\relax 
\bibstyle{aaai23}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sanchez2018inverse,yao2021inverse}
\citation{lavin2021simulation}
\citation{liu2022machine}
\citation{wang2020deep}
\citation{asim2020invertible}
\citation{ertl2009estimation}
\citation{zhang2019learning}
\citation{zhang2021enabling}
\citation{sun2020deep}
\@writefile{toc}{\contentsline {section}{Introduction}{1}{section*.1}\protected@file@percent }
\newlabel{intro}{{}{1}{Introduction}{section*.1}{}}
\citation{yang2018predictive}
\citation{zhang2018effect,zhang2018quantification}
\citation{wang2018high}
\citation{sohn2015learning}
\citation{tonolini2020variational}
\citation{asim2020invertible,whang2021composing,whang2021solving,daras2021intermediate,sun2020deep,kothari2021trumpets,song2021solving}
\citation{asim2020invertible}
\citation{whang2021composing}
\citation{whang2021solving}
\citation{sun2020deep,asim2020invertible,kothari2021trumpets}
\citation{rezende2015variational,dinh2016density,kingma2018glow,grathwohl2018ffjord,wu2020stochastic,nielsen2020survae}
\citation{ardizzone2018analyzing,kruse2021benchmarking}
\citation{ren2020benchmarking}
\citation{fung2021inverse,fung2022atomic}
\citation{forrester2009recent,gomez2018automatic,white2019multiscale}
\citation{ren2020benchmarking}
\citation{deng2021neural}
\citation{rezende2015variational,dinh2016density,ardizzone2018analyzing}
\@writefile{toc}{\contentsline {section}{Related Work}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Methodology}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Augmented Inverse Learning Formulation}{2}{section*.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Original inverse problem often encounters the ill-posed issue due to one-to-many mappings. An augmented inverse problem is formulated based on bijective mapping with additional latent variable $\mathbf  {z}$.}}{2}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:eq_bijectiv}{{1}{2}{Original inverse problem often encounters the ill-posed issue due to one-to-many mappings. An augmented inverse problem is formulated based on bijective mapping with additional latent variable $\mathbf {z}$}{figure.caption.5}{}}
\citation{durkan2019neural}
\citation{gretton2012kernel,gretton2012optimal}
\newlabel{eq:bijective}{{1}{3}{Augmented Inverse Learning Formulation}{equation.0.1}{}}
\@writefile{toc}{\contentsline {subsection}{Dynamical Bi-directional Training}{3}{section*.6}\protected@file@percent }
\newlabel{eq:total_loss}{{4}{3}{Dynamical Bi-directional Training}{equation.0.4}{}}
\newlabel{eq:loss_y}{{5}{3}{Dynamical Bi-directional Training}{equation.0.5}{}}
\newlabel{eq:loss_z}{{6}{3}{Dynamical Bi-directional Training}{equation.0.6}{}}
\newlabel{eq:loss_x}{{7}{3}{Dynamical Bi-directional Training}{equation.0.7}{}}
\@writefile{toc}{\contentsline {subsection}{Localization from Posterior Samples}{3}{section*.8}\protected@file@percent }
\citation{li2006probability}
\citation{stein1987large,shields2016generalization}
\citation{mckay2000comparison}
\citation{zhang2021modern}
\citation{caflisch1998monte}
\citation{kucherenko2015exploring}
\citation{bishop1994mixture}
\citation{ardizzone2018analyzing}
\citation{ardizzone2019guided,rombach2020network}
\citation{sohn2015learning}
\citation{ren2020benchmarking}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces {Localizing inverse solutions from intelligent priors (posterior samples)}. The approximated posterior and its MAP estimator both deviate from the exact solution but successfully narrow down the search space. Our objective is to localize the exact solution by leveraging these posterior samples as intelligent initialization such that the process can be accelerated.}}{4}{figure.caption.7}\protected@file@percent }
\newlabel{fig:localize}{{2}{4}{{Localizing inverse solutions from intelligent priors (posterior samples)}. The approximated posterior and its MAP estimator both deviate from the exact solution but successfully narrow down the search space. Our objective is to localize the exact solution by leveraging these posterior samples as intelligent initialization such that the process can be accelerated}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces {Space-filling sampling}. 10 random samples are used to illustrate three different sampling strategies: (a) simple random sampling (SRS), (b) classical LHS, and (c) maximin LHS.}}{4}{figure.caption.10}\protected@file@percent }
\newlabel{fig:lhs}{{3}{4}{{Space-filling sampling}. 10 random samples are used to illustrate three different sampling strategies: (a) simple random sampling (SRS), (b) classical LHS, and (c) maximin LHS}{figure.caption.10}{}}
\newlabel{eq:gradient}{{8}{4}{Localization from Posterior Samples}{equation.0.8}{}}
\@writefile{toc}{\contentsline {subsection}{Space-filling Sampling on Latent Space}{4}{section*.9}\protected@file@percent }
\newlabel{eq:lhs}{{9}{4}{Space-filling Sampling on Latent Space}{equation.0.9}{}}
\@writefile{toc}{\contentsline {section}{Experiments}{4}{section*.11}\protected@file@percent }
\citation{ardizzone2018analyzing,ren2020benchmarking,kruse2021benchmarking}
\newlabel{algo:1}{{\caption@xref {algo:1}{ on input line 342}}{5}{Space-filling Sampling on Latent Space}{equation.0.9}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \hspace  {-0.1cm}: iPage algorithm}}{5}{algorithm.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Training dataset size, input/output dimensionality, and target observation $y^*$ for benchmark tasks and real-world applications.}}{5}{table.caption.12}\protected@file@percent }
\newlabel{tab:dataset}{{1}{5}{Training dataset size, input/output dimensionality, and target observation $y^*$ for benchmark tasks and real-world applications}{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{Illustrative Example: 2D Sinewave Function}{5}{section*.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces {Localization and exploration of inverse solutions for the 2D sinewave function}. Given a specific target $y^*=1.2$, there exits a multimodal disconnected solution space (labeled as 1-9 in the left panel). The inverse solution using four baseline methods (INN, cINN, cVAE, and NA) and iPage (with SRS) are illustrated and compared at different sampling counts ranging from 25 to 200.}}{5}{figure.caption.15}\protected@file@percent }
\newlabel{fig:sine9}{{4}{5}{{Localization and exploration of inverse solutions for the 2D sinewave function}. Given a specific target $y^*=1.2$, there exits a multimodal disconnected solution space (labeled as 1-9 in the left panel). The inverse solution using four baseline methods (INN, cINN, cVAE, and NA) and iPage (with SRS) are illustrated and compared at different sampling counts ranging from 25 to 200}{figure.caption.15}{}}
\citation{mao2020designing}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Performance comparison of tested methods on five tasks given 1000 different observations $\mathbf  {y}^*$). The re-simulation error measures how well the generated $\hat  {\mathbf  {x}}$ is conditioned on the observation $\mathbf  {y}^*$. Each task is performed 50 times to obtain the standard deviation.}}{6}{table.caption.13}\protected@file@percent }
\newlabel{tab:1000_y}{{2}{6}{Performance comparison of tested methods on five tasks given 1000 different observations $\mathbf {y}^*$). The re-simulation error measures how well the generated $\hat {\mathbf {x}}$ is conditioned on the observation $\mathbf {y}^*$. Each task is performed 50 times to obtain the standard deviation}{table.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces iPage (with mLHS) performance. Blue dots represent the final solutions, showing that our approach yields uniformly distributed solutions that capture all local modes.}}{6}{figure.caption.16}\protected@file@percent }
\newlabel{fig:iPage}{{5}{6}{iPage (with mLHS) performance. Blue dots represent the final solutions, showing that our approach yields uniformly distributed solutions that capture all local modes}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{Artificial Benchmark Tasks}{6}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Real-world Applications}{6}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Computational Cost Comparison}{6}{section*.22}\protected@file@percent }
\citation{zhang2021scalable}
\citation{li2022auditing}
\bibdata{aaai23}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces {Two real-world design applications}: (Left) Crystal structure design problem in quantum chemistry and (Right) Architected materials design problem in additive manufacturing.}}{7}{figure.caption.17}\protected@file@percent }
\newlabel{fig:crystal}{{6}{7}{{Two real-world design applications}: (Left) Crystal structure design problem in quantum chemistry and (Right) Architected materials design problem in additive manufacturing}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces {Total time cost (inference and localization) for 1000 solutions}. The time-to-solution using iPage with other baselines on three benchmarks are compared side-by-side.}}{7}{figure.caption.19}\protected@file@percent }
\newlabel{fig:time}{{7}{7}{{Total time cost (inference and localization) for 1000 solutions}. The time-to-solution using iPage with other baselines on three benchmarks are compared side-by-side}{figure.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performance comparison of tested methods on five tasks for 1000 solutions conditioned on a specific observation $\mathbf  {y}^*$. We repeat 50 times to obtain the standard deviation for each case.}}{7}{table.caption.21}\protected@file@percent }
\newlabel{tab:1_y}{{3}{7}{Performance comparison of tested methods on five tasks for 1000 solutions conditioned on a specific observation $\mathbf {y}^*$. We repeat 50 times to obtain the standard deviation for each case}{table.caption.21}{}}
\@writefile{toc}{\contentsline {section}{Conclusion}{7}{section*.23}\protected@file@percent }
\gdef \@abspage@last{8}
