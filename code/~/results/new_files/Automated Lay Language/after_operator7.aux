\relax 
\bibstyle{aaai21}
\citation{parker1999health}
\citation{howes2004evidence}
\citation{korsch1968gaps}
\citation{friedman2002two}
\citation{cao2020expertise}
\citation{allahyari2017text}
\citation{Moradi2019TextSI}
\@LN@col{1}
\@LN@col{2}
\citation{nallapati2016abstractive}
\citation{goldenberg2017probiotics}
\citation{brok2010ribavirin}
\citation{paravastu2016endovenous}
\citation{cirocchi2012non}
\citation{alfirevic2017fetal}
\citation{Paice1980TheAG,teufel2002summarization}
\citation{Cohan2018ADA}
\citation{Luu2020CitationTG}
\citation{Cachola2020TLDRES}
\citation{Sarkar2011UsingML}
\citation{Andr2007ASO}
\citation{Erkan2004LexRankGC,Cheng2016NeuralSB}
\citation{rush2015abstractive,nallapati2016abstractive}
\citation{Altmami2020AutomaticSO}
\citation{Moradi2019TextSI}
\citation{Shardlow2014ASO}
\citation{qenam2017text}
\citation{habibi2017deep}
\citation{baumgartner2008concept}
\citation{jonnalagadda2010towards}
\citation{jonnalagadda2010towards}
\citation{miller1995wordnet}
\citation{bodenreider2004unified}
\citation{zesch2008extracting}
\citation{peters2018elmo,Radford2018ImprovingLU,Devlin2019BERTPO}
\citation{brown2020language}
\citation{Hendrycks2020PretrainedTI}
\citation{Gururangan2020DontSP}
\citation{Pruksachatkun2020IntermediateTaskTL}
\citation{pivovarov2015automated,feblowitz2011summarization,molla2011development}
\citation{goeuriot2020overview}
\@LN@col{1}
\@LN@col{2}
\citation{uman2011systematic}
\citation{murad2016new}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{transformation_case_study}{{1}{3}{Typical transformation phenomena from \textit  {source} to \textit  {target}, and the corresponding generated summaries using two best-performing models. We only show part of the long text here for brevity.}{}{}}
\@LN@col{1}
\@LN@col{2}
\newlabel{Data_analysis}{{}{3}{}{}{}}
\citation{mcilwain2014standards}
\citation{lin2004rouge}
\citation{kincaid1975derivation}
\citation{gunning1952technique}
\citation{coleman1975computer}
\newlabel{data_description}{{2}{4}{Dataset statistics across the different splits.}{}{}}
\@LN@col{1}
\@LN@col{2}
\newlabel{evaluation_metrics}{{}{4}{}{}{}}
\citation{liu2019text}
\citation{devlin2018bert}
\citation{see2017get}
\citation{lewis2019bart}
\citation{nallapati2016abstractive}
\@LN@col{1}
\@LN@col{2}
\citation{sen1968estimates}
\newlabel{rouge_readability}{{3}{6}{Test set performance evaluated by ROUGE and readability score. BART model pretrained on CNN/DM and PubMed is the best-performing model based on ROUGE, while BART model pretrained on PubMed is the best one based on readability score (Best model performance is in bold). $x_\pm $ indicates 95\% interval: $[x-, x+]$}{}{}}
\@LN@col{1}
\newlabel{sec:inter-pretraining}{{}{6}{}{}{}}
\@LN@col{2}
\bibdata{main}
\newlabel{result_human_evaluation}{{4}{7}{Human evaluation scores of the expert-generated summaries (\textit  {Target}) and the model-generated summaries (\textit  {Generated}) for two abstracts from the test set. Generated abstracts from BART+CNN/DM+PubMed model have better scores in grammaticality, meaning preservation, and correctness of key information.}{}{}}
\@LN@col{1}
\@LN@col{2}
\gdef \@abspage@last{7}
