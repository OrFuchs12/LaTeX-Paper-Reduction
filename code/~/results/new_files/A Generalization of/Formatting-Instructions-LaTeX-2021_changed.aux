\relax 
\bibstyle{aaai21}
\citation{vaswani2017attention}
\citation{bahdanau2014neural}
\citation{gilmer2017neural}
\citation{joshi2020transformers}
\citation{devlin2018bert,radford2018improving,brown2020language}
\citation{schlichtkrull2018modeling,chami2020low}
\citation{monti2019fake}
\citation{cranmer2019learning,sanchez2020learning}
\citation{li2019graph,Nguyen2019U2GNN,zhang2020graph}
\citation{yun2019graph,xu2019multi,hu2020heterogeneous,zhou2020data}
\citation{li2019graph}
\citation{zhang2020graph}
\citation{belkin2003laplacian,dwivedi2020benchmarking}
\citation{you2019position}
\citation{li2015gated}
\citation{zhang2020graph}
\newlabel{sec:related_work}{{1.1}{1}{}{}{}}
\citation{yun2019graph}
\citation{hu2020heterogeneous}
\citation{zhou2020data}
\citation{Nguyen2019U2GNN}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{gt-architecture}{{1}{2}{Block Diagram of Graph Transformer with Laplacian Eigvectors ($\lambda $) used as positional encoding (LapPE). LapPE is added to input node embeddings before passing the features to the first layer. \textit  {\textbf  {Left}}: Graph Transformer operating on node embeddings only to compute attention scores; \textit  {\textbf  {Right}}: Graph Transformer with edge features with designated feature pipeline to maintain layer wise edge representations. In this extension, the available edge attributes in a graph is used to explicitly modify the corresponding pairwise attention scores.}{}{}}
\citation{vaswani2017attention}
\citation{NIPS2016_6081,kipf2017semi,Monti_2017,gilmer2017neural,velickovic2018graph,bresson2017residual,xu2018how}
\citation{murphy2019relational}
\citation{srinivasan2019equivalence}
\citation{velickovic2018graph}
\citation{murphy2019relational,you2019position,srinivasan2019equivalence,dwivedi2020benchmarking,li2020distance}
\citation{dwivedi2020benchmarking}
\citation{belkin2003laplacian}
\citation{vaswani2017attention}
\citation{dwivedi2020benchmarking}
\newlabel{sec:on_positional_encodings}{{2.2}{3}{}{}{}}
\newlabel{eigenvec}{{1}{3}{}{}{}}
\newlabel{eqn:input_embd}{{2}{3}{}{}{}}
\citation{vaswani2017attention}
\citation{ba2016layer}
\citation{ioffe2015batch}
\newlabel{eqn:pe_embd_add}{{3}{4}{}{}{}}
\newlabel{eqn:gt_layer}{{4}{4}{}{}{}}
\newlabel{eqn:softmax}{{5}{4}{}{}{}}
\newlabel{eqn:rc_norm1}{{6}{4}{}{}{}}
\newlabel{eqn:ffn}{{7}{4}{}{}{}}
\newlabel{eqn:rc_norm2}{{8}{4}{}{}{}}
\newlabel{eqn:gt_layer_edge_h}{{9}{4}{}{}{}}
\newlabel{eqn:gt_layer_edge_e}{{10}{4}{}{}{}}
\newlabel{eqn:softmax_edge_1}{{11}{4}{}{}{}}
\newlabel{eqn:softmax_edge_2}{{12}{4}{}{}{}}
\newlabel{eqn:rc_norm1_h}{{13}{4}{}{}{}}
\newlabel{eqn:ffn_h}{{14}{4}{}{}{}}
\newlabel{eqn:rc_norm2_h}{{15}{4}{}{}{}}
\newlabel{eqn:rc_norm1_e}{{16}{4}{}{}{}}
\newlabel{eqn:ffn_e}{{17}{4}{}{}{}}
\newlabel{eqn:rc_norm2_e}{{18}{4}{}{}{}}
\citation{dwivedi2020benchmarking}
\citation{kipf2017semi}
\citation{velickovic2018graph}
\citation{bresson2017residual}
\citation{kipf2017semi}
\citation{velickovic2018graph}
\citation{bresson2017residual}
\citation{irwin2012zinc}
\citation{abbe2017community}
\citation{dwivedi2020benchmarking}
\citation{irwin2012zinc}
\citation{dwivedi2020benchmarking}
\citation{abbe2017community}
\citation{dwivedi2020benchmarking}
\citation{dwivedi2020benchmarking}
\citation{paszke2019pytorch}
\citation{wang2019dgl}
\citation{vaswani2017attention}
\citation{zhang2020graph}
\citation{hu2020heterogeneous}
\citation{zhang2020graph,niepert2016learning}
\citation{zhang2020graph}
\citation{zhang2020graph}
\bibdata{bibfile}
\newlabel{tab:results}{{1}{6}{ Results of GraphTransformer (GT) on all datasets. Performance Measure for ZINC is MAE, for PATTERN and CLUSTER is Acc. Results (higher is better for all except ZINC) are averaged over 4 runs with 4 different seeds. \textbf  {Bold}: the best performing model for each dataset. We perform each experiment with given graphs \textbf  {(Sparse Graph)} and \textbf  {(Full Graph)} in which we create full connections among all nodes; For ZINC full graphs, edge features are discarded given our motive of the full graph experiments without any sparse structure information. }{}{}}
\newlabel{tab:results_comparison}{{2}{6}{ Comparison of our best performing scores (from Table \ref {tab:results}) on each dataset against the GNN baselines (GCN \citep  {kipf2017semi}, GAT \citep  {velickovic2018graph}, GatedGCN\citep  {bresson2017residual}) of 500k model parameters. \textbf  {Note:} Only GatedGCN and GT models use the available edge attributes in ZINC. }{}{}}
\newlabel{tab:results_ablation}{{3}{6}{ Analysis of GraphTransformer (GT) using different PE schemes. Notations \textbf  {x}: No PE; \textbf  {L}: LapPE (ours); \textbf  {W}: WL-PE \citep  {zhang2020graph}. \textbf  {Bold}: the best performing model for each dataset. }{}{}}
\newlabel{sec:graph_bert_comparison}{{4.1}{6}{}{}{}}
\gdef \@abspage@last{7}
