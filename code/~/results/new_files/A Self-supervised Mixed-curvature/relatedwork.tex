%!TEX root = ./SelfMGNN.tex


\section{Related Work}

%We briefly discuss the related work as follows:
%Our work is related to Riemannian representation learning and contrastive learning. We briefly discuss them as follows:
\textsc{SelfMGNN} learns representations via a mixed-curvature graph neural network with the dual contrastive approach. 
Here, we briefly discuss the related work on the \emph{graph neural network} and \emph{contrastive learning}:
% \noindent\textbf{Graph Neural Network (GNNs)}
% emerge as the state-of-the-art representation learning methods on graphs.
% Existing methods in the literature belong to two major categories: spectral methods and message passing methods.
% The former studies the spectral domain of graphs to learn the representations \cite{BrunaZSL13,kipf2016semi},
% while the latter builds message passing architectures to aggregate neighborsâ€™ features \cite{velickovic2018graph,hamilton2017inductive,ma2019Disentangled}.
% A detailed survey can be found in \cite{WuPCLZY21}.
% Our work attempts to address the limitation of prior GNNs in terms of representation space and learning paradigm.

\subsection{Graph Neural Network}
Graph Neural Networks (GNNs) achieve the state-of-the-art results in graph analysis tasks \cite{DouL0DPY20,pengh2021tois}.
Recently, a few attempts propose to marry GNN and Riemannian representation learning
\cite{HAN,mathieu2019continuous,monath2019gradient}
as graphs are non-Euclidean inherently \cite{krioukov2010hyperbolic}.
In hyperbolic space, \citet{nickel2017poincare,suzuki2019hyperbolic} introduce shallow models,
while \citet{HGNN,HGCN,ZhangWSLS21} formulate deep models (GNNs).
Furthermore, \citet{Sun21hvgnn} generalize hyperbolic GNN to dynamic graphs with insights in temporal analysis. 
\citet{fuxc2020icdm} study the curvature exploration.
\citet{Sun20jointAlign,Wang0Z20} propose promising methods to apply the hyperbolic geometry to network alignment.

Beyond hyperbolic space,
\citet{CruceruBG21} studies the matrix manifold of Riemannian spaces,
and
\citet{BachmannBG20} generalizes GCN to arbitrary constant-curvature spaces.
%\textbf{Constant-curvature space} models work well on the particular kinds of structure they were designed for.
To generalize representation learning, \citet{GuSGR19}  propose to learn in the mixed-curvature space.
\citet{SkopekGB20} introduce the mixed-curvature VAE.
Recently, \citet{WangWSWNAXYC21} model the knowledge graph triples in mixed-curvature space specifically with the supervised manner. 
Distinguishing from these studies, we propose the first self-supervised mixed-curvature model, allowing multiple hyperbolic (spherical) subspaces each with distinct curvatures.  


\subsection{Contrastive Learning}
Contrastive Learning is an attractive self-supervised learning method that learns representations by contrasting positive and negative pairs \cite{ChenK0H20}. 
Here, we discuss the contrastive learning methods on graphs.
Specifically,
%DGI \cite{VelickovicFHLBH19} builds local patches and global summary pairs, and contrast via Infomax theory,
\citet{VelickovicFHLBH19} contrast patch-summary pairs via infomax theory.
\citet{HassaniA20} leverage multiple views for contrastive learning.
\citet{QiuCDZYDWT20} formulate a general framework for pre-training.
\citet{WanPY021} incorporate the generative learning concurrently.
\citet{PengHLZRXH20} explore the graph-specific infomax for contrastive learning.
\citet{pmlr-v139-xu21g} aim to learn graph level representations.
\citet{ParkK0Y20,abs-2105-09111} consider the heterogeneous graphs.
To the best of our knowledge, 
existing methods cannot apply to Riemannian spaces % due to the intrinsic difference in the geometry,
and we bridge this gap in this paper.


%  \begin{table}
%   \scriptsize
%     \centering
%           \caption{\textsc{SelfMGNN} ($\mathbb H^8\times\mathbb S^8\times\mathbb E^8$): curvature (\textcolor{magenta}{attentional weight}) for node classification on datasets.}
%             \vspace{-0.1in}
%     \begin{tabular}{ p{1.5cm}<{\centering}|p{1.7cm}<{\centering} p{1.7cm}<{\centering} p{1.7cm}<{\centering} }
%       \toprule
%   \footnotesize{\textbf{Component}} &   \footnotesize{Hyperbolic}       &    \footnotesize{Euclidean}    & \footnotesize{Spherical}        \\
% \toprule
% \footnotesize{Citeseer}&       $  -0.67 \ (\textcolor{magenta}{0.39})$      &    $ 0\ (\textcolor{magenta}{0.28})$     &  $  +0.82\ (\textcolor{magenta}{0.33})$     \\
% \footnotesize{Cora}      &       $  -0.90\  (\textcolor{magenta}{0.48})$      &    $ 0\ (\textcolor{magenta}{0.24})$     & $  +0.76\ (\textcolor{magenta}{0.28})$       \\
% \footnotesize{Pubmed}&       $  -1.12\  (\textcolor{magenta}{0.56})$      &    $ 0\ (\textcolor{magenta}{0.25})$      &  $ +0.59\ (\textcolor{magenta}{0.19})$      \\
% \footnotesize{Amazon}&       $  -0.78\ (\textcolor{magenta}{0.72})$      &    $ 0\ (\textcolor{magenta}{0.13})$      &  $  +1.13\ (\textcolor{magenta}{0.15})$     \\
% \footnotesize{Airport}  &       $  -1.26\ (\textcolor{magenta}{0.67})$      &    $ 0\ (\textcolor{magenta}{0.15})$      &  $ +1.85\ (\textcolor{magenta}{0.18})$     \\
%       \bottomrule
%     \end{tabular} 
%         \label{component}
%                   \vspace{-0.1in}
%   \end{table}


\section{Conclusion}

In this paper, we take the first attempt to study the self-supervised graph representation learning in the mixed-curvature Riemannian space, and present a novel \textsc{SelfMGNN}.
Specifically, we first construct the mixed-curvature space via Cartesian product  of Riemannian manifolds
and design hierarchical attention mechanisms within and among component spaces to learn graph representations in the mixed-curvature space.
%In this mixed-curvature space, 
%we reveal different Riemannian views by the designed Riemannian projector and propose the dual contrastive approach, i.e., 
Then, we introduce the
single-view
and cross-view contrastive learning to learn graph representations without labels.
Extensive experiments show the superiority of \textsc{SelfMGNN}.
% outperforms state-of-the-art baselines. % on several benchmark datasets.

 % \begin{table}
 %  \footnotesize
 %    \centering
 %          \caption{The summary of related work}
 %          \vspace{-0.1in}
 %    \begin{tabular}{c| c|c}
 %      \toprule
      
 %      \bottomrule
 %    \end{tabular} 
 %  \end{table}
