%!TEX root = ./SelfMGNN.tex

\section{Preliminaries and Problem Definition}

% Prior to introducing the studied problem and proposed approach, we provide some important preliminaries in this section. 
% Throughout the paper, 
% we denote the Euclidean norm and inner product by $\| \cdot \|_2$ and $\langle \cdot, \cdot \rangle$, respectively.
In this section, we first present the preliminaries and notations necessary to construct a mixed-curvature space.
%(more details can be found in textbooks [28, 39]), 
%including the Riemannian manifold and constant-curvature space. 
Then, we formulate the problem of \emph{self-supervised graph representation learning in the mixed-curvature space}.


\subsection{Riemannian Manifold}

A smooth \emph{manifold} $\mathcal M$ generalizes the notion of the surface to higher dimensions.
Each point $\mathbf x \in \mathcal M$ associates with a \emph{tangent space} $\mathcal T_\mathbf x\mathcal M$, the first order approximation of $\mathcal M$ around $\mathbf x$, which is locally Euclidean.
On tangent space $\mathcal T_\mathbf x\mathcal M$, the \emph{Riemannian metric}, $g_\mathbf x (\cdot, \cdot) : \mathcal T_\mathbf x\mathcal M  \times \mathcal T_\mathbf x\mathcal M \to \mathbb R$, defines an inner product so that geometric notions can be induced.
The tuple $(\mathcal M, g)$ is called a \emph{Riemannian manifold}.

Transforming between the tangent space and the manifold is done via exponential and logarithmic maps, respectively.
For $\mathbf x \in \mathcal M$, 
the  \emph{exponential map} at $\mathbf x$, 
$\mathbf{exp}_\mathbf x(\mathbf v): \mathcal T_\mathbf x\mathcal M \to \mathcal M$, 
projects the vector $\mathbf v \in \mathcal T_\mathbf x\mathcal M$ onto the manifold $\mathcal M$.
The \emph{logarithmic map} at $\mathbf x$, 
$\mathbf{log}_\mathbf x(\mathbf y): \mathcal M \to \mathcal T_\mathbf x\mathcal M$, 
projects the vector $\mathbf y \in \mathcal M$ back to the tangent space $\mathcal T_\mathbf x\mathcal M$.
For further expositions, please refer to mathematical materials \cite{Spivak1979,Hopper2010}.

\subsection{Constant Curvature Space}

The Riemannian metric also defines a curvature at each point $\kappa(\mathbf x)$, 
which determines how the space is curved.
If the curvature is uniformly distributed,  
$(\mathcal M, g)$ is called a \emph{constant curvature space} of curvature $\kappa$. 
There are $3$ canonical types of constant curvature space that we can define with respect to the sign of the curvature: 
a positively curved spherical space $\mathbb S$ with $\kappa>0$, 
a negatively curved hyperbolic space $\mathbb H$ with $\kappa<0$ 
and the flat Euclidean space $\mathbb E$  with $\kappa=0$.

\noindent\textbf{Note that}, $\| \cdot \|_2$ denotes the Euclidean norm in this paper.

\subsection{Problem Definition}
In this paper, we propose to study the self-supervised graph representation learning in the mixed-curvature space.
Without loss of generality,
a graph is described as $G = (V, E, \mathbf X)$, 
where $V = \{v_1,  \cdots, v_n\}$ is the node set and $E =\{ (v_i,  v_j ) | \  v_i,  v_j \in V\}$ is the edge set. 
We summarize the edges in the adjacency matrix $\mathbf G$, where $\mathbf G_{ij}=1$ iff $(v_i,  v_j ) \in E$, otherwise $0$.
Each node $v_i$ is associated with a feature vector $\mathbf x_i \in \mathbb R^d$, and matrix $\mathbf{X} \in \mathbb{R}^{|V| \times d}$ represents the features of all nodes.
Now, we give the studied problem:
\newtheorem*{def1}{Problem Definition (Self-supervised graph representation learning in the mixed-curvature space)} 
\begin{def1}
Given a graph $G = (V, E, \mathbf X)$, 
the problem of self-supervised graph representation learning in the mixed-curvature space
is to learn an encoding function $\Phi: V \to \mathcal P$ that maps the node $v$ to a vector $\boldsymbol z$ in a mixed-curvature space $\mathcal P$ that captures the intrinsic complexity of graph structure without using any label information. 
\end{def1}

In other words, the graph representation model should align with the complex graph structures 
â€” hierarchical as well as cyclical structure, 
and can be learned without external guidance (labels). 
% The representation space of constant curvature works well on particular kinds of structure that they were designed for. % \cite{HGNN,ZhangWSLS21}.
% Instead of the constant-curvature space, the mixed-curvature space is called in fact to cover the intrinsic complicated structures of the graphs.
Graphs in reality are usually mixed-curvatured rather than structured uniformly, i.e., in some regions hierarchical, while in others cyclical.
A constant-curvature model (e.g., hyperbolic, spherical or the Euclidean model) benefits from their specific bias to better fit particular structure types. 
To bridge this gap, we propose to work with the \textbf{mixed-curvature space} to cover the  complex graph structures in real-world applications.


