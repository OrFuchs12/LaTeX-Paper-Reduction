%!TEX root = ./SelfMGNN.tex


  
\section{Experiments}
In this section, we evaluate  \textsc{SelfMGNN} with the link prediction and node classification tasks against $10$ strong baselines on $5$ benchmark datasets. 
%We repeat each experiment $10$ times and report the mean with the standard deviations.
We report the mean with the standard deviations of $10$ independent runs for each model to achieve fair comparisons.

\subsection{Experimental Setups}
\noindent\textbf{Datasets:} We utilize $5$ benchmark datasets, i.e., 
the widely-used 
\textbf{Citeseer}, 
\textbf{Cora},
and
\textbf{Pubmed} \cite{kipf2016semi,VelickovicFHLBH19}, 
and the latest
\textbf{Amazon}
and
\textbf{Airport} \cite{ZhangWSLS21}.
%We list the statistics in Supplementary Material.

\noindent\textbf{Euclidean Baselines:} %We compare the proposed \textsc{SelfMGNN} with the state-of-the-art baselines of two categories: % of constant-curvature space and self-supervised learning.
i) \emph{Supervised Models}: 
\textbf{GCN} \cite{kipf2016semi},
\textbf{GraphSage} \cite{hamilton2017inductive},
\textbf{GAT} \cite{velickovic2018graph}.
ii) \emph{Self-supervised Models}: 
\textbf{DGI} \cite{VelickovicFHLBH19},
\textbf{MVGRL} \cite{HassaniA20},
\textbf{GMI} \cite{PengHLZRXH20}.

\noindent\textbf{Riemannian Baselines:}
i) \emph{Supervised Models}: 
\textbf{HGCN} \cite{HGCN}, 
\textbf{HAT} \cite{HAN}
and
\textbf{LGCN} \cite{ZhangWSLS21} for hyperbolic space; 
$\boldsymbol \kappa$-\textbf{GCN} \cite{BachmannBG20} with positive $\kappa$ for spherical space.
ii)  
\emph{Self-supervised Models}: There is no self-supervised Riemannian models in the literature, and thus we propose \textsc{SelfMGNN} to fill this gap.

%\emph{Refer Supplementary Material for details.}

\subsection{Implementation Details}

%$\kappa$-stereographic model
% \noindent\textbf{Euclidean input:} The input feature $\mathbf x$ is Euclidean by default. 
% In this case, we map it to the Riemannian space to feed into \textsc{SelfMGNN}. 
% Specifically, the map $\rho: \mathbb R^d \to \mathcal M^d_\kappa$ is defined as 
% $\rho(\mathbf x)=( F_\kappa
% |\kappa|^{\frac{1}{2}}
% \sinh(
% |\kappa|^{-\frac{1}{2}}
% ||\mathbf x||_2
% )
% )\mathbf x$,
% where projection factor $F$ is given as follows:
%    \vspace{-0.09in}
% \begin{equation}
% F_\kappa=(1+\kappa\cosh(|\kappa|^{-\frac{1}{2}}||\mathbf x||_2))||\mathbf x||_2.
%    \vspace{-0.06in}
% \end{equation}
% \emph{We derive the map  $\rho(\mathbf x)$ in the Supplementary Material.}

\noindent\textbf{Congruent graph:} 
As suggested by \citet{HassaniA20}, we opt for the diffusion to generate a congruent augmentation.
%Generate congruent views with diffusion matrices
% \begin{equation}
% \Gamma(\mathbf G)=\alpha\left(\mathbf{I}_{n}-(1-\alpha) \mathbf{D}^{-\frac{1}{2}} \mathbf{G D}^{-\frac{1}{2}}\right)^{-1}
% \end{equation}
Specifically, given an adjacency matrix $\mathbf G^\alpha$, we use the congruent graph generation function \emph{$\Gamma(\cdot)$} to obtain a diffusion matrix $\mathbf G^\beta$  and treat it as the adjacency matrix of the congruent augmentation. 
% In practice, we utilize the Personalized PageRank diffusion which has a closed-form formulation: 
%    \vspace{-0.05in}
% \begin{equation}
% \Gamma(\mathbf G^\alpha)=\alpha\left(\mathbf{I}_{n}-(1-\alpha) \mathbf{D}^{-\frac{1}{2}} \mathbf{G^\alpha D}^{-\frac{1}{2}}\right)^{-1},
%    \vspace{-0.05in}
% \end{equation}
% where $\alpha$ denotes teleport probability in the diffusion. 
% Note that, 
The diffusion is computed once via fast approximated and sparsified method \cite{KlicperaWG19}.

\noindent\textbf{Signature:} The mixed-curvature space is parameterized by the signature, i.e., space type, curvature and dimensionality of the component spaces.
The space type of component $\mathcal M_i$ can be hyperbolic $\mathbb H$, spherical $\mathbb S$ or Euclidean $\mathbb E$, and we utilize the combination of them to cover the mixed and complicated graph structures.
The dimensionality $d_{\mathcal M_i}$ is a hyperparameter.
The curvature $\kappa_{\mathcal M_i}$ is a learnable parameters as our loss is differentiable with respect to the curvature. 

\noindent\textbf{Learning manner:} Similar to \citet{VelickovicFHLBH19}, self-supervised models first learn representations without labels, and then were evaluated by specific learning task, which is performed by directly using these representations to train and test for learning tasks.
Supervised models were trained and tested by following \citet{HGCN}.
Please refer to the Supplementary Material for further experimental details.

%We learn the curvatures using gradient based optimization.

%We stack the attentive aggregation layer twice in the experiment.

 \begin{table}
  \scriptsize
    \centering
          \caption{Ablation study of \textsc{SelfMGNN} for node classification task in classification accuracy ($\%$).}
    \begin{tabular}{p{0.05cm}<{\centering} p{2.7cm}<{\centering}|p{1.2cm}<{\centering} p{1.2cm}<{\centering} p{1.2cm}<{\centering} }
      \toprule
\multicolumn{2}{c|}{\footnotesize{\textbf{Variants}}} & \footnotesize{Citesser }& \footnotesize{Core} & \footnotesize{Pubmed}\\[0.5pt]
\toprule
\multirow{3}{*}{\rotatebox{90}{\footnotesize{CCS} } }
                                      &  \footnotesize{$\mathbb H^{24}$}                               &       $  72.2(0.7)$      &    $ 82.1(0.4)$      &  $  78.6(0.3)$        \\
                                      &  \footnotesize{$\mathbb S^{24}$}                                &       $  70.5(0.8)$      &    $ 82.3(0.5)$      &  $  77.5(0.4)$           \\
                                      & \footnotesize{$\mathbb E^{24}$}                                 &       $  71.8(1.1)$      &    $ 81.0(0.7)$      &  $  77.3(0.8)$       \\
\midrule
\multirow{3}{*}{\rotatebox{90}{\footnotesize{Single} }  }
&  \footnotesize{ $\mathbb H^8\times\mathbb S^8 \times \mathbb E^8  $}          &       $ 72.6 (0.3)$      &    $  82.7(0.8)$      &  $  78.9(0.9)$        \\
&  \footnotesize{$(\mathbb H^4)^2\times (\mathbb S^4)^2\times\mathbb E^8 $}&       $ 72.8(0.6)$      &    $  83.1(0.6)$      &  $  79.2(0.2)$       \\
&  \footnotesize{$(\mathbb H^2)^4\times(\mathbb S^2)^4\times\mathbb E^8 $} &       $ 72.9(0.2)$      &    $  83.5(0.5)$      &  $  79.3(0.5)$            \\
\midrule
\multirow{3}{*}{\rotatebox{90}{\footnotesize{Ours} } }
& \footnotesize{ $\mathbb H^8\times\mathbb S^8 \times \mathbb E^8  $ }          &       $ 72.8(1.0)$       &    $  83.3(0.9)$      &  $  79.2(0.6)$           \\
& \footnotesize{ $(\mathbb H^4)^2\times (\mathbb S^4)^2\times\mathbb E^8 $}&  $\blue{73.1}(0.9)$ &$\blue{83.8}(0.5)$ &$\blue{79.6}(0.7)$         \\
& \footnotesize{ $(\mathbb H^2)^4\times(\mathbb S^2)^4\times\mathbb E^8 $} &$\mathbf{73.3}(0.5)$ & $\mathbf{84.1}(0.8)$  &$\mathbf{79.9}(1.1)$           \\
      \bottomrule
    \end{tabular} 
        \label{ablation}
  \end{table}

\subsection{Link Prediction}

%The task of link prediction is to predict the probability of two nodes being connected.  is a generalized sigmoid function, and
For link perdition, we utilize the Fermi-Dirac decoder with distance function to define the probability based on model outputs $\boldsymbol z$.
Formally, we have the probability as follows: % representations. 
%we have Formally, 
\begin{equation}
\resizebox{0.888\hsize}{!}{$
p((i,j) \in E| \boldsymbol z_i, \boldsymbol z_j)=\left(\exp\left((d_\mathcal M(\boldsymbol z_i, \boldsymbol z_j)^2-r)/t\right)+1\right)^{-1},
$}
\end{equation}
where $r$, $t$ are hyperparameters. 
For each method, $d_\mathcal M(\boldsymbol z_i, $ $\boldsymbol z_j)$ is the distance function of corresponding representation space, e.g., $||\boldsymbol z_i - \boldsymbol z_j||_2$ for Euclidean models, and we have
\begin{equation}
\resizebox{0.72\hsize}{!}{$
d_\mathcal P(\boldsymbol z_i, \boldsymbol z_j)^2=\sum\nolimits_{l=1}^K d^{\ \kappa_l}_{\mathcal M_l}\left(({\boldsymbol z_i})_{\mathcal M_l}, ({\boldsymbol z_j})_{\mathcal M_l} \right)^2,
$}
\label{dist}
\end{equation}
for \textsc{SelfMGNN}.
We utilize AUC as the evaluation metric and summarize the performance in Table \ref{results}.
We set output dimensionality to be $24$ for all models for fair comparisons.
Table \ref{results} shows that \textsc{SelfMGNN} outperforms the self-supervised models in Euclidean space consistently since it better matches the mixed structures of graphs with the mixed-curvature space.
\textsc{SelfMGNN} achieves competitive and even better results with the supervised Riemannian baselines. 
The reason lies in that we leverage dual contrastive approach to exploit the rich information of data themselves in the mixed-curvature Riemannian space.


\subsection{Node Classification}
% %\textsc{SelfMGNN} output node representations in the mixed-curvature space. 
% %In this case, our task is to classify nodes based on node representations.
% Most of existing classifiers work with Euclidean spaces, and cannot apply to Riemannian spaces.
% Recently, \cite{HNN} presents a hyperbolic classifier, but it still cannot work with mixed-curvature spaces. % in general.
% %To bridge this gap, similar to the study \cite{HGNN}, we employ an Euclidean classification space.
For node classification, we first discuss the classifier as none of existing classifiers, to our knowledge, can work with mixed-curvature spaces.
To bridge this gap, inspired by \cite{HGNN}, for Riemannian models, we introduce the Euclidean transformation to generate an encoding, summarizing the structure of node representations.
%we utilize the Euclidean encodings which summarize the structure of node representations for classification, similar to \cite{HGNN}.
Specifically, we first introduce a set of centroids $\{\boldsymbol \mu_1,  \cdots, \boldsymbol \mu_C\}$,
where $\boldsymbol \mu_c$ is the centroid in Riemannian space learned jointly with the learning model. %using backpropagation.
% Then, we transform the output representation $\boldsymbol z_j \in \mathcal M$ into an Euclidean encoding $\boldsymbol \xi\in \mathbb R^C$, 
% which summarizes the position of $\boldsymbol z_i $ relative to the centroids, i.e., $\boldsymbol \xi=\left(\xi_{1j}, \ldots, \xi_{Cj}\right) $ and $\xi_{ij}=d_{\mathcal M}(\boldsymbol \mu_i, \boldsymbol z_j)$.
Then, for output representation $\boldsymbol z_j \in \mathcal M$, its encoding is defined as $\boldsymbol \xi=\left(\xi_{1j}, \ldots, \xi_{Cj}\right) $, where $\xi_{ij}=d_{\mathcal M}(\boldsymbol \mu_i, \boldsymbol z_j)$, summarizing the position of $\boldsymbol z_i $ relative to the centroids.
Now, we are ready to use logistic regression for node classification and the likelihood is 
\begin{equation}
p( y | \boldsymbol h )=\text{sigmoid}(\mathbf w_C^\top \boldsymbol h),
\end{equation}
where $\mathbf w_C \in \mathbb R^{|C|}$ is the weight matrix, and $y$ is the label.
$\boldsymbol h=\boldsymbol \xi$ for Riemannian models and $\boldsymbol h$ is the output of Euclidean ones.
We utilize classification accuracy \cite{kipf2016semi} as the evaluation metric and summarize the performance in Table \ref{results}.
\textsc{SelfMGNN} achieves the best results on all the datasets.
% Additionally, to test self-supervised learning, we conduct experiments when the number of labels is extremely small.
% Specifically, we follow \cite{LiHW18} to select labeled examples for Cora and show the results in Table \ref{cora}.
% We find that self-supervised models achieve better results as they can extract rich information from data themselves. %, and our \textsc{SelfMGNN} achieves the best results.

%  \begin{table}
%   \scriptsize
%     \centering
%           \caption{Classification accuracy with low label rates (Cora). The best results are in \textbf{bold}, and the second best \textcolor{blue}{blue}.}
%         \vspace{-0.135in}
%     \begin{tabular}{p{0.1cm}<{\centering} p{1.65cm}<{\centering}|p{1.05cm}<{\centering} p{1.05cm}<{\centering}  p{1.05cm}<{\centering} p{1.05cm}<{\centering}}
%       \toprule
%   \multicolumn{2}{c|}{ \footnotesize{\textbf{Label Rate}} }& \footnotesize{$0.5\%$ }& \footnotesize{$1\%$ }& \footnotesize{$2\%$ }& \footnotesize{$3\%$}\\
%      \toprule
%        \multirow{7}{*}{\rotatebox{90}{\footnotesize{Supervised} }  }
%                          & \footnotesize{GCN}  &$ 42.2(0.9)$      &    $ 56.1(1.1)$      &  $ 64.8(0.7)$    &  $ 70.9(0.6)$         \\
%                 &  \footnotesize{GraphSage} &$ 39.6(0.7)$      &    $ 55.3(1.0)$      &  $ 62.5(0.3)$    &  $  69.1(0.5)$         \\
%                            &\footnotesize{GAT} & $ 51.8(0.6)$     &    $ 59.2(0.9)$      &  $ 65.2(0.9)$    &  $ 71.5(0.8)$         \\
%                       &  \footnotesize{HGCN } &$ 54.5(1.1)$     &    $  62.0(0.7)$     &  $ 68.1(0.6)$    &  $ 72.5 (0.5)$         \\
%                           &  \footnotesize{HAT }&$ 55.7(0.8)$      &    $  61.8(0.4)$      &  $68.9 (0.6)$    &  $73.1(0.8)$         \\
%                        &  \footnotesize{LGCN }&$ 56.3 (0.7)$      &    $  61.5(1.0)$      &  $  67.3(0.5)$    &  $72.3(0.7)$         \\
%           &  \footnotesize{$\kappa$-GCN} &$ 54.1(0.9)$      &    $ 62.2 (0.8)$      &  $  68.8(0.7)$    &  $72.6(0.9)$         \\
%        \midrule
%        \multirow{4}{*}{\rotatebox{90}{\footnotesize{Contrastive}} }
%                         &  \footnotesize{DGI }    & $\blue{66.3}(0.5)$      &  $\mathbf{71.2}(0.5)$    &  $  75.0(0.2)$     &$\blue{78.1}(0.6)$       \\
%                     &  \footnotesize{MVGRL}  &$  61.2(1.2)$      &  $  63.8(0.7)$     &  $  74.1(0.6)$     &$  77.2(0.7)$       \\
%                         &   \footnotesize{GMI }   &$  65.1(0.3)$      &  $  69.7(0.8)$    &  $\blue{75.6}(1.1)$      &$  77.8(0.5)$       \\
% &\footnotesize{\textbf{\textsc{SelfMGNN}} }&$\mathbf{65.6}(0.7)$ & $\blue{70.9}(0.6)$ & $\mathbf{76.0}(0.5)$ & $\mathbf{78.5}(0.3)$   \\
%       \bottomrule
%     \end{tabular} 
%         \vspace{-0.12in}
%         \label{cora}
%   \end{table}
%         \vspace{-0.1in}



 \begin{table}
  \scriptsize
    \centering
          \caption{Learning results of the mixed-curvature space on the datasets — curvature (\textcolor{magenta}{weight}) of  each component space.}
    \begin{tabular}{ p{0.93cm}<{\centering}|p{1.1cm}<{\centering} p{1.1cm}<{\centering} p{1.1cm}<{\centering} p{1.1cm}<{\centering} p{0.65cm}<{\centering}}
      \toprule
\footnotesize{\textbf{Dataset}}&   \footnotesize{$\mathbb H^4$}    &  \footnotesize{$\mathbb H^4$} &     \footnotesize{$\mathbb S^4$}    &  \footnotesize{$\mathbb S^4$} & \footnotesize{$\mathbb E^8$}         \\
\toprule
\footnotesize{Citeseer}&$-0.67(\textcolor{magenta}{0.29})$    &  $-0.58(\textcolor{magenta}{0.19})$   &  $  +0.82 (\textcolor{magenta}{0.21})$   &  $  +2.72 (\textcolor{magenta}{0.13})$ & $ 0 (\textcolor{magenta}{0.18})$     \\
\footnotesize{Cora}      &$-0.90  (\textcolor{magenta}{0.18})$    &  $-1.31  (\textcolor{magenta}{0.25})$   & $  +0.76 (\textcolor{magenta}{0.28})$    & $+0.19 (\textcolor{magenta}{0.08})$ &  $ 0 (\textcolor{magenta}{0.21})$     \\
\footnotesize{Pubmed}&$-1.12  (\textcolor{magenta}{0.26})$     &   $-0.79  (\textcolor{magenta}{0.34})$   &  $ +0.59 (\textcolor{magenta}{0.16})$  & $+1.05 (\textcolor{magenta}{0.15})$  &$ 0 (\textcolor{magenta}{0.09})$    \\
\footnotesize{Amazon}&$-0.78 (\textcolor{magenta}{0.11})$      &  $-1.02  (\textcolor{magenta}{0.48})$   &  $  +1.13 (\textcolor{magenta}{0.05})$  & $+2.24 (\textcolor{magenta}{0.24})$ &$ 0 (\textcolor{magenta}{0.12})$   \\
\footnotesize{Airport}  &$-1.26 (\textcolor{magenta}{0.30})$     &  $-2.15  (\textcolor{magenta}{0.17})$   &  $ +1.85 (\textcolor{magenta}{0.20})$   & $+0.67 (\textcolor{magenta}{0.18})$  & $ 0 (\textcolor{magenta}{0.15})$  \\
      \bottomrule
    \end{tabular} 
        \label{component}
  \end{table}

\subsection{Ablation Study} 
We give the ablation study on the importance of i)  mixed-curvature space (MCS) and 
%as well as the differences between different constant-curvature space, e.g., $\mathbb H$ v.s. $\mathbb E$. 
ii) cross-view contrastive learning.
To this end, we include two kinds of variants: CCS and Single, the degenerated \textsc{SelfMGNN}s without some functional module.
CCS variants work without Cartesian product, and thus are learned by the single-view contrastive loss in a constant-curvature space. 
e.g., $\mathbb H^{24}$ is the variant in the hyperbolic space, where the superscript is the dimensionality.
Single variants work with the mixed-curvature space, but disable the cross-view contrastive learning.
The ours are the proposed \textsc{SelfMGNN}s.
A specific instantiation is denoted by Cartesian product, e.g., 
we use $(\mathbb H^4)^2\times(\mathbb S^4)^2\times\mathbb E^8$ as default, 
whose mixed-curvature space is constructed by Cartesian product of  $2$ $\mathbb H^4$,  $2$ $\mathbb S^4$ and $1$ $\mathbb E^8$ component spaces. 

We show the classification accuracy of these variants in Table \ref{ablation}, and we find that:
i) Mixed-curvature variant with single or dual contrastive learning outperforms its CCS counterpart. The reason lies in that the mixed-curvature space is more flexible than a constant-curvature space to cover the complicated graph structures.
ii) Disabling cross-view contrastive learning decreases the performance as the cross-view contrasting further unleashes the rich information of data in the mixed-curvature space. 
iii) Allowing  more than one hyperbolic and spherical spaces can also improve performance as $(\mathbb H^4)^2\times (\mathbb S^4)^2\times\mathbb E^8$
and  $(\mathbb H^2)^4\times (\mathbb S^2)^4\times\mathbb E^8$ both outperform $\mathbb H^8 \times\mathbb S^8\times\mathbb E^8$.

%we list each component space of  $\mathbb H^8\times\mathbb S^8\times\mathbb E^8$ for node classification in Table \ref{component}.
Furthermore, we discuss the curvatures of the datasets. 
We report the learned curvatures and weights of each component space for the real-world datasets in Table \ref{component}.
%Different datasets present different curvature distributions. Hence, the models with fixed curvature is limited.
As shown in Table \ref{component}, component spaces of the same space type are learned with different curvatures, showing that the curvatures over different hierarchical or cyclical regions can still be different.
$(\mathbb H^4)^2\times (\mathbb S^4)^2\times\mathbb E^8$ has $2$ component spaces for hyperbolic (spherical) geometry, 
%which includes the curvatures of $\mathbb H^8\times\mathbb S^8\times\mathbb E^8$.
and allowing multiple hyperbolic (spherical) components enables us to cover a wider range of curvatures of the graph, better matching the graph structures.
This also explains why $(\mathbb H^4)^2\times (\mathbb S^4)^2\times\mathbb E^8$ outperforms  $\mathbb H^8 \times\mathbb S^8\times\mathbb E^8$ in Table \ref{ablation}.
With learnable curvatures and weights of each component, \textsc{SelfMGNN} matches the mixed and complicated graph structures, and learns more promising graph representations. 
%This shows that graph structures are usually heterogeneous. 






