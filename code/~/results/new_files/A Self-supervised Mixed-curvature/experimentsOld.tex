%!TEX root = ./draft.tex

 \begin{table*}
  \footnotesize
    \centering
          \caption{The summary of classification accuracy (\%) for node classification (NC) and AUC (\%) for link prediction (LP) on Citeseer, Cora, Pubmed, Amazon and USA datasets. The best experimental result is marked by \textbf{bold}, and the second \textcolor{blue}{blue}.}
          \vspace{-0.1in}
    \begin{tabular}{p{0.22cm}<{\centering} p{1.75cm}<{\centering}|p{0.97cm}<{\centering} p{1.15cm}<{\centering} |p{0.97cm}<{\centering} p{1.15cm}<{\centering} |p{0.97cm}<{\centering} p{1.15cm}<{\centering}| p{0.97cm}<{\centering} p{1.15cm}<{\centering}| p{0.97cm}<{\centering} p{1.1cm}<{\centering}}
      \toprule
  \multicolumn{2}{c|}{}        & \multicolumn{2}{c|}{\textbf{Citeseer}} &  \multicolumn{2}{c|}{\textbf{Cora} } &  \multicolumn{2}{c|}{\textbf{Pubmed}}  &  \multicolumn{2}{c|}{\textbf{Amazon} } &  \multicolumn{2}{c}{\textbf{Airport}} \\
  \multicolumn{2}{c|}{\textbf{Method}} & LP& NC & LP & NC& LP & NC& LP & NC& LP & NC\\
     \toprule
       \multirow{6}{*}{\rotatebox{90}{Euclidean} } 
                         & GCN   
&  $  92.9(0.7)$   &  $  70.3()$   &  $ 90.4(0.2)$   &  $  81.5(0.3)$   &   $  91.1(0.5)$    &    $  79.0(0.2)$   &  $  92.8(0.8)$  & $ 70.9(1.1)$ &  $ 90.2(0.4)$  &  $  50.9(0.6)$ \\
                &  GraphSage
&  $  87.1(0.9)$   &    $  ()$       &  $  85.5(0.6)$  &    $  77.9(2.4)$  &  $  86.2(1.0)$    &    $  77.4(2.2)$   &  $  90.8(0.5)$  & $  72.3(1.6)$ &  $ 84.4(1.0)$    &  $  47.5(0.8)$ \\
                        &GAT     
&  $  92.2(0.7)$   &  $72.5(0.7)$ & $  93.7(0.1)$ &   $  83.0(0.7)$   &  $  91.2(0.1)$     &   $  79.0(0.3)$   &   $  92.8(0.9)$  & $  72.7(0.8)$ &  $ 91.1(0.5)$  &   $  49.5(0.7)$\\
                           &  DGI 
&  $  ()$              &  $ 71.8(0.7)$ & $  ()$             &  $  82.3(0.6)$    &   $  ()$                &    $  76.8(0.6)$  &  $  ()$               & $  ()$ &  $  ()$               &    $  ()$          \\
                    &  MVGRL 
&     $  ()$           &  $ 73.3(0.5)$ &$  ()$              &  $  86.8(0.5)$    &   $  ()$               &    $   80.1(0.7)$  &  $  ()$               & $  ()$ &  $  ()$               &    $  ()$          \\
                        &   GMI   
&    $  ()$           &   $ 73.0(0.3)$ & $ 96.3(0.1)$  &  $  82.7(0.2)$   &   $  ()$                &    $  80.1(0.2)$   &  $  ()$               & $  ()$ & $  ()$                &    $  ()$          \\
       \midrule
       \multirow{5}{*}{\rotatebox{90}{Riemannian}}
                      &  HGCN 
&  $  94.3(0.4)$  &    $  ()$       &  $  92.9(0.1)$   &   $  79.9(0.2)$  &  $  96.3(0.0)$    &    $  80.3(0.3)$   &  $  96.3(0.9)$   & $  72.7(1.3)$ & $  93.4(0.3)$    &  $  51.1(1.0)$ \\
                          &  HAT 
&  $  93.6(0.5)$  &    $  ()$      &  $  93.0(0.3)$    &    $  ()$            &  $  92.3(0.3)$     &    $  ()$               &  $  96.9(1.0)$   &$  73.3(1.0)$  & $  93.6(0.6)$    &   $  51.3(0.9)$ \\
                       &  LGCN 
&  $  95.4(0.5)$  &    $  ()$      &  $  93.6(0.4)$    &   $  ()$             &  $  96.6(0.1)$     &    $  ()$                &  $  97.3(0.8)$ & $  75.0(1.1)$  & $  96.3(0.2)$    &   $  51.9(0.9)$ \\
          &  $\kappa$-GCN 
&  $  93.8(0.5)$  &    $  ()$      &  $  92.6(0.4)$    &   $  ()$             &  $  94.9(0.3)$     &    $  ()$                &  $  94.8(0.5)$ & $  71.9(1.1)$  & $  93.3(0.6)$     &  $  50.5(1.2)$ \\
&\textbf{\textsc{SelfMGNN}} &  $\mathbf{94.5}(0.2)$ & $\mathbf{94.5}(0.2)$ & $\mathbf{94.5}(0.2)$ & $\mathbf{94.5}(0.2)$ 
                                                    & $\mathbf{94.5}(0.2)$ & $\mathbf{94.5}(0.2)$ & $\mathbf{94.5}(0.2)$ & $\mathbf{94.5}(0.2)$ 
                                                    & $\mathbf{94.5}(0.2)$ & $\mathbf{94.5}(0.2)$  \\
      \bottomrule
    \end{tabular} 
        \vspace{-0.2in}
        \label{results}
  \end{table*}
  
\section{Experiments}
In this section, we evaluate  \textsc{SelfMGNN} by link prediction and node classification against $10$ strong baselines on $5$ benchmark datasets. 
We repeat each experiment $10$ times and report the mean with the standard deviations.

\subsection{Experimental Setups}
\noindent\textbf{Datasets:} We utilize several benchmark datasets, i.e., 
\textbf{Citeseer}, 
\textbf{Cora} 
and 
\textbf{Pubmed} \cite{kipf2016semi,VelickovicFHLBH19}, 
\textbf{Amazon} \cite{ZhangWSLS21}
and 
\textbf{USA} \cite{HGCN}.
We list the statistics in Supplementary Material.

\noindent\textbf{Euclidean Baselines:} %We compare the proposed \textsc{SelfMGNN} with the state-of-the-art baselines of two categories: % of constant-curvature space and self-supervised learning.
i) \emph{Supervised Models}: 
\textbf{GCN} \cite{kipf2016semi},
\textbf{GraphSage} \cite{hamilton2017inductive},
\textbf{GAT} \cite{velickovic2018graph}.
ii) \emph{Self-supervised Models}: 
\textbf{DGI} \cite{VelickovicFHLBH19},
\textbf{MVGRL} \cite{HassaniA20},
\textbf{GMI} \cite{PengHLZRXH20}.

\noindent\textbf{Riemannian Baselines:}
i) \emph{Supervised Models}: 
\textbf{HGCN} \cite{HGCN}, 
\textbf{HAT} \cite{HAN}, 
\textbf{LGCN} \cite{ZhangWSLS21},
$\boldsymbol \kappa$-\textbf{GCN} \cite{BachmannBG20}.
ii)  
\emph{Self-supervised Models}: There is no self-supervised Riemannian methods in the literature, and we propose \textsc{SelfMGNN} to fill this gap.

%\emph{Refer Supplementary Material for details.}
\vspace{-0.05in}
\subsection{Technical Details}
%$\kappa$-stereographic model
\noindent\textbf{Euclidean input:} The input feature $\mathbf v$ is Euclidean by default. 
In this case, we will map it to the Riemannian space to feed into \textsc{SelfMGNN}. 
Specifically, the map $\rho: \mathbb R^d \to \mathcal M^d_\kappa$ is defined as 
$\rho(\mathbf v)=( F_\kappa
|\kappa|^{\frac{1}{2}}
\sinh(
|\kappa|^{-\frac{1}{2}}
||\mathbf v||_2
)
)\mathbf v$,
where projection factor $F$ is given as follows:
   \vspace{-0.05in}
\begin{equation}
F_\kappa=\left(1+\kappa\cosh(|\kappa|^{-\frac{1}{2}}||\mathbf v||_2)\right)||\mathbf v||_2.
   \vspace{-0.05in}
\end{equation}
\emph{We derive the map  $\rho(\mathbf v)$ in Supplementary Material.}

\noindent\textbf{Congruent graph:} 
As suggested by \cite{HassaniA20}, we opt for diffusion to generate the congruent graph.
%Generate congruent views with diffusion matrices
% \begin{equation}
% \Gamma(\mathbf G)=\alpha\left(\mathbf{I}_{n}-(1-\alpha) \mathbf{D}^{-\frac{1}{2}} \mathbf{G D}^{-\frac{1}{2}}\right)^{-1}
% \end{equation}
Given an the adjacency matrix $\mathbf G^\alpha$, we compute the diffusion matrix $\mathbf G^\beta$ via the diffusion function \emph{$\Gamma(\cdot)$}, and treat it as the adjacency matrix of the congruent counterpart. 
In practice, we utilize the Personalized PageRank diffusion which has a closed-form formulation: 
   \vspace{-0.05in}
\begin{equation}
\Gamma(\mathbf G^\alpha)=\alpha\left(\mathbf{I}_{n}-(1-\alpha) \mathbf{D}^{-\frac{1}{2}} \mathbf{G^\alpha D}^{-\frac{1}{2}}\right)^{-1},
   \vspace{-0.05in}
\end{equation}
where $\alpha$ denotes teleport probability in the diffusion. 
Note that, 
the diffusion is computed once using fast approximation and sparsification methods \cite{KlicperaWG19}.

\noindent\textbf{Signature:} The mixed-curvature space is parameterized by the signature, i.e., space type, curvature and dimensionality of component spaces.
The component space $\mathcal M_i$ can be hyperbolic $\mathbb H$, spherical $\mathbb S$ or Euclidean $\mathbb E$, and we utilize the combination of them to cover the intrinsic complicated structures of the graphs.
For each component space, we treat the dimensionality $d_{\mathcal M_i}$ as hyperparameters, 
and treat the curvature $\kappa_{\mathcal M_i}$ as parameters as our loss is differentiable with respect to them. We learn the curvatures using gradient based optimization.

%We stack the attentive aggregation layer twice in the experiment.

\subsection{Link Prediction}
\vspace{-0.05in}
In the experiments, similar to \cite{VelickovicFHLBH19}, self-supervised models first learn representations without labels, and then were evaluated by specific learning task, which is performed by directly using these representations to train and test for learning tasks.
Supervised models were trained and tested following \cite{HGCN}.
%The task of link prediction is to predict the probability of two nodes being connected.  is a generalized sigmoid function, and
For link perdition, we utilize the Fermi-Dirac decoder with distance function to define the probability based on model outputs $\boldsymbol z$: % representations. 
%we have Formally, 
   \vspace{-0.07in}
\begin{equation}
\resizebox{0.887\hsize}{!}{$
p((i,j) \in E| \boldsymbol z_i, \boldsymbol z_j)=\left(\exp\left((d_\mathcal M(\boldsymbol z_i, \boldsymbol z_j)^2-r)/t\right)+1\right)^{-1}
$}
   \vspace{-0.05in}
\end{equation}
where $r$, $t$ are hyperparameters. 
For each method, $d_\mathcal M(\boldsymbol z_i, $ $\boldsymbol z_j)$ is the distance function of corresponding representation space, e.g., $||\boldsymbol z_i - \boldsymbol z_j||_2$ for Euclidean models, and naturally, 
   \vspace{-0.12in}
\begin{equation}
\resizebox{0.67\hsize}{!}{$
d_\mathcal P(\boldsymbol z_i, \boldsymbol z_j)^2=\sum\nolimits_{i=1}^K d^{\ \kappa_i}_{\mathcal M_i}\left({\boldsymbol z_i}_{[\mathcal M_i]}, {\boldsymbol z_j}_{[\mathcal M_i]} \right)^2
$}
\label{dist}
   \vspace{-0.05in}
\end{equation}
for \textsc{SelfMGNN}.
We utilize AUC as evaluation metric and summarize the performances in Table \ref{results}.
As shown in the table, \textsc{SelfMGNN} achieves the best results.
The reasons lie in that 
\textsc{SelfMGNN} i) better matches heterogeneous structures of graphs with the mixed-curvature space,
and ii) leverages dual contrastive approach to extract the rich supervision information in the mixed-curvature space.

\vspace{-0.1in}
\subsection{Node Classification}
%\vspace{-0.03in}
% %\textsc{SelfMGNN} output node representations in the mixed-curvature space. 
% %In this case, our task is to classify nodes based on node representations.
% Most of existing classifiers work with Euclidean spaces, and cannot apply to Riemannian spaces.
% Recently, \cite{HNN} presents a hyperbolic classifier, but it still cannot work with mixed-curvature spaces. % in general.
% %To bridge this gap, similar to the study \cite{HGNN}, we employ an Euclidean classification space.
For node classification, we first discuss the classifier as none of existing classifiers, to our knowledge, can work with mixed-curvature spaces.
To bridge this gap, inspired by \cite{HGNN}, for Riemannian models, we introduce the Euclidean transformation to generate an encoding, summarizing the structure of node representations.
%we utilize the Euclidean encodings which summarize the structure of node representations for classification, similar to \cite{HGNN}.
Specifically, we first introduce a set of centroids $\{\boldsymbol \mu_1,  \cdots, \boldsymbol \mu_C\}$,
where $\boldsymbol \mu_c$ is the centroid in Riemannian space learned jointly with the learning model. %using backpropagation.
% Then, we transform the output representation $\boldsymbol z_j \in \mathcal M$ into an Euclidean encoding $\boldsymbol \xi\in \mathbb R^C$, 
% which summarizes the position of $\boldsymbol z_i $ relative to the centroids, i.e., $\boldsymbol \xi=\left(\xi_{1j}, \ldots, \xi_{Cj}\right) $ and $\xi_{ij}=d_{\mathcal M}(\boldsymbol \mu_i, \boldsymbol z_j)$.
Then, for output representation $\boldsymbol z_j \in \mathcal M$, its encoding is defined as $\boldsymbol \xi=\left(\xi_{1j}, \ldots, \xi_{Cj}\right) $, where $\xi_{ij}=d_{\mathcal M}(\boldsymbol \mu_i, \boldsymbol z_j)$, summarizing the position of $\boldsymbol z_i $ relative to the centroids.
Now, we are ready to use logistic regression for node classification and the likelihood is 
\vspace{-0.09in}
\begin{equation}
p( y | \boldsymbol h )=\text{sigmoid}(\mathbf w_C^\top \boldsymbol h),
\vspace{-0.07in}
\end{equation}
where $\mathbf w_C \in \mathbb R^{|C|}$ is the weight matrix, and $y$ is the label.
$\boldsymbol h=\boldsymbol \xi$ for Riemannian models and $\boldsymbol h$ is the output of Euclidean ones.
We utilize classification accuracy \cite{kipf2016semi} as evaluation metric and summarize the performances in Table \ref{results}.
Additionally, to test self-supervised learning, we conduct experiments when the number of labels is extremely small.
Specifically, we follow \cite{LiHW18} to select labeled examples for Cora and show the results in Table \ref{cora}.
We find that self-supervised models achieve better results as they can extract supervision from data themselves, and our \textsc{SelfMGNN} achieves the best results.

 \begin{table}
  \footnotesize
    \centering
          \caption{Classification accuracy with low label rates (Cora). The best results are in \textbf{bold}, and the second best \textcolor{blue}{blue}.}
        \vspace{-0.135in}
    \begin{tabular}{p{0.1cm}<{\centering} p{1.65cm}<{\centering}|p{1.05cm}<{\centering} p{1.05cm}<{\centering}  p{1.05cm}<{\centering} p{1.05cm}<{\centering}}
      \toprule
  \multicolumn{2}{c|}{\textbf{Label Rate}} & $0.5\%$ & $1\%$ & $2\%$ & $3\%$\\
     \toprule
       \multirow{7}{*}{\rotatebox{90}{Supervised} } 
                         & GCN  & $  ()$      &    $  ()$      &  $  ()$    &  $  ()$         \\
                &  GraphSage &$  ()$      &    $  ()$      &  $  ()$    &  $  ()$         \\
                        &GAT    & $  ()$      &    $  ()$      &  $  ()$    &  $  ()$         \\
                      &  HGCN &$  ()$      &    $  ()$      &  $  ()$    &  $  ()$         \\
                          &  HAT &$  ()$      &    $  ()$      &  $  ()$    &  $  ()$         \\
                       &  LGCN &$  ()$      &    $  ()$      &  $  ()$    &  $  ()$         \\
          &  $\kappa$-GCN &$  ()$      &    $  ()$      &  $  ()$    &  $  ()$         \\
       \midrule
       \multirow{4}{*}{\rotatebox{90}{Contrastive}}
                        &  DGI $  ()$      &    $  ()$      &  $  ()$    &  $  ()$         \\
                    &  MVGRL $  ()$      &    $  ()$      &  $  ()$    &  $  ()$         \\
                        &   GMI   $  ()$      &    $  ()$      &  $  ()$    &  $  ()$         \\
&\textbf{\textsc{SelfMGNN}} &  $\mathbf{94.5}(0.2)$ & $\mathbf{94.5}(0.2)$ & $\mathbf{94.5}(0.2)$ & $\mathbf{94.5}(0.2)$   \\
      \bottomrule
    \end{tabular} 
        \vspace{-0.225in}
        \label{cora}
  \end{table}

\vspace{-0.1in}
\subsection{Ablation Study} 
\vspace{-0.03in}
We give the ablation study on i) the differences between constant-curvature space (CCS) and mixed-curvature space (MCS), and 
%as well as the differences between different constant-curvature space, e.g., $\mathbb H$ v.s. $\mathbb E$. 
ii) the importance of cross-view contrastive learning in MCS.
To this end, we give several variants of \textsc{SelfMGNN}.
We use Cartesian product to denote a specific instantiation, e.g., $\mathbb H^8 \times \mathbb S^8 \times \mathbb E^8$ denotes the \textsc{SelfMGNN} of the Cartesian product of $\mathbb H^8$, $\mathbb S^8$ and $\mathbb E^8$,  where the superscript is dimensionality.
%a $8$-dimensional hyperbolic space,  a $8$-dimensional spherical space and a $8$-dimensional Euclidean space.
%We further include degenerate versions of \textsc{SelfMGNN} to the constant-curvature space. 
We give several variants (referred to as single) that disable the cross-view contrastive learning.
Particularly, $\mathbb H$, $\mathbb S$ and $\mathbb E$ denote degenerate \textsc{SelfMGNN} of corresponding CCS with sing-view contrastive learning only.
%hyperbolic, spherical and Euclidean space, respectively.
As shown in Table \ref{ablation}, we find that: 
i) Disabling cross-view contrastive learning decreases the performance as cross-view contrasting unleashes the rich information in MCS.
ii) MCS model outperforms the CCS model. 
Additionally, we list each component space of \textsc{SelfMGNN} ($\mathbb H^8\times\mathbb S^8\times\mathbb E^8$) for node classification in Table \ref{component}.
This suggests that graph structures are usually heterogeneous. 
MCS model is more flexible than CCS to capture structural heterogeneity, and thus outputs more promising representations. 
%\subsection{Something More?}


















