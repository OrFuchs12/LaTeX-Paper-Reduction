%!TEX root = ./SelfMGNN.tex

\begin{abstract}
Graph representation learning received increasing attentions in recent years.
Most of the existing methods ignore the complexity of the graph structures and restrict graphs in a single constant-curvature representation space, which is only suitable to particular kinds of graph structure indeed.
Additionally, these methods follow the supervised or semi-supervised learning paradigm, and thereby notably limit their deployment on the unlabeled graphs in real applications.
To address these aforementioned limitations, 
we take the first attempt to study the self-supervised graph representation learning in the mixed-curvature spaces.
In this paper, 
we present a novel \textbf{Self}-supervised \textbf{M}ixed-curvature \textbf{G}raph \textbf{N}eural \textbf{N}etwork (\textbf{\textsc{SelfMGNN}}).
To capture the complex graph structures, 
we construct a \emph{mixed-curvature}  space via the Cartesian product of multiple Riemannian component spaces, 
and design hierarchical attention mechanisms for learning and fusing graph representations across these component spaces. 
% To enable the self-supervisd learning, we propose a novel \emph{dual contrastive approach} that con.
% Specifically, we first introduce a Riemannian projector to reveal different Riemannian views. 
% Then, we utilize a well-designed Riemannian discriminator for 
% single-view contrastive learning, contrasting positive and negative samples in the same Riemannian view, 
% and cross-view contrastive learning, contrasting between different Riemannian views concurrently.
To enable the self-supervisd learning, we propose a novel \emph{dual contrastive approach}.
The constructed mixed-curvature space actually provides multiple Riemannian views for the contrastive learning. 
We introduce a Riemannian projector to reveal these views,
 and utilize a well-designed Riemannian discriminator for the \emph{single-view} and \emph{cross-view contrastive learning} within and across the Riemannian views.
Finally, extensive experiments show that \textsc{SelfMGNN} captures the complex graph structures and outperforms state-of-the-art baselines.
\end{abstract}

