\relax 
\bibstyle{aaai23}
\citation{Carl2015,chen2020simple,he2020momentum}
\citation{Morgado_2021_CVPR,patrick2020multi,alwassel2019self}
\citation{misra2016shuffle,jenni2021time,dave2021tclr}
\citation{wei2018learning,benaim2020speednet,jenni2020video}
\citation{wu2018unsupervised}
\citation{dwibedi2021little,koohpayegani2021mean}
\citation{lee2008robust,black2021vpn}
\citation{dosovitskiy2015discriminative,wu2018unsupervised}
\citation{chen2020simple,he2020momentum}
\citation{grill2020bootstrap,chen2020exploring,caron2020unsupervised,wang2020unsupervised}
\citation{dwibedi2021little,koohpayegani2021mean}
\citation{qian2020spatiotemporal,feichtenhofer2021large}
\citation{dave2021tclr,patrick2020multi}
\citation{bai2020can,jenni2021time}
\citation{Carl2015,noroozi2016unsupervised}
\citation{zhang2016colorful,zhang2016split}
\citation{gidaris2018unsupervised,jenni2018artifacts,jenni2020steering}
\citation{misra2016shuffle,brattoli2017lstm,fernando2017self,lee2017unsupervised}
\citation{xu2019self,kim2019self}
\citation{wei2018learning}
\citation{epstein2020oops,benaim2020speednet,yao2020video}
\citation{jenni2020video}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:contrastive_terms}{{1}{2}{\textbf  {Illustration of Contrastive Loss Terms in our Model.} We demonstrate the main contrastive pairs in our formulation given an example video clip (yellow box in the middle) and its corresponding audio clip. Positives (solid green arrows) are constructed from differently augmented video clips of the same training instance (blue box) and \emph  {temporally aligned} pairs of the corresponding video and audio clips. Negatives (dashed red arrows) stem from $m$ other video and audio clips from the current mini-batch or a memory bank of prior embeddings (gray box on the right). Additional positives from the memory bank are omitted from the figure. Note that our formulation does not contain any contrastive terms among audio clips. }{}{}}
\newlabel{fig:temp_ssl}{{2}{2}{\textbf  {Illustration of the Temporal Reasoning Tasks.} Besides contrastive terms, our model encompasses both per-clip classification tasks (blue arrows) about the playback-speed and -direction, and temporal ordering tasks (green arrows) which are performed both intra- and cross-modal (V: RGB frames, A: audio). }{}{}}
\citation{owens2016ambient}
\citation{Owens_2018_ECCV,arandjelovic2017look}
\citation{korbar2018cooperative}
\citation{recasens2021broaden}
\citation{alwassel2020self}
\citation{Morgado_2021_CVPR}
\citation{alayrac2020self,akbari2021vatt}
\citation{grill2020bootstrap}
\newlabel{eq:crl_loss}{{1}{3}{}{}{}}
\newlabel{eq:sim}{{2}{3}{}{}{}}
\newlabel{eq:weight}{{3}{3}{}{}{}}
\citation{jenni2020video}
\citation{jenni2021time,xu2019self,kim2019self}
\citation{hara2018can}
\citation{tran2018closer}
\citation{he2016deep}
\citation{loshchilov2018decoupled}
\citation{loshchilov2016sgdr}
\citation{chen2020simple}
\citation{zisserman2017kinetics}
\citation{soomro2012ucf101}
\citation{hmdb51}
\citation{piczak2015esc}
\citation{chen2020vggsound}
\citation{papakipos2022augly}
\newlabel{eq:crl_obj}{{4}{4}{}{}{}}
\newlabel{sec:temp}{{}{4}{}{}{}}
\citation{koohpayegani2021mean}
\citation{dwibedi2021little}
\citation{chen2020simple}
\citation{chen2020exploring}
\newlabel{tab:crl_abl}{{1}{5}{\textbf  {Contrastive Loss Design.} We explore different configurations of the contrastive loss formulation in Eq.\nobreakspace  {}\ref {eq:crl_loss} in combination with temporal SSL when applied to video-video learning (no audio is being used). We report nearest-neighbor classifier accuracy on UCF101 and HMDB51 and recall @1 for robust video fingerprinting on VGG-Sound. }{}{}}
\newlabel{tab.ablations}{{1}{5}{\textbf  {Contrastive Loss Design.} We explore different configurations of the contrastive loss formulation in Eq.\nobreakspace  {}\ref {eq:crl_loss} in combination with temporal SSL when applied to video-video learning (no audio is being used). We report nearest-neighbor classifier accuracy on UCF101 and HMDB51 and recall @1 for robust video fingerprinting on VGG-Sound. }{}{}}
\newlabel{tab:audio_abl}{{2}{5}{\textbf  {Temporal Self-Supervision for Audio Feature Learning.} We explore how the different temporal self-supervision signals impact the audio representation performance for downstream audio classification on ESC50 and audio fingerprinting on VGG-Sound. The audio encoder is pre-trained with temporal supervision and audio-audio contrastive learning (no RGB frames were used). }{}{}}
\citation{jenni2021time}
\citation{qian2020spatiotemporal}
\citation{alayrac2020self}
\citation{recasens2021broaden}
\citation{korbar2018cooperative}
\citation{alwassel2019self}
\citation{patrick2020multi}
\citation{Morgado_2021_CVPR}
\citation{recasens2021broaden,qian2020spatiotemporal}
\citation{dave2021tclr}
\citation{patrick2020multi}
\citation{morgado2021robust}
\citation{jenni2021time}
\newlabel{tab:mm_abl}{{3}{6}{\textbf  {Audio-Visual Model Ablations.} We perform ablation experiments to demonstrate the influence of the different self-supervised learning signals in our approach (first block) and various implementation details (second block). The video encoder is evaluated in transfer to action recognition on UCF101 and HMDB51, and the audio encoder for classification on ESC50. The fused audio-video feature is used for fingerprinting on VGG-Sound. }{}{}}
\newlabel{tab.ablations}{{3}{6}{\textbf  {Audio-Visual Model Ablations.} We perform ablation experiments to demonstrate the influence of the different self-supervised learning signals in our approach (first block) and various implementation details (second block). The video encoder is evaluated in transfer to action recognition on UCF101 and HMDB51, and the audio encoder for classification on ESC50. The fused audio-video feature is used for fingerprinting on VGG-Sound. }{}{}}
\bibdata{aaai23}
\newlabel{tab:comparison}{{4}{7}{\textbf  {Action Recognition on UCF101 and HMDB51 and Audio Classification on ESC50.} We report action recognition accuracy after full fine-tuning and linear probe evaluation. We indicate the pre-training dataset, resolution, the number of frames, iterations (or epochs in brackets), and pre-training data modalities (V=RGB, A=audio). }{}{}}
\newlabel{fig:comp_augvgg}{{3}{7}{\textbf  {Video Fingerprinting Performance.} We report instance retrieval performance under video content manipulation on the different AugVGG variants. We show results using a video only (V), audio only (A), and a joint audio-visual model (A+V). }{}{}}
\newlabel{tab:nn}{{5}{7}{\textbf  {Video Retrieval on UCF101 and HMDB51.} We report recall at $k$ (R@$k$) for $k$-NN video retrieval. All methods use a R(2+1)D-18 network. }{}{}}
\newlabel{tab:fusion}{{6}{7}{\textbf  {Modality Fusion.} We explore the fusion of our audio-visual features for downstream video classification. }{}{}}
\gdef \@abspage@last{8}
