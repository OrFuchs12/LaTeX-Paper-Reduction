\section{Audit Methods \label{sec:methods}}

In this section we detail the methods used to answer the questions from our conceptual audit framework, first addressing the primary tools used (Amazon Mechanical Turk and Appium), and then detailing our experiments. We will later discuss how future audit studies can and should employ these various methods. We investigate the mechanism behind the Trending Stories section first, because by knowing aspects of the algorithm such as its update frequency and its degree of personalization, we can better design and more efficiently execute the content audit phase. For example, if content is not personalized, then we can collect data from a single source instead of applying a crowdsourced audit. Once the mechanism is clarified, we design the content audit accordingly.

\subsection{Auditing Tools}

\subsubsection{Crowdsourced Auditing via Mechanical Turk}
For crowdsourced auditing, we leveraged Amazon Mechanical Turk (AMT), for which we designed a Human Intelligence Task (HIT) to collect screenshots of Apple News from workers on the platform. Our Institutional Review Board (IRB) reviewed the HIT description and determined it was not considered human research. Still, an important ethical consideration in using AMT is setting the wage for completing a task \citep{Hara2018}. To ensure we compensated workers according to the United States federal minimum wage of \$7.25 per hour, we deployed a pilot version of our HIT to fifteen users and measured the median time to completion, which was 6 minutes. We round up and consider \$0.75 to be the \textit{minimum} pay for the work of taking a  screenshot of the app for our data collection.

To audit the algorithm for personalization and eliminate time as a potential cause of variation in content, we needed multiple users to take screenshots in synchrony. Since crowdworkers perform tasks on their own schedule, synchronization represents a significant challenge. We therefore modify an approach from \citet{Bernstein2012}, in which we incentivize workers to wait until a designated time by offering a ``high-reward'' wage of \$4.00 per screenshot. For instance, users checking in at 10:36am and 10:42am would both be instructed to wait until 11:00am to take the screenshot in exchange for the increased wages. Once users uploaded their screenshot, we manually verify that the time displayed in the screenshot is the time we requested, then record the headlines shown.

After iterating on the task to ensure consistency in the data collection process, the task ultimately instructed workers to take the following steps: 

\begin{enumerate}
\item Unblock any previously blocked channels in Apple News
\item Force-close the Apple News app
\item Wait until the time was a round hour (e.g. 1:00, 2:00, etc.)
\item Re-open Apple News and take a screenshot showing the Trending Stories section
\item Upload the screenshot to Imgur and provide the URL to the screenshot, as well as input the zipcode of the device at the time the screenshot was taken

\end{enumerate}

\subsubsection{Sock Puppet Auditing via Appium}
Since the Apple News app lacks public APIs and implements security measures such as SSL pinning, we could not collect data via direct scraping. Crowdsourced auditing circumvents this and is ideal for auditing personalization, but to collect data for other experiments, we used Appium. Appium allows automatic control of the iPhone simulator on macOS, and was designed mainly for software verification. Appium's API can perform a number of actions in the iOS simulator, such as opening and closing applications, scrolling, finding and tapping buttons, and more. Using the API, we designed and built a suite of Python programs to collect stories from and run experiments in the Apple News app. We make our programs available with this paper\footnote{\url{https://github.com/comp-journalism/apple-news-scraper}}. The programs work by automatically opening the Apple News app, scrolling to Trending Stories, copying the ``share'' links to each of the stories, and saving them to a .csv file. Appium's API also allowed us to change the geolocation of the device, a feature we used in an experiment to test for localization. These experiments are detailed in the following section.

\begin{figure}[t]
    \centering
\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [cloud] (init) {Apple News Servers};
    \node [below of=init,node distance=1.5cm] (dummy) {};
    \node [block, left of=dummy,node distance=1.5cm] (sim) {iPhone Simulator};
    \node [block, right of=dummy,node distance=1.5cm] (dev) {iPhone Devices};
    \node [dashblock, below of=sim] (app) {Appium Sock Puppet};
    \node [dashblock, below of=dev] (amt) {AMT Crowd Workers};
    % Draw edges
    \path [line] (init) -- (sim);
    \path [line] (sim) -- (init);
    \path [line] (init) -- (dev);
    \path [line] (dev) -- (init);

\end{tikzpicture}
    \caption{A depiction of our two audit methods: crowdsourced auditing via Amazon Mechanical Turk workers, and sock puppet auditing via an Appium-controlled iPhone simulator.}
    \label{fig:my_label}
\end{figure}


\subsection{Mechanism Audit}

\subsubsection{Experiment 1a: Measuring platform-wide update frequency.}\label{platform-wide}
We identify two content update mechanisms on Apple News: platform-wide updates and user-specific updates. In platform-wide updates, the ``master list'' of Trending Stories changes on the Apple News servers. The two types of updates may not always correlate: an update to the master list may not cause an update to the stories seen by a given user. We tested the platform-wide update frequency by continually checking for new content and measuring how often content changed. In this experiment, we set Apple News to not use any personalization data from other apps by turning off the ``Find Content in Other Apps'' toggle in the settings. We also removed files from the app's cache folder before every refresh. These measures were intended to minimize any potential content adaptation while we investigated the platform-wide update frequency of the Trending Stories section. However, the measures took time to perform, as did the process of opening the app, locating the buttons, and pressing the buttons via Appium. Thus, the maximum frequency with which we could record Trending Stories this way was approximately every 2 minutes.

\subsubsection{Experiment 1b: Measuring user-specific update frequency.}\label{user-specific}
Our initial experiment to test for user-specific update frequency happened by accident when we collected data for 12 continuous hours without closing the app and observed no changes in the Trending Stories. This led us to ask: Under what conditions does Apple News allow new Trending Stories from the ``master list'' to populate a user's app? In other words, what might be the user-specific update frequency? To answer this, we performed the same process as in experiment 1a, continuously collecting data from the app, but \textit{without deleting the cache and user identification files}. This allowed us to determine whether Apple News updates Trending Stories on a different schedule for individual users compared to the ``master list.''


\subsubsection{Experiment 2: Testing for location-based adaptation.}
While Apple News is known to adapt stories at a national level \citep{Brown2018}, we wondered whether stories changed with respect to a more fine-grain measure of location such as city or state. To test for fine-grain localization, we designed an experiment to elicit differences in content that could be accounted for by differences in a device's location.

In this experiment, we gathered stories from different simulated locations via Appium. Because it was not possible to run 50 simulators in parallel, we designed a way to sequentially collect and analyze data for evidence of localization, while still controlling for differences owed to time. We ran two iPhone simulators in parallel: one control, and one experimental. The control simulator was virtually located at Apple's headquarters and collected a set of control headlines, while the experimental simulator was virtually moved around the country to the 50 state capitals to collect local headlines in each place. Both simulators collected all Trending Stories shown in the app. We defined location-based adaptation (i.e. localization) as local headlines differing from  control headlines at a single point in time. Algorithm \ref{localization_algorithm} details the experiment.

\begin{algorithm}
\caption{Check for localization via sock-puppeting}\label{localization_algorithm}
\begin{algorithmic} 
\REQUIRE{set of locations $L$, $control\_location$}
\STATE set control simulator location to $control\_location$
\FOR{each $location$ in $L$}
\STATE set experimental simulator location to $location$
\STATE collect $local\_headlines$ and $control\_headlines$
\IF{$control\_headlines$ != $local\_headlines$}
\RETURN True \COMMENT{control headlines differed from local headlines, thus localization}
\ENDIF
\ENDFOR
\RETURN False \COMMENT{No localization observed}
\end{algorithmic}
\end{algorithm}

\subsubsection{Experiment 3: Measuring user-based adaptation.}
Location is one variable that might be used to adapt content across users. However, in Apple News, individual users can ``follow'' specific topics and publishers. This feature is used to populate the app with personalized content outside of the Trending Stories and Top Stories sections, but it is unclear whether it may also be used to adapt stories in the Trending Stories section. Because of the social and political implications of personalizing content, we wanted to test whether any personalization was applied to the algorithmically-curated Trending Stories section -- we do not examine Top Stories for personalization because Apple's editorial team is known to ``select five stories to lead the app'' in the United States \citep{Nicas2018}. In the personalization experiment, we used AMT to collect synchronized user screenshots, and compared the list of headlines to a control list that we collected concurrently from the simulator (which contained no cache files or user profile data).

Meaningful quantitative comparison of ranked lists is nontrivial. As \cite{Webber2010} point out, ``testing for statistical significance becomes problematic,'' since ``any finding of difference'' disproves a null hypothesis that rankings are identical. Researchers have therefore proposed and used various metrics to quantify the \textit{degree} of similarity between ranked lists, depending on the characteristics of the list. For example, rank-biased overlap (RBO) is designed for lists in which (1) the length is indefinite and (2) items at the top of the list are more important than items in the tail \citep{Webber2010}, and can be useful, for example, to investigate the degree of personalization in search engines  \citep{Robertson2018a}. 

While the Top Stories and Trending Stories lists vary in length across devices, their lengths are not ``indefinite'' since there is a known maximum length. Further, the weighting schemes in metrics such as RBO do not conceptually fit our data. This is because, as previously mentioned, one subset of Trending Stories (\#1-2) is displayed on iOS home screens, one subset (\#1-4) is shown within the app on all devices, and the full set of Trending Stories (\#1-6) is shown within the app on devices with large screens. Thus, a headline moving \textit{between} these subsets (ex. from \#5 to \#4) has more significant implications because the change will more substantively affect visibility compared to a headline moving \textit{within} a subset (e.g., from \#4 to \#3).

We therefore define user-based adaptation as a unique \textit{set} of stories appearing to a given user, quantified by the overlap coefficient (Szymkiewicz-Simpson coefficient)  \citep{Vijaymeena2016} between a user's Trending Stories and the Trending Stories on a control device. The overlap coefficient is a slight modification of Jaccard index, which is also used by \citet{Hannak}, \citet{Kliman-Silver2015}, \citet{Vincent2019}, and other studies to measure personalization in web search results. It operationalizes the definition of user-based adaptation as the size of the intersection divided by the smallest size of the two sets, and ranges from 0 (no overlap between sets) to 1 (one set includes all items in the other set). One key benefit of the metric is that it conceptually accommodates data from devices with smaller screens that showed only four Trending Stories, since our control list was on a large device and showed six Trending Stories.


\subsection{Content Audit}

\subsubsection{Experiment 4: Extended data collection.}
Finally, we designed and performed an extended data collection using Appium based on findings from the first three experiments. Each data point included a timestamp and a list of shortened URLs to the news stories, gathered by a simulated user tapping ``Share'' then ``Copy'' on each headline. To analyze the stories, a Python script followed each shortened URL along redirects until it reached a final web address, where it parsed the web page for a story title. The domain of the final address identified the publisher of the story (e.g. \texttt{cnn.com}).

%The URLs include a 23-character hash value, preceded by  \texttt{https://apple.news/}.

We compared the Top Stories and Trending Stories according to the update schedule, source diversity, and headline content. To quantify source diversity, we relied on the Shannon diversity index \citep{Shannon1948}, a metric which accounts for both abundance and evenness of different categories in a sample and derives from the probability that two randomly-chosen items belong to the same category (in our case, the same news source). Normalizing this value (dividing it by the maximum possible Shannon diversity index for the population) provides the Shannon \textit{equitability} index, also known as Pielou's index \citep{Pielou1966}, a more interpretable value between 0 and 1 where 1 indicates that all sources appeared with the same relative frequency (e.g., 50\% of Trending Stories from Fox and 50\% of Trending Stories from CNN). To statistically compare the source diversity of the Trending Stories section with that of the Top Stories section, we use the Hutcheson t-test \citep{Hutcheson1970}, which was specifically designed to compare the Shannon diversity index between two samples (in our case, the Trending Stories and the Top Stories).

%We tally the total number of stories appearing from each individual source and calculate the mean, median, standard deviation, and relative distribution of stories across all sources. We also calculate the amount of time each story spends in a given section (which can further characterize source concentration), as well as the total time each source was featured, the overall average duration stories were featured, and the source-specific average duration.

To compare content between the two sections, we first create a corpus of all headlines from Top Stories, and another corpus of all headlines from Trending Stories. We then count bigrams and trigrams within each set of headlines. Before doing this, each headline is stripped of punctuation and stopwords, then tokenized into bigrams and trigrams. We calculate the log ratio \citep{Hardie2014} of each n--gram to determine which are most salient relative to the other section.

%first stripped of punctuation, then tokenized using the nltk python package \citep{Bird2016a}. Finally, the bigrams from each headline are tallied into an aggregate table, which we manually filter for repetitive phrases and stopwords such as ``the'', ``is,'' ``and,'' etc.
