\section{Discussion}\label{sec:discussion}
%<insert>
%In this section we discuss the helpfulness of a theoretical ``audit standard'' to study content curators and note how our framework might contribute to such a standard. We also note the various audit methods used in this study and clarify the strengths of the different methods. In regards to the audit results, we discuss possible motivations for the mechanism design 
In this section we first discuss the specific results found in our audit of Apple News and how our results paint a distinction between algorithmic logic and human editorial logic. We then consider the broader implications of our study in terms of the various audit methods used, clarifying their different strengths. And finally, we suggest that the audit framework we develop may contribute to informing a conceptual ``audit standard'' which helps make the questions and methods more consistent when approaching similar audits of other news curators. 


\subsection{Mechanisms behind Apple News}
The absence of localization and personalization in the sections we audited highlights a possible tension between equitable economic distribution and vulnerability to echo-chambers. By showing the same Top Stories and Trending Stories to every user, Apple has taken a measure that minimizes individual filter bubble effects. However, this design choice may also translate to a highly-skewed distribution of economic winnings for publishers, since the few sources that frequent the top slots reap disproportionate traffic and potential advertising revenue from the 85 million users. 

Certainly, a balance could mitigate filter bubble effects while creating more equitable monetization. For example, Google News provides all users with an algorithmically-selected five-story briefing, but the same two top stories are shown to all users \citep{Wang2018}. Apple might explore a similar hybrid approach, using their editorial team to choose a mix of regionally and nationally relevant news. It should also be noted that Apple News includes a ``For You" section based on ``topics \& channels you read," which may help balance these tensions. 


\subsection{Algorithmic vs. Editorial Logic}
Our results illustrate what \citet{Gillespie2014} refers to as a possible competition between ``editorial logic'' and ``algorithmic logic.'' While the algorithmic logic behind Trending Stories is intended to ``automate some proxy of human judgment or unearth patterns across collected social traces,'' the editorial logic behind Top Stories ``depends on the subjective choices of experts, themselves made and authorized through institutional processes of training and certification'' \citep{Gillespie2014}. These logics manifest in our audit results for both mechanism and  content: Trending Stories and Top Stories exhibit distinct update schedules, sources, and topics that reflect their respective logics.

According to interviews with Apple's staff by \citet{Nicas2018}, the editorial logic behind the Top Stories pushes new content ``depending on the news,'' and also ``prioritizes accuracy over speed.'' In contrast, the Trending Stories section continually churns out new content (over 50 stories per day on average, more than twice as many as the Top Stories section). The algorithmic logic continually updates `what's trending,' and does so at all hours of the day (see Figure \ref{fig-trending-stories-appearance-times}), whereas the editorial logic espoused by Apple's staff strives for ``subtly following the news cycle and what's important'' \citep{Nicas2018}.

The contrast in logics extends to source concentration and source diversity. The editorially-selected Top Stories section exhibited more diverse and more equitable source distribution than the algorithmically-selected Trending Stories, as detailed in Table \ref{summary-table} and visualized in Figures \ref{fig-trending_stories_distribution} and \ref{fig-top_stories_distribution}. During the two-month data collection, human editors chose from a slightly wider array of sources than the algorithm behind Trending Stories, and also distributed selections more equitably across those sources, although there was a core set of 40 sources that were selected by \textit{both} algorithm and editor. Apple's editors also appeared to seek out regional news sources when the topic called for it. For example, during our data collection, the team chose stories from The San Diego Union-Tribune, The Miami Herald, The Chicago Tribune, TwinCities.com, and The Baltimore Sun. While the Trending Stories algorithm surfaced some content from smaller sources (e.g., esimoney.com, a single-author website dedicated to money management), no sources were regionally-specific.

Finally, perhaps the most intuitive distinction between the editorial logic and the algorithmic logic exhibited in Apple News is differences in content. Trending Stories uniquely included ``soft news'' \citep{Reinemann2012} pieces about celebrities and entertainment (ex. Kate Middleton, Justin Bieber), while the Top Stories uniquely included ``hard news'' topics including international stories and news about political policy (ex. Brexit, Affordable Care Act). Our data corroborates initial reports that headlines in Trending Stories ``tend to focus on Mr. Trump or celebrities'' \citep{Nicas2018}.


\subsection{Broader Implications}

\subsubsection{Audit Methods}
To study Apple News we used scraping, sock-puppet, and crowdsourced auditing. While these techniques have been employed in previous audit studies, we next reflect on our experience of how we found each technique helpful for addressing different aspects of our audit framework, in hopes this can inform future audit studies. 

First, audit studies should consider crowdsourcing whenever real-world observation is important, or when seeking higher parallel throughput in the data collection process. By using the crowd, we were able to assess the degree of personalization Apple News performs in practice, rather than fully relying on simulated sock-puppet data. Also, since resource constraints limited us to run at most two simulators at a time, crowdsourcing allowed us to significantly increase the throughput of our data collection, as many crowd workers could take screenshots in parallel.

Sock-puppet auditing -- using computer programs to impersonate users \citep{Sandvig2014} -- is most helpful for isolating variables that might affect a given system. Sock puppets provide clear and precise information about the input to an algorithm in cases where the crowd falls short. For example, precise temporal synchronization was still challenging with crowdworkers, as 6.7\% of screenshots from the crowd showed times that did not match the requested time in minutes, and even screenshots at the correct hour and minute may have been unsynchronized if seconds were taken into account. On the other hand, when using sock puppets, we could perform time-locked data collection on multiple devices with same-second precision.
%fine-grained location reporting is invasive: in a pilot study we attempted collecting ZIP codes, which some crowd workers either forged or simply declined to report. However, when using sock puppets, we could precisely situate the device at a given latitude and longitude while guaranteeing the absence of other factors such as user-level blocked sources. 

Lastly, we found the scraping audit most helpful for extended data collection, and we suggest scraping whenever researchers seek continuous data or to monitor over time. While we initially deployed a crowdsourced method for the extended data collection \citep{Bandy}, we found that we could not rely on the crowd for consistent data over time. Namely, in the United States, it was difficult to collect screenshots between the hours of 1am and 5am. However, after observing no evidence of personalization or localization in our initial experiments, we needed data from just one user account. We could therefore scrape content from a single simulated device to run the extended data collection.

It should be noted that we categorize our Appium-based data collection as a scraping audit since it centers around ``repeated queries to a platform and observing the results'' \citep{Sandvig2014}, however, we used a simulated iPhone to impersonate a user, making it somewhat of a hybrid with sock-puppet auditing. The Apple News platform lacks a public or private endpoint from which to scrape stories, so our experiment required additional layers of operation -- every data point we collected required simulating a user opening the application, refreshing for new content, locating buttons, and pressing buttons, thus requiring more time to collect data compared to a traditional scrape.

\subsubsection{Audit Framework}
To guide this work, we developed a conceptual framework that articulates three common aspects of a curation system that an audit might address: mechanism, content, and consumption. We showed how each of these aspects can have consequences for individual users, publishing companies, and even political discourse. As news curation systems change, consequential aspects may also change and prompt an expanded or revised framework.

Still, future research might leverage our proposed framework for auditing other content curators, revising and elaborating it to suit the nuances of specific systems. We believe this framework helps advance towards a conceptual ``audit standard'' for curation systems, which might allow the research community to compare and contrast different curation platforms, as well as characterize the evolution of a single platform over time.


