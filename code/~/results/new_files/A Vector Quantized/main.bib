@INPROCEEDINGS{8461368,
  author={Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and Saurous, Rif A. and Agiomvrgiannakis, Yannis and Wu, Yonghui},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions}, 
  year={2018},
  volume={},
  number={},
  pages={4779-4783},
  doi={10.1109/ICASSP.2018.8461368}
}

@InProceedings{pmlr-v139-kim21f,
  title = 	 {Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech},
  author =       {Kim, Jaehyeon and Kong, Jungil and Son, Juhee},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {5530--5540},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/kim21f/kim21f.pdf},
  url = 	 {https://proceedings.mlr.press/v139/kim21f.html},
  abstract = 	 {Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth.}
}

@article{Li_Liu_Liu_Zhao_Liu_2019, title={Neural Speech Synthesis with Transformer Network}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4642}, DOI={10.1609/aaai.v33i01.33016706}, abstractNote={&lt;p&gt;Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2) are proposed and achieve state-of-theart performance, they still suffer from two problems: 1) low efficiency during training and inference; 2) hard to model long dependency using current recurrent neural networks (RNNs). Inspired by the success of Transformer network in neural machine translation (NMT), in this paper, we introduce and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mechanism in Tacotron2. With the help of multi-head self-attention, the hidden states in the encoder and decoder are constructed in parallel, which improves training efficiency. Meanwhile, any two inputs at different times are connected directly by a self-attention mechanism, which solves the long range dependency problem effectively. Using phoneme sequences as input, our Transformer TTS network generates mel spectrograms, followed by a WaveNet vocoder to output the final audio results. Experiments are conducted to test the efficiency and performance of our new network. For the efficiency, our Transformer TTS network can speed up the training about 4.25 times faster compared with Tacotron2. For the performance, rigorous human tests show that our proposed model achieves state-of-the-art performance (outperforms Tacotron2 with a gap of 0.048) and is very close to human quality (4.39 vs 4.44 in MOS).&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Li, Naihan and Liu, Shujie and Liu, Yanqing and Zhao, Sheng and Liu, Ming}, year={2019}, month={Jul.}, pages={6706-6713} }

@inproceedings{NEURIPS2020_5c3b99e8,
 author = {Kim, Jaehyeon and Kim, Sungwon and Kong, Jungil and Yoon, Sungroh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {8067--8077},
 publisher = {Curran Associates, Inc.},
 title = {Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search},
 url = {https://proceedings.neurips.cc/paper/2020/file/5c3b99e8f92532e5ad1556e53ceea00c-Paper.pdf},
 volume = {33},
 year = {2020}
}
@inproceedings{NEURIPS2019_f63f65b5,
 author = {Ren, Yi and Ruan, Yangjun and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {FastSpeech: Fast, Robust and Controllable Text to Speech},
 url = {https://proceedings.neurips.cc/paper/2019/file/f63f65b503e22cb970527f23c9ad7db1-Paper.pdf},
 volume = {32},
 year = {2019}
}



@misc{ljspeech17,
  author       = {Keith Ito and Linda Johnson},
  title        = {The {LJ Speech} Dataset},
  url = {https://keithito.com/LJ-Speech-Dataset/},
  year         = {2017}
}

@misc{VCTK,
  doi = {10.7488/ds/2645},
  url = {https://doi.org/10.7488/ds/2645},
  author = {Yamagishi, Junichi and Veaux, Christophe and MacDonald, Kirsten},
  keywords = {speech synthesis, HMM, Mathematical and Computer Sciences::Speech and Natural Language Processing},
  language = {en},
  title = {{CSTR VCTK Corpus}: English Multi-speaker Corpus for {CSTR} Voice Cloning Toolkit (version 0.92)},
  publisher = {University of Edinburgh. The Centre for Speech Technology Research (CSTR)},
  year = {2019}
}

@inproceedings{zen19_interspeech,
  author={Heiga Zen and Viet Dang and Rob Clark and Yu Zhang and Ron J. Weiss and Ye Jia and Zhifeng Chen and Yonghui Wu},
  title={{LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={1526--1530},
  doi={10.21437/Interspeech.2019-2441}
}
@inproceedings{GigaSpeech2021,
  title={GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio},
  booktitle={Proc. Interspeech 2021},
  year=2021,
  author={Guoguo Chen and Shuzhou Chai and Guanbo Wang and Jiayu Du and Wei-Qiang Zhang and Chao Weng and Dan Su and Daniel Povey and Jan Trmal and Junbo Zhang and Mingjie Jin and Sanjeev Khudanpur and Shinji Watanabe and Shuaijiang Zhao and Wei Zou and Xiangang Li and Xuchen Yao and Yongqing Wang and Yujun Wang and Zhao You and Zhiyong Yan}
}

@InProceedings{pmlr-v139-ramesh21a,
  title = 	 {Zero-Shot Text-to-Image Generation},
  author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8821--8831},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/ramesh21a.html},
  abstract = 	 {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.}
}

@InProceedings{Esser_2021_CVPR,
    author    = {Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
    title     = {Taming Transformers for High-Resolution Image Synthesis},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {12873-12883}
}

@inproceedings{NIPS2017_7a98af17,
 author = {van den Oord, Aaron and Vinyals, Oriol and kavukcuoglu, koray},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Discrete Representation Learning},
 url = {https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{DBLP:conf/iclr/BaevskiSA20,
  author    = {Alexei Baevski and
               Steffen Schneider and
               Michael Auli},
  title     = {vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=rylwJxrYDS},
  timestamp = {Thu, 24 Mar 2022 16:25:27 +0100},
  biburl    = {https://dblp.org/rec/conf/iclr/BaevskiSA20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{10.5555/3495724.3497152,
author = {Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
title = {HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1428},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@article{DBLP:journals/ijcv/WuH20,
  author    = {Yuxin Wu and
               Kaiming He},
  title     = {Group Normalization},
  journal   = {Int. J. Comput. Vis.},
  volume    = {128},
  number    = {3},
  pages     = {742--755},
  year      = {2020},
  url       = {https://doi.org/10.1007/s11263-019-01198-w},
  doi       = {10.1007/s11263-019-01198-w},
  timestamp = {Thu, 19 Mar 2020 10:24:13 +0100},
  biburl    = {https://dblp.org/rec/journals/ijcv/WuH20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
Holtzman2020The,
title={The Curious Case of Neural Text Degeneration},
author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rygGQyrFvH}
}
@inproceedings{fan-etal-2018-hierarchical,
    title = "Hierarchical Neural Story Generation",
    author = "Fan, Angela  and
      Lewis, Mike  and
      Dauphin, Yann",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1082",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
    abstract = "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
}

@article{Radford2019,
abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset-matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
file = {:Users/shanest/Documents/Library/Radford et al/Unknown/Radford et al. - 2019 - Language Models are Unsupervised Multitask Learners.pdf:pdf},
keywords = {model},
title = {{Language Models are Unsupervised Multitask Learners}},
url = {https://openai.com/blog/better-language-models/},
year = {2019}
}

@inproceedings{Wang2018StyleTU,
  title={Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis},
  author={Yuxuan Wang and Daisy Stanton and Yu Zhang and R. J. Skerry-Ryan and Eric Battenberg and Joel Shor and Ying Xiao and Fei Ren and Ye Jia and Rif A. Saurous},
  booktitle={ICML},
  year={2018}
}
@Article{Nagrani19,
              author       = "Arsha Nagrani and Joon~Son Chung and Weidi Xie and Andrew Zisserman",
              title        = "Voxceleb: Large-scale speaker verification in the wild",
              journal      = "Computer Science and Language",
              year         = "2019",
              publisher    = "Elsevier",
            }
@inproceedings{mcauliffe17_interspeech,
  author={Michael McAuliffe and Michaela Socolof and Sarah Mihuc and Michael Wagner and Morgan Sonderegger},
  title={{Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi}},
  year=2017,
  booktitle={Proc. Interspeech 2017},
  pages={498--502},
  doi={10.21437/Interspeech.2017-1386}
}
@inproceedings{DBLP:conf/nips/BaevskiZMA20,
  author    = {Alexei Baevski and
               Yuhao Zhou and
               Abdelrahman Mohamed and
               Michael Auli},
  editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
  title     = {wav2vec 2.0: {A} Framework for Self-Supervised Learning of Speech
               Representations},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 15:57:22 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/BaevskiZMA20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Hsu2021HuBERTSS,
  title={HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},
  author={Wei-Ning Hsu and Benjamin Bolte and Yao-Hung Hubert Tsai and Kushal Lakhotia and Ruslan Salakhutdinov and Abdelrahman Mohamed},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2021},
  volume={29},
  pages={3451-3460}
}
@article{DBLP:journals/corr/abs-2103-16710,
  author    = {Albert Zeyer and
               Ralf Schl{\"{u}}ter and
               Hermann Ney},
  title     = {A study of latent monotonic attention variants},
  journal   = {CoRR},
  volume    = {abs/2103.16710},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.16710},
  eprinttype = {arXiv},
  eprint    = {2103.16710},
  timestamp = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-16710.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{10.5555/3305890.3305974,
author = {Raffel, Colin and Luong, Minh-Thang and Liu, Peter J. and Weiss, Ron J. and Eck, Douglas},
title = {Online and Linear-Time Attention by Enforcing Monotonic Alignments},
year = {2017},
publisher = {JMLR.org},
abstract = {Recurrent neural network models with an attention mechanism have proven to be extremely effective on a wide variety of sequence-to-sequence problems. However, the fact that soft attention mechanisms perform a pass over the entire input sequence when producing each element in the output sequence precludes their use in online settings and results in a quadratic time complexity. Based on the insight that the alignment between input and output sequence elements is monotonic in many problems of interest, we propose an end-to-end differentiable method for learning monotonic alignments which, at test time, enables computing attention online and in linear time. We validate our approach on sentence summarization, machine translation, and online speech recognition problems and achieve results competitive with existing sequence-to-sequence models.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2837–2846},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}
@inproceedings{DBLP:conf/interspeech/HeDH19,
  author    = {Mutian He and
               Yan Deng and
               Lei He},
  editor    = {Gernot Kubin and
               Zdravko Kacic},
  title     = {Robust Sequence-to-Sequence Acoustic Modeling with Stepwise Monotonic
               Attention for Neural {TTS}},
  booktitle = {Interspeech 2019, 20th Annual Conference of the International Speech
               Communication Association, Graz, Austria, 15-19 September 2019},
  pages     = {1293--1297},
  publisher = {{ISCA}},
  year      = {2019},
  url       = {https://doi.org/10.21437/Interspeech.2019-1972},
  doi       = {10.21437/Interspeech.2019-1972},
  timestamp = {Mon, 17 May 2021 07:42:25 +0200},
  biburl    = {https://dblp.org/rec/conf/interspeech/HeDH19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{Boersma2009,
  added-at = {2009-10-10T00:08:43.000+0200},
  author = {Boersma, Paul and Weenink, David},
  biburl = {https://www.bibsonomy.org/bibtex/232367795348acf2f7ca08c2bccf47c28/lran022},
  interhash = {aa077a0d2d1779b7712a998e30e6f989},
  intrahash = {32367795348acf2f7ca08c2bccf47c28},
  keywords = {imported},
  timestamp = {2009-10-10T00:08:47.000+0200},
  title = {Praat: doing phonetics by computer (Version 5.1.13)},
  url = {http://www.praat.org},
  year = 2009
}
@ARTICLE{Scheerer2018-cb,
  title    = "The Role of Auditory Feedback at Vocalization Onset and
              {Mid-Utterance}",
  author   = "Scheerer, Nichole E and Jones, Jeffery A",
  abstract = "Auditory feedback plays an important role in monitoring and
              correcting for errors during speech production. Previous research
              suggests that at vocalization onset, auditory feedback is
              compared to a sensory prediction generated by the motor system to
              ensure the desired fundamental frequency (F0) is produced. After
              vocalization onset, auditory feedback is compared to the most
              recently perceived F0 in order to stabilize the vocalization.
              This study aimed to further investigate whether after
              vocalization onset, auditory feedback is used strictly to
              stabilize speakers' F0, or if it is also influenced by the
              sensory prediction generated by the motor system. Event-related
              potentials (ERP) were recorded while participants produced
              vocalizations and heard the F0 of their auditory feedback
              perturbed suddenly mid-utterance by half a semitone. For half of
              the vocalizations, at vocalization onset, participants' F0 was
              also raised by half a semitone. Thus, half of the perturbations
              occurred while participants heard their unaltered auditory
              feedback, and the other half occurred in auditory feedback that
              had also been perturbed 50 cents at vocalization onset. If after
              vocalization onset auditory feedback is strictly used to
              stabilize speakers' F0, then similarly sized vocal and ERP
              responses would be expected across all trials, regardless of
              whether the perturbation occurred while listening to altered or
              unaltered auditory feedback. Results indicate that the
              perturbations to the participants' unaltered auditory feedback
              resulted in larger vocal and N1 and P2 ERP responses than
              perturbations to their altered auditory feedback. These results
              suggest that after vocalization onset auditory feedback is not
              strictly used to stabilize speakers' F0, but is also used to
              ensure the desired F0 is produced.",
  journal  = "Front Psychol",
  volume   =  9,
  pages    = "2019",
  month    =  oct,
  year     =  2018,
  keywords = "auditory feedback; event-related potential (ERP); fundamental
              frequency; speech motor control; vocal pitch",
  language = "en"
}
@MISC{Yates1963-hg,
  title     = "Delayed auditory feedback",
  author    = "Yates, Aubrey J",
  abstract  = "When S hears his own voice with a small time delay his speech
               may be seriously affected. The effects produced by delayed
               auditory feedback (DAF) include prolongation of vowels,
               repetition of consonants, increased intensity of utterance, and
               other articulatory changes. The significance of individual
               differences in susceptibility to DAF is considered in relation
               to personality and physiological characteristics. The technique
               may prove useful in the detection of auditory malingering and
               has possible implications for the understanding of stammering.
               The discussion relates the findings to models of speech control.
               Methodological problems and future research needs are outlined.
               (74 ref.) (PsycINFO Database Record (c) 2016 APA, all rights
               reserved)",
  journal   = "Psychological Bulletin",
  publisher = "American Psychological Association",
  volume    =  60,
  number    =  3,
  pages     = "213--232",
  year      =  1963,
  address   = "US",
  keywords  = "*Consonants; *Delayed Auditory Feedback; *Individual
               Differences; Vowels"
}
@inproceedings{kim08e_interspeech,
  author={Chanwoo Kim and Richard M. Stern},
  title={{Robust signal-to-noise ratio estimation based on waveform amplitude distribution analysis}},
  year=2008,
  booktitle={Proc. Interspeech 2008},
  pages={2598--2601},
  doi={10.21437/Interspeech.2008-644}
}
@inproceedings{DBLP:conf/icassp/BattenbergSMSKS20,
  author    = {Eric Battenberg and
               R. J. Skerry{-}Ryan and
               Soroosh Mariooryad and
               Daisy Stanton and
               David Kao and
               Matt Shannon and
               Tom Bagby},
  title     = {Location-Relative Attention Mechanisms for Robust Long-Form Speech
               Synthesis},
  booktitle = {2020 {IEEE} International Conference on Acoustics, Speech and Signal
               Processing, {ICASSP} 2020, Barcelona, Spain, May 4-8, 2020},
  pages     = {6194--6198},
  publisher = {{IEEE}},
  year      = {2020},
  url       = {https://doi.org/10.1109/ICASSP40776.2020.9054106},
  doi       = {10.1109/ICASSP40776.2020.9054106},
  timestamp = {Thu, 23 Jul 2020 16:20:10 +0200},
  biburl    = {https://dblp.org/rec/conf/icassp/BattenbergSMSKS20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{watanabe2018espnet,
  author={Shinji Watanabe and Takaaki Hori and Shigeki Karita and Tomoki Hayashi and Jiro Nishitoba and Yuya Unno and Nelson {Enrique Yalta Soplin} and Jahn Heymann and Matthew Wiesner and Nanxin Chen and Adithya Renduchintala and Tsubasa Ochiai},
  title={{ESPnet}: End-to-End Speech Processing Toolkit},
  year={2018},
  booktitle={Proceedings of Interspeech},
  pages={2207--2211},
  doi={10.21437/Interspeech.2018-1456},
  url={http://dx.doi.org/10.21437/Interspeech.2018-1456}
}
@inproceedings{kingma2014adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
}
@INPROCEEDINGS{9445238,
  author={Hassan, Farman and Javed, Ali},
  booktitle={2021 International Conference on Artificial Intelligence (ICAI)}, 
  title={Voice Spoofing Countermeasure for Synthetic Speech Detection}, 
  year={2021},
  volume={},
  number={},
  pages={209-212},
  doi={10.1109/ICAI52203.2021.9445238}
}
@inproceedings{DBLP:conf/icassp/Tak0TNEL21,
  author    = {Hemlata Tak and
               Jose Patino and
               Massimiliano Todisco and
               Andreas Nautsch and
               Nicholas W. D. Evans and
               Anthony Larcher},
  title     = {End-to-End anti-spoofing with RawNet2},
  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
               {ICASSP} 2021, Toronto, ON, Canada, June 6-11, 2021},
  pages     = {6369--6373},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/ICASSP39728.2021.9414234},
  doi       = {10.1109/ICASSP39728.2021.9414234},
  timestamp = {Fri, 09 Jul 2021 13:04:25 +0200},
  biburl    = {https://dblp.org/rec/conf/icassp/Tak0TNEL21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{
alibi,
title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
author={Ofir Press and Noah Smith and Mike Lewis},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=R8sQPpGCv0}
}
@article{DBLP:journals/corr/abs-2005-05525,
  author    = {Tomoki Hayashi and
               Shinji Watanabe},
  title     = {DiscreTalk: Text-to-Speech as a Machine Translation Problem},
  journal   = {CoRR},
  volume    = {abs/2005.05525},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.05525},
  eprinttype = {arXiv},
  eprint    = {2005.05525},
  timestamp = {Tue, 30 Jun 2020 10:16:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-05525.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{10.1162/tacl_a_00430,
    author = {Lakhotia, Kushal and Kharitonov, Eugene and Hsu, Wei-Ning and Adi, Yossi and Polyak, Adam and Bolte, Benjamin and Nguyen, Tu-Anh and Copet, Jade and Baevski, Alexei and Mohamed, Abdelrahman and Dupoux, Emmanuel},
    title = "{On Generative Spoken Language Modeling from Raw Audio}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {1336-1354},
    year = {2021},
    month = {12},
    abstract = "{We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo- text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder- dependent way, and that some combinations approach text-based systems.1}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00430},
    url = {https://doi.org/10.1162/tacl\_a\_00430},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00430/1976784/tacl\_a\_00430.pdf},
}
@article{DBLP:journals/corr/abs-2010-04301,
  author    = {Jonathan Shen and
               Ye Jia and
               Mike Chrzanowski and
               Yu Zhang and
               Isaac Elias and
               Heiga Zen and
               Yonghui Wu},
  title     = {Non-Attentive Tacotron: Robust and Controllable Neural {TTS} Synthesis
               Including Unsupervised Duration Modeling},
  journal   = {CoRR},
  volume    = {abs/2010.04301},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.04301},
  eprinttype = {arXiv},
  eprint    = {2010.04301},
  timestamp = {Tue, 13 Oct 2020 15:25:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-04301.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2107-02530,
  author    = {Yuzi Yan and
               Xu Tan and
               Bohan Li and
               Guangyan Zhang and
               Tao Qin and
               Sheng Zhao and
               Yuan Shen and
               Wei{-}Qiang Zhang and
               Tie{-}Yan Liu},
  title     = {AdaSpeech 3: Adaptive Text to Speech for Spontaneous Style},
  journal   = {CoRR},
  volume    = {abs/2107.02530},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.02530},
  eprinttype = {arXiv},
  eprint    = {2107.02530},
  timestamp = {Thu, 08 Jul 2021 14:04:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-02530.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/icassp/Zhang00LZQZL21,
  author    = {Chen Zhang and
               Yi Ren and
               Xu Tan and
               Jinglin Liu and
               Kejun Zhang and
               Tao Qin and
               Sheng Zhao and
               Tie{-}Yan Liu},
  title     = {Denoispeech: Denoising Text to Speech with Frame-Level Noise Modeling},
  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
               {ICASSP} 2021, Toronto, ON, Canada, June 6-11, 2021},
  pages     = {7063--7067},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/ICASSP39728.2021.9413934},
  doi       = {10.1109/ICASSP39728.2021.9413934},
  timestamp = {Sat, 09 Apr 2022 12:47:15 +0200},
  biburl    = {https://dblp.org/rec/conf/icassp/Zhang00LZQZL21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}
@Article{dtw,
    title = {Computing and Visualizing Dynamic Time Warping Alignments
      in {R}: The {dtw} Package},
    author = {Toni Giorgino},
    journal = {Journal of Statistical Software},
    year = {2009},
    volume = {31},
    number = {7},
    pages = {1--24},
    doi = {10.18637/jss.v031.i07},
}
@inproceedings{NIPS2017_8a1d6947,
 author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
 url = {https://proceedings.neurips.cc/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf},
 volume = {30},
 year = {2017}
}
@article{Lotfian_2019_3,
	author = {R. Lotfian and C. Busso},
	title = {Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech From Existing Podcast Recordings},
	journal = {IEEE Transactions on Affective Computing},
	volume = {10},
	number = {4},
	year = {2019},
	pages = {471-483},
	month = {October-December},
	doi={10.1109/TAFFC.2017.2736999},
}
@misc{https://doi.org/10.48550/arxiv.2203.07378,
  doi = {10.48550/ARXIV.2203.07378},
  
  url = {https://arxiv.org/abs/2203.07378},
  
  author = {Wagner, Johannes and Triantafyllopoulos, Andreas and Wierstorf, Hagen and Schmitt, Maximilian and Burkhardt, Felix and Eyben, Florian and Schuller, Björn W.},
  
  keywords = {Audio and Speech Processing (eess.AS), Machine Learning (cs.LG), Sound (cs.SD), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Dawn of the transformer era in speech emotion recognition: closing the valence gap},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}
@article{DBLP:journals/corr/abs-2110-07840,
  author    = {Tomoki Hayashi and
               Ryuichi Yamamoto and
               Takenori Yoshimura and
               Peter Wu and
               Jiatong Shi and
               Takaaki Saeki and
               Yooncheol Ju and
               Yusuke Yasuda and
               Shinnosuke Takamichi and
               Shinji Watanabe},
  title     = {ESPnet2-TTS: Extending the Edge of {TTS} Research},
  journal   = {CoRR},
  volume    = {abs/2110.07840},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.07840},
  eprinttype = {arXiv},
  eprint    = {2110.07840},
  timestamp = {Fri, 22 Oct 2021 13:33:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-07840.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/icassp/HayashiYIY0TTZT20,
  author    = {Tomoki Hayashi and
               Ryuichi Yamamoto and
               Katsuki Inoue and
               Takenori Yoshimura and
               Shinji Watanabe and
               Tomoki Toda and
               Kazuya Takeda and
               Yu Zhang and
               Xu Tan},
  title     = {Espnet-TTS: Unified, Reproducible, and Integratable Open Source End-to-End
               Text-to-Speech Toolkit},
  booktitle = {2020 {IEEE} International Conference on Acoustics, Speech and Signal
               Processing, {ICASSP} 2020, Barcelona, Spain, May 4-8, 2020},
  pages     = {7654--7658},
  publisher = {{IEEE}},
  year      = {2020},
  url       = {https://doi.org/10.1109/ICASSP40776.2020.9053512},
  doi       = {10.1109/ICASSP40776.2020.9053512},
  timestamp = {Thu, 11 Aug 2022 12:41:30 +0200},
  biburl    = {https://dblp.org/rec/conf/icassp/HayashiYIY0TTZT20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/iclr/ChiuR18,
  author    = {Chung{-}Cheng Chiu and
               Colin Raffel},
  title     = {Monotonic Chunkwise Attention},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=Hko85plCW},
  timestamp = {Thu, 25 Jul 2019 14:26:02 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ChiuR18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2005-00205,
  author    = {Baiji Liu and
               Songjun Cao and
               Sining Sun and
               Weibin Zhang and
               Long Ma},
  title     = {Multi-head Monotonic Chunkwise Attention For Online Speech Recognition},
  journal   = {CoRR},
  volume    = {abs/2005.00205},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.00205},
  eprinttype = {arXiv},
  eprint    = {2005.00205},
  timestamp = {Fri, 08 May 2020 15:04:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-00205.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{du22b_interspeech,
  author={Chenpeng Du and Yiwei Guo and Xie Chen and Kai Yu},
  title={{VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={1596--1600},
  doi={10.21437/Interspeech.2022-489}
}