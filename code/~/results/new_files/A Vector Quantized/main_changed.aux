\relax 
\bibstyle{aaai23}
\citation{Li_Liu_Liu_Zhao_Liu_2019,NEURIPS2020_5c3b99e8,NEURIPS2019_f63f65b5}
\citation{ljspeech17,VCTK}
\citation{NEURIPS2020_1457c0d6}
\citation{8461368,Li_Liu_Liu_Zhao_Liu_2019}
\citation{8461368}
\citation{DBLP:conf/interspeech/HeDH19,DBLP:journals/corr/abs-2103-16710}
\citation{DBLP:conf/iclr/ChiuR18}
\citation{DBLP:journals/corr/abs-2005-00205}
\citation{DBLP:journals/corr/abs-2010-04301}
\newlabel{sec:intro}{{1}{1}{}{}{}}
\citation{pmlr-v139-kim21f,NEURIPS2020_5c3b99e8,NEURIPS2019_f63f65b5}
\citation{pmlr-v139-kim21f}
\citation{VCTK}
\citation{pmlr-v139-ramesh21a,Esser_2021_CVPR}
\citation{DBLP:journals/corr/abs-2005-05525}
\citation{10.1162/tacl_a_00430}
\citation{du22b_interspeech}
\citation{DBLP:conf/icassp/Zhang00LZQZL21}
\citation{DBLP:journals/corr/abs-2107-02530}
\citation{10.5555/3495724.3497152}
\citation{DBLP:journals/ijcv/WuH20}
\citation{DBLP:conf/iclr/BaevskiSA20}
\newlabel{sec:model}{{3}{2}{}{}{}}
\newlabel{ssec:quant}{{3.1}{2}{}{}{}}
\citation{alibi}
\citation{Holtzman2020The,fan-etal-2018-hierarchical}
\citation{Holtzman2020The}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:VQmodule}{{1}{3}{Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks $G_i$.}{}{}}
\newlabel{eq:l_f}{{3}{3}{}{}{}}
\newlabel{eq:l_vq}{{4}{3}{}{}{}}
\newlabel{fig:tts}{{2}{3}{Detailed view of the multi-output transformer during training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding.}{}{}}
\citation{Holtzman2020The}
\citation{GigaSpeech2021}
\citation{Nagrani19}
\citation{kingma2014adam}
\citation{Li_Liu_Liu_Zhao_Liu_2019}
\citation{8461368}
\citation{Wang2018StyleTU}
\citation{DBLP:journals/corr/abs-2010-04301}
\newlabel{ssec:inf}{{3.3}{4}{}{}{}}
\newlabel{sec:exp-stepup}{{4}{4}{}{}{}}
\newlabel{ssec:data}{{4.1}{4}{}{}{}}
\citation{DBLP:conf/icassp/HayashiYIY0TTZT20}
\citation{watanabe2018espnet}
\citation{NIPS2017_8a1d6947}
\citation{https://doi.org/10.48550/arxiv.2203.07378}
\citation{Lotfian_2019_3}
\citation{DBLP:journals/corr/abs-2110-07840}
\citation{dtw}
\citation{ljspeech17}
\newlabel{tab:vocoder:Giga}{{1}{5}{Comparison of quantizer and vocoder reconstruction quality on VoxCeleb test set. HF-GAN is HiFi-GAN.}{}{}}
\newlabel{sec:res}{{5}{5}{}{}{}}
\newlabel{ssec:quantana}{{5.1}{5}{}{}{}}
\newlabel{ssec:performance}{{5.2}{5}{}{}{}}
\newlabel{fig:alignment}{{3}{6}{Comparison of inference encoder-decoder alignment of different models. For Transformer TTS we picked one of the cross-attentions which learn alignment. See Appendix C for the cross-attentions of all heads and layers.}{}{}}
\newlabel{tab:architecture:Giga}{{2}{6}{Comparison of TTS models. MOS is with 95\% confidence interval. MCD is with one standard deviation.}{}{}}
\citation{kim08e_interspeech}
\bibdata{main}
\newlabel{fig:pitch_variation_vits}{{4a}{7}{}{}{}}
\newlabel{sub@fig:pitch_variation_vits}{{(a)}{a}}
\newlabel{fig:pitch_variation_vqtts}{{4b}{7}{}{}{}}
\newlabel{sub@fig:pitch_variation_vqtts}{{(b)}{b}}
\newlabel{fig:pitch_variation}{{4}{7}{Pitch contour for the utterance: ``How much variation is there?'' from two models within the same speaker.}{}{}}
\newlabel{fig:snr}{{5}{7}{Comparison of SNR with different level of noise as audio prompt. SNR is calculated with 1472 syntheses.}{}{}}
\newlabel{sec:conclusion}{{6}{7}{}{}{}}
\gdef \@abspage@last{8}
