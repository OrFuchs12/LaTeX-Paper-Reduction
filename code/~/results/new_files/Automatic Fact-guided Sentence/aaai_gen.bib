@article{bleu,
 author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
 title = {BLEU: A Method for Automatic Evaluation of Machine Translation},
 journal = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
 series = {ACL '02},
 year = {2002},
 location = {Philadelphia, Pennsylvania},
 pages = {311--318},
 numpages = {8},
 url = {https://doi.org/10.3115/1073083.1073135},
 doi = {10.3115/1073083.1073135},
 acmid = {1073135},
 publisher = {ACL},
 address = {Stroudsburg, PA, USA},
} 

@article{kim2018bilinear,
  title={Bilinear attention networks},
  author={Kim, Jin-Hwa and Jun, Jaehyun and Zhang, Byoung-Tak},
  journal={NIPS},
  pages={1564--1574},
  year={2018},
  url={https://arxiv.org/abs/1611.01144}
}

@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}


@article{xu-etal-2016-optimizing,
    title = "Optimizing Statistical Machine Translation for Text Simplification",
    author = "Xu, Wei  and
      Napoles, Courtney  and
      Pavlick, Ellie  and
      Chen, Quanze  and
      Callison-Burch, Chris",
    journal = "TACL",
    volume = "4",
    month = dec,
    year = "2016",
    url = "https://www.aclweb.org/anthology/Q16-1029",
    doi = "10.1162/tacl_a_00107",
    pages = "401--415",
    abstract = "Most recent sentence simplification systems use basic machine translation models to learn lexical and syntactic paraphrases from a manually simplified parallel corpus. These methods are limited by the quality and quantity of manually simplified corpora, which are expensive to build. In this paper, we conduct an in-depth adaptation of statistical machine translation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task.",
}

@article{li-etal-2018-delete,
    title = "Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer",
    author = "Li, Juncen  and
      Jia, Robin  and
      He, He  and
      Liang, Percy",
    journal = {NAACL HLT},
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1169",
    doi = "10.18653/v1/N18-1169",
    pages = "1865--1874",
}

@article{hu2017toward,
  title={Toward controlled generation of text},
  author={Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P},
  journal={ICML},
  pages={1587--1596},
  year={2017},
  organization={JMLR. org}
}

@article{perez2017effectiveness,
  title={The effectiveness of data augmentation in image classification using deep learning},
  author={Perez, Luis and Wang, Jason},
  journal={arXiv preprint arXiv:1712.04621},
  year={2017}
}

@article{zhang2019paws,
  title={PAWS: Paraphrase Adversaries from Word Scrambling},
  author={Zhang, Yuan and Baldridge, Jason and He, Luheng},
  journal={arXiv preprint arXiv:1904.01130},
  year={2019}
}

@article{lei-rational,
    title = "Rationalizing Neural Predictions",
    author = "Lei, Tao  and
      Barzilay, Regina  and
      Jaakkola, Tommi",
    journal = "EMNLP",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1011",
    doi = "10.18653/v1/D16-1011",
    pages = "107--117",
}
@inproceedings{faruqui-etal-2018-wikiatomicedits,
    title = "{W}iki{A}tomic{E}dits: A Multilingual Corpus of {W}ikipedia Edits for Modeling Language and Discourse",
    author = "Faruqui, Manaal  and
      Pavlick, Ellie  and
      Tenney, Ian  and
      Das, Dipanjan",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1028",
    doi = "10.18653/v1/D18-1028",
    pages = "305--315",
    abstract = "We release a corpus of 43 million atomic edits across 8 languages. These edits are mined from Wikipedia edit history and consist of instances in which a human editor has inserted a single contiguous phrase into, or deleted a single contiguous phrase from, an existing sentence. We use the collected data to show that the language generated during editing differs from the language that we observe in standard corpora, and that models trained on edits encode different aspects of semantics and discourse than models trained on raw text. We release the full corpus as a resource to aid ongoing research in semantics, discourse, and representation learning.",
}

@article{pointer-generator,
    title = "Get To The Point: Summarization with Pointer-Generator Networks",
    author = "See, Abigail  and
      Liu, Peter J.  and
      Manning, Christopher D.",
    journal = "ACL",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1099",
    doi = "10.18653/v1/P17-1099",
    pages = "1073--1083",
    abstract = "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
}

@article{fever,
    title = "{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification",
    author = "Thorne, James  and
      Vlachos, Andreas  and
      Christodoulopoulos, Christos  and
      Mittal, Arpit",
    journal = {NAACL},
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1074",
    doi = "10.18653/v1/N18-1074",
    pages = "809--819",
    abstract = "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\%}, while if we ignore the evidence we achieve 50.91{\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.",
}

@article{fakenews-review,
  title={Fake News: A Survey of Research, Detection Methods, and Opportunities},
  url = "https://arxiv.org/abs/1812.00315",
  author={Zhou, Xinyi and Zafarani, Reza},
  journal={arXiv preprint arXiv:1812.00315},
  year={2018}
}

@article{vlachos-riedel-2014-fact,
    title = "Fact Checking: Task definition and dataset construction",
    author = "Vlachos, Andreas  and
      Riedel, Sebastian",
    journal = "ACL Workshop on Language Technologies and Computational Social Science",
    month = jun,
    year = "2014",
    address = "Baltimore, MD, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-2508",
    doi = "10.3115/v1/W14-2508",
    pages = "18--22",
}

@article{wang-2017-liar,
    title = "{``}Liar, Liar Pants on Fire{''}: A New Benchmark Dataset for Fake News Detection",
    author = "Wang, William Yang",
    journal = "ACL",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-2067",
    doi = "10.18653/v1/P17-2067",
    pages = "422--426",
    abstract = "Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present LIAR: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model.",
}

@article{rashkin-etal-2017-truth,
    title = "Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking",
    author = "Rashkin, Hannah  and
      Choi, Eunsol  and
      Jang, Jin Yea  and
      Volkova, Svitlana  and
      Choi, Yejin",
    journal = "EMNLP",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1317",
    doi = "10.18653/v1/D17-1317",
    pages = "2931--2937",
    abstract = "We present an analytic study on the language of news media in the context of political fact-checking and fake news detection. We compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. To probe the feasibility of automatic political fact-checking, we also present a case study based on PolitiFact.com using their factuality judgments on a 6-point scale. Experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.",
}

@article{ferreira-vlachos-2016-emergent,
    title = "{E}mergent: a novel data-set for stance classification",
    author = "Ferreira, William  and
      Vlachos, Andreas",
    journal = {NAACL},
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N16-1138",
    doi = "10.18653/v1/N16-1138",
    pages = "1163--1168",
}

@article{nsmns,
  title={Combining Fact Extraction and Verification with Neural Semantic Matching Networks},
  author={Nie, Yixin and Chen, Haonan and Bansal, Mohit},
  journal={arXiv preprint arXiv:1811.07039},
  url={https://arxiv.org/abs/1811.07039},
  year={2018}
}

@article{poliak2018hypothesis,
  title={Hypothesis Only Baselines in Natural Language Inference},
  author={Poliak, Adam and Naradowsky, Jason and Haldar, Aparajita and Rudinger, Rachel and Van Durme, Benjamin},
  journal={NAACL HLT},
  url={http://www.aclweb.org/anthology/S18-2#page=198},
  pages={180},
  year={2018}
}

@article{gururangan2018annotation,
  author = 	"Gururangan, Suchin
		and Swayamdipta, Swabha
		and Levy, Omer
		and Schwartz, Roy
		and Bowman, Samuel
		and Smith, Noah A.",
  title = 	"Annotation Artifacts in Natural Language Inference Data",
  journal={NAACL HLT},
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"107--112",
  location = 	"New Orleans, Louisiana",
  doi = 	"10.18653/v1/N18-2017",
  url = 	"http://aclweb.org/anthology/N18-2017"
}

@incollection{NIPS2018_7959,
title = {Unsupervised Text Style Transfer using Language Models as Discriminators},
author = {Yang, Zichao and Hu, Zhiting and Dyer, Chris and Xing, Eric P and Berg-Kirkpatrick, Taylor},
journal = {NIPS},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {7287--7298},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7959-unsupervised-text-style-transfer-using-language-models-as-discriminators.pdf}
}

@incollection{NIPS2018_7757,
title = {Content preserving text generation with attribute controls},
author = {Logeswaran, Lajanugen and Lee, Honglak and Bengio, Samy},
journal = {NIPS},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {5103--5113},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7757-content-preserving-text-generation-with-attribute-controls.pdf}
}

@article{zhang2018style,
  title={Style Transfer as Unsupervised Machine Translation},
  author={Zhang, Zhirui and Ren, Shuo and Liu, Shujie and Wang, Jianyong and Chen, Peng and Li, Mu and Zhou, Ming and Chen, Enhong},
  journal={arXiv preprint arXiv:1808.07894},
  year={2018}
}

@article{chen-etal-2018-learning,
    title = "Learning to Flip the Bias of News Headlines",
    author = "Chen, Wei-Fan  and
      Wachsmuth, Henning  and
      Al Khatib, Khalid  and
      Stein, Benno",
    journal = "ICNLG",
    month = nov,
    year = "2018",
    address = "Tilburg University, The Netherlands",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6509",
    pages = "79--88",
    abstract = "This paper introduces the task of {``}flipping{''} the bias of news articles: Given an article with a political bias (left or right), generate an article with the same topic but opposite bias. To study this task, we create a corpus with bias-labeled articles from allsides.com. As a first step, we analyze the corpus and discuss intrinsic characteristics of bias. They point to the main challenges of bias flipping, which in turn lead to a specific setting in the generation process. The paper in hand scales down the general bias flipping task to focus on bias flipping for news article headlines. A manual annotation of headlines from each side reveals that headlines are self-informative in general and often convey bias. We apply an autoencoder incorporating information from an article{'}s content to learn how to automatically flip the bias. From 200 generated headlines, 73 are classified as understandable by annotators, and 83 maintain the topic while having opposite bias. Insights from our analysis shed light on how to solve the main challenges of bias flipping.",
}

@article{shen2017style,
  title={Style transfer from non-parallel text by cross-alignment},
  author={Shen, Tianxiao and Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
  journal = {NIPS},
  pages={6830--6841},
  year={2017}
}

@article{geva2019discofuse,
    title = "{D}isco{F}use: A Large-Scale Dataset for Discourse-Based Sentence Fusion",
    author = "Geva, Mor  and
      Malmi, Eric  and
      Szpektor, Idan  and
      Berant, Jonathan",
    journal = "NAACL HLT",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1348",
    doi = "10.18653/v1/N19-1348",
    pages = "3443--3455",
    abstract = "Sentence fusion is the task of joining several independent sentences into a single coherent text. Current datasets for sentence fusion are small and insufficient for training modern neural models. In this paper, we propose a method for automatically-generating fusion examples from raw text and present DiscoFuse, a large scale dataset for discourse-based sentence fusion. We author a set of rules for identifying a diverse set of discourse phenomena in raw text, and decomposing the text into two independent sentences. We apply our approach on two document collections: Wikipedia and Sports articles, yielding 60 million fusion examples annotated with discourse information required to reconstruct the fused text. We develop a sequence-to-sequence model on DiscoFuse and thoroughly analyze its strengths and weaknesses with respect to the various discourse phenomena, using both automatic as well as human evaluation. Finally, we conduct transfer learning experiments with WebSplit, a recent dataset for text simplification. We show that pretraining on DiscoFuse substantially improves performance on WebSplit when viewed as a sentence fusion task.",
}
@article{barzilay2005sentence,
  title={Sentence fusion for multidocument news summarization},
  author={Barzilay, Regina and McKeown, Kathleen R},
  journal={Computational Linguistics},
  volume={31},
  number={3},
  pages={297--328},
  year={2005},
  publisher={MIT Press}
}

@article{botha-etal-2018-learning,
    title = "Learning To Split and Rephrase From {W}ikipedia Edit History",
    author = "Botha, Jan A.  and
      Faruqui, Manaal  and
      Alex, John  and
      Baldridge, Jason  and
      Das, Dipanjan",
    journal = {EMNLP},
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1080",
    pages = "732--737",
    abstract = "Split and rephrase is the task of breaking down a sentence into shorter ones that together convey the same meaning. We extract a rich new dataset for this task by mining Wikipedia{'}s edit history: WikiSplit contains one million naturally occurring sentence rewrites, providing sixty times more distinct split examples and a ninety times larger vocabulary than the WebSplit corpus introduced by Narayan et al. (2017) as a benchmark for this task. Incorporating WikiSplit as training data produces a model with qualitatively better predictions that score 32 BLEU points above the prior best result on the WebSplit benchmark.",
}

@article{narayan-etal-2017-split,
    title = "Split and Rephrase",
    author = "Narayan, Shashi  and
      Gardent, Claire  and
      Cohen, Shay B.  and
      Shimorina, Anastasia",
    journal = "EMNLP",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1064",
    doi = "10.18653/v1/D17-1064",
    pages = "606--616",
    abstract = "We propose a new sentence simplification task (Split-and-Rephrase) where the aim is to split a complex sentence into a meaning preserving sequence of shorter sentences. Like sentence simplification, splitting-and-rephrasing has the potential of benefiting both natural language processing and societal applications. Because shorter sentences are generally better processed by NLP systems, it could be used as a preprocessing step which facilitates and improves the performance of parsers, semantic role labellers and machine translation systems. It should also be of use for people with reading disabilities because it allows the conversion of longer sentences into shorter ones. This paper makes two contributions towards this new task. First, we create and make available a benchmark consisting of 1,066,115 tuples mapping a single complex sentence to a sequence of sentences expressing the same meaning. Second, we propose five models (vanilla sequence-to-sequence to semantically-motivated models) to understand the difficulty of the proposed task.",
}

@article{thorne-vlachos-2018-automated,
    title = "Automated Fact Checking: Task Formulations, Methods and Future Directions",
    author = "Thorne, James  and
      Vlachos, Andreas",
    journal = "ICCL",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/C18-1283",
    pages = "3346--3359",
    abstract = "The recently increased focus on misinformation has stimulated research in fact checking, the task of assessing the truthfulness of a claim. Research in automating this task has been conducted in a variety of disciplines including natural language processing, machine learning, knowledge representation, databases, and journalism. While there has been substantial progress, relevant papers and articles have been published in research communities that are often unaware of each other and use inconsistent terminology, thus impeding understanding and further progress. In this paper we survey automated fact checking research stemming from natural language processing and related disciplines, unifying the task formulations and methodologies across papers and authors. Furthermore, we highlight the use of evidence as an important distinguishing factor among them cutting across task formulations and methods. We conclude with proposing avenues for future NLP research on automated fact checking.",
}

@article{kobayashi2018contextual,
  title={Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations},
  author={Kobayashi, Sosuke},
  journal={NAACL HLT},
  pages={452--457},
  year={2018}
}

@article{wu2018conditional,
  title={Conditional BERT Contextual Augmentation},
  author={Wu, Xing and Lv, Shangwen and Zang, Liangjun and Han, Jizhong and Hu, Songlin},
  journal={arXiv preprint arXiv:1812.06705},
  year={2018}
}

@article{iyyer-etal-2018-adversarial,
    title = "Adversarial Example Generation with Syntactically Controlled Paraphrase Networks",
    author = "Iyyer, Mohit  and
      Wieting, John  and
      Gimpel, Kevin  and
      Zettlemoyer, Luke",
    journal = {NAACL HLT},
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1170",
    doi = "10.18653/v1/N18-1170",
    pages = "1875--1885",
    abstract = "We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoder-decoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) {``}fool{''} pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data.",
}

@article{Gardner2017AllenNLP,
    title = "{A}llen{NLP}: A Deep Semantic Natural Language Processing Platform",
    author = "Gardner, Matt  and
      Grus, Joel  and
      Neumann, Mark  and
      Tafjord, Oyvind  and
      Dasigi, Pradeep  and
      Liu, Nelson F.  and
      Peters, Matthew  and
      Schmitz, Michael  and
      Zettlemoyer, Luke",
    journal = "Workshop for {NLP}-{OSS}",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2501",
    doi = "10.18653/v1/W18-2501",
    pages = "1--6",
    abstract = "Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the research. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.",
}

@article{chen2017enhanced,
  title={Enhanced LSTM for Natural Language Inference},
  author={Chen, Qian and Zhu, Xiaodan and Ling, Zhen-Hua and Wei, Si and Jiang, Hui and Inkpen, Diana},
  journal = {ACL},
  pages={1657--1668},
  year={2017}
}

@article{pennington-etal-2014-glove,
    title = "{G}love: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    journal = {EMNLP},
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@article{sak2014long,
  title={Long short-term memory recurrent neural network architectures for large scale acoustic modeling},
  author={Sak, Ha{\c{s}}im and Senior, Andrew and Beaufays, Fran{\c{c}}oise},
  journal = {ISCA},
  year={2014}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{mccoy2019right,
  title={Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference},
  author={McCoy, R Thomas and Pavlick, Ellie and Linzen, Tal},
  journal={arXiv preprint arXiv:1902.01007},
  year={2019},
  url={https://arxiv.org/abs/1902.01007}
}

@article{bao-etal-2018-deriving,
    title = "Deriving Machine Attention from Human Rationales",
    author = "Bao, Yujia  and
      Chang, Shiyu  and
      Yu, Mo  and
      Barzilay, Regina",
    journal = "EMNLP",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1216",
    pages = "1903--1913",
    abstract = "Attention-based models are successful when trained on large amounts of data. In this paper, we demonstrate that even in the low-resource scenario, attention can be learned effectively. To this end, we start with discrete human-annotated rationales and map them into continuous attention. Our central hypothesis is that this mapping is general across domains, and thus can be transferred from resource-rich domains to low-resource ones. Our model jointly learns a domain-invariant representation and induces the desired mapping between rationales and attention. Our empirical results validate this hypothesis and show that our approach delivers significant gains over state-of-the-art baselines, yielding over 15{\%} average error reduction on benchmark datasets.",
}

@article{wieting-gimpel-2018-paranmt,
    title = "{P}ara{NMT}-50{M}: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations",
    author = "Wieting, John  and
      Gimpel, Kevin",
    journal = "ACL",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1042",
    pages = "451--462",
    abstract = "We describe ParaNMT-50M, a dataset of more than 50 million English-English sentential paraphrase pairs. We generated the pairs automatically by using neural machine translation to translate the non-English side of a large parallel corpus, following Wieting et al. (2017). Our hope is that ParaNMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use ParaNMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation.",
}

@article{stern-etal-2017-minimal,
    title = "A Minimal Span-Based Neural Constituency Parser",
    author = "Stern, Mitchell  and
      Andreas, Jacob  and
      Klein, Dan",
    journal = "ACL",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1076",
    doi = "10.18653/v1/P17-1076",
    pages = "818--827",
    abstract = "In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).",
}

@article{daxenberger-gurevych-2013-automatically,
    title = "Automatically Classifying Edit Categories in {W}ikipedia Revisions",
    author = "Daxenberger, Johannes  and
      Gurevych, Iryna",
    journal = "EMNLP",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1055",
    pages = "578--589",
}

@article{daxenberger-gurevych-2012-corpus,
    title = "A Corpus-Based Study of Edit Categories in Featured and Non-Featured {W}ikipedia Articles",
    author = "Daxenberger, Johannes  and
      Gurevych, Iryna",
    journal = "COLING",
    month = dec,
    year = "2012",
    address = "Mumbai, India",
    publisher = "The COLING 2012 Organizing Committee",
    url = "https://www.aclweb.org/anthology/C12-1044",
    pages = "711--726",
}

@article{yang-etal-2017-identifying-semantic,
    title = "Identifying Semantic Edit Intentions from Revisions in {W}ikipedia",
    author = "Yang, Diyi  and
      Halfaker, Aaron  and
      Kraut, Robert  and
      Hovy, Eduard",
    journal = "EMNLP",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1213",
    doi = "10.18653/v1/D17-1213",
    pages = "2000--2010",
    abstract = "Most studies on human editing focus merely on syntactic revision operations, failing to capture the intentions behind revision changes, which are essential for facilitating the single and collaborative writing process. In this work, we develop in collaboration with Wikipedia editors a 13-category taxonomy of the semantic intention behind edits in Wikipedia articles. Using labeled article edits, we build a computational classifier of intentions that achieved a micro-averaged F1 score of 0.621. We use this model to investigate edit intention effectiveness: how different types of edits predict the retention of newcomers and changes in the quality of articles, two key concerns for Wikipedia today. Our analysis shows that the types of edits that users make in their first session predict their subsequent survival as Wikipedia editors, and articles in different stages need different types of edits.",
}

@article{yamangil-nelken-2008-mining,
    title = "Mining {W}ikipedia Revision Histories for Improving Sentence Compression",
    author = "Yamangil, Elif  and
      Nelken, Rani",
    journal = "ACL",
    month = jun,
    year = "2008",
    address = "Columbus, Ohio",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P08-2035",
    pages = "137--140",
}

@article{yatskar-etal-2010-sake,
    title = "For the sake of simplicity: Unsupervised extraction of lexical simplifications from {W}ikipedia",
    author = "Yatskar, Mark  and
      Pang, Bo  and
      Danescu-Niculescu-Mizil, Cristian  and
      Lee, Lillian",
    journal = {NAACL},
    month = jun,
    year = "2010",
    address = "Los Angeles, California",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N10-1056",
    pages = "365--368",
}

@article{max-wisniewski-2010-mining,
    title = "Mining Naturally-occurring Corrections and Paraphrases from {W}ikipedia{'}s Revision History",
    author = "Max, Aur{\'e}lien  and
      Wisniewski, Guillaume",
    journal = "Proceedings of the Seventh conference on International Language Resources and Evaluation ({LREC}{'}10)",
    month = may,
    year = "2010",
    address = "Valletta, Malta",
    publisher = "European Languages Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2010/pdf/827_Paper.pdf",
}

@article{zesch-2012-measuring,
    title = "Measuring Contextual Fitness Using Error Contexts Extracted from the {W}ikipedia Revision History",
    author = "Zesch, Torsten",
    journal = "ACL",
    month = apr,
    year = "2012",
    address = "Avignon, France",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E12-1054",
    pages = "529--538",
}

@article{cahill-etal-2013-robust,
    title = "Robust Systems for Preposition Error Correction Using {W}ikipedia Revisions",
    author = "Cahill, Aoife  and
      Madnani, Nitin  and
      Tetreault, Joel  and
      Napolitano, Diane",
    journal = {NAACL HLT},
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N13-1055",
    pages = "507--517",
}

@article{schuster2019towards,
  title="Towards Debiasing Fact Verification Models",
  author="Schuster, Tal and Shah, Darsh J and Yeo, Yun Jie Serene and Filizzola, Daniel and Santus, Enrico and Barzilay, Regina",
  journal = 	"EMNLP-IJCNLP",
  year = 	"2019",
  publisher = 	"Association for Computational Linguistics",
}