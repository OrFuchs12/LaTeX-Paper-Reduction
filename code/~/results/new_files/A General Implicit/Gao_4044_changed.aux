\relax 
\bibstyle{aaai24}
\citation{mildenhall2020nerf}
\citation{wang2022morf,jiang2022nerffaceediting}
\citation{tancik2022block,kundu2022panoptic}
\citation{gu2021stylenerf}
\citation{wang2021neus,sun2022neural}
\citation{boss2021nerd,boss2021neural,zhang2021nerfactor}
\citation{Ma_Li_He_Dong_Tan_2023,yang2021learning,wu2022object,shuai2022multinb,ost2021neural}
\citation{kurz2022adanerf,neff2021donerf}
\citation{liu2020neural,mueller2022instant}
\citation{hedman2021baking,chen2022mobilenerf}
\providecommand \oddpage@label [2]{}
\citation{krishnan2023lane}
\citation{yang2021learning}
\citation{wu2022object}
\citation{shuai2022multinb}
\citation{kundu2022panoptic,tancik2022block}
\citation{liu2020neural}
\citation{ost2021neural,shuai2022multinb}
\citation{yu2021plenoctrees}
\citation{hedman2021baking,chen2022mobilenerf}
\citation{bakedsdf2023sig}
\citation{piala2021terminerf,neff2021donerf,kurz2022adanerf,Niemeyer2021Regnerf}
\citation{liu2020neural}
\citation{meng2019high,wizadwongsa2021nex,attal2022learning,sitzmann2021light,suhail2022light}
\citation{cao2022real,wang2022r2l}
\citation{1328805,6910019,Kim2013scene}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:overview}{{1}{2}{An overview of our framework (left) and data flow (right). In terms of framework, we use Neural Depth Fields (NeDF) as the representation of the implicit surface within each object's local space during training, under the supervision of pre-trained NeRF. There are three main steps in render time: 1) Step1 is "NeDFs generation step", depth buffer and ID buffer are created by a collection of NeDFs; 2) Step2 is "Deferred shading step", those 2 buffers are then used by a collection of NeRFs to fill a color buffer with no shadows; 3) Step3 is "Shadow step", shadow rays are generated based on the depth buffer and NeDFs are used again to provide a shadow map. Finally, the color buffer and shadow buffer are multiplied to produce the results.}{}{}}
\citation{neff2021donerf}
\newlabel{fig:rep_ray}{{2}{3}{(a) Given a ray traveling through an object's local space, the first intersection on the surface can be obtained by the distance between the intersection and the tangency point $\mathbf  {p}_{\perp }$ (The sphere is in the center of local space). Since $\mathbf  {p}_{\perp }$ can be quickly and uniquely determined, this representation leads to a direct intersection computation. (b) To improve the stability of NeDF, we regularize the main body of each ray, approximated by a tuple of points sampled along the ray, within the relaxed bounding box before fedding it into the intersection network. }{}{}}
\newlabel{eq:NeDF_reg}{{1}{3}{}{}{}}
\newlabel{eq:convert_depth}{{2}{3}{}{}{}}
\newlabel{eq:NeDF_mulcla}{{3}{3}{}{}{}}
\newlabel{eq:vol_depth}{{5}{3}{}{}{}}
\citation{Pumarola_2021_CVPR,park2021nerfies,park2021hypernerf,ost2021neural}
\citation{Andersson2020}
\citation{Andersson2020}
\citation{max1995optical}
\citation{williams1978casting}
\newlabel{fig:multi-classifier}{{3}{4}{Directly regressing $\mu $ may result in significant errors in the discontinuity area (a) during composition (see the top row of c). We address this problem by replacing the regression of $\mu $ with a multi-level classifier (b), which yields a better result (see the bottom row of c).}{}{}}
\newlabel{eq:loss_nedf}{{6}{4}{}{}{}}
\newlabel{eq:deferred_color}{{8}{4}{}{}{}}
\citation{barron2021mip}
\citation{snerf2022}
\citation{wang2021neus}
\citation{liu2020neural}
\newlabel{fig:3based}{{4}{5}{Visual quality illustration of the general plugin. For each method, the left column shows the previewing result, the middle column shows the naive result, and the right column visualizes the flip errors \citep  {Andersson2020}.}{}{}}
\newlabel{tab:3based}{{1}{5}{Our proposed framework serves as a plugin for three representative NeRF models: Mip-NeRF, SNeRF, and Neus. Term "s-time" refers to the percentage of time that our method saves. The optional outlier processing in Eq. \ref {eq:deferred_color} provides better quality but a slight speed penalty. The resolution is set to $800\times 800$.}{}{}}
\citation{mildenhall2020nerf,zhang2022modeling,verbin2022ref}
\citation{Andersson2020}
\citation{Andersson2020}
\citation{huang2022hdr}
\citation{huang2022hdr}
\newlabel{fig:bx_table}{{5}{6}{Number of objects contained in \textit  {b3},\textit  {b10},\textit  {b20},\textit  {b50} scene. The number in two ball cases is empty, which is because we use them for the capability pressure test and repeated basketballs and footballs (sharing fifty-fifty) in the 100-ball and 1k-ball scenes.}{}{}}
\newlabel{fig:bx-obj}{{6}{6}{Results on our $N$-object dataset. Each 4-image pair shows: ID map, depth map, results with shadows using our previewing framework, and results with only naive NeRFs (shadowless). The Rightside of last two rows is the flip error \citep  {Andersson2020} between previewing and NeRF's naive composition results). From top to bottom: \textit  {b3 and b10, b20 and b50, 100-ball and 1k-ball} virtual scenes. Scenes \textit  {b20} and \textit  {b50} are both dynamic, with carousels and ferris wheels moving through user control settings; scenes \textit  {100-ball} and \textit  {1k-ball} have 100 and 1,000 balls (half basketballs and half footballs), respectively. }{}{}}
\newlabel{fig:mixed_pipeline}{{7}{6}{Given the information of camera view and light position, the buffer produced by the proposed framework (NN Buffer) and the \textit  {Cycles} engine of Blender 3D can be combined to form a mixed render result (see our video for a dynamic result). The \textit  {Bathroom} scene is from \citep  {huang2022hdr}.}{}{}}
\citation{martinbrualla2020nerfw,tancik2022block,snerf2022,tojo2022posternerf,zhang2021nerfactor,zhang2022modeling,verbin2022ref}
\bibdata{aaai24}
\newlabel{tab:bxobj}{{2}{7}{Performance of the implemented Pytorch framework on an $N$-object dataset ('wo s' means no shadow is calculated; 'rs' stands for resampling, i.e., outlier processing in Table\nobreakspace  {}1). The resolution is set to $900\times 600$. We use the naive composition (right side in Fig. \ref {fig:bx-obj}) by NeRF as ground truth for composition without shadows. We only test the average time (in seconds) in each frame for shadow composition. It should be noted that the quality of the 1K-ball pressure testing was superior to the former due to the simpler geometry of the objects. Nonetheless, due to excessive overlapping between balls, the time increases rapidly.}{}{}}
\newlabel{fig:step_ratio}{{8}{7}{The time consumption ratio of our framework for the main steps. \textit  {Green} and \textit  {Red} bars represent the results with and without outliers resampling, respectively. Regarding the green bar, refer to this figure in conjunction with Fig. \ref {fig:bx-obj}, when the camera view varies, certain objects which occupy a larger portion of the frame, e.g., the trees, have a higher resampling ratio and thus increase the processing time. Focusing attention on the red bar, the Step 2 (deferred shading step) consistently assumes a relatively minor temporal allocation across all scenarios. This is because, regardless of the number of objects in the scene, a pixel will be shaded only once with a single NeRF object. }{}{}}
\gdef \@abspage@last{8}
