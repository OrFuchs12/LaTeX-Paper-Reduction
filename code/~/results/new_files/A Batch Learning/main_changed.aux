\relax 
\citation{hu2008collaborative}
\citation{rendle2009bpr}
\citation{he2016fast}
\citation{linden2003amazon}
\citation{chen2009make}
\citation{liu2014exploiting}
\citation{rendle2009bpr}
\citation{rendle2014improving}
\citation{srebro2005maximum}
\citation{hidasi2015session}
\citation{covington2016deep}
\citation{shi2012tfmap}
\citation{shi2012climf}
\citation{burges2005learning}
\citation{burges2007learning}
\citation{weston2010large}
\citation{hong2013co}
\citation{yuan2016lambdafm}
\providecommand \oddpage@label [2]{}
\citation{hidasi2015session}
\citation{covington2016deep}
\citation{usunier2009ranking}
\citation{shi2012tfmap}
\citation{shi2012climf}
\citation{weston2010large}
\citation{yuan2016lambdafm}
\newlabel{sec:notation}{{}{2}{}{}{}}
\newlabel{eq:rank}{{1}{2}{}{}{}}
\newlabel{eq:element}{{2}{2}{}{}{}}
\newlabel{eq:warp3}{{3}{2}{}{}{}}
\citation{yager1988ordered}
\citation{usunier2009ranking}
\citation{covington2016deep}
\newlabel{eq:owc}{{4}{3}{}{}{}}
\newlabel{sec:method}{{}{3}{}{}{}}
\newlabel{eq:mr}{{5}{3}{}{}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:rapp}{{1a}{3}{Indicator approximations.}{}{}}
\newlabel{sub@fig:rapp}{{a}{3}{Indicator approximations.}{}{}}
\newlabel{fig:rsl}{{1b}{3}{Rank-sensitive loss functions.}{}{}}
\newlabel{sub@fig:rsl}{{b}{3}{Rank-sensitive loss functions.}{}{}}
\newlabel{fig:approach}{{1}{3}{Illustrations of rank approximations and smooth rank-sensitive loss functions. \ref {fig:rapp} shows different approximations of indicator functions. \ref {fig:rsl} shows smooth loss functions used to generalize the loss (\ref {eq:owc}).}{}{}}
\newlabel{eq:smr}{{6}{3}{}{}{}}
\newlabel{eq:batch}{{7}{3}{}{}{}}
\newlabel{pic:std}{{2}{4}{Standard deviations (relative values) of two types of rank estimators at different item ranks. Simulation is done with item set size N=100,000. `online' uses estimator (\ref {eq:warp3}) and `sampled-batch q' uses (\ref {eq:mrmini}) where $q=|\textbf  {Z}|/|\textbf  {Y}|$.}{}{}}
\newlabel{eq:mrmini}{{8}{4}{}{}{}}
\newlabel{eq:alg}{{9}{4}{}{}{}}
\newlabel{eq:gradient}{{10}{4}{}{}{}}
\newlabel{alg}{{1}{4}{}{}{}}
\citation{weimer2008cofi}
\citation{taylor2008softrank}
\citation{agarwal2011infinite}
\citation{boyd2012accuracy}
\citation{burges2005learning}
\citation{burges2007learning}
\citation{wu2010adapting}
\citation{burges2007learning}
\citation{wu2010adapting}
\citation{hu2008collaborative}
\citation{rendle2009bpr}
\citation{rendle2014improving}
\citation{shi2012tfmap}
\citation{shi2012climf}
\citation{weston2010large}
\citation{yuan2016lambdafm}
\citation{weston2010large}
\citation{yuan2016lambdafm}
\citation{burges2005learning}
\citation{he2016fast}
\citation{abel2016recsys}
\citation{rendle2009bpr}
\citation{weston2010large}
\citation{hong2013co}
\citation{hidasi2015session}
\citation{covington2016deep}
\newlabel{eq:lambda}{{11}{5}{}{}{}}
\newlabel{sec:related}{{}{5}{}{}{}}
\newlabel{sec:exp}{{}{5}{}{}{}}
\newlabel{t:data1}{{1}{5}{Dataset statistics. U: users; I: items; S: interactions.}{}{}}
\citation{kula_metadata_2015}
\citation{shmueli2012care}
\newlabel{t:accuracy}{{2}{6}{Recommendation accuracy comparisons (in \%). Results are averaged over 5 experiments with different random seeds. Best and second best numbers are in bold and italic, respectively.}{}{}}
\newlabel{fig:qual1}{{3a}{6}{0-50000.}{}{}}
\newlabel{sub@fig:qual1}{{a}{6}{0-50000.}{}{}}
\newlabel{fig:qual2}{{3b}{6}{0-200.}{}{}}
\newlabel{sub@fig:qual2}{{b}{6}{0-200.}{}{}}
\newlabel{fig:rankapprox}{{3}{6}{Approximated rank values compared to the true rank values. \ref {fig:qual2} is a zoomed-in version with error bars.}{}{}}
\bibstyle{aaai}
\bibdata{main}
\newlabel{t:time}{{3}{7}{Dataset/model complexity comparisons.}{}{}}
\newlabel{fig:tconv}{{4a}{7}{Across datasets.}{}{}}
\newlabel{sub@fig:tconv}{{a}{7}{Across datasets.}{}{}}
\newlabel{fig:tepoch}{{4b}{7}{Across time.}{}{}}
\newlabel{sub@fig:tepoch}{{b}{7}{Across time.}{}{}}
\newlabel{fig:time}{{4}{7}{Training time comparisons between WARP and BARS. Fig. \ref {fig:tconv} plots how training time changes across datasets with different scales; Fig. \ref {fig:tepoch} plots how epoch time changes as the training progresses. }{}{}}
\newlabel{t:sampled}{{4}{7}{Comparisons of objective values (obj) and recommendation accuracies (NDCG) on development set among full batch and sampled batch algorithms. $q=|\textbf  {Z}|/|\textbf  {Y}|$, $q=1.0$ means full batch.}{}{}}
\newlabel{sec:summary}{{}{7}{}{}{}}
\gdef \@abspage@last{8}
