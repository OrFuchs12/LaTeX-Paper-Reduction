\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Sutton2018}
\citation{Powell2011}
\citation{Sutton2018}
\citation{Lattimore2014}
\citation{Li2006,Abel2016,Hutter2016}
\citation{Majeed2019}
\citation{Bertsekas1996}
\citation{Mnih2015}
\citation{Hutter2009}
\citation{McCallum1995}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{McCallum1995,Hutter2016,Majeed2019}
\citation{Strehl2009,Lattimore2014}
\citation{Powell2011}
\citation{Hutter2016}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The agent-environment interaction.}}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:interaction}{{1}{2}{The agent-environment interaction}{figure.caption.1}{}}
\newlabel{fig:interaction@cref}{{[figure][1][]1}{[1][1][]2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A simple sequentialization example in an MDP. To see how the actions sequentialized, consider an agent which has to choose among four alternatives, e.g.\ $\mathscr  {A}= \{a_{00}, a_{01}, a_{10}, a_{11}\}$. Let the agent receive a state signal $s$ from the environment. It first decides between a partition of actions, say two actions each, $\{a_{00}, a_{01}\}$ and $\{a_{10}, a_{11}\}$. \emph  {After} it has decided on the bifurcation, the \emph  {extended} state becomes $s_x$, where $x$ is the decision of the first stage. Now the agent \emph  {on} this \emph  {extended} state $s_x$ makes its second decision to choose from the \emph  {short-listed} set of actions. This way, the agent only selects among \emph  {two} alternatives at each stage by \emph  {tripling} the effective state-space.}}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig:example}{{2}{3}{A simple sequentialization example in an MDP. To see how the actions sequentialized, consider an agent which has to choose among four alternatives, e.g.\ $\A = \{a_{00}, a_{01}, a_{10}, a_{11}\}$. Let the agent receive a state signal $s$ from the environment. It first decides between a partition of actions, say two actions each, $\{a_{00}, a_{01}\}$ and $\{a_{10}, a_{11}\}$. \emph {After} it has decided on the bifurcation, the \emph {extended} state becomes $s_x$, where $x$ is the decision of the first stage. Now the agent \emph {on} this \emph {extended} state $s_x$ makes its second decision to choose from the \emph {short-listed} set of actions. This way, the agent only selects among \emph {two} alternatives at each stage by \emph {tripling} the effective state-space}{figure.caption.2}{}}
\newlabel{fig:example@cref}{{[figure][2][]2}{[1][3][]3}}
\citation{Sutton2018}
\citation{Hutter2009}
\@writefile{toc}{\contentsline {section}{\numberline {2}Notation}{4}{section.2}\protected@file@percent }
\newlabel{sec:notation}{{2}{4}{Notation}{section.2}{}}
\newlabel{sec:notation@cref}{{[section][2][]2}{[1][3][]4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Problem Setup}{4}{section.3}\protected@file@percent }
\newlabel{sec:setup}{{3}{4}{Problem Setup}{section.3}{}}
\newlabel{sec:setup@cref}{{[section][3][]3}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}General Reinforcement Learning}{4}{subsection.3.1}\protected@file@percent }
\citation{Sutton2018}
\citation{Hutter2016,Sutton2018}
\newlabel{eq:history}{{1}{5}{General Reinforcement Learning}{equation.3.1}{}}
\newlabel{eq:history@cref}{{[equation][1][]1}{[1][4][]5}}
\newlabel{eq:bellman}{{2}{5}{General Reinforcement Learning}{equation.3.2}{}}
\newlabel{eq:bellman@cref}{{[equation][2][]2}{[1][5][]5}}
\newlabel{eq:obe}{{3}{5}{General Reinforcement Learning}{equation.3.3}{}}
\newlabel{eq:obe@cref}{{[equation][3][]3}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Sequential Decisions}{5}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The agent-environment interaction through the sequentialization scheme. Note that the sequentialized-environment block (or a $\mathscr  {B}$-ary ``mock'') manages two different time-scales $t$ and $k$. It is simply a buffer block which knows (de)coders $C$ and $D$ (see text for details). It buffers the input $\mathscr  {B}$-ary actions and dispatches the buffered observation and reward. Once a complete $\mathscr  {B}$-ary decision sequence is produced by the agent the $\mathscr  {B}$-ary mock decodes the encoded actions to the original environment to continue the interaction loop. We can consider this sequentialized environment as a ``middle layer'' between the agent and the original environment.}}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig:bianary-interaction}{{3}{6}{The agent-environment interaction through the sequentialization scheme. Note that the sequentialized-environment block (or a $\B $-ary ``mock'') manages two different time-scales $t$ and $k$. It is simply a buffer block which knows (de)coders $C$ and $D$ (see text for details). It buffers the input $\B $-ary actions and dispatches the buffered observation and reward. Once a complete $\B $-ary decision sequence is produced by the agent the $\B $-ary mock decodes the encoded actions to the original environment to continue the interaction loop. We can consider this sequentialized environment as a ``middle layer'' between the agent and the original environment}{figure.caption.6}{}}
\newlabel{fig:bianary-interaction@cref}{{[figure][3][]3}{[1][6][]6}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}3.1\else \numberline {3.1}Definition\fi \thmtformatoptarg {History transformation function}}{6}{definition.3.1}\protected@file@percent }
\citation{Watkins1992}
\newlabel{eq:tobs}{{6}{7}{Sequential Decisions}{equation.3.6}{}}
\newlabel{eq:tobs@cref}{{[equation][6][]6}{[1][7][]7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A simple sequentialization/binarization example in a deterministic history-based process. The $\mathscr  {B}$-ary/binary decisions are on the edges. For brevity, we do not represent $o_\bot $ and $r_\bot $ in the figure. For example, it should be apparent that $\tau 1 o_\bot r_\bot \equiv \tau 1$. The circles represent complete histories while the squares indicate partial histories.}}{7}{figure.caption.10}\protected@file@percent }
\newlabel{fig:example-2}{{4}{7}{A simple sequentialization/binarization example in a deterministic history-based process. The $\B $-ary/binary decisions are on the edges. For brevity, we do not represent $o_\bot $ and $r_\bot $ in the figure. For example, it should be apparent that $\tau 1 o_\bot r_\bot \equiv \tau 1$. The circles represent complete histories while the squares indicate partial histories}{figure.caption.10}{}}
\newlabel{fig:example-2@cref}{{[figure][4][]4}{[1][7][]7}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}3.2\else \numberline {3.2}Definition\fi \thmtformatoptarg {Sequentialized environment}}{7}{definition.3.2}\protected@file@percent }
\newlabel{def:uP}{{3.2}{7}{Sequentialized environment}{definition.3.2}{}}
\newlabel{def:uP@cref}{{[definition][2][3]3.2}{[1][7][]7}}
\newlabel{eq:ube}{{8}{8}{Sequential Decisions}{equation.3.8}{}}
\newlabel{eq:ube@cref}{{[equation][8][]8}{[1][8][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Sequentialized Processes and Values}{8}{section.4}\protected@file@percent }
\newlabel{sec:bin-esa}{{4}{8}{Sequentialized Processes and Values}{section.4}{}}
\newlabel{sec:bin-esa@cref}{{[section][4][]4}{[1][8][]8}}
\@writefile{loe}{\contentsline {proposition}{\ifthmt@listswap Proposition\nobreakspace  {}4.1\else \numberline {4.1}Proposition\fi \thmtformatoptarg {Sequentialized Process}}{8}{proposition.4.1}\protected@file@percent }
\newlabel{pro:uPtoP}{{4.1}{8}{Sequentialized Process}{proposition.4.1}{}}
\newlabel{pro:uPtoP@cref}{{[proposition][1][4]4.1}{[1][8][]8}}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem\nobreakspace  {}4.2\else \numberline {4.2}Theorem\fi \thmtformatoptarg {Sequentialization preserves Markov property}}{9}{theorem.4.2}\protected@file@percent }
\newlabel{pro:mdpismdp}{{4.2}{9}{Sequentialization preserves Markov property}{theorem.4.2}{}}
\newlabel{pro:mdpismdp@cref}{{[theorem][2][4]4.2}{[1][9][]9}}
\@writefile{loe}{\contentsline {proposition}{\ifthmt@listswap Proposition\nobreakspace  {}4.3\else \numberline {4.3}Proposition\fi \thmtformatoptarg {$\breve  {Q}^*$ $\max $-relationship}}{9}{proposition.4.3}\protected@file@percent }
\newlabel{prep:expandsion}{{4.3}{9}{$\u Q^*$ $\max $-relationship}{proposition.4.3}{}}
\newlabel{prep:expandsion@cref}{{[proposition][3][4]4.3}{[1][9][]9}}
\@writefile{loe}{\contentsline {lemma}{\ifthmt@listswap Lemma\nobreakspace  {}4.4\else \numberline {4.4}Lemma\fi \thmtformatoptarg {$\breve  {Q}^*$ $\boldsymbol  {x}$-relationship}}{9}{lemma.4.4}\protected@file@percent }
\newlabel{lem:q-relation}{{4.4}{9}{$\u Q^*$ $\v x$-relationship}{lemma.4.4}{}}
\newlabel{lem:q-relation@cref}{{[lemma][4][4]4.4}{[1][9][]9}}
\citation{Lattimore2014b}
\newlabel{eq:obe-2}{{{13}}{10}{Sequentialized Processes and Values}{AMS.32}{}}
\newlabel{eq:obe-2@cref}{{[equation][2147483647][]{13}}{[1][9][]10}}
\newlabel{eq:bpi}{{15}{10}{Sequentialized Processes and Values}{equation.4.15}{}}
\newlabel{eq:bpi@cref}{{[equation][15][]15}{[1][10][]10}}
\@writefile{loe}{\contentsline {lemma}{\ifthmt@listswap Lemma\nobreakspace  {}4.5\else \numberline {4.5}Lemma\fi \thmtformatoptarg {$\breve  {Q}^{\breve  {\Pi }}$ $\boldsymbol  {x}$-relationship}}{11}{lemma.4.5}\protected@file@percent }
\newlabel{lem:fixed-policy}{{4.5}{11}{$\u Q^{\u \Pi }$ $\v x$-relationship}{lemma.4.5}{}}
\newlabel{lem:fixed-policy@cref}{{[lemma][5][4]4.5}{[1][10][]11}}
\newlabel{eq:v-v}{{17}{11}{Sequentialized Processes and Values}{equation.4.17}{}}
\newlabel{eq:v-v@cref}{{[equation][17][]17}{[1][11][]11}}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem\nobreakspace  {}4.6\else \numberline {4.6}Theorem\fi \thmtformatoptarg {Sequentialization preserves $\varepsilon $-optimality}}{11}{theorem.4.6}\protected@file@percent }
\newlabel{lem:uplift}{{4.6}{11}{Sequentialization preserves $\eps $-optimality}{theorem.4.6}{}}
\newlabel{lem:uplift@cref}{{[theorem][6][4]4.6}{[1][11][]11}}
\citation{Hutter2009}
\citation{Sutton2018}
\citation{Sutton2018}
\citation{Hutter2016}
\citation{Mnih2015,Silver2016,Silver2018}
\citation{Kaelbling1998}
\citation{Pendrith1998}
\newlabel{eq:near-opt}{{19}{12}{Sequentialized Processes and Values}{equation.4.19}{}}
\newlabel{eq:near-opt@cref}{{[equation][19][]19}{[1][11][]12}}
\newlabel{eq:opt-v-v}{{{20}}{12}{Sequentialized Processes and Values}{AMS.47}{}}
\newlabel{eq:opt-v-v@cref}{{[equation][2147483647][]{20}}{[1][12][]12}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Extreme State Aggregation}{12}{section.5}\protected@file@percent }
\newlabel{sec:esa}{{5}{12}{Extreme State Aggregation}{section.5}{}}
\newlabel{sec:esa@cref}{{[section][5][]5}{[1][12][]12}}
\citation{Li2006,Abel2016}
\citation{Hutter2016}
\citation{Majeed2019}
\citation{Majeed2018}
\citation{Hutter2016}
\citation{Hutter2016}
\citation{Hutter2016}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}5.1\else \numberline {5.1}Definition\fi \thmtformatoptarg {$\varepsilon $-Q-uniform abstraction}}{13}{definition.5.1}\protected@file@percent }
\citation{Hutter2016}
\citation{Hutter2016}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem\nobreakspace  {}5.2\else \numberline {5.2}Theorem\fi \thmtformatoptarg {ESA {\cite  [Theorem 11]{Hutter2016}}}}{14}{theorem.5.2}\protected@file@percent }
\newlabel{thm:esa}{{5.2}{14}{ESA {\cite [Theorem 11]{Hutter2016}}}{theorem.5.2}{}}
\newlabel{thm:esa@cref}{{[theorem][2][5]5.2}{[1][14][]14}}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem\nobreakspace  {}5.3\else \numberline {5.3}Theorem\fi \thmtformatoptarg {Binary ESA}}{14}{theorem.5.3}\protected@file@percent }
\newlabel{thm:bin-esa}{{5.3}{14}{Binary ESA}{theorem.5.3}{}}
\newlabel{thm:bin-esa@cref}{{[theorem][3][5]5.3}{[1][14][]14}}
\newlabel{eq:bound}{{22}{14}{Extreme State Aggregation}{equation.5.22}{}}
\newlabel{eq:bound@cref}{{[equation][22][]22}{[1][14][]14}}
\newlabel{eq:bound2}{{{23}}{14}{Extreme State Aggregation}{AMS.54}{}}
\newlabel{eq:bound2@cref}{{[equation][2147483647][]{23}}{[1][14][]14}}
\citation{Hutter2000}
\citation{Hutter2016}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion \& Outlook}{15}{section.6}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{15}{Conclusion \& Outlook}{section.6}{}}
\newlabel{sec:conclusion@cref}{{[section][6][]6}{[1][15][]15}}
\bibstyle{unsrtnat}
\bibdata{references}
\gdef \@abspage@last{16}
