

@book{Sutton2018,
  author =        {Sutton, Richard S. and Barto, Andrew G.},
  booktitle =     {A Bradford Book},
  edition =       {2nd},
  month =         {sep},
  publisher =     {MIT press Cambridge},
  title =         {{Reinforcement Learning: An Introduction}},
  year =          {2018},
  isbn =          {0262039249},
}

@book{Powell2011,
  author =        {Powell, Warren B.},
  booktitle =     {Approximate Dynamic Programming: Solving the Curses
                   of Dimensionality: Second Edition},
  number =        {1},
  pages =         {1--638},
  publisher =     {Wiley},
  title =         {{Approximate Dynamic Programming: Solving the Curses
                   of Dimensionality: Second Edition}},
  volume =        {136},
  year =          {2011},
  abstract =      {Praise for the First Edition "Finally, a book devoted
                   to dynamic programming and written using the language
                   of operations research (OR)! This beautiful book
                   fills a gap in the libraries of OR specialists and
                   practitioners." -Computing Reviews. This new edition
                   showcases a focus on modeling and computation for
                   complex classes of approximate dynamic programming
                   problems. Understanding approximate dynamic
                   programming (ADP) is vital in order to develop
                   practical and high-quality solutions to complex
                   industrial problems, particularly when those problems
                   involve making decisions in the presence of
                   uncertainty. Approximate Dynamic Programming, Second
                   Edition uniquely integrates four distinct
                   disciplines-Markov decision processes, mathematical
                   programming, simulation, and statistics-to
                   demonstrate how to successfully approach, model, and
                   solve a wide range of real-life problems using ADP.
                   The book continues to bridge the gap between computer
                   science, simulation, and operations research and now
                   adopts the notation and vocabulary of reinforcement
                   learning as well as stochastic search and simulation
                   optimization. The author outlines the essential
                   algorithms that serve as a starting point in the
                   design of practical solutions for real problems. The
                   three curses of dimensionality that impact complex
                   problems are introduced and detailed coverage of
                   implementation challenges is provided. The Second
                   Edition also features: • A new chapter describing
                   four fundamental classes of policies for working with
                   diverse stochastic optimization problems: myopic
                   policies, look-ahead policies, policy function
                   approximations, and policies based on value function
                   approximations • A new chapter on policy search
                   that brings together stochastic search and simulation
                   optimization concepts and introduces a new class of
                   optimal learning strategies • Updated coverage of
                   the exploration exploitation problem in ADP, now
                   including a recently developed method for doing
                   active learning in the presence of a physical state,
                   using the concept of the knowledge gradient • A new
                   sequence of chapters describing statistical methods
                   for approximating value functions, estimating the
                   value of a fixed policy, and value function
                   approximation while searching for optimal policies.
                   The presented coverage of ADP emphasizes models and
                   algorithms, focusing on related applications and
                   computation while also discussing the theoretical
                   side of the topic that explores proofs of convergence
                   and rate of convergence. A related website features
                   an ongoing discussion of the evolving fields of
                   approximation dynamic programming and reinforcement
                   learning, along with additional readings, software,
                   and datasets. Requiring only a basic understanding of
                   statistics and probability, Approximate Dynamic
                   Programming, Second Edition is an excellent book for
                   industrial engineering and operations research
                   courses at the upper-undergraduate and graduate
                   levels. It also serves as a valuable reference for
                   researchers and professionals who utilize dynamic
                   programming, stochastic programming, and control
                   theory to solve problems in their everyday work.},
  doi =           {10.1002/9781118029176},
  isbn =          {9781118029176},
}

@article{Lattimore2014,
  author =        {Lattimore, Tor and Hutter, Marcus},
  journal =       {Theoretical Computer Science},
  number =        {C},
  pages =         {125--143},
  title =         {{Near-optimal PAC bounds for discounted MDPs}},
  volume =        {558},
  year =          {2014},
  abstract =      {We study upper and lower bounds on the
                   sample-complexity of learning near-optimal behaviour
                   in finite-state discounted Markov Decision Processes
                   (mdps). We prove a new bound for a modified version
                   of Upper Confidence Reinforcement Learning (ucrl)
                   with only cubic dependence on the horizon. The bound
                   is unimprovable in all parameters except the size of
                   the state/action space, where it depends linearly on
                   the number of non-zero transition probabilities. The
                   lower bound strengthens previous work by being both
                   more general (it applies to all policies) and
                   tighter. The upper and lower bounds match up to
                   logarithmic factors provided the transition matrix is
                   not too dense.},
  doi =           {10.1016/j.tcs.2014.09.029},
  isbn =          {9783642341052},
  issn =          {03043975},
}

@article{Li2006,
  author =        {Li, Lihong and Walsh, Thomas J. and
                   Littman, Michael L.},
  journal =       {9th International Symposium on Artificial
                   Intelligence and Mathematics, ISAIM 2006},
  pages =         {531--539},
  title =         {{Towards a unified theory of state abstraction for
                   MDPs}},
  year =          {2006},
  abstract =      {State abstraction (or state aggregation) has been
                   extensively studied in the fields of artificial
                   intelligence and operations research. Instead of
                   working in the ground state space, the decision maker
                   usually finds solutions in the abstract state space
                   much faster by treating groups of states as a unit by
                   ignoring irrelevant state information. A number of
                   abstractions have been proposed and studied in the
                   reinforcement-learning and planning literatures, and
                   positive and negative results are known. We provide a
                   unified treatment of state abstraction for Markov
                   decision processes. We study five particular
                   abstraction schemes, some of which have been proposed
                   in the past in different forms, and analyze their
                   usability for planning and learning.},
}

@inproceedings{Abel2016,
  author =        {Abel, David and Hershkowitz, D. Ellis and
                   Littman, Michael L.},
  booktitle =     {33rd International Conference on Machine Learning,
                   ICML 2016},
  month =         {jan},
  pages =         {4287--4295},
  title =         {{Near optimal behavior via approximate state
                   abstraction}},
  volume =        {6},
  year =          {2016},
  abstract =      {The combinatorial explosion that plagues planing and
                   reinforcement learning (RL) algorithms can be
                   moderated using state abstraction. Prohibitively
                   large task representations can be con-densed such
                   that essential information is preserved, and
                   consequently, solutions arc tractably computable.
                   However, exact abstractions, which treat only
                   fully-identical situations as equivalent, fail to
                   present opportunities for abstraction in environments
                   where no two situations arc exactly alike. In this
                   work, we investigate approximate state abstractions,
                   which treat nearly-identical situations as
                   equivalent. We present theoretical guarantees of the
                   quality of behaviors derived from four types of
                   approximate abstractions. Additionally, we
                   empirically demonstrate that ap-proximate
                   abstractions lead to reduction in task complexity and
                   bounded loss of optimality of be-havior in a variety
                   of environments.},
  isbn =          {9781510829008},
}

@article{Hutter2016,
  author =        {Hutter, Marcus},
  journal =       {Theoretical Computer Science},
  pages =         {73--91},
  title =         {{Extreme state aggregation beyond Markov decision
                   processes}},
  volume =        {650},
  year =          {2016},
  abstract =      {We consider a Reinforcement Learning setup where an
                   agent interacts with an environment in
                   observation–reward–action cycles without any
                   (esp. MDP) assumptions on the environment. State
                   aggregation and more generally feature reinforcement
                   learning is concerned with mapping
                   histories/raw-states to reduced/aggregated states.
                   The idea behind both is that the resulting reduced
                   process (approximately) forms a small stationary
                   finite-state MDP, which can then be efficiently
                   solved or learnt. We considerably generalize existing
                   aggregation results by showing that even if the
                   reduced process is not an MDP, the (q-)value
                   functions and (optimal) policies of an associated MDP
                   with same state-space size solve the original
                   problem, as long as the solution can approximately be
                   represented as a function of the reduced states. This
                   implies an upper bound on the required state space
                   size that holds uniformly for all RL problems. It may
                   also explain why RL algorithms designed for MDPs
                   sometimes perform well beyond MDPs.},
  doi =           {10.1016/j.tcs.2016.07.032},
  isbn =          {9783319116617},
  issn =          {03043975},
}

@article{Majeed2019,
  author =        {Majeed, Sultan Javed and Hutter, Marcus},
  journal =       {Proceedings of the AAAI Conference on Artificial
                   Intelligence},
  month =         {nov},
  pages =         {7659--7666},
  title =         {{Performance Guarantees for Homomorphisms beyond
                   Markov Decision Processes}},
  volume =        {33},
  year =          {2019},
  abstract =      {Most real-world problems have huge state and/or
                   action spaces. Therefore, a naive application of
                   existing tabular solution methods is not tractable on
                   such problems. Nonetheless, these solution methods
                   are quite useful if an agent has access to a
                   relatively small state-action space homomorphism of
                   the true environment and near-optimal performance is
                   guaranteed by the map. A plethora of research is
                   focused on the case when the homomorphism is a
                   Markovian representation of the underlying process.
                   However, we show that nearoptimal performance is
                   sometimes guaranteed even if the homomorphism is
                   non-Markovian.},
  doi =           {10.1609/aaai.v33i01.33017659},
  issn =          {2159-5399},
}

@book{Bertsekas1996,
  author =        {Bertsekas, Dimitri and Tsitsiklis, John},
  booktitle =     {Control and Cybernetics},
  number =        {3},
  pages =         {513--533},
  publisher =     {Belmont: Athena Scientific},
  title =         {{Dynamic programming: An overview}},
  volume =        {35},
  year =          {1996},
  abstract =      {Dynamic programing is one of the major problemsolving
                   methodologies in a number of disciplines such as
                   operations research and computer science. It is also
                   a very important and powerful tool of thought. But
                   not all is well on the dynamic programming front.
                   There is definitely lack of commercial software
                   support and the situation in the classroom is not as
                   good as it should be. In this paper we take a bird's
                   view of dynamic programming so as to identify ways to
                   make it more accessible to students, academics and
                   practitioners alike.},
  annote =        {NULL},
  doi =           {10.1109/cdc.1995.478953},
  isbn =          {0-7803-2685-7},
  issn =          {03248569},
}

@article{Mnih2015,
  author =        {Mnih, Volodymyr and Kavukcuoglu, Koray and
                   Silver, David and Rusu, Andrei A. and Veness, Joel and
                   Bellemare, Marc G. and Graves, Alex and
                   Riedmiller, Martin and Fidjeland, Andreas K. and
                   Ostrovski, Georg and Petersen, Stig and
                   Beattie, Charles and Sadik, Amir and
                   Antonoglou, Ioannis and King, Helen and
                   Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and
                   Hassabis, Demis},
  journal =       {Nature},
  number =        {7540},
  pages =         {529--533},
  title =         {{Human-level control through deep reinforcement
                   learning}},
  volume =        {518},
  year =          {2015},
  abstract =      {The theory of reinforcement learning provides a
                   normative account, deeply rooted in psychological and
                   neuroscientific perspectives on animal behaviour, of
                   how agents may optimize their control of an
                   environment. To use reinforcement learning
                   successfully in situations approaching real-world
                   complexity, however, agents are confronted with a
                   difficult task: they must derive efficient
                   representations of the environment from
                   high-dimensional sensory inputs, and use these to
                   generalize past experience to new situations.
                   Remarkably, humans and other animals seem to solve
                   this problem through a harmonious combination of
                   reinforcement learning and hierarchical sensory
                   processing systems, the former evidenced by a wealth
                   of neural data revealing notable parallels between
                   the phasic signals emitted by dopaminergic neurons
                   and temporal difference reinforcement learning
                   algorithms. While reinforcement learning agents have
                   achieved some successes in a variety of domains,
                   their applicability has previously been limited to
                   domains in which useful features can be handcrafted,
                   or to domains with fully observed, low-dimensional
                   state spaces. Here we use recent advances in training
                   deep neural networks to develop a novel artificial
                   agent, termed a deep Q-network, that can learn
                   successful policies directly from high-dimensional
                   sensory inputs using end-to-end reinforcement
                   learning. We tested this agent on the challenging
                   domain of classic Atari 2600 games. We demonstrate
                   that the deep Q-network agent, receiving only the
                   pixels and the game score as inputs, was able to
                   surpass the performance of all previous algorithms
                   and achieve a level comparable to that of a
                   professional human games tester across a set of 49
                   games, using the same algorithm, network architecture
                   and hyperparameters. This work bridges the divide
                   between high-dimensional sensory inputs and actions,
                   resulting in the first artificial agent that is
                   capable of learning to excel at a diverse array of
                   challenging tasks.},
  doi =           {10.1038/nature14236},
  isbn =          {1476-4687 (Electronic) 0028-0836 (Linking)},
  issn =          {14764687},
}

@article{Hutter2009,
  author =        {Hutter, Marcus},
  journal =       {Journal of Artificial General Intelligence},
  month =         {jan},
  number =        {1},
  pages =         {3--24},
  title =         {{Feature Reinforcement Learning: Part I. Unstructured
                   MDPs}},
  volume =        {1},
  year =          {2009},
  abstract =      {General-purpose, intelligent, learning agents cycle
                   through sequences of observations, actions, and
                   rewards that are complex, uncertain, unknown, and
                   non-Markovian. On the other hand, reinforcement
                   learning is well-developed for small finite state
                   Markov decision processes (MDPs). Up to now,
                   extracting the right state representations out of
                   bare observations, that is, reducing the general
                   agent setup to the MDP framework, is an art that
                   involves significant effort by designers. The primary
                   goal of this work is to automate the reduction
                   process and thereby significantly expand the scope of
                   many existing reinforcement learning algorithms and
                   the agents that employ them. Before we can think of
                   mechanizing this search for suitable MDPs, we need a
                   formal objective criterion. The main contribution of
                   this article is to develop such a criterion. I also
                   integrate the various parts into one learning
                   algorithm. Extensions to more realistic dynamic
                   Bayesian networks are developed in Part II. The role
                   of POMDPs is also considered there.},
  doi =           {10.2478/v10229-011-0002-8},
  issn =          {1946-0163},
}

@article{McCallum1995,
  author =        {McCallum, Andrew},
  journal =       {Machine Learning Proceedings 1995},
  pages =         {387--395},
  title =         {{Instance-Based Utile Distinctions for Reinforcement
                   Learning with Hidden State}},
  year =          {1995},
  abstract =      {We present Utile Suffix Memory, a reinforcement
                   learning algorithm that uses short-term memory to
                   overcome the state aliasing that results from hidden
                   state. By combining the advantages of previous work
                   in instance-based (or "memorybased ") learning and
                   previous work with statistical tests for separating
                   noise from task structure, the method learns quickly,
                   creates only as much memory as needed for the task at
                   hand, and handles noise well. Utile Suffix Memory
                   uses a tree-structured representation, and is related
                   to work on Prediction Suffix Trees [Ron et al., 1994]
                   , Parti-game [Moore, 1993] , G-algorithm [Chapman and
                   Kaelbling, 1991] , and Variable Resolution Dynamic
                   Programming [Moore, 1991] . 1 INTRODUCTION The
                   sensory systems of embedded agents are inherently
                   limited. When a reinforcement learning agent's
                   sensory limitations hide features of the environment
                   from the agent, we say that the agent suffers from
                   hidden state. There are many reasons why important
                   features can be hidden...},
  doi =           {10.1016/b978-1-55860-377-6.50055-4},
  isbn =          {9781558603776},
}

@article{Strehl2009,
  author =        {Strehl, Alexander L. and Li, Hong and
                   Littman, Michael L.},
  journal =       {Journal of Machine Learning Research},
  pages =         {2413--2444},
  title =         {{Reinforcement learning in finite MDPs: PAC
                   analysis}},
  volume =        {10},
  year =          {2009},
  abstract =      {We study the problem of learning near-optimal
                   behavior in finite Markov Decision Processes (MDPs)
                   with a polynomial number of samples. These "PAC-MDP"
                   algorithms include the wellknown E3 and R-MAX
                   algorithms as well as the more recent Delayed
                   Q-learning algorithm. We summarize the current
                   state-of-the-art by presenting bounds for the problem
                   in a unified theoretical framework. A more refined
                   analysis for upper and lower bounds is presented to
                   yield insight into the differences between the
                   model-free Delayed Q-learning and the model-based
                   R-MAX. {\textcopyright} 2009 Alexander L. Strehl and
                   Lihong Li and Michael L. Littman.},
  doi =           {10.1.1.153.9841},
  isbn =          {1532-4435},
  issn =          {15324435},
}

@article{Watkins1992,
  author =        {Watkins, Christopher J. C. H. and Dayan, Peter},
  journal =       {Machine Learning},
  number =        {3-4},
  pages =         {279--292},
  title =         {{Q-learning}},
  volume =        {8},
  year =          {1992},
  abstract =      {Q-learning (Watkins, 1989) is a simple way for agents
                   to learn how to act optimally in controlled Markovian
                   domains. It amounts to an incremental method for
                   dynamic programming which imposes limited
                   computational demands. It works by successively
                   improving its evaluations of the quality of
                   particular actions at particular states. This paper
                   presents and proves in detail a convergence theorem
                   forQ-learning based on that outlined in Watkins
                   (1989). We show thatQ-learning converges to the
                   optimum action-values with probability 1 so long as
                   all actions are repeatedly sampled in all states and
                   the action-values are represented discretely. We also
                   sketch extensions to the cases of non-discounted, but
                   absorbing, Markov environments, and where manyQ
                   values can be changed each iteration, rather than
                   just one.},
  doi =           {10.1007/bf00992698},
  isbn =          {0885-6125},
  issn =          {0885-6125},
}

@article{Lattimore2014b,
  author =        {Lattimore, Tor and Hutter, Marcus},
  journal =       {Theoretical Computer Science},
  pages =         {140--154},
  title =         {{General time consistent discounting}},
  volume =        {519},
  year =          {2014},
  abstract =      {Modeling inter-temporal choice is a key problem in
                   both computer science and economic theory. The
                   discounted utility model of Samuelson is currently
                   the most popular model for measuring the global
                   utility of a time-series of local utilities. The
                   model is limited by not allowing the discount
                   function to change with the age of the agent. This is
                   despite the fact that many agents, in particular
                   humans, are best modelled with age-dependent discount
                   functions. It is well known that discounting can lead
                   to time-inconsistent behaviour where agents change
                   their preferences over time. In this paper we
                   generalise the discounted utility model to allow
                   age-dependent discount functions. We then extend
                   previous work in time-inconsistency to our new
                   setting, including a complete characterisation of
                   time-(in)consistent discount functions, the existence
                   of sub-game perfect equilibrium policies where the
                   discount function is time-inconsistent and a
                   continuity result showing that "nearly"
                   time-consistent discount rates lead to "nearly"
                   time-consistent behaviour.{\textcopyright} 2013
                   Elsevier B.V.},
  doi =           {10.1016/j.tcs.2013.09.022},
  isbn =          {9783642244117},
  issn =          {03043975},
}

@article{Silver2016,
  author =        {Silver, David and Huang, Aja and Maddison, Chris J. and
                   Guez, Arthur and Sifre, Laurent and
                   {Van Den Driessche}, George and Schrittwieser, Julian and
                   Antonoglou, Ioannis and Panneershelvam, Veda and
                   Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and
                   Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and
                   Lillicrap, Timothy and Leach, Madeleine and
                   Kavukcuoglu, Koray and Graepel, Thore and
                   Hassabis, Demis},
  journal =       {Nature},
  month =         {jan},
  number =        {7587},
  pages =         {484--489},
  publisher =     {Nature Publishing Group, a division of Macmillan
                   Publishers Limited. All Rights Reserved.},
  title =         {{Mastering the game of Go with deep neural networks
                   and tree search}},
  volume =        {529},
  year =          {2016},
  abstract =      {The game of Go has long been viewed as the most
                   challenging of classic games for artificial
                   intelligence owing to its enormous search space and
                   the difficulty of evaluating board positions and
                   moves. Here we introduce a new approach to computer
                   Go that uses 'value networks' to evaluate board
                   positions and 'policy networks' to select moves.
                   These deep neural networks are trained by a novel
                   combination of supervised learning from human expert
                   games, and reinforcement learning from games of
                   self-play. Without any lookahead search, the neural
                   networks play Go at the level of state-of-the-art
                   Monte Carlo tree search programs that simulate
                   thousands of random games of self-play. We also
                   introduce a new search algorithm that combines Monte
                   Carlo simulation with value and policy networks.
                   Using this search algorithm, our program AlphaGo
                   achieved a 99.8{\%} winning rate against other Go
                   programs, and defeated the human European Go champion
                   by 5 games to 0. This is the first time that a
                   computer program has defeated a human professional
                   player in the full-sized game of Go, a feat
                   previously thought to be at least a decade away.},
  doi =           {10.1038/nature16961},
  isbn =          {1476-4687 (Electronic)$\backslash$r0028-0836
                   (Linking)},
  issn =          {14764687},
}

@article{Silver2018,
  author =        {Silver, David and Hubert, Thomas and
                   Schrittwieser, Julian and Antonoglou, Ioannis and
                   Lai, Matthew and Guez, Arthur and Lanctot, Marc and
                   Sifre, Laurent and Kumaran, Dharshan and
                   Graepel, Thore and Lillicrap, Timothy and
                   Simonyan, Karen and Hassabis, Demis},
  journal =       {Science},
  number =        {6419},
  pages =         {1140--1144},
  title =         {{A general reinforcement learning algorithm that
                   masters chess, shogi, and Go through self-play}},
  volume =        {362},
  year =          {2018},
  abstract =      {The game of chess is the longest-studied domain in
                   the history of artificial intelligence. The strongest
                   programs are based on a combination of sophisticated
                   search techniques, domain-specific adaptations, and
                   handcrafted evaluation functions that have been
                   refined by human experts over several decades. By
                   contrast, the AlphaGo Zero program recently achieved
                   superhuman performance in the game of Go by
                   reinforcement learning from self-play. In this paper,
                   we generalize this approach into a single AlphaZero
                   algorithm that can achieve superhuman performance in
                   many challenging games. Starting from random play and
                   given no domain knowledge except the game rules,
                   AlphaZero convincingly defeated a world champion
                   program in the games of chess and shogi (Japanese
                   chess), as well as Go.},
  doi =           {10.1126/science.aar6404},
  issn =          {10959203},
}

@article{Kaelbling1998,
  author =        {Kaelbling, Leslie Pack and Littman, Michael L. and
                   Cassandra, Anthony R.},
  journal =       {Artificial Intelligence},
  number =        {1-2},
  pages =         {99--134},
  title =         {{Planning and acting in partially observable
                   stochastic domains}},
  volume =        {101},
  year =          {1998},
  abstract =      {In this paper, we bring techniques from operations
                   research to bear on the problem of choosing optimal
                   actions in partially observable stochastic domains.
                   We begin by introducing the theory of Markov decision
                   processes (mdps) and partially observable MDPs
                   (pomdps). We then outline a novel algorithm for
                   solving pomdps off line and show how, in some cases,
                   a finite-memory controller can be extracted from the
                   solution to a POMDP. We conclude with a discussion of
                   how our approach relates to previous work, the
                   complexity of finding exact solutions to pomdps, and
                   of some possibilities for finding approximate
                   solutions.},
  annote =        {Very citable introduction to POMDPs (Wikipedia seems
                   loosely based on this one - or possibly it's a very
                   standard formulation).},
  doi =           {10.1016/S0004-3702(98)00023-X},
  isbn =          {00043702 (ISSN)},
  issn =          {00043702},
}

@article{Pendrith1998,
  author =        {Pendrith, Mark D and McGarity, Michael J},
  journal =       {Proceedings of the Fifteenth International Conference
                   on Machine Learning},
  pages =         {421--429},
  title =         {{An analysis of direct reinforcement learning in
                   non-Markovian domains}},
  year =          {1998},
  abstract =      {It is well known that for Markov decision processes,
                   the policies stable under policy iteration and the
                   standard reinforcement learning methods are exactly
                   the optimal policies. In this paper, we investigate
                   the conditions for policy stability in the more
                   general situation when the Markov property cannot be
                   assumed. We show that for a general class of
                   non-Markov decision processes, if actual return
                   (Monte Carlo) credit assignment is used with
                   undiscounted returns, we are still guaranteed the
                   optimal observation-based policies will be
                   equilibrium points in the policy space when using the
                   standard direct reinforcement learning approaches.
                   However, if either discounted rewards, or a temporal
                   differences style of credit assignment method is
                   used, this is not the case.},
  isbn =          {1-55860-556-8},
}

@article{Majeed2018,
  author =        {Majeed, Sultan Javed and Hutter, Marcus},
  journal =       {IJCAI International Joint Conference on Artificial
                   Intelligence},
  pages =         {2546--2552},
  title =         {{On Q-learning convergence for non-Markov decision
                   processes}},
  volume =        {2018-July},
  year =          {2018},
  abstract =      {Temporal-difference (TD) learning is an attractive,
                   computationally efficient framework for model-free
                   reinforcement learning. Q-learning is one of the most
                   widely used TD learning technique that enables an
                   agent to learn the optimal action-value function,
                   i.e. Q-value function. Contrary to its widespread
                   use, Q-learning has only been proven to converge on
                   Markov Decision Processes (MDPs) and Q-uniform
                   abstractions of finite-state MDPs. On the other hand,
                   most real-world problems are inherently
                   non-Markovian: the full true state of the environment
                   is not revealed by recent observations. In this
                   paper, we investigate the behavior of Q-learning when
                   applied to non-MDP and non-ergodic domains which may
                   have infinitely many underlying states. We prove that
                   the convergence guarantee of Q-learning can be
                   extended to a class of such non-MDP problems, in
                   particular, to some non-stationary domains. We show
                   that state-uniformity of the optimal Q-value function
                   is a necessary and sufficient condition for
                   Q-learning to converge even in the case of infinitely
                   many internal states.},
  doi =           {10.24963/ijcai.2018/353},
  isbn =          {9780999241127},
  issn =          {10450823},
}

@article{Hutter2000,
  author =        {Hutter, Marcus},
  journal =       {Ecml},
  month =         {apr},
  pages =         {226--238},
  title =         {{Universal Artificial Intellegence}},
  year =          {2005},
  abstract =      {This book presents sequential decision theory from a
                   novel algorithmic information theory perspective.
                   While the former is suited for active agents in known
                   environments, the latter is suited for passive
                   prediction in unknown environments. The book
                   introduces these two different ideas and removes the
                   limitations by unifying them to one parameter-free
                   theory of an optimal reinforcement learning agent
                   embedded in an unknown environment. Most AI problems
                   can easily be formulated within this theory, reducing
                   the conceptual problems to pure computational ones.
                   Considered problem classes include sequence
                   prediction, strategic games, function minimization,
                   reinforcement and supervised learning. The discussion
                   includes formal definitions of intelligence order
                   relations, the horizon problem and relations to other
                   approaches. One intention of this book is to excite a
                   broader AI audience about abstract algorithmic
                   information theory concepts, and conversely to inform
                   theorists about exciting applications to AI.},
  doi =           {10.1007/b138233},
  isbn =          {978-3-540-22139-5},
  issn =          {0160-6972},
}

