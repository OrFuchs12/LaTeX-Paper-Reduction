\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Anthony, Tian, and
  Barber}{2017}]{anthony2017thinking}
Anthony, T.; Tian, Z.; and Barber, D.
\newblock 2017.
\newblock Thinking fast and slow with deep learning and tree search.
\newblock In {\em NIPS},  5360--5370.

\bibitem[\protect\citeauthoryear{Arulkumaran \bgroup et al\mbox.\egroup
  }{2017}]{arulkumaran2017deep}
Arulkumaran, K.; Deisenroth, M.~P.; Brundage, M.; and Bharath, A.~A.
\newblock 2017.
\newblock Deep reinforcement learning: A brief survey.
\newblock {\em IEEE Signal Processing Magazine} 34(6):26--38.

\bibitem[\protect\citeauthoryear{Auer, Cesa-Bianchi, and
  Fischer}{2002}]{auer2002finite}
Auer, P.; Cesa-Bianchi, N.; and Fischer, P.
\newblock 2002.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock {\em Machine Learning} 47(2-3):235--256.

\bibitem[\protect\citeauthoryear{Baier and Winands}{2018}]{baier2018mcts}
Baier, H., and Winands, M.~H.
\newblock 2018.
\newblock {MCTS-minimax hybrids with state evaluations}.
\newblock {\em JMLR} 62:193--231.

\bibitem[\protect\citeauthoryear{Bejjani \bgroup et al\mbox.\egroup
  }{2018}]{bejjani2018planning}
Bejjani, W.; Papallas, R.; Leonetti, M.; and Dogar, M.~R.
\newblock 2018.
\newblock Planning with a receding horizon for manipulation in clutter using a
  learned value function.
\newblock In {\em 2018 IEEE-RAS 18th International Conference on Humanoid
  Robots (Humanoids)},  1--9.

\bibitem[\protect\citeauthoryear{Best \bgroup et al\mbox.\egroup
  }{2019}]{best2019dec}
Best, G.; Cliff, O.~M.; Patten, T.; Mettu, R.~R.; and Fitch, R.
\newblock 2019.
\newblock Dec-mcts: Decentralized planning for multi-robot active perception.
\newblock {\em The International Journal of Robotics Research}
  38(2-3):316--337.

\bibitem[\protect\citeauthoryear{Borovikov \bgroup et al\mbox.\egroup
  }{2019}]{borovikov2019winning}
Borovikov, I.; Zhao, Y.; Beirami, A.; Harder, J.; Kolen, J.; Pestrak, J.;
  Pinto, J.; Pourabolghasem, R.; Chaput, H.; Sardari, M.; et~al.
\newblock 2019.
\newblock Winning isnâ€™t everything: Training agents to playtest modern games.
\newblock In {\em AAAI Workshop on Reinforcement Learning in Games}.

\bibitem[\protect\citeauthoryear{Bouzy}{2005}]{bouzy2005associating}
Bouzy, B.
\newblock 2005.
\newblock Associating domain-dependent knowledge and {Monte Carlo} approaches
  within a {Go} program.
\newblock {\em Information Sciences} 175(4):247--257.

\bibitem[\protect\citeauthoryear{Browne \bgroup et al\mbox.\egroup
  }{2012}]{browne2012survey}
Browne, C.~B.; Powley, E.; Whitehouse, D.; Lucas, S.~M.; Cowling, P.~I.;
  Rohlfshagen, P.; Tavener, S.; Perez, D.; Samothrakis, S.; and Colton, S.
\newblock 2012.
\newblock {A survey of Monte Carlo tree search methods}.
\newblock {\em IEEE Transactions on Computational Intelligence and AI in games}
  4(1):1--43.

\bibitem[\protect\citeauthoryear{Coulom}{2006}]{coulom2006efficient}
Coulom, R.
\newblock 2006.
\newblock Efficient selectivity and backup operators in {Monte-Carlo} tree
  search.
\newblock In van~den Herik, H.~J.; Ciancarini, P.; and Donkers, J., eds., {\em
  Computers and Games}, volume 4630 of LNCS. Springer.
\newblock  72--83.

\bibitem[\protect\citeauthoryear{Garc{\i}a and
  Fern{\'a}ndez}{2015}]{garcia2015comprehensive}
Garc{\i}a, J., and Fern{\'a}ndez, F.
\newblock 2015.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock {\em JMLR} 16(1).

\bibitem[\protect\citeauthoryear{Guo \bgroup et al\mbox.\egroup
  }{2014}]{guo2014deep}
Guo, X.; Singh, S.; Lee, H.; Lewis, R.~L.; and Wang, X.
\newblock 2014.
\newblock Deep learning for real-time atari game play using offline
  {M}onte-{C}arlo tree search planning.
\newblock In {\em NIPS},  3338--3346.

\bibitem[\protect\citeauthoryear{Hernandez-Leal, Kartal, and
  Taylor}{2018}]{hernandez2018multiagent}
Hernandez-Leal, P.; Kartal, B.; and Taylor, M.~E.
\newblock 2018.
\newblock {Is multiagent deep reinforcement learning the answer or the
  question? A brief survey}.
\newblock {\em arXiv preprint arXiv:1810.05587}.

\bibitem[\protect\citeauthoryear{Hester \bgroup et al\mbox.\egroup
  }{2017}]{hester2017deep}
Hester, T.; Vecerik, M.; Pietquin, O.; Lanctot, M.; Schaul, T.; Piot, B.;
  Horgan, D.; Quan, J.; Sendonaris, A.; Dulac-Arnold, G.; et~al.
\newblock 2017.
\newblock {Deep Q-learning from Demonstrations}.
\newblock {\em arXiv preprint arXiv:1704.03732}.

\bibitem[\protect\citeauthoryear{Holmg{\aa}rd \bgroup et al\mbox.\egroup
  }{2018}]{holmgaard2018automated}
Holmg{\aa}rd, C.; Green, M.~C.; Liapis, A.; and Togelius, J.
\newblock 2018.
\newblock Automated playtesting with procedural personas through {MCTS} with
  evolved heuristics.
\newblock {\em arXiv preprint arXiv:1802.06881}.

\bibitem[\protect\citeauthoryear{Ilhan and Etaner-Uyar}{2017}]{ilhan2017monte}
Ilhan, E., and Etaner-Uyar, A.~{\c{S}}.
\newblock 2017.
\newblock Monte carlo tree search with temporal-difference learning for general
  video game playing.
\newblock In {\em 2017 IEEE Conference on Computational Intelligence and Games
  (CIG)},  317--324.

\bibitem[\protect\citeauthoryear{Jaderberg \bgroup et al\mbox.\egroup
  }{2016}]{jaderberg2016reinforcement}
Jaderberg, M.; Mnih, V.; Czarnecki, W.~M.; Schaul, T.; Leibo, J.~Z.; Silver,
  D.; and Kavukcuoglu, K.
\newblock 2016.
\newblock Reinforcement learning with unsupervised auxiliary tasks.
\newblock {\em arXiv preprint arXiv:1611.05397}.

\bibitem[\protect\citeauthoryear{Kartal \bgroup et al\mbox.\egroup
  }{2015}]{kartal2015stochastic}
Kartal, B.; Godoy, J.; Karamouzas, I.; and Guy, S.~J.
\newblock 2015.
\newblock Stochastic tree search with useful cycles for patrolling problems.
\newblock In {\em 2015 IEEE International Conference on Robotics and Automation
  (ICRA)},  1289--1294.

\bibitem[\protect\citeauthoryear{Kartal, Koenig, and
  Guy}{2014}]{kartal2014user}
Kartal, B.; Koenig, J.; and Guy, S.~J.
\newblock 2014.
\newblock User-driven narrative variation in large story domains using {M}onte
  {C}arlo tree search.
\newblock In {\em AAMAS},  69--76.

\bibitem[\protect\citeauthoryear{Kim \bgroup et al\mbox.\egroup
  }{2013}]{kim2013learning}
Kim, B.; Farahmand, A.-m.; Pineau, J.; and Precup, D.
\newblock 2013.
\newblock Learning from limited demonstrations.
\newblock In {\em NIPS},  2859--2867.

\bibitem[\protect\citeauthoryear{Kocsis and
  Szepesv{\'a}ri}{2006}]{kocsis2006bandit}
Kocsis, L., and Szepesv{\'a}ri, C.
\newblock 2006.
\newblock {Bandit based Monte-Carlo planning}.
\newblock In {\em Machine Learning: ECML 2006}.
\newblock  282--293.

\bibitem[\protect\citeauthoryear{LeCun, Bengio, and
  Hinton}{2015}]{lecun2015deep}
LeCun, Y.; Bengio, Y.; and Hinton, G.
\newblock 2015.
\newblock Deep learning.
\newblock {\em Nature} 521(7553):436.

\bibitem[\protect\citeauthoryear{Lee \bgroup et al\mbox.\egroup
  }{2019}]{lee2019wisemove}
Lee, J.; Balakrishnan, A.; Gaurav, A.; Czarnecki, K.; and Sedwards, S.
\newblock 2019.
\newblock Wisemove: A framework for safe deep reinforcement learning for
  autonomous driving.
\newblock {\em arXiv preprint arXiv:1902.04118}.

\bibitem[\protect\citeauthoryear{Leonetti, Iocchi, and
  Stone}{2016}]{leonetti2016synthesis}
Leonetti, M.; Iocchi, L.; and Stone, P.
\newblock 2016.
\newblock A synthesis of automated planning and reinforcement learning for
  efficient, robust decision-making.
\newblock {\em Artificial Intelligence} 241:103--130.

\bibitem[\protect\citeauthoryear{Li}{2017}]{li2017deep}
Li, Y.
\newblock 2017.
\newblock Deep reinforcement learning: An overview.
\newblock {\em arXiv preprint arXiv:1701.07274}.

\bibitem[\protect\citeauthoryear{Matiisen}{2018}]{matiisen2018pommerman}
Matiisen, T.
\newblock 2018.
\newblock Pommerman baselines.
\newblock \url{https://github.com/tambetm/pommerman-baselines}.

\bibitem[\protect\citeauthoryear{Mnih \bgroup et al\mbox.\egroup
  }{2015}]{mnih2015human}
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A.~A.; Veness, J.; Bellemare,
  M.~G.; Graves, A.; Riedmiller, M.; Fidjeland, A.~K.; Ostrovski, G.; et~al.
\newblock 2015.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature} 518(7540):529.

\bibitem[\protect\citeauthoryear{Mnih \bgroup et al\mbox.\egroup
  }{2016}]{mnih2016asynchronous}
Mnih, V.; Badia, A.~P.; Mirza, M.; Graves, A.; Lillicrap, T.; Harley, T.;
  Silver, D.; and Kavukcuoglu, K.
\newblock 2016.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em ICML},  1928--1937.

\bibitem[\protect\citeauthoryear{OpenAI}{2018}]{openfive}
OpenAI.
\newblock 2018.
\newblock {OpenAI Five}.
\newblock \url{https://blog.openai.com/openai-five/}.
\newblock [Online; accessed 15-October-2018].

\bibitem[\protect\citeauthoryear{Resnick \bgroup et al\mbox.\egroup
  }{2018}]{resnick2018pommerman}
Resnick, C.; Eldridge, W.; Ha, D.; Britz, D.; Foerster, J.; Togelius, J.; Cho,
  K.; and Bruna, J.
\newblock 2018.
\newblock Pommerman: A multi-agent playground.
\newblock {\em arXiv preprint arXiv:1809.07124}.

\bibitem[\protect\citeauthoryear{Ross, Gordon, and
  Bagnell}{2011}]{ross2011reduction}
Ross, S.; Gordon, G.; and Bagnell, D.
\newblock 2011.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In {\em Proceedings of the fourteenth international conference on
  artificial intelligence and statistics},  627--635.

\bibitem[\protect\citeauthoryear{Silver \bgroup et al\mbox.\egroup
  }{2016}]{silver2016mastering}
Silver, D.; Huang, A.; Maddison, C.~J.; et~al.
\newblock 2016.
\newblock Mastering the game of {Go} with deep neural networks and tree search.
\newblock {\em Nature} 529(7587):484--489.

\bibitem[\protect\citeauthoryear{Silver \bgroup et al\mbox.\egroup
  }{2017}]{silver2017mastering}
Silver, D.; Schrittwieser, J.; Simonyan, K.; Antonoglou, I.; Huang, A.; Guez,
  A.; Hubert, T.; Baker, L.; Lai, M.; Bolton, A.; et~al.
\newblock 2017.
\newblock Mastering the game of go without human knowledge.
\newblock {\em Nature} 550(7676):354.

\bibitem[\protect\citeauthoryear{Soemers \bgroup et al\mbox.\egroup
  }{2016}]{soemers2016enhancements}
Soemers, D.~J.; Sironi, C.~F.; Schuster, T.; and Winands, M.~H.
\newblock 2016.
\newblock Enhancements for real-time {Monte Carlo} tree search in general video
  game playing.
\newblock In {\em 2016 IEEE Conference on Computational Intelligence and Games
  (CIG)},  1--8.

\bibitem[\protect\citeauthoryear{Sturtevant}{2015}]{sturtevant2015monte}
Sturtevant, N.~R.
\newblock 2015.
\newblock Monte {C}arlo tree search and related algorithms for games.
\newblock {\em Game AI Pro 2: Collected Wisdom of Game AI Professionals}.

\bibitem[\protect\citeauthoryear{Vodopivec, Samothrakis, and
  Ster}{2017}]{vodopivec2017monte}
Vodopivec, T.; Samothrakis, S.; and Ster, B.
\newblock 2017.
\newblock On {Monte Carlo} tree search and reinforcement learning.
\newblock {\em JAIR} 60:881--936.

\bibitem[\protect\citeauthoryear{Yu}{2018}]{yu2018towards}
Yu, Y.
\newblock 2018.
\newblock Towards sample efficient reinforcement learning.
\newblock In {\em IJCAI},  5739--5743.

\bibitem[\protect\citeauthoryear{Zook, Harrison, and
  Riedl}{2015}]{zook2015monte}
Zook, A.; Harrison, B.; and Riedl, M.~O.
\newblock 2015.
\newblock Monte-{C}arlo tree search for simulation-based strategy analysis.
\newblock In {\em Proceedings of the 10th Conference on the Foundations of
  Digital Games}.

\end{thebibliography}
