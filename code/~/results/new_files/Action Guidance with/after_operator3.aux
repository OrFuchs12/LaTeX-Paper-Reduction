\relax 
\citation{arulkumaran2017deep}
\citation{li2017deep}
\citation{hernandez2018multiagent}
\citation{mnih2015human}
\citation{silver2016mastering}
\citation{openfive}
\citation{yu2018towards}
\citation{browne2012survey}
\citation{silver2017mastering}
\citation{anthony2017thinking}
\citation{guo2014deep}
\citation{anthony2017thinking}
\citation{guo2014deep}
\citation{jaderberg2016reinforcement}
\citation{resnick2018pommerman}
\citation{mnih2016asynchronous}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pom8x8}{{1}{1}{An example of the $8 \times 8$ Mini-Pommerman board, randomly generated by the simulator. Agents' initial positions are randomized among four corners at each episode.}{}{}}
\citation{garcia2015comprehensive}
\citation{ross2011reduction}
\citation{anthony2017thinking}
\citation{silver2017mastering}
\citation{hester2017deep}
\citation{hester2017deep}
\citation{kim2013learning}
\citation{bejjani2018planning}
\citation{leonetti2016synthesis}
\citation{lee2019wisemove}
\citation{vodopivec2017monte}
\citation{silver2016mastering}
\citation{lecun2015deep}
\citation{mnih2016asynchronous}
\citation{jaderberg2016reinforcement}
\citation{coulom2006efficient}
\citation{sturtevant2015monte}
\citation{silver2016mastering}
\citation{zook2015monte}
\citation{holmgaard2018automated}
\citation{borovikov2019winning}
\citation{kartal2015stochastic}
\citation{best2019dec}
\citation{auer2002finite}
\citation{kocsis2006bandit}
\citation{guo2014deep}
\citation{matiisen2018pommerman}
\citation{guo2014deep}
\citation{anthony2017thinking}
\newlabel{eqn:ucb}{{1}{3}{}{}{}}
\citation{resnick2018pommerman}
\newlabel{fig:pi_a3c}{{2}{4}{\textbf  {a)} In the A3C framework, each worker independently interacts with the environment and computes gradients. Then, each worker \emph  {asynchronously} passes the gradients to the global neural network which updates parameters and synchronizes with the respective worker. \textbf  {b)} In Planner Imitation based A3C (PI-A3C), $k\ge 1$ CPU workers are assigned as demonstrators taking MCTS actions, while keeping track of what action its actor network would take. The demonstrator workers have an additional auxiliary supervised loss. PI-A3C enables the network to simultaneously optimize the policy and learn to imitate MCTS.}{}{}}
\newlabel{sec:results}{{}{4}{}{}{}}
\citation{kartal2014user}
\citation{bouzy2005associating}
\citation{ilhan2017monte}
\newlabel{fig:ablate_planner}{{3}{5}{ a) Against Static agent, all variants have been trained for 12 hours. The ${\text  {PI-A3C}}$ framework using MCTS demonstrator with 75 and 150 rollouts learns significantly faster compared to the standard A3C. b) Against Rule-based opponent, all variants have been trained for 3 days. Against this more skilled opponent, PI-A3C provides significant speed up in learning performance, and finds better best response policies. c) Against Rule-based opponent, employing different number $(n=1,3,6)$ of Demonstrators, i.e. increasing from 1 to 3, slightly improved the results, however, there is almost no variation from 3 to 6 demonstrators. For both (a) and (b), increasing the expertise level of MCTS through doubling the number of rollouts (from 75 to 150) does not yield improvement, and can even hurt performance. Our hypothesis is that slower planning decreases the number of demonstrator actions too much for the model-free RL workers to learn to imitate for safe exploration.}{}{}}
\newlabel{tab:table_mcts}{{1}{5}{Vanilla MCTS-based demonstrator evaluation: average episodic rewards of 200 games. Note that mean rewards are affected by too many tie games (rewarded -1 by the Pommerman simulator), e.g. against Static opponent, MCTS wins 88 games, loses 4 games by suicide, and ties 108 games, which results in average reward of -0.12.}{}{}}
\citation{soemers2016enhancements}
\citation{baier2018mcts}
\citation{resnick2018pommerman}
\citation{browne2012survey}
\bibstyle{aaai}
\newlabel{fig:simple_rollout_biasing}{{4}{6}{Learning against Rule-based opponent: using the policy head during MCTS rollout phase within the demonstrator provides improvement in learning speed, but it has higher variance compared to the default random policy.}{}{}}
\bibdata{ref}
\gdef \@abspage@last{7}
