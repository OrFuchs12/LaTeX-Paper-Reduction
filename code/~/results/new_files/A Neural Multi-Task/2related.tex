\textbf{MER and MEN.} 
Several existing studies typically run a medical named entity recognition model to extract entity names first, then run a medical named entity normalization model to link extracted names to a controlled vocabulary~\cite{Doan2010,Sahu2016}.
Such decoupled approaches used pipeline models to implement MER and MEN separately, leading to errors cascade and absence of mutual benefits. 
There has been a line of research on joint modeling MER and MEN, which has demonstrated the superiority over pipeline implementation. 
For example, semi-CRF has been used for joint entity recognition and disambiguation \cite{Luo2015Joint}. \citeauthor{Leaman2016TaggerOne} (\citeyear{Leaman2016TaggerOne}) leverage a joint scoring function for MER and MEN. \citeauthor{Leaman2015tmChem} (\citeyear{Leaman2015tmChem}) developed a chemical named entity recognizer and normalizer created by combining two independent machine learning models in an ensemble. \citeauthor{Lou2017A} (\citeyear{Lou2017A}) propose a transition-based model to jointly perform disease named entity recognition and normalization. 

\textbf{Methodology of NER.}
Traditional approaches to NER include handcrafted features for Maximum Entropy models~\cite{Curran-Clark:2003:CONLL}, Conditional Random
Fields~\cite{McCallum-Li:2003:CONLL}, and Hidden Markov Models~\cite{Klein-etAl:2003:CONLL}.
State-of-the-art neural NER techniques use a combination of pre-trained word embeddings and character embeddings derived from a convolutional neural network (CNN) layer or bidirectional long short-term memory (Bi-LSTM) layer. These features are passed to a Bi-LSTM layer, which may be followed by a CRF layer~\cite{lample-EtAl:2016:N16-1,MaH16,TACL792}. \citeauthor{strubell-EtAl:2017:EMNLP2017} (\citeyear{strubell-EtAl:2017:EMNLP2017}) proposed a faster alternative
to Bi-LSTMs for NER: Iterated Dilated Convolutional Neural Networks (ID-CNNs), which have better capacity than traditional CNNs for large context and structured prediction.

\textbf{Neural Multi-Task Learning.} 
Multi-Task Learning  is a learning paradigm in machine learning and its aim is to leverage useful information
contained in multiple related tasks to help improve the generalization performance of all the tasks. It has been used successfully across many tasks of NLP \cite{Collobert2011}. 
In the context of deep learning for NLP, the most notable work was proposed by \citeauthor{CollobertW08} (\citeyear{CollobertW08}), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings.
In recent years, the idea of neural deep multi-task learning becomes popular to sequence-to-sequence problems with LSTM~\cite{DongWHYW15,luong2016iclr_multi,Liuyang2016,AugensteinS17}. There are also a few studies which make use of multi-task learning for biomedical named entity recognition, such as cross-type biomedical named entity recognition~\cite{DBLP:journals/corr/abs-1801-09851} and multiple independent tasks modeling with MER involved~\cite{Crichton2017A}. 

