\section{Proposed Method}

In this section, we introduce thoroughly our proposed Graph-based Relevance Matching Model (GRMM). We first formulate the problem and demonstrate how to construct the graph-of-word formation from the query and document, and then describe the graph-based matching method in details. Figure \ref{fig:2} illustrates the overall process of our proposed architecture.

\subsection{Problem Statement}
Given a query $q$ and a document $d$, they are represented as a sequence of  words 
$q=\left[w_{1}^{(q)}, \ldots, w_{M}^{(q)}\right]$  and $d=\left[w_{1}^{(d)}, \ldots, w_{N}^{(d)}\right]$, where $w_{i}^{(q)}$ denotes the $i$-th word in the query, $w_{i}^{(d)}$ denotes the $i$-th word in the document, $M$ and $N$ denote the length of the query and the document respectively.
The aim is to compute a relevance score $rel(q,d)$ regarding the query words and the document words.


\subsection{Graph Construction}
\label{sec:graphconstruct}
To leverage the long-distance term dependency information, the first step is to construct a graph $\mathcal{G}$ for the document. It typically consists of two components denoted as $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, 
where $\mathcal{V}$ is the set of vertexes with \emph{node features}, and $\mathcal{E}$ is the set of edges as the \emph{topological structure}.

\subsubsection{Node features.}
We represent each unique word instead of sentence or paragraph in the document as a node. Thus the word sequence is squeezed to a node set $\left\{w_{1}^{(d)}, \ldots, w_{n}^{(d)}\right\}$, where $n$ is the number of unique words in the document ($|\mathcal{V}| = n  \leq N$). Each node feature is set the interaction signal between its word embedding and query term embeddings. We simply employ the cosine similarity matrix as the interaction matrix, denoted as $\mathbf{S} \in \mathbb{R}^{n \times M}$, where each element $\mathbf{S}_{ij}$ between document node $w^{(d)}_i$ and query term $w^{(q)}_j$ is defined as:
\begin{equation}\mathbf{S}_{i j}=cosine\left(\mathbf{e}_i^{(d)}, \mathbf{e}_j^{(q)}\right)
\end{equation}
where $\mathbf{e}_{i}^{(d)}$ and $\mathbf{e}_{j}^{(q)} $ are embedding vectors for $w_{i}^{(d)}$ and $w_{j}^{(q)}$ respectively. In this work, we use word2vec \cite{mikolov2013distributed} technique to convert words into dense and semantic embedding vectors.

\subsubsection{Topological structure.}
In addition to the node feature matrix, the adjacency matrix representing the topological structure constitutes for the graph as well. The structure generally describes the connection between the nodes and reveals their relationships. We build bi-directional connections for each pair of word nodes that co-occur within a sliding window, along with the original document word sequence $d$. By restricting the size of the window, every word can connect with their neighbourhood words which may share related contextual meanings. However, GRMM differs from those local relevance matching methods in that the combined word node can bridge all neighbourhoods together and therefore possess a document-level receptive field. In other words, it breaks the constraints of local context and can model the long-distance word dependencies that we concern. Note that in the worst case where there are no duplicate words, the graph would still perform as a sequential and local scheme. 

Formally, the adjacency matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ is defined as:
\begin{equation}
\mathbf{A}_{i j}=\left\{\begin{array}{ll}
count(i, j) & \text{if } i \not= j \\
0 & \text{otherwise}
\end{array}\right.
\end{equation}
where $count(i, j)$ is the number of times that the words $w_{i}^{(d)}$ and $w_{j}^{(d)}$ appear in the same sliding window. To alleviate the exploding/vanishing gradient problem \cite{kipf2017semi}, we normalise the adjacency matrix as $\tilde{\mathbf{A}} = \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}$, where $\mathbf{D} \in \mathbb{R}^{n \times n}$ is the diagonal degree matrix and $\mathbf{D}_{ii} = \sum_j \mathbf{A}_{ij}$.



\subsection{Graph-based Matching}
Once we obtain the graph $\mathcal{G}$, we focus on making use of its node features and structure information with graph neural networks. In particular, the query-document interaction and the intra-document word interaction are learned mutually following the procedures - \emph{neighbourhood aggregation}, \emph{state update} and \emph{feature election}. 

\subsubsection{Neighbourhood Aggregation.}
As discussed in Section \ref{sec:graphconstruct}, we initialise the node state $\mathbf{h}^0_i$ with the query-document interaction matrix:
\begin{equation}\mathbf{h}^0_i =  \mathbf{S}_{i,:}
\end{equation}
where $\forall i\in [1, n]$ denotes the $i$-th node in the graph, and $\mathbf{S}_{i,:}$ is the $i$-th row of the interaction matrix $\mathbf{S}$.

Assume each word node either holds the core information or serves as a bridge connecting others, it is necessary to make the information flow and enrich the related fractions on the graph.
Through propagating the state representations to a node from its neighbours, it can receive the contextual information within the first-order connectivity as:
\begin{equation}\mathbf{a}_{i}^{t}=\sum_{(w_{i}, w_{j}) \in \mathcal{E}} \mathbf{\tilde{A}}_{ij} \mathbf{W}_{a} \mathbf{h}_{j}^{t}\end{equation}
where $\mathbf{a}_i^t \in \mathbb{R}^{M}$ denotes the summed message from neighbours, $t$ denotes the current timestamp, and $\mathbf{W}_a$ is a trainable transformation matrix to project features into a new relation space. When aggregate $t$ times recursively, a node can receive the information propagated from its $t$-hop neighbours. In this way, the model can achieve \emph{high-order aggregation} of the query-document interaction as well as the intra-document interaction.

\subsubsection{State Update.}
To incorporate the contextual information into the word nodes, we engage a GRU-like function \cite{li2016gated} to automatically adjust the merge proportion of its current representation $\mathbf{h}^{t}_i$ and the received representation $\mathbf{a}^{t}_i$, which is formulated as:
\begin{equation}\begin{array}{l}
\mathbf{z}_{i}^{t}=\sigma\left(\mathbf{W}_{z} \mathbf{a}_{i}^{t}+\mathbf{U}_{z} \mathbf{h}_{i}^{t}+\mathbf{b}_{z}\right)
\end{array}\end{equation}
\begin{equation}
\mathbf{r}_{i}^{t}=\sigma\left(\mathbf{W}_{r} \mathbf{a}_{i}^{t}+\mathbf{U}_{r} \mathbf{h}_{i}^{t}+\mathbf{b}_{r}\right)
\end{equation}
\begin{equation}\tilde{\mathbf{h}}_{i}^{t}=\tanh \left(\mathbf{W}_{h} \mathbf{a}_{i}^{t}+\mathbf{U}_{h}\left(\mathbf{r}_{i}^{t} \odot \mathbf{h}_{i}^{t}\right)+\mathbf{b}_{h}\right)\end{equation}
\begin{equation}\mathbf{h}_{i}^{t+1}=\tilde{\mathbf{h}}_{i}^{t} \odot \mathbf{z}_{i}^{t}+\mathbf{h}_{i}^{t} \odot\left(1-\mathbf{z}_{i}^{t}\right)\end{equation}
where $\sigma(\cdot)$ is the sigmoid function, $\odot$ is the Hardamard product operation, tanh$(\cdot)$ is the non-linear tangent hyperbolic activation function, and all $\mathbf{W_*, U_*}$ and $\mathbf{b_*}$ are trainable weights and biases. 

Specifically, $\mathbf{r}^{t}_i$ determines irrelevant information for hidden state $\tilde{\mathbf{h}}^{t}_i$ to forget (reset gate), while $\mathbf{z}^{t}_i$ determines which part of past information to discard and which to push forward (update gate). With the layer $t$ going deep, high-order information becomes complicated, and it is necessary to identify useful dependencies with the two gates. We have also tried plain updater such as GCN \cite{kipf2017semi} in our experiments but did not observe satisfying performance due to its simplicity.


\subsubsection{Graph Readout.}
The last phase involves locating the position where relevance matching happens as a delegate for the entire graph. Since it is suggested that not all words make contributions, and some may cause adverse influences \cite{guo2016deep}, here we only select the most informative features to represent the query-document matching signals. Intuitively, higher similarity means higher relevance possibility. Hence we perform a $k$-max-pooling strategy over the query dimension and select the top $k$ signals for each query term, which also prevents the model from being biased by the document length. The formulas are expressed as:
\begin{equation}\mathbf{H}=\mathbf{h}_{1}^{t} \parallel \mathbf{h}_{2}^{t} \parallel \ldots \parallel \mathbf{h}_{n}^{t}\end{equation}
\begin{equation}
\mathbf{x}_{j} = {topk}(\mathbf{H}_{:,j})
\end{equation}
where $\forall j\in [1, M]$ denotes the $j$-th query term, and $\mathbf{H}_{:,j}$ is the $j$-th column of the feature matrix $\mathbf{H}$.

\subsection{Matching Score and Training}
After obtaining low-dimensional and informative matching features $\mathbf{x}_j$, we move towards converting them into actual relevance scores for training and inference. Considering different terms may have different importances \cite{guo2016deep}, we assign each with a soft gating network as:
\begin{equation}g_{j}=\frac{\exp \left({c} \cdot idf_j \right)}{\sum_{j=1}^{M} \exp \left({c} \cdot idf_j \right)}\end{equation}
where $g_j$ denotes the term weight, $idf_j$ is the inverse document frequency of the $j$-th query term, and $c$ is a trainable parameter. To reduce the amount of parameters and avoid over-fitting, we score each query term with a weight-shared multi-layer perceptron (MLP) and sum them up as the final result:
\begin{equation}{rel}(q, d)=\sum_{j=1}^{M} g_j \cdot \tanh \left(\mathbf{W}_x \mathbf{x}_{j}+{b}_x \right)\end{equation}
where $\mathbf{W}_x, b_x$ are trainable parameters for MLP.

Finally, we adopt the pairwise hinge loss which is commonly used in information retrieval to optimise the model parameters:
\begin{small}
	\begin{equation}\mathcal{L}\left(q, d^{+}, d^{-}\right)=\max \left(0, 1-rel\left(q, d^{+}\right)+rel\left(q, d^{-}\right)\right)\end{equation} 
\end{small}
where $\mathcal{L}\left(q, d^{+}, d^{-}\right)$ denotes the pairwise loss based on a triplet of the query $q$, a relevant (positive) document sample $d^+$, and an irrelevant (negative) document sample $d^-$.

