@misc{Wyman:22,
  title        = "After COVID-19, Aviation Faces a Pilot Shortage",
  author       = "{Oliver Wyman}",
  howpublished = "\url{https://www.oliverwyman.com/our-expertise/insights/2021/mar/after-covid-19-aviation-faces-a-pilot-shortage.html}",
  year         = 2022,
  note         = "Accessed: 2022-09-15"
}

@inproceedings{cui:21,
  title={Understanding the Relationship between Interactions and Outcomes in Human-in-the-Loop Machine Learning.},
  author={Cui, Yuchen and Koppol, Pallavi and Admoni, Henny and Niekum, Scott and Simmons, Reid G and Steinfeld, Aaron and Fitzgerald, Tesca},
  booktitle={IJCAI},
  pages={4382--4391},
  year={2021}
}

@article{shute:08,
  title={Focus on formative feedback},
  author={Shute, Valerie J},
  journal={Review of educational research},
  volume={78},
  number={1},
  pages={153--189},
  year={2008},
  publisher={Sage Publications}
}

@misc{OpenAIFive,
 title={Open {AI} {F}ive},
 year = {2018}, 
 howpublished = {https://blog.openai.com/openai-five},
 note = {Online; accessed 7-September-2018}
 }

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={Proceedings of the International Conference on Machine Learning (ICML)},
  year={1999}
}

@inproceedings{wiewiora2003principled,
  title={Principled methods for advising reinforcement learning agents},
  author={Wiewiora, Eric and Cottrell, Garrison W and Elkan, Charles},
  booktitle={Proceedings of the 20th International Conference on Machine Learning (ICML-03)},
  pages={792--799},
  year={2003}
}

@inproceedings{brys2015reinforcement,
  title={Reinforcement learning from demonstration through shaping},
  author={Brys\bftext{*}, Tim and Harutyunyan, Anna and Suay, Halit Bener and Chernova, Sonia and Taylor, Matthew E and Now{\'e}, Ann},
  booktitle={Twenty-fourth international joint conference on artificial intelligence},
  year={2015}
}

@inproceedings{knox2010combining,
  title={Combining manual feedback with subsequent MDP reward signals for reinforcement learning},
  author={Knox, W Bradley and Stone, Peter},
  booktitle={Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: volume 1-Volume 1},
  pages={5--12},
  year={2010},
  organization={Citeseer}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@article{zhang2020teaching,
  title={The Teaching Dimension of {Q-}learning},
  author={Zhang, Xuezhou and Bharti, Shubham Kumar and Ma, Yuzhe and Singla, Adish and Zhu, Xiaojin},
  journal={arXiv preprint arXiv:2006.09324},
  year={2020}
}

@article{bignold2020conceptual,
  title={A Conceptual Framework for Externally-influenced Agents: An Assisted Reinforcement Learning Review},
  author={Bignold, Adam and Cruz, Francisco and Taylor, Matthew E and Brys, Tim and Dazeley, Richard and Vamplew, Peter and Foale, Cameron},
  journal={arXiv preprint arXiv:2007.01544},
  year={2020}
}

@article{thomaz2008teachable,
  title={Teachable robots: Understanding human teaching behavior to build more effective robot learners},
  author={Thomaz, Andrea L and Breazeal, Cynthia},
  journal={Artificial Intelligence},
  volume={172},
  number={6-7},
  pages={716--737},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{HAT,
  author = {\textbf{M.~E.~Taylor} and Suay, Halit Bener and Chernova, Sonia},
  title = {{Integrating Reinforcement Learning with Human Demonstrations of Varying Ability}},
  booktitle = {Proceedings of the International Conference on Autonomous Agents and Mulit Agent Systems (AAMAS)},
  month = may,
  year = {2011},
  month_numeric = {5}
}

@article{2018curriculadesign,
  author = {Peng\textbf{*}, Bei and MacGlashan, James and Loftin, Robert and Littman, Michael L. and Roberts, David L. and Taylor, Matthew E.},
  title = {Curriculum Design for Machine Learners in Sequential Decision Tasks},
  journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  year = {2018},
  volume = {2},
  issue = {4},
  pages = {268--277},
  doi = {10.1109/TETCI.2018.2829980}
}

@misc{2020ala-paniz,
  author = {Behboudian\textbf{*}, Paniz and Satsangi, Yash and Taylor, Matthew E. and Harutyunyan, Anna and Bowling, Michael},
  title = {Useful Policy Invariant Shaping from Arbitrary Advice},
  booktitle = {Proceedings of the Adaptive and Learning Agents Workshop at the AAMAS-20 conference},
  month = may,
  year = {2020},
  month_numeric = {5}
}

@article{rosenfeld_cohen_taylor_kraus_2018,
  title = {Leveraging human knowledge in tabular reinforcement learning: a study of human subjects},
  volume = {33},
  doi = {10.1017/S0269888918000206},
  journal = {The Knowledge Engineering Review},
  publisher = {Cambridge University Press},
  author = {Rosenfeld, Ariel and Cohen, Moshe and Taylor, Matthew E. and Kraus, Sarit},
  year = {2018}
}


@article{LfD,
author = {Argall, Brenna D. and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
title = {A Survey of Robot Learning from Demonstration},
year = {2009},
issue_date = {May, 2009},
publisher = {North-Holland Publishing Co.},
address = {NLD},
volume = {57},
number = {5},
issn = {0921-8890},
url = {https://doi.org/10.1016/j.robot.2008.10.024},
doi = {10.1016/j.robot.2008.10.024},
abstract = {We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research.},
journal = {Robotics and Autonomous Systems},
month = may,
pages = {469–483},
numpages = {15},
keywords = {Learning from demonstration, Machine learning, Robotics, Autonomous systems}
}

@inproceedings{DQFD,
  author    = {Todd Hester and
               Matej Vecer{\'{\i}}k and
               Olivier Pietquin and
               Marc Lanctot and
               Tom Schaul and
               Bilal Piot and
               Dan Horgan and
               John Quan and
               Andrew Sendonaris and
               Ian Osband and
               Gabriel Dulac{-}Arnold and
               John P. Agapiou and
               Joel Z. Leibo and
               Audrunas Gruslys},
  title     = {Deep {Q-}learning From Demonstrations},
  booktitle = {Proceedings of  AAAI Conference on Artificial Intelligence},
  year      = {2018}
}

@article{gabe_du_taylor_2019,
  title = {Pre-training with non-expert human demonstration for deep reinforcement learning},
  volume = {34},
  doi = {10.1017/S0269888919000055},
  journal = {The Knowledge Engineering Review},
  publisher = {Cambridge University Press},
  author = {de la Cruz Jr.\textbf{*}, Gabriel V. and Du\textbf{*}, Yunshu and Taylor, Matthew E.},
  year = {2019}
}

@inproceedings{2019IJCAI-wang,
  author = {Wang\textbf{*}, Zhaodong and Taylor, Matthew E.},
  title = {Interactive Reinforcement Learning with Dynamic Reuse of Prior Knowledge from Human and Agent Demonstrations},
  booktitle = {Proceedings of  International Joint Conference on Artificial Intelligence},
  year = {2019}
}

@article{2019sociology,
  title = {It’s What You Call It: Gendered Framing and Women’s and Men’s Interest in a Robotics Instruction Task},
  author = {Morton, Sarah and Kmec, Julie and Taylor, Matthew E.},
  journal = {Int. J. of Gender, Science and Technology},
  volume = {11},
  number = {2},
  year = {2019}
}

@article{SuayLfD,
  author    = {Halit Bener Suay and
               Russell Toris and
               Sonia Chernova},
  title     = {A Practical Comparison of Three Robot Learning from Demonstration
               Algorithm},
  journal   = {International Journal of Social Robotics},
  volume    = {4},
  number    = {4},
  pages     = {319--330},
  year      = {2012},
  url       = {https://doi.org/10.1007/s12369-012-0158-7},
  doi       = {10.1007/s12369-012-0158-7},
  timestamp = {Sat, 25 Apr 2020 13:55:29 +0200},
  biburl    = {https://dblp.org/rec/journals/ijsr/SuayTC12.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{NASATLX,
  added-at = {2013-04-22T17:33:29.000+0200},
  author = {Hart, Sandra G and Staveland, Lowell E},
  biburl = {https://www.bibsonomy.org/bibtex/277872d08d9d519aa9764e8534a03d653/rnesselrath},
  interhash = {de54f7e65db49d4f80d32d6c89a8e006},
  intrahash = {77872d08d9d519aa9764e8534a03d653},
  journal = {Human mental workload},
  keywords = {cognitive_load},
  number = 3,
  pages = {139--183},
  publisher = {Amsterdam, Holland},
  timestamp = {2013-04-22T17:33:29.000+0200},
  title = {Development of {NASA-TLX} (Task Load Index): Results of empirical and theoretical research},
  volume = 1,
  year = 1988
}


@article{jmlr09-taylor,
  author = {\textbf{M.~E.~Taylor} and Stone, Peter},
  title = {{Transfer Learning for Reinforcement Learning Domains: A Survey}},
  journal = {{Journal of Machine Learning Research}},
  volume = {10},
  number = {1},
  pages = {1633--1685},
  year = {2009}
}

@article{2014connectionscience-taylor,
  author = {Taylor, Matthew E. and Carboni\textbf{*}, Nicholas and Fachantidis, Anestis and Vlahavas, Ioannis and Torrey, Lisa},
  title = {{Reinforcement learning agents providing advice in complex video games}},
  journal = {{Connection Science}},
  volume = {26},
  number = {1},
  pages = {45-63},
  year = {2014},
  doi = {10.1080/09540091.2014.885279},
  url = {http://dx.doi.org/10.1080/09540091.2014.885279},
  eprint = {http://dx.doi.org/10.1080/09540091.2014.885279}
}


@inproceedings{omidshafiei2018learning,
  author    = {Shayegan Omidshafiei and
               Dong{-}Ki Kim and
               Miao Liu and
               Gerald Tesauro and
               Matthew Riemer and
               Christopher Amato and
               Murray Campbell and
               Jonathan P. How},
  title     = {Learning to Teach in Cooperative Multiagent Reinforcement Learning},
  booktitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  year      = {2019},
}


@article{2017Fachantidis-Taylor-Vlahavas,
  author = {Fachantidis, Anestis and Taylor, Matthew and Vlahavas, I.},
  year = {2017},
  month = jul,
  title = {Learning to Teach Reinforcement Learning Agents},
  volume = {1},
  journal = {Machine Learning and Knowledge Extraction},
  doi = {10.3390/make1010002},
  url = {https://www.researchgate.net/publication/318785192_Learning_to_Teach_Reinforcement_Learning_Agents},
  month_numeric = {7}
}

@inproceedings{Ofra16,
author = {Amir, Ofra and Kamar, Ece and Kolobov, Andrey and Grosz, Barbara},
year = {2016},
pages = {},
booktitle = {Proceedings of  International Joint Conference on Artificial Intelligence (IJCAI)},
title = {Interactive Teaching Strategies for Agent Training}
}


@article{agentsTeachingAgents,
  title={Agents teaching agents: a survey on inter-agent transfer learning.},
  author={Felipe Leno {Da Silva} and Garrett Warnell and Anna Helena Reali Costa and Peter Stone},
  journal={Autonomous Agents and Multi-Agent Systems (AAMAS)},
  volume={34},
  number={9},
  year={2020}
}

@inproceedings{da2020uncertainty,
  title = {Uncertainty-Aware Action Advising for Deep Reinforcement Learning Agents},
  author = {Da Silva\textbf{*}, Felipe Leno and Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E.},
  booktitle = {Proceedings of  AAAI Conference on Artificial Intelligence},
  month = jan,
  year = {2020},
  month_numeric = {1}
}

@inproceedings{2019aiide-mcts,
  author = {Kartal, Bilal and Hernandez-Leal, Pablo and Taylor, Matthew E.},
  title = {Action Guidance with {MCTS} for Deep Reinforcement Learning},
  booktitle = {Proceedings of  AIIDE},
  year = {2019}
}

@misc{2020ala-yunshu,
  author = {Du\textbf{*}, Yunshu and Warnell, Garrett and Gebremedhin, Assefaw and Stone, Peter and Taylor, Matthew E.},
  title = {Work-in-progress: Corrected Self Imitation Learning via Demonstrations},
  booktitle = {Proceedings of the Adaptive and Learning Agents Workshop at AAMAS},
  month = may,
  year = {2020},
  month_numeric = {5}
}

@article{DBLP:journals/ijsr/ChernovaV10,
  author    = {Sonia Chernova and
               Manuela M. Veloso},
  title     = {Confidence-Based Multi-Robot Learning from Demonstration},
  journal   = {International Journal of Social Robotics},
  volume    = {2},
  number    = {2},
  pages     = {195--215},
  year      = {2010},
  url       = {https://doi.org/10.1007/s12369-010-0060-0},
  doi       = {10.1007/s12369-010-0060-0},
  timestamp = {Sat, 25 Apr 2020 13:55:22 +0200},
  biburl    = {https://dblp.org/rec/journals/ijsr/ChernovaV10.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{2019ker,
  title = {Team learning from human demonstration with coordination confidence},
  author = {Banerjee, Bikramjit and Vittanala, Syamala and Taylor, Matthew E.},
  journal = {The Knowledge Engineering Review},
  year = {2019},
  volume = {34}
}

@InProceedings{IROS20,
    author = "Gucsi, Balint and Tarapore, Danesh and Yeoh, William and Amato, Christopher and Tran-Thanh, Long",
    title = "To Ask or Not to Ask: A User Annoyance Aware Preference Elicitation Framework for Social Robots",
    booktitle = "Proceedings of  IROS",
    year = "2020"
}

@article{10.1145/3068663,
author = {Lasecki, Walter S. and Miller, Christopher D. and Naim, Iftekhar and Kushalnagar, Raja and Sadilek, Adam and Gildea, Daniel and Bigham, Jeffrey P.},
title = {Scribe: Deep Integration of Human and Machine Intelligence to Caption Speech in Real Time},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/3068663},
doi = {10.1145/3068663},
abstract = {Quickly converting speech to text allows deaf and hard of hearing people to interactively follow along with live speech. Doing so reliably requires a combination of perception, understanding, and speed that neither humans nor machines possess alone. In this article, we discuss how our Scribe system combines human labor and machine intelligence in real time to reliably convert speech to text with less than 4s latency. To achieve this speed while maintaining high accuracy, Scribe integrates automated assistance in two ways. First, its user interface directs workers to different portions of the audio stream, slows down the portion they are asked to type, and adaptively determines segment length based on typing speed. Second, it automatically merges the partial input of multiple workers into a single transcript using a custom version of multiple-sequence alignment. Scribe illustrates the broad potential for deeply interleaving human labor and machine intelligence to provide intelligent interactive services that neither can currently achieve alone.},
journal = {Communications of the Association of Computing Machinery},
month = aug,
pages = {93–100},
numpages = {8}
}


@misc{cogment,
    author       = {{Artificial Intelligence Redefined}},
    title        = {Cogment v0.5},
    month        = {September},
    year         = 2020,
    version      = {0.5},
    howpublished = {https://gitlab.com/cogment/cogment},
    note          = {Online; accessed 1-October-2020}
    }

@inproceedings{TAMER,
author = {Knox, W. Bradley and Stone, Peter},
title = {Interactively Shaping Agents via Human Reinforcement: The {TAMER} Framework},
year = {2009},
isbn = {9781605586588},
abstract = {As computational learning agents move into domains that incur real costs (e.g., autonomous driving or financial investment), it will be necessary to learn good policies without numerous high-cost learning trials. One promising approach to reducing sample complexity of learning a task is knowledge transfer from humans to agents. Ideally, methods of transfer should be accessible to anyone with task knowledge, regardless of that person's expertise in programming and AI. This paper focuses on allowing a human trainer to interactively shape an agent's policy via reinforcement signals. Specifically, the paper introduces "Training an Agent Manually via Evaluative Reinforcement," or TAMER, a framework that enables such shaping. Differing from previous approaches to interactive shaping, a TAMER agent models the human's reinforcement and exploits its model by choosing actions expected to be most highly reinforced. Results from two domains demonstrate that lay users can train TAMER agents without defining an environmental reward function (as in an MDP) and indicate that human training within the TAMER framework can reduce sample complexity over autonomous learning algorithms.},
booktitle = {Proceedings of the International Conference on Knowledge Capture (KCap)}
}

@inproceedings{TAMERRL,
author = {Knox, W. Bradley and Stone, Peter},
title = {Reinforcement Learning from Simultaneous Human and {MDP} Reward},
year = {2012},
abstract = {As computational agents are increasingly used beyond research labs, their success will depend on their ability to learn new skills and adapt to their dynamic, complex environments. If human users---without programming skills---can transfer their task knowledge to agents, learning can accelerate dramatically, reducing costly trials. The tamer framework guides the design of agents whose behavior can be shaped through signals of approval and disapproval, a natural form of human feedback. More recently, tamer+rl was introduced to enable human feedback to augment a traditional reinforcement learning (RL) agent that learns from a Markov decision process's (MDP) reward signal. We address limitations of prior work on tamer and tamer+rl, contributing in two critical directions. First, the four successful techniques for combining human reward with RL from prior tamer+rl work are tested on a second task, and these techniques' sensitivities to parameter changes are analyzed. Together, these examinations yield more general and prescriptive conclusions to guide others who wish to incorporate human knowledge into an RL algorithm. Second, tamer+rl has thus far been limited to a sequential setting, in which training occurs before learning from MDP reward. In this paper, we introduce a novel algorithm that shares the same spirit as tamer+rl but learns simultaneously from both reward sources, enabling the human feedback to come at any time during the reinforcement learning process. We call this algorithm simultaneous tamer+rl. To enable simultaneous learning, we introduce a new technique that appropriately determines the magnitude of the human model's influence on the RL algorithm throughout time and state-action space.},
booktitle = {Proceedings of the International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)}
}

@article{SABL,
  author = {Loftin, Robert and Peng\textbf{*}, Bei and MacGlashan, James and Littman, Michael L. and Taylor, Matthew E. and Huang, Jeff and Roberts, David L.},
  title = {{Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning}},
  journal = {{Journal of Autonomous Agents and Multi-Agent Systems}},
  pages = {1--30},
  year = {2015},
  doi = {10.1007/s10458-015-9283-7},
  publisher = {Springer},
  url = {http://link.springer.com/article/10.1007%2Fs10458-015-9283-7}
}

@inproceedings{COACH,
  author = {MacGlashan, James and Ho, Mark and Loftin, Robert and Peng\textbf{*}, Bei and Wang, Guan and Roberts, David L. and Taylor, Matthew E. and Littman, Michael L.},
  title = {Interactive Learning from Policy-Dependent Human Feedback},
  booktitle = {{Proc. ICML}},
  month = aug,
  year = {2017},
  month_numeric = {8}
}

@inproceedings{PolicyShaping,
  title={Policy Shaping with Human Teachers},
  author={Thomas Cederborg and Ishaan Grover and C. Isbell and A. Thomaz},
  booktitle={International Joint Conference on Artificial Intelligence (IJCAI)},
  year={2015}
}

@InProceedings(AAAI04ws-pillar,
 author="Gregory Kuhlmann and Peter Stone and Raymond Mooney and Jude Shavlik",
 title="Guiding a Reinforcement Learner with Natural Language Advice: Initial Results in {R}obo{C}up Soccer",
 booktitle="The {AAAI} Workshop on Supervisory Control of Learning and Adaptive Systems",
 month="July",year="2004",
 abstract={
           We describe our current efforts towards creating a
           reinforcement learner that learns \emph{both} from
           reinforcements provided by its environment \emph{and} from
           human-generated advice.  Our research involves two
           complementary components: (a)~mapping advice expressed in
           English to a formal advice language and (b)~using advice
           expressed in a formal notation in a reinforcement learner.
           We use a subtask of the challenging RoboCup simulated
           soccer task~\cite{Noda98} as our testbed.
          },
)

@article{Pengo,
author = {Maclin, Richard and Shavlik, Jude},
year = {1996},
month = {March},
pages = {251-281},
title = {Creating Advice-Taking Reinforcement Learners},
volume = {22},
isbn = {978-0-7923-9705-2},
journal = {Machine Learning},
doi = {10.1023/A:1018020625251}
}

@article{2017stemtransactions-yang,
  author = {Hu\textbf{*}, Yang and Taylor, Matthew E.},
  title = {A Computer-Aided Design Intelligent Tutoring System Teaching Strategic Flexibility},
  journal = {Transactions on Techniques for STEM Education},
  month = oct,
  year = {2016},
  month_numeric = {10}
}

 
@misc{99AndBeyondICLR,
 author = {Sai {Krishna Gottipati} and Yashaswi Pathak and Rohan Nuttall\textbf{*} and Sahir\textbf{*} and Raviteja Chunduru and Ahmed Touati and Sriram Ganapathi Subramanian and \textbf{M.~E.~Taylor} and Sarath Chandar},
 year = {2021},
 title = "Maximum Reward Formulation In Reinforcement Learning",
 note = {\underline{Under Submission} to the International Conference on Learning Representations (ICLR)}
 }
 
 @misc{99AndBeyondAAAI,
 author = {Sai {Krishna Gottipati} and Yashaswi Pathak and Boris Sattarov and Sahir\textbf{*} and Rohan Nuttall\textbf{*} and Mohammad Amini and \textbf{M.~E.~Taylor} and Sarath Chandar},
 year = {2021},
 title = "{TAC}: Towered Actor Critic For Handling Multiple Action Types In Reinforcement Learning For Drug Discovery",
 note = {\underline{Under Submission} to the {AAAI} Conference on Artificial Intelligence}
 }
 
 @inproceedings{mujoco,
  author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle = {Proceedings of the International Conference on Intelligent Robots and Systems (IROS)},
  year = 2012
}


@article{ALE,
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
title = {The Arcade Learning Environment: An Evaluation Platform for General Agents},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
journal = {Journal of Artificial Intelligence Research},
month = may,
pages = {253–279},
numpages = {27}
}

@article{BatchRL,
  author    = {Scott Fujimoto and
               Edoardo Conti and
               Mohammad Ghavamzadeh and
               Joelle Pineau},
  title     = {Benchmarking Batch Deep Reinforcement Learning Algorithms},
  journal   = {CoRR},
  volume    = {abs/1910.01708},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.01708},
  archivePrefix = {arXiv},
  eprint    = {1910.01708},
  timestamp = {Wed, 09 Oct 2019 14:07:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-01708.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{offpolicy,
      title={Empirical Study of Off-Policy Policy Evaluation for Reinforcement Learning}, 
      author={Cameron Voloshin and Hoang M. Le and Nan Jiang and Yisong Yue},
      year={2020},
      eprint={1911.06854},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@MastersThesis{ColinThesis,
    author     =     {Colin Vandenhof},
    title     =     {{Asking for Help with a Cost in Reinforcement Learning}},
    school     =     {University of Waterloo},
    year     =     {2020},
    }

@inproceedings{DBLP:conf/corl/MahmoodKVMB18,
  author    = {A. Rupam Mahmood and
               Dmytro Korenkevych and
               Gautham Vasan and
               William Ma and
               James Bergstra},
  title     = {Benchmarking Reinforcement Learning Algorithms on Real-World Robots},
  booktitle = {2nd Annual Conference on Robot Learning (CoRL)},
  year      = {2018},
}

@article{mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}

@misc{Datacenter,
 title={{DeepMind} {AI} Reduces {Google} Data Centre Cooling Bill by 40\%},
 year = {2016}, 
 howpublished = {https://deepmind.com/blog/article/deepmind-ai-reduces-google-data-centre-cooling-bill-40},
 note = {Online; accessed 1-October-2020}
 }
 
@inproceedings{10.1145/1143844.1143929,
author = {Nevmyvaka, Yuriy and Feng, Yi and Kearns, Michael},
title = {Reinforcement Learning for Optimized Trade Execution},
year = {2006},
abstract = {We present the first large-scale empirical application of reinforcement learning to the important problem of optimized trade execution in modern financial markets. Our experiments are based on 1.5 years of millisecond time-scale limit order data from NASDAQ, and demonstrate the promise of reinforcement learning methods to market microstructure problems. Our learning algorithm introduces and exploits a natural "low-impact" factorization of the state space.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning (ICML)}
}

@misc{Aiden,
 title={Aiden: Reinforcement Learning Applied to Electronic Trading},
 year={2020},
 howpublished = {https://www.borealisai.com/en/applying-ai/aiden/},
 note = {Online; accessed 15-October-2020}
}

@inproceedings{10.1145/3330430.3333634,
author = {Borrella, Inma and Caballero-Caballero, Sergio and Ponce-Cueto, Eva},
title = {Predict and Intervene: Addressing the Dropout Problem in a MOOC-Based Program},
year = {2019},
isbn = {9781450368049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330430.3333634},
doi = {10.1145/3330430.3333634},
abstract = {Massive Open Online Courses (MOOCs) are an efficient way of delivering knowledge to thousands of learners. However, even among learners who show a clear intention to complete a MOOC, the dropout rate is substantial. This is particularly relevant in the context of MOOC-based educational programs where a funnel of participation can be observed and high dropout rates at early stages of the program significantly reduce the number of learners successfully completing it. In this paper, we propose an approach to identify learners at risk of dropping out from a course, and we design and test an intervention intended to mitigate that risk. We collect course clickstream data from MOOCs of the MITx MicroMasters® in Supply Chain Management program and apply machine learning algorithms to predict potential dropouts. Our final model is able to predict 80% of actual dropouts. Based on these results, we design an intervention aimed to increase learners' motivation and engagement with a MOOC. The intervention consists on sending tailored encouragement emails to at-risk learners, but despite the high email opening rate, it shows no effect in dropout reduction.},
booktitle = {Proceedings of the Sixth (2019) ACM Conference on Learning @ Scale},
articleno = {24},
numpages = {9},
keywords = {Online education, Predictive model, Dropout, Intervention, Higher education, Machine learning, MOOC, Retention},
location = {Chicago, IL, USA},
series = {L@S '19}
}

@inproceedings{DBLP:conf/ijcai/LuketinaNFFAGWR19,
  author    = {Jelena Luketina and
               Nantas Nardelli and
               Gregory Farquhar and
               Jakob N. Foerster and
               Jacob Andreas and
               Edward Grefenstette and
               Shimon Whiteson and
               Tim Rockt{\"{a}}schel},
  title     = {A Survey of Reinforcement Learning Informed by Natural Language},
  booktitle = {Proceedings of the International Joint Conference on
               Artificial Intelligence ({IJCAI})},
  year      = {2019}
}

@article{wang2016does,
  title={Where does AlphaGo go: From church-turing thesis to AlphaGo thesis and beyond},
  author={Wang, Fei-Yue and Zhang, Jun Jason and Zheng, Xinhu and Wang, Xiao and Yuan, Yong and Dai, Xiaoxiao and Zhang, Jie and Yang, Liuqing},
  journal={IEEE/CAA Journal of Automatica Sinica},
  year={2016},
  publisher={IEEE}
}

@article{jaderberg2019human,
  title={Human-level performance in 3D multiplayer games with population-based reinforcement learning},
  author={Jaderberg, Max and Czarnecki, Wojciech M and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C and Morcos, Ari S and Ruderman, Avraham and others},
  journal={Science},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@article{navidi2020human,
  title={Human AI interaction loop training: New approach for interactive reinforcement learning},
  author={Navidi, Neda},
  journal={arXiv preprint arXiv:2003.04203},
  year={2020}
}

@inproceedings{tran2015reinforcement,
  title={Reinforcement Learning with Autonomous Small Unmanned Aerial Vehicles in Cluttered Environments-" After all these years among humans, you still haven't learned to smile."},
  author={Tran, Loc D and Cross, Charles D and Motter, Mark A and Neilan, James H and Qualls, Garry and Rothhaar, Paul M and Trujillo, Anna and Allen, Bonnie D},
  booktitle={15th AIAA aviation technology, integration, and operations conference},
  year={2015}
}

@inproceedings{ross2013learning,
  title={Learning monocular reactive uav control in cluttered natural environments},
  author={Ross, St{\'e}phane and Melik-Barkhudarov, Narek and Shankar, Kumar Shaurya and Wendel, Andreas and Dey, Debadeepta and Bagnell, J Andrew and Hebert, Martial},
  booktitle={2013 IEEE international conference on robotics and automation},
  year={2013}
}

@inproceedings{mericcli2010complementary,
  title={Complementary humanoid behavior shaping using corrective demonstration},
  author={Meri{\c{c}}li, Cetin and Veloso, Manuela and Akin, H Levent},
  booktitle={2010 10th IEEE-RAS International Conference on Humanoid Robots},
  year={2010}
}

@inproceedings{ChristianoLBMLA17,
  author    = {Paul F. Christiano and
               Jan Leike and
               Tom B. Brown and
               Miljan Martic and
               Shane Legg and
               Dario Amodei},
  title     = {Deep Reinforcement Learning from Human Preferences},
  booktitle = {Neural Information Processing Systems},
  pages     = {4299--4307},
  year      = {2017}
}

@article{loftin2016learning,
  title={Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning},
  author={Loftin, Robert and Peng, Bei and MacGlashan, James and Littman, Michael L and Taylor, Matthew E and Huang, Jeff and Roberts, David L},
  journal={Autonomous agents and multi-agent systems},
  year={2016}
}

@inproceedings{TAMER,
author={W. Bradley Knox and P. Stone},
booktitle={2008 7th IEEE International Conference on Development and Learning},
title={TAMER: Training an Agent Manually via Evaluative Reinforcement},
year={2008}}

@article{reddy2018shared,
  title={Shared autonomy via deep reinforcement learning},
  author={Reddy, Siddharth and Dragan, Anca D and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.01744},
  year={2018}
}

@inproceedings{ng2003autonomous,
  title={Autonomous helicopter flight via reinforcement learning.},
  author={Ng, Andrew Y and Kim, H Jin and Jordan, Michael I and Sastry, Shankar and Ballianda, Shiv},
  booktitle={NIPS},
  year={2003},
  organization={Citeseer}
}

@inproceedings{bagnell2001autonomous,
  title={Autonomous helicopter control using reinforcement learning policy search methods},
  author={Bagnell, J Andrew and Schneider, Jeff G},
  booktitle={Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No. 01CH37164)},
  year={2001}
}

@inproceedings{morales2004learning,
  title={Learning to fly by combining reinforcement learning with behavioural cloning},
  author={Morales, Eduardo F and Sammut, Claude},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={76},
  year={2004}
}

@inproceedings{martin2004agentx,
  title={AgentX: Using reinforcement learning to improve the effectiveness of intelligent tutoring systems},
  author={Martin, Kimberly N and Arroyo, Ivon},
  booktitle={International Conference on Intelligent Tutoring Systems},
  year={2004}
}

@inproceedings{sarma2007intelligent,
  title={Intelligent tutoring systems using reinforcement learning to teach autistic students},
  author={Sarma, BH Sreenivasa and Ravindran, Balaraman},
  booktitle={International Conference on Home-Oriented Informatics and Telematics},
  year={2007}
}

@inproceedings{georgila2019using,
  title={Using reinforcement learning to optimize the policies of an intelligent tutoring system for interpersonal skills training},
  author={Georgila, Kallirroi and Core, Mark G and Nye, Benjamin D and Karumbaiah, Shamya and Auerbach, Daniel and Ram, Maya},
  booktitle={Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  year={2019}
}

@article{wang2018reinforcement,
  title={Reinforcement learning in a pomdp based intelligent tutoring system for optimizing teaching strategies},
  author={Wang, Fangju},
  journal={International Journal of Information and Education Technology},
  year={2018}
}

@inproceedings{sandstrom2022fighter,
  title={Fighter Pilot Behavior Cloning},
  author={Sandstr{\"o}m, Viktor and Luotsinen, Linus and Oskarsson, Daniel},
  booktitle={2022 International Conference on Unmanned Aircraft Systems (ICUAS)},
  pages={686--695},
  year={2022},
  organization={IEEE}
}

@misc{stable-baselines3,
  author = {Raffin, Antonin and Hill, Ashley and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Dormann, Noah},
  title = {Stable Baselines3},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/DLR-RM/stable-baselines3}},
}

@article{pomerleau1988alvinn,
  title={Alvinn: An autonomous land vehicle in a neural network},
  author={Pomerleau, Dean A},
  journal={Advances in neural information processing systems},
  volume={1},
  year={1988}
}

@article{bignold21,
  title = {A conceptual framework for externally-influenced agents: an assisted reinforcement learning review},
  author = {Bignold, Adam and Cruz, Francisco and Taylor, Matthew E. and Brys, Tim and Dazeley, Richard and Vamplew, Peter and Foale, Cameron},
  journal = {Journal of Ambient Intelligence and Humanized Computing},
  month = sep,
  day = {18},
  year = {2021},
  doi = {https://doi.org/10.1007/s12652-021-03489-y},
  publisher = {Springer},
  url = {https://link.springer.com/article/10.1007/s12652-021-03489-y},
  month_numeric = {9}
}