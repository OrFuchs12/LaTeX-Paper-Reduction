\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Bertasius, Wang, and Torresani(2021)}]{Bertasius2021is}
Bertasius, G.; Wang, H.; and Torresani, L. 2021.
\newblock {Is Space-Time Attention All You Need for Video Understanding?}
\newblock In \emph{Proc. ICML}, 813--824.

\bibitem[{Carreira and Zisserman(2017)}]{Carreira2017quo}
Carreira, J.; and Zisserman, A. 2017.
\newblock Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset.
\newblock In \emph{Proc. CVPR}, 6299--6308.

\bibitem[{Chen et~al.(2020)Chen, He, Bao, and Wang}]{Chen2020pretraining}
Chen, J.; He, H.; Bao, S.; and Wang, F. 2020.
\newblock {Pre-training Assisted Scene-aware Dialogue Generation}.
\newblock In \emph{DSTC8 at AAAI2020 workshop}.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{Devlin2018bert}
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
\newblock {BERT}: Pre-training of Deep Bidirectional Transformers for Language
  Understanding.
\newblock In \emph{Proc. NAACL-HLT}, 4171--4186.

\bibitem[{Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly
  et~al.}]{Dosovitskiy2020image}
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.;
  Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et~al.
  2020.
\newblock An Image is Worth 16x16 Words: Transformers for Image Recognition at
  Scale.
\newblock In \emph{Proc. ICLR}.

\bibitem[{Geng et~al.(2021)Geng, Gao, Chatterjee, Hori, Roux, Zhang, Li, and
  Cherian}]{Geng2021dynamic}
Geng, S.; Gao, P.; Chatterjee, M.; Hori, C.; Roux, J.~L.; Zhang, Y.; Li, H.;
  and Cherian, A. 2021.
\newblock {Dynamic Graph Representation Learning for Video Dialog via
  Multi-Modal Shuffled Transformers}.
\newblock In \emph{Proc. AAAI}, 1415--1423.

\bibitem[{Hara, Kataoka, and Satoh(2018)}]{Hara2018can}
Hara, K.; Kataoka, H.; and Satoh, Y. 2018.
\newblock Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and
  ImageNet?
\newblock In \emph{Proc. CVPR}, 6546--6555.

\bibitem[{He et~al.(2016)He, Zhang, Ren, and Sun}]{He2015deep}
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016.
\newblock Deep Residual Learning for Image Recognition.
\newblock In \emph{Proc. CVPR}, 770--778.

\bibitem[{Heck et~al.(2020)Heck, van Niekerk, Lubis, Geishauser, Lin, Moresi,
  and Ga{\v{s}}i{\'c}}]{Heck2020trippy}
Heck, M.; van Niekerk, C.; Lubis, N.; Geishauser, C.; Lin, H.-C.; Moresi, M.;
  and Ga{\v{s}}i{\'c}, M. 2020.
\newblock TripPy: A Triple Copy Strategy for Value Independent Neural Dialog
  State Tracking.
\newblock In \emph{Proc. SIGDIAL}, 35--44.

\bibitem[{Hershey et~al.(2017)Hershey, Chaudhuri, Ellis, Gemmeke, Jansen,
  Moore, Plakal, Platt, Saurous, Seybold et~al.}]{Hershey2017cnn}
Hershey, S.; Chaudhuri, S.; Ellis, D.~P.; Gemmeke, J.~F.; Jansen, A.; Moore,
  R.~C.; Plakal, M.; Platt, D.; Saurous, R.~A.; Seybold, B.; et~al. 2017.
\newblock CNN Architectures for Large-Scale Audio Classification.
\newblock In \emph{Proc. ICASSP}, 131--135.

\bibitem[{Hori et~al.(2019{\natexlab{a}})Hori, Alamri, Wang, Wichern, Hori,
  Cherian, Marks, Cartillier, Lopes, Das, Essa, Batra, and
  Parikh}]{Hori2019end}
Hori, C.; Alamri, H.; Wang, J.; Wichern, G.; Hori, T.; Cherian, A.; Marks,
  T.~K.; Cartillier, V.; Lopes, R.~G.; Das, A.; Essa, I.; Batra, D.; and
  Parikh, D. 2019{\natexlab{a}}.
\newblock {End-to-end Audio Visual Scene-aware Dialog Using Multimodal
  Attention-based Video Features}.
\newblock In \emph{Proc. ICASSP}, 2352--2356.

\bibitem[{Hori et~al.(2019{\natexlab{b}})Hori, Cherian, Marks, and
  Hori}]{Hori2019joint}
Hori, C.; Cherian, A.; Marks, T.~K.; and Hori, T. 2019{\natexlab{b}}.
\newblock {Joint Student-Teacher Learning for Audio-Visual Scene-Aware Dialog}.
\newblock In \emph{Proc. INTERSPEECH}, 1886--1890.

\bibitem[{Le, Chen, and Hoi(2021)}]{Le2021vgnmn}
Le, H.; Chen, N.~F.; and Hoi, S. C.~H. 2021.
\newblock {VGNMN: Video-grounded Neural Module Network to Video-Grounded
  Language Tasks}.
\newblock \emph{arXiv preprint arXiv:2104.07921}.

\bibitem[{Le et~al.(2020)Le, Sahoo, Chen, and Hoi}]{Le2020bist}
Le, H.; Sahoo, D.; Chen, N.; and Hoi, S.~C. 2020.
\newblock {BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded
  Dialogues}.
\newblock In \emph{Proc. EMNLP}, 1846--1859.

\bibitem[{Li et~al.(2021)Li, Li, Zhang, Feng, and Zhou}]{Li2021bridging}
Li, Z.; Li, Z.; Zhang, J.; Feng, Y.; and Zhou, J. 2021.
\newblock {Bridging Text and Video: A Universal Multimodal Transformer for
  Video-Audio Scene-Aware Dialog}.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, 29: 2476--2483.

\bibitem[{Loshchilov and Hutter(2019)}]{Loshchilov2017decoupled}
Loshchilov, I.; and Hutter, F. 2019.
\newblock {Decoupled Weight Decay Regularization}.
\newblock In \emph{Proc. ICLR}.

\bibitem[{Miech et~al.(2019)Miech, Zhukov, Alayrac, Tapaswi, Laptev, and
  Sivic}]{Miech2019howto100m}
Miech, A.; Zhukov, D.; Alayrac, J.-B.; Tapaswi, M.; Laptev, I.; and Sivic, J.
  2019.
\newblock {HowTo100M: Learning a Text-Video Embedding by Watching Hundred
  Million Narrated Video Clips}.
\newblock In \emph{Proc. ICCV}, 2630--2640.

\bibitem[{Nguyen et~al.(2019)Nguyen, Sharma, Schulz, and Asri}]{Nguyen2018from}
Nguyen, D.~T.; Sharma, S.; Schulz, H.; and Asri, L.~E. 2019.
\newblock {From FiLM to Video: Multi-turn Question Answering with Multi-modal
  Context}.
\newblock In \emph{DSTC7 at AAAI2019 workshop}.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever
  et~al.}]{Radford2019language}
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et~al.
  2019.
\newblock Language Models are Unsupervised Multitask Learners.
\newblock \emph{OpenAI blog}, 1(8): 9.

\bibitem[{Raghu et~al.(2021)Raghu, Unterthiner, Kornblith, Zhang, and
  Dosovitskiy}]{Raghu2021vision}
Raghu, M.; Unterthiner, T.; Kornblith, S.; Zhang, C.; and Dosovitskiy, A. 2021.
\newblock Do Vision Transformers See Like Convolutional Neural Networks?
\newblock \emph{arXiv preprint arXiv:2108.08810}.

\bibitem[{Ren et~al.(2015)Ren, He, Girshick, and Sun}]{Ren2015faster}
Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015.
\newblock Faster R-CNN: Towards Real-Time Object Detection with Region Proposal
  Networks.
\newblock In \emph{Proc. NIPS}, volume~28, 91--99.

\bibitem[{Sanabria, Palaskar, and Metze(2019)}]{Sanabria2019cmu}
Sanabria, R.; Palaskar, S.; and Metze, F. 2019.
\newblock {CMU Sinbad's Submission for the DSTC7 AVSD Challenge}.
\newblock In \emph{DSTC7 at AAAI2019 workshop}.

\bibitem[{Shah et~al.(2021)Shah, Geng, Gao, Cherian, Hori, Marks, Roux, and
  Hori}]{Shah2021audio}
Shah, A.~P.; Geng, S.; Gao, P.; Cherian, A.; Hori, T.; Marks, T.~K.; Roux,
  J.~L.; and Hori, C. 2021.
\newblock Audio-Visual Scene-Aware Dialog and Reasoning using Audio-Visual
  Transformers with Joint Student-Teacher Learning.
\newblock \emph{arXiv preprint arXiv:2110.06894}.

\bibitem[{Sigurdsson et~al.(2016)Sigurdsson, Varol, Wang, Farhadi, Laptev, and
  Gupta}]{Sigurdsson2016hollywood}
Sigurdsson, G.~A.; Varol, G.; Wang, X.; Farhadi, A.; Laptev, I.; and Gupta, A.
  2016.
\newblock {Hollywood in Homes: Crowdsourcing Data Collection for Activity
  Understanding}.
\newblock In \emph{Proc. ECCV}, volume 9905 LNCS, 510--526.

\bibitem[{Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush}]{Wolf2020transformers}
Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.; Moi, A.; Cistac, P.;
  Rault, T.; Louf, R.; Funtowicz, M.; Davison, J.; Shleifer, S.; von Platen,
  P.; Ma, C.; Jernite, Y.; Plu, J.; Xu, C.; Scao, T.~L.; Gugger, S.; Drame, M.;
  Lhoest, Q.; and Rush, A.~M. 2020.
\newblock Transformers: State-of-the-Art Natural Language Processing.
\newblock In \emph{Proc. EMNLP}, 38--45.

\bibitem[{Wu et~al.(2016)Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun,
  Cao, Gao, Macherey, Klingner, Shah, Johnson, Liu, Łukasz Kaiser, Gouws,
  Kato, Kudo, Kazawa, Stevens, Kurian, Patil, Wang, Young, Smith, Riesa,
  Rudnick, Vinyals, Corrado, Hughes, and Dean}]{Wu2016googles}
Wu, Y.; Schuster, M.; Chen, Z.; Le, Q.~V.; Norouzi, M.; Macherey, W.; Krikun,
  M.; Cao, Y.; Gao, Q.; Macherey, K.; Klingner, J.; Shah, A.; Johnson, M.; Liu,
  X.; Łukasz Kaiser; Gouws, S.; Kato, Y.; Kudo, T.; Kazawa, H.; Stevens, K.;
  Kurian, G.; Patil, N.; Wang, W.; Young, C.; Smith, J.; Riesa, J.; Rudnick,
  A.; Vinyals, O.; Corrado, G.; Hughes, M.; and Dean, J. 2016.
\newblock Google's Neural Machine Translation System: Bridging the Gap between
  Human and Machine Translation.
\newblock \emph{arXiv preprint arXiv:1609.08144}.

\bibitem[{Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and
  He}]{Xie2017aggregated}
Xie, S.; Girshick, R.; Doll{\'a}r, P.; Tu, Z.; and He, K. 2017.
\newblock Aggregated Residual Transformations for Deep Neural Networks.
\newblock In \emph{Proc. CVPR}, 1492--1500.

\bibitem[{Xu, Szlam, and Weston(2021)}]{Xu2021beyond}
Xu, J.; Szlam, A.; and Weston, J. 2021.
\newblock Beyond Goldfish Memory: Long-Term Open-Domain Conversation.
\newblock \emph{arXiv preprint arXiv:2107.07567}.

\end{thebibliography}
