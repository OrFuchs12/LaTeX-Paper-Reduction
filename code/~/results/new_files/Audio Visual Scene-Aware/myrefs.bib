@inproceedings{Lin2019entropy,
abstract = {With increasing information from social media, there are more and more videos available. Therefore, the ability to reason on a video is important and deserves to be discussed. TheDialog System Technology Challenge (DSTC7) (Yoshino et al. 2018) proposed an Audio Visual Scene-aware Dialog (AVSD) task, which contains five modalities including video, dialogue history, summary, and caption, as a scene-aware environment. In this paper, we propose the entropy-enhanced dynamic memory network (DMN) to effectively model video modality. The attention-based GRU in the proposed model can improve the model's ability to comprehend and memorize sequential information. The entropy mechanism can control the attention distribution higher, so each to-be-answered question can focus more specifically on a small set of video segments. After the entropy-enhanced DMN secures the video context, we apply an attention model that in-corporates summary and caption to generate an accurate answer given the question about the video. In the official evaluation, our system can achieve improved performance against the released baseline model for both subjective and objective evaluation metrics.},
archivePrefix = {arXiv},
arxivId = {1908.08191},
author = {Lin, Kuan-Yen and Hsu, Chao-Chun and Chen, Yun-Nung and Ku, Lun-Wei},
eprint = {1908.08191},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - 2019 - Entropy-Enhanced Multimodal Attention Model for Scene-Aware Dialogue Generation.pdf:pdf},
title = {{Entropy-Enhanced Multimodal Attention Model for Scene-Aware Dialogue Generation}},
url = {http://arxiv.org/abs/1908.08191},
year = {2019}
}
@inproceedings{Li2020tmt,
abstract = {Audio Visual Scene-aware Dialog (AVSD) is a task to generate responses when discussing about a given video. The previous state-of-the-art model shows superior performance for this task using Transformer-based architecture. However, there remain some limitations in learning better representation of modalities. Inspired by Neural Machine Translation (NMT), we propose the Transformer-based Modal Translator (TMT) to learn the representations of the source modal sequence by translating the source modal sequence to the related target modal sequence in a supervised manner. Based on Multimodal Transformer Networks (MTN), we apply TMT to video and dialog, proposing MTN-TMT for the video-grounded dialog system. On the AVSD track of the Dialog System Technology Challenge 7, MTN-TMT outperforms the MTN and other submission models in both Video and Text task and Text Only task. Compared with MTN, MTN-TMT improves all metrics, especially, achieving relative improvement up to 14.1% on CIDEr.},
address = {ISCA},
archivePrefix = {arXiv},
arxivId = {2010.10839},
author = {Li, Wubo and Jiang, Dongwei and Zou, Wei and Li, Xiangang},
booktitle = {Interspeech 2020},
doi = {10.21437/Interspeech.2020-2359},
eprint = {2010.10839},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2020 - TMT A Transformer-Based Modal Translator for Improving Multimodal Sequence Representations in Audio Visual Scene-Awar.pdf:pdf},
issn = {19909772},
keywords = {Audio-visual scene-aware dialog,Multi-task learning,Multimodal learning,Neural machine translation},
month = {oct},
pages = {3501--3505},
publisher = {ISCA},
title = {{TMT: A Transformer-Based Modal Translator for Improving Multimodal Sequence Representations in Audio Visual Scene-Aware Dialog}},
url = {http://www.isca-speech.org/archive/Interspeech_2020/abstracts/2359.html},
volume = {2020-Octob},
year = {2020}
}
@article{Le2021c3,
abstract = {Video-grounded dialogue systems aim to integrate video understanding and dialogue understanding to generate responses that are relevant to both the dialogue and video context. Most existing approaches employ deep learning models and have achieved remarkable performance, given the relatively small datasets available. However, the results are partly accomplished by exploiting biases in the datasets rather than developing multimodal reasoning, resulting in limited generalization. In this paper, we propose a novel approach of Compositional Counterfactual Contrastive Learning ($C^3$) to develop contrastive training between factual and counterfactual samples in video-grounded dialogues. Specifically, we design factual/counterfactual sampling based on the temporal steps in videos and tokens in dialogues and propose contrastive loss functions that exploit object-level or action-level variance. Different from prior approaches, we focus on contrastive hidden state representations among compositional output tokens to optimize the representation space in a generation setting. We achieved promising performance gains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the benefits of our approach in grounding video and dialogue context.},
archivePrefix = {arXiv},
arxivId = {2106.08914},
author = {Le, Hung and Chen, Nancy F. and Hoi, Steven C. H.},
eprint = {2106.08914},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Chen, Hoi - 2021 - $C3$ Compositional Counterfactual Constrastive Learning for Video-grounded Dialogues.pdf:pdf},
month = {jun},
title = {{$C^3$: Compositional Counterfactual Constrastive Learning for Video-grounded Dialogues}},
url = {http://arxiv.org/abs/2106.08914},
year = {2021}
}
@inproceedings{Le2020video,
abstract = {Pre-trained language models have shown remarkable success in improving various downstream NLP tasks due to their ability to capture dependencies in textual data and generate natural responses. In this paper, we leverage the power of pre-trained language models for improving video-grounded dialogue, which is very challenging and involves complex features of different dynamics: (1) Video features which can extend across both spatial and temporal dimensions; and (2) Dialogue features which involve semantic dependencies over multiple dialogue turns. We propose a framework by extending GPT-2 models to tackle these challenges by formulating video-grounded dialogue tasks as a sequence-to-sequence task, combining both visual and textual representation into a structured sequence, and fine-tuning a large pre-trained GPT-2 network. Our framework allows fine-tuning language models to capture dependencies across multiple modalities over different levels of information: spatio-temporal level in video and token-sentence level in dialogue context. We achieve promising improvement on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark from DSTC7, which supports a potential direction in this line of research.},
address = {Stroudsburg, PA, USA},
archivePrefix = {arXiv},
arxivId = {2006.15319},
author = {Le, Hung and Hoi, Steven C.H.},
booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
doi = {10.18653/v1/2020.acl-main.518},
eprint = {2006.15319},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Hoi - 2020 - Video-Grounded Dialogues with Pretrained Generation Language Models.pdf:pdf},
pages = {5842--5848},
publisher = {Association for Computational Linguistics},
title = {{Video-Grounded Dialogues with Pretrained Generation Language Models}},
url = {https://www.aclweb.org/anthology/2020.acl-main.518},
year = {2020}
}
@article{Kumar2019leveraging,
abstract = {With the recent advancements in Artificial Intelligence (AI), Intelligent Virtual Assistants (IVA) such as Alexa, Google Home, etc., have become a ubiquitous part of many homes. Currently, such IVAs are mostly audio-based, but going forward, we are witnessing a confluence of vision, speech and dialog system technologies that are enabling the IVAs to learn audio-visual groundings of utterances. This will enable agents to have conversations with users about the objects, activities and events surrounding them. In this work, we present three main architectural explorations for the Audio Visual Scene-Aware Dialog (AVSD): 1) investigating `topics' of the dialog as an important contextual feature for the conversation, 2) exploring several multimodal attention mechanisms during response generation, 3) incorporating an end-to-end audio classification ConvNet, AclNet, into our architecture. We discuss detailed analysis of the experimental results and show that our model variations outperform the baseline system presented for the AVSD task.},
archivePrefix = {arXiv},
arxivId = {1912.10131},
author = {Kumar, Shachi H and Okur, Eda and Sahay, Saurav and Huang, Jonathan and Nachman, Lama},
eprint = {1912.10131},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumar et al. - 2019 - Leveraging Topics and Audio Features with Multimodal Attention for Audio Visual Scene-Aware Dialog.pdf:pdf},
number = {NeurIPS 2019},
title = {{Leveraging Topics and Audio Features with Multimodal Attention for Audio Visual Scene-Aware Dialog}},
url = {http://arxiv.org/abs/1912.10131},
year = {2019}
}
@inproceedings{Yeh2019reactive,
abstract = {Visual question answering and visual dialogue tasks have been increasingly studied in the multimodal field towards more practical real-world scenarios. A more challenging task, audio visual scene-aware dialogue (AVSD), is proposed to further advance the technologies that connect audio, vision, and language, which introduces temporal video information and dialogue interactions between a questioner and an answerer. This paper proposes an intuitive mechanism that fuses features and attention in multiple stages in order to well integrate multimodal features, and the results demonstrate its capability in the experiments. Also, we apply several state-of-the-art models in other tasks to the AVSD task, and further analyze their generalization across different tasks.},
archivePrefix = {arXiv},
arxivId = {1908.05067},
author = {Yeh, Yi-Ting and Lin, Tzu-Chuan and Cheng, Hsiao-Hua and Deng, Yu-Hsuan and Su, Shang-Yu and Chen, Yun-Nung},
eprint = {1908.05067},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yeh et al. - 2019 - Reactive Multi-Stage Feature Fusion for Multimodal Dialogue Modeling.pdf:pdf},
title = {{Reactive Multi-Stage Feature Fusion for Multimodal Dialogue Modeling}},
url = {http://arxiv.org/abs/1908.05067},
year = {2019}
}
@article{Chu2020multi,
abstract = {Understanding dynamic scenes and dialogue contexts in order to converse with users has been challenging for multimodal dialogue systems. The 8-th Dialog System Technology Challenge (DSTC8) proposed an Audio Visual Scene-Aware Dialog (AVSD) task, which contains multiple modalities including audio, vision, and language, to evaluate how dialogue systems understand different modalities and response to users. In this paper, we proposed a multi-step joint-modality attention network (JMAN) based on recurrent neural network (RNN) to reason on videos. Our model performs a multi-step attention mechanism and jointly considers both visual and textual representations in each reasoning process to better integrate information from the two different modalities. Compared to the baseline released by AVSD organizers, our model achieves a relative 12.1% and 22.4% improvement over the baseline on ROUGE-L score and CIDEr score.},
archivePrefix = {arXiv},
arxivId = {2001.06206},
author = {Chu, Yun-Wei and Lin, Kuan-Yen and Hsu, Chao-Chun and Ku, Lun-Wei},
eprint = {2001.06206},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chu et al. - 2020 - Multi-step Joint-Modality Attention Network for Scene-Aware Dialogue System.pdf:pdf},
month = {jan},
title = {{Multi-step Joint-Modality Attention Network for Scene-Aware Dialogue System}},
url = {http://arxiv.org/abs/2001.06206},
year = {2020}
}
@inproceedings{Jin2019video,
abstract = {Video dialog is a new and challenging task, which requires the agent to answer questions combining video information with dialog history. And different from single-turn video question answering, the additional dialog history is important for video dialog, which often includes contextual information for the question. Existing visual dialog methods mainly use RNN to encode the dialog history as a single vector representation, which might be rough and straightforward. Some more advanced methods utilize hierarchical structure, attention and memory mechanisms, which still lack an explicit reasoning process. In this paper, we introduce a novel progressive inference mechanism for video dialog, which progressively updates query information based on dialog history and video content until the agent think the information is sufficient and unambiguous. In order to tackle the multimodal fusion problem, we propose a cross-transformer module, which could learn more fine-grained and comprehensive interactions both inside and between the modalities. And besides answer generation, we also consider question generation, which is more challenging but significant for a complete video dialog system. We evaluate our method on two large-scale datasets, and the extensive experiments show the effectiveness of our method.},
address = {Stroudsburg, PA, USA},
author = {Jin, Weike and Zhao, Zhou and Gu, Mao and Xiao, Jun and Wei, Furu and Zhuang, Yueting},
booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
doi = {10.18653/v1/D19-1217},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jin et al. - 2019 - Video Dialog via Progressive Inference and Cross-Transformer.pdf:pdf},
isbn = {9781950737901},
pages = {2109--2118},
publisher = {Association for Computational Linguistics},
title = {{Video Dialog via Progressive Inference and Cross-Transformer}},
url = {https://www.aclweb.org/anthology/D19-1217},
year = {2019}
}
@inproceedings{Le2019,
abstract = {We present our work on the Dialog System Technology Challenges 7 (DSTC7). We participated in Track 3, which evaluated how dialog systems understand video scenes and response to users about the video visual and audio content. Our system is built upon the baseline system (Hori et al. 2018) with changes adopted similarly to (Anderson et al. 2018). The model utilizes different types of attentions on video caption and the video audio and visual input that contribute to the improved evaluation results. We also applied a nonlinear feature fusioning of the visual and audio features to improve the results further. Our proposed model showed improved performance in terms of both objective evaluation and human rating, surpassing the performance of the baseline.},
author = {Le, Hung and Hoi, S and Sahoo, Doyen and Workshop, N Chen - DSTC7 at AAAI2019 and 2019, Undefined},
booktitle = {Workshop.Colips.Org},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le et al. - 2019 - End-to-End Multimodal Dialog Systems with Hierarchical Multimodal Attention on Video Features.pdf:pdf},
keywords = {Le2019end},
number = {4},
title = {{End-to-End Multimodal Dialog Systems with Hierarchical Multimodal Attention on Video Features}},
url = {www.aaai.org},
year = {2019}
}
@inproceedings{Sanabria2019cmu,
abstract = {AudioVisual Scene-Aware Dialog (AVSD) is understood as an extension of Visual Question Answering, the task of generating a textual answer in response to a textual question on multi-media content. The input typically consists of text features (either speech recognition output, or a summary describing the video contents), video features (object, scene, and/ or action features), and dialog history or context. In this paper, we describe our submission to the AVSD track of the 7 th Dialog State Tracking Challenge. We use hierarchical attention to fuse contributions from different modalities, and investigate transfer learning using a background corpus of 2,000 hours of how-to videos. Our approach uses dialog context, but we do not use dialog history explicitly. Our system achieves the best performance in both automatic and human evaluations.},
author = {Sanabria, Ramon and Palaskar, Shruti and Metze, Florian},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sanabria, Palaskar, Metze - 2019 - CMU Sinbad's Submission for the DSTC7 AVSD Challenge.pdf:pdf},
title = {{CMU Sinbad's Submission for the DSTC7 AVSD Challenge}},
url = {https://github.com/KaimingHe/},
booktitle = {DSTC7 at AAAI2019 workshop},
year = {2019}
}
@inproceedings{Schwartz2019simple,
abstract = {The recently proposed audio-visual scene-aware dialog task paves the way to a more data-driven way of learning virtual assistants, smart speakers and car navigation systems. However, very little is known to date about how to effectively extract meaningful information from a plethora of sensors that pound the computational engine of those devices. Therefore, in this paper, we provide and carefully analyze a simple baseline for audio-visual scene-aware dialog which is trained end-to-end. Our method differentiates in a data-driven manner useful signals from distracting ones using an attention mechanism. We evaluate the proposed approach on the recently introduced and challenging audio-visual scene-aware dataset, and demonstrate the key features that permit to outperform the current state-of-the-art by more than 20% on CIDEr.},
archivePrefix = {arXiv},
arxivId = {1904.05876},
author = {Schwartz, Idan and Schwing, Alexander G. and Hazan, Tamir},
booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2019.01283},
eprint = {1904.05876},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwartz, Schwing, Hazan - 2019 - A Simple Baseline for Audio-Visual Scene-Aware Dialog.pdf:pdf},
isbn = {978-1-7281-3293-8},
issn = {10636919},
keywords = {Vision + Language},
month = {jun},
pages = {12548--12558},
publisher = {IEEE},
title = {{A Simple Baseline for Audio-Visual Scene-Aware Dialog}},
url = {https://ieeexplore.ieee.org/document/8953318/},
volume = {2019-June},
year = {2019}
}
@inproceedings{Kim2021structured,
abstract = {A video-grounded dialogue system referred to as the Structured Co-reference Graph Attention (SCGA) is presented for decoding the answer sequence to a question regarding a given video while keeping track of the dialogue context. Although recent efforts have made great strides in improving the quality of the response, performance is still far from satisfactory. The two main challenging issues are as follows: (1) how to deduce co-reference among multiple modalities and (2) how to reason on the rich underlying semantic structure of video with complex spatial and temporal dynamics. To this end, SCGA is based on (1) Structured Co-reference Resolver that performs dereferencing via building a structured graph over multiple modalities, (2) Spatio-temporal Video Reasoner that captures local-to-global dynamics of video via gradually neighboring graph attention. SCGA makes use of pointer network to dynamically replicate parts of the question for decoding the answer sequence. The validity of the proposed SCGA is demonstrated on AVSD@DSTC7 and AVSD@DSTC8 datasets, a challenging video-grounded dialogue benchmarks, and TVQA dataset, a large-scale videoQA benchmark. Our empirical results show that SCGA outperforms other state-of-the-art dialogue systems on both benchmarks, while extensive ablation study and qualitative analysis reveal performance gain and improved interpretability.},
archivePrefix = {arXiv},
arxivId = {2103.13361},
author = {Kim, Junyeong and Yoon, Sunjae and Kim, Dahyun and Yoo, Chang D.},
booktitle = {Proc. AAAI},
eprint = {2103.13361},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2021 - Structured Co-reference Graph Attention for Video-grounded Dialogue.pdf:pdf},
title = {{Structured Co-reference Graph Attention for Video-grounded Dialogue}},
url = {http://arxiv.org/abs/2103.13361},
pages = {1789--1797},
year = {2021}
}
@inproceedings{Zhuang2019investigation,
abstract = {In this paper, we show our effort on the Audio Visual Scene-aware dialog (AVSD) task which is proposed in DSTC7. We investigate the effectiveness of different modality fusion methods as well as the different input modalities. We also employ the Maximum Mutual Information (MMI) objective to replace the original objective function in the AVSD system. In our experiments, the proposed simplified attention-based multimodal fusion method outperforms the prior work slightly. Besides, the final system which uses MMI as the new objective obtains a 6.6% relative improvement over the base-line system on BLEU-1 metric.},
author = {Zhuang, Bairong and Wang, Wenbo and Shinozaki, Takahiro},
booktitle = {DSTC7 at AAAI 2019 workshop},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhuang, Wang, Shinozaki - 2019 - Investigation of Attention-Based Multimodal Fusion and Maximum Mutual Information Objective for DSTC7 T.pdf:pdf},
keywords = {Attention-Based Multimodal Fusion,DSTC7,Index Terms: Audio Visual Scene-Aware Dialog,Maximum Mutual Information},
number = {2},
title = {{Investigation of Attention-Based Multimodal Fusion and Maximum Mutual Information Objective for DSTC7 Track3}},
url = {http://www.ts.ip.titech.ac.jp},
year = {2019}
}
@inproceedings{Nguyen2018from,
abstract = {Understanding audio-visual content and the ability to have an informative conversation about it have both been challenging areas for intelligent systems. The Audio Visual Scene-aware Dialog (AVSD) challenge, organized as a track of the Dialog System Technology Challenge 7 (DSTC7), proposes a combined task, where a system has to answer questions pertaining to a video given a dialogue with previous question-answer pairs and the video itself. We propose for this task a hierarchical encoder-decoder model which computes a multi-modal embedding of the dialogue context. It first embeds the dialogue history using two LSTMs. We extract video and audio frames at regular intervals and compute semantic features using pre-trained I3D and VGGish models, respectively. Before summarizing both modalities into fixed-length vectors using LSTMs, we use FiLM blocks to condition them on the embeddings of the current question, which allows us to reduce the dimensionality considerably. Finally, we use an LSTM decoder that we train with scheduled sampling and evaluate using beam search. Compared to the modality-fusing baseline model released by the AVSD challenge organizers, our model achieves a relative improvements of more than 16%, scoring 0.36 BLEU-4 and more than 33%, scoring 0.997 CIDEr.},
archivePrefix = {arXiv},
arxivId = {1812.07023},
author = {Nguyen, Dat Tien and Sharma, Shikhar and Schulz, Hannes and Asri, Layla El},
eprint = {1812.07023},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen et al. - 2018 - From FiLM to Video Multi-turn Question Answering with Multi-modal Context.pdf:pdf},
title = {{From FiLM to Video: Multi-turn Question Answering with Multi-modal Context}},
url = {http://arxiv.org/abs/1812.07023},
booktitle = {DSTC7 at AAAI2019 workshop},
pages = {},
year = {2019}
}
@inproceedings{Le2020bist,
abstract = {Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns. However, existing approaches to video-grounded dialogues often focus on superficial temporal-level visual cues, but neglect more fine-grained spatial signals from videos. To address this drawback, we propose Bi-directional Spatio-Temporal Learning (BiST), a vision-language neural framework for high-resolution queries in videos based on textual cues. Specifically, our approach not only exploits both spatial and temporal-level information, but also learns dynamic information diffusion between the two feature spaces through spatial-to-temporal and temporal-to-spatial reasoning. The bidirectional strategy aims to tackle the evolving semantics of user queries in the dialogue setting. The retrieved visual cues are used as contextual information to construct relevant responses to the users. Our empirical results and comprehensive qualitative analysis show that BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA setting, and substantially outperform prior approaches on the TGIF-QA benchmark.},
archivePrefix = {arXiv},
arxivId = {2010.10095},
author = {Le, Hung and Sahoo, Doyen and Chen, Nancy and Hoi, Steven C.H.},
booktitle = {Proc. EMNLP},
doi = {10.18653/v1/2020.emnlp-main.145},
eprint = {2010.10095},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le et al. - 2020 - BiST Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues.pdf:pdf},
pages = {1846--1859},
title = {{BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues}},
url = {https://www.aclweb.org/anthology/2020.emnlp-main.145},
year = {2020}
}
@inproceedings{Chu2018face,
author = {Chu, Hang and Li, Daiqing and Fidler, Sanja},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00743},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chu, Li, Fidler - 2018 - A Face-to-Face Neural Conversation Model.pdf:pdf},
isbn = {978-1-5386-6420-9},
issn = {0023-2173},
month = {jun},
number = {17},
pages = {7113--7121},
pmid = {13756986},
publisher = {IEEE},
title = {{A Face-to-Face Neural Conversation Model}},
url = {http://link.springer.com/10.1007/BF01482140 https://ieeexplore.ieee.org/document/8578841/},
volume = {38},
year = {2018}
}
@inproceedings{Chen2021reasoning,
abstract = {In multi-turn dialog, utterances do not always take the full form of sentences (Carbonell 1983), which naturally makes understanding the dialog context more difficult. However, it is essential to fully grasp the dialog context to generate a reasonable response. Hence, in this paper, we propose to improve the response generation performance by examining the model's ability to answer a reading comprehension question, where the question is focused on the omitted information in the dialog. Enlightened by the multi-task learning scheme, we propose a joint framework that unifies these two tasks, sharing the same encoder to extract the common and task-invariant features with different decoders to learn task-specific features. To better fusing information from the question and the dialog history in the encoding part, we propose to augment the Transformer architecture with a memory updater, which is designed to selectively store and update the history dialog information so as to support downstream tasks. For the experiment, we employ human annotators to write and examine a large-scale dialog reading comprehension dataset. Extensive experiments are conducted on this dataset, and the results show that the proposed model brings substantial improvements over several strong baselines on both tasks. In this way, we demonstrate that reasoning can indeed help better response generation and vice versa. We release our large-scale dataset for further research1.},
archivePrefix = {arXiv},
arxivId = {2012.07410},
author = {Chen, Xiuying and Cui, Zhi and Zhang, Jiayi and Wei, Chen and Cui, Jianwei and Wang, Bin and Zhao, Dongyan and Yan, Rui},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
eprint = {2012.07410},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2021 - Reasoning in dialog Improving response Generation by context reading comprehension.pdf:pdf},
issn = {23318422},
title = {{Reasoning in dialog: Improving response Generation by context reading comprehension}},
year = {2021}
}
@inproceedings{Zhu2020convlab-2,
abstract = {We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. As the successor of ConvLab (Lee et al., 2019b), ConvLab-2 inherits ConvLab's framework but integrates more powerful dialogue models and supports more datasets. Besides, we have developed an analysis tool and an interactive tool to assist researchers in diagnosing dialogue systems. The analysis tool presents rich statistics and summarizes common mistakes from simulated dialogues, which facilitates error analysis and system improvement. The interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component.},
address = {Stroudsburg, PA, USA},
archivePrefix = {arXiv},
arxivId = {2002.04793},
author = {Zhu, Qi and Zhang, Zheng and Fang, Yan and Li, Xiang and Takanobu, Ryuichi and Li, Jinchao and Peng, Baolin and Gao, Jianfeng and Zhu, Xiaoyan and Huang, Minlie},
booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
doi = {10.18653/v1/2020.acl-demos.19},
eprint = {2002.04793},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu et al. - 2020 - ConvLab-2 An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems.pdf:pdf},
pages = {142--149},
publisher = {Association for Computational Linguistics},
title = {{ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems}},
url = {https://www.aclweb.org/anthology/2020.acl-demos.19},
year = {2020}
}
@inproceedings{Chen2020dmrm,
abstract = {Visual Dialog is a vision-language task that requires an AI agent to engage in a conversation with humans grounded in an image. It remains a challenging task since it requires the agent to fully understand a given question before making an appropriate response not only from the textual dialog history, but also from the visually-grounded information. While previous models typically leverage single-hop reasoning or single-channel reasoning to deal with this complex multimodal reasoning task, which is intuitively insufficient. In this paper, we thus propose a novel and more powerful Dual-channel Multi-hop Reasoning Model for Visual Dialog, named DMRM. DMRM synchronously captures information from the dialog history and the image to enrich the semantic representation of the question by exploiting dual-channel reasoning. Specifically, DMRM maintains a dual channel to obtain the question- and history-aware image features and the question- and image-aware dialog history features by a mulit-hop reasoning process in each channel. Additionally, we also design an effective multimodal attention to further enhance the decoder to generate more accurate responses. Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate that the proposed model is effective and outperforms compared models by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1912.08360},
author = {Chen, Feilong and Meng, Fandong and Xu, Jiaming and Li, Peng and Xu, Bo and Zhou, Jie},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i05.6248},
eprint = {1912.08360},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2020 - DMRM A Dual-Channel Multi-Hop Reasoning Model for Visual Dialog.pdf:pdf},
issn = {2374-3468},
month = {apr},
number = {05},
pages = {7504--7511},
title = {{DMRM: A Dual-Channel Multi-Hop Reasoning Model for Visual Dialog}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/6248},
volume = {34},
year = {2020}
}
@inproceedings{Li2020aloha,
abstract = {For conversational AI and virtual assistants to communicate with humans in a realistic way, they must exhibit human characteristics such as expression of emotion and personality. Current attempts toward constructing human-like dialogue agents have presented significant difficulties. We propose Human Level Attributes (HLAs) based on tropes as the basis of a method for learning dialogue agents that can imitate the personalities of fictional characters. Tropes are characteristics of fictional personalities that are observed recurrently and determined by viewers' impressions. By combining detailed HLA data with dialogue data for specific characters, we present a dataset, HLA-Chat, that models character profiles and gives dialogue agents the ability to learn characters' language styles through their HLAs. We then introduce a three-component system, ALOHA (which stands for Artificial Learning of Human Attributes), that combines character space mapping, character community detection, and language style retrieval to build a character (or personality) specific language model. Our preliminary experiments demonstrate that two variations of ALOHA, combined with our proposed dataset, can outperform baseline models at identifying the correct dialogue responses of chosen target characters, and are stable regardless of the character's identity, the genre of the show, and the context of the dialogue.},
archivePrefix = {arXiv},
arxivId = {1910.08293},
author = {Li, Aaron W. and Jiang, Veronica and Feng, Steven Y. and Sprague, Julia and Zhou, Wei and Hoey, Jesse},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i05.6328},
eprint = {1910.08293},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2020 - ALOHA Artificial Learning of Human Attributes for Dialogue Agents.pdf:pdf},
issn = {2374-3468},
keywords = {Natural Language Processing},
month = {apr},
number = {05},
pages = {8155--8163},
title = {{ALOHA: Artificial Learning of Human Attributes for Dialogue Agents}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/6328},
volume = {34},
year = {2020}
}
@inproceedings{Arabshahi2021conversational,
abstract = {In order for conversational AI systems to hold more natural and broad-ranging conversations, they will require much more commonsense, including the ability to identify unstated presumptions of their conversational partners. For example, in the command "If it snows at night then wake me up early because I don't want to be late for work" the speaker relies on commonsense reasoning of the listener to infer the implicit presumption that they wish to be woken only if it snows enough to cause traffic slowdowns. We consider here the problem of understanding such imprecisely stated natural language commands given in the form of "if-(state), then-(action), because-(goal)" statements. More precisely, we consider the problem of identifying the unstated presumptions of the speaker that allow the requested action to achieve the desired goal from the given state (perhaps elaborated by making the implicit presumptions explicit). We release a benchmark data set for this task, collected from humans and annotated with commonsense presumptions. We present a neuro-symbolic theorem prover that extracts multi-hop reasoning chains, and apply it to this problem. Furthermore, to accommodate the reality that current AI commonsense systems lack full coverage, we also present an interactive conversational framework built on our neuro-symbolic system, that conversationally evokes commonsense knowledge from humans to complete its reasoning chains.},
archivePrefix = {arXiv},
arxivId = {2006.10022},
author = {Arabshahi, Forough and Lee, Jennifer and Gawarecki, Mikayla and Mazaitis, Kathryn and Azaria, Amos and Mitchell, Tom},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
eprint = {2006.10022},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arabshahi et al. - 2021 - Conversational Neuro-Symbolic Commonsense Reasoning.pdf:pdf},
title = {{Conversational Neuro-Symbolic Commonsense Reasoning}},
url = {http://arxiv.org/abs/2006.10022},
year = {2021}
}
@article{Wang2021audio,
abstract = {This paper presents the details of the Audio-Visual Scene Classification task in the DCASE 2021 Challenge (Task 1 Subtask B). The task is concerned with classification using audio and video modalities, using a dataset of synchronized recordings. Here we describe the datasets and baseline systems. After the challenge submission deadline, challenge results and analysis of the submissions will be added.},
archivePrefix = {arXiv},
arxivId = {2105.13675},
author = {Wang, Shanshan and Heittola, Toni and Mesaros, Annamaria and Virtanen, Tuomas},
eprint = {2105.13675},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2021 - Audio-visual scene classification analysis of DCASE 2021 Challenge submissions.pdf:pdf},
month = {may},
title = {{Audio-visual scene classification: analysis of DCASE 2021 Challenge submissions}},
url = {http://arxiv.org/abs/2105.13675},
year = {2021}
}
@inproceedings{Chen2020Multimodal,
abstract = {Visual Dialog: aiming at holding a meaningful conversation with humans based on natural images, is a 'high-level' AI task of multimodal fusion. Since the challenge for visual dialog was proposed in 2017, multimodal fusion has been developed and made significant breakthroughs with the help of deep learning techniques. The goal of this paper is to provide a comprehensive survey of the recent achievements in the Visual Dialog task. This survey covers many aspects of multimodal fusion research: Visual Co-reference Resolution, Attention Mechanism, Graph Neural Networks, evaluation issues, specifically benchmark datasets, evaluation metrics, and state of the art performance.},
address = {New York, NY, USA},
author = {Chen, Xiaofan and Lao, Songyang and Duan, Ting},
booktitle = {Proceedings of the 2020 2nd International Conference on Robotics, Intelligent Control and Artificial Intelligence},
doi = {10.1145/3438872.3439098},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Lao, Duan - 2020 - Multimodal Fusion of Visual Dialog A Survey(2).pdf:pdf},
isbn = {9781450388306},
keywords = {Visual Dialog,deep learning,multimodal fusion,vision and language},
month = {oct},
pages = {302--308},
publisher = {ACM},
title = {{Multimodal Fusion of Visual Dialog: A Survey}},
url = {https://dl.acm.org/doi/10.1145/3438872.3439098},
year = {2020}
}
@inproceedings{Budzianowski2018multiwoz,
abstract = {Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available. To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora. The contribution of this work apart from the open-sourced dataset labelled with dialogue belief states and dialogue actions is two-fold: firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators; secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.},
address = {Stroudsburg, PA, USA},
archivePrefix = {arXiv},
arxivId = {1810.00278},
author = {Budzianowski, Pawe{\l} and Wen, Tsung-Hsien and Tseng, Bo-Hsiang and Casanueva, I{\~{n}}igo and Ultes, Stefan and Ramadan, Osman and Ga{\v{s}}i{\'{c}}, Milica},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
doi = {10.18653/v1/D18-1547},
eprint = {1810.00278},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Budzianowski et al. - 2018 - MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling.pdf:pdf},
isbn = {9781948087841},
pages = {5016--5026},
publisher = {Association for Computational Linguistics},
title = {{MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling}},
url = {http://aclweb.org/anthology/D18-1547},
year = {2018}
}
@inproceedings{Shen2020counseling,
abstract = {We introduce a counseling dialogue system that seeks to assist counselors while they are learning and refining their counseling skills. The system generates counselors{'}reflections {--} i.e., responses that reflect back on what the client has said given the dialogue history. Our method builds upon the new generative pretrained transformer architecture and enhances it with context augmentation techniques inspired by traditional strategies used during counselor training. Through a set of comparative experiments, we show that the system that incorporates these strategies performs better in the reflection generation task than a system that is just fine-tuned with counseling conversations. To confirm our findings, we present a human evaluation study that shows that our system generates naturally-looking reflections that are also stylistically and grammatically correct.},
author = {Shen, Siqi and Welch, Charles and Mihalcea, Rada and P{\'{e}}rez-Rosas, Ver{\'{o}}nica},
booktitle = {Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shen et al. - 2020 - Counseling-Style Reflection Generation Using Generative Pretrained Transformers with Augmented Context.pdf:pdf},
number = {July},
pages = {10--20},
title = {{Counseling-Style Reflection Generation Using Generative Pretrained Transformers with Augmented Context}},
url = {https://www.aclweb.org/anthology/2020.sigdial-1.2},
year = {2020}
}
@inproceedings{Chiang2020empirical,
abstract = {With a lot of work about context-free question answering systems, there is an emerging trend of conversational question answering models in the natural language processing field. Thanks to the recently collected datasets, including QuAC and CoQA, there has been more work on conversational question answering, and recent work has achieved competitive performance on both datasets. However, to best of our knowledge, two important questions for conversational comprehension research have not been well studied: 1) How well can the benchmark dataset reflect models' content understanding? 2) Do the models well utilize the conversation content when answering questions? To investigate these questions, we design different training settings, testing settings, as well as an attack to verify the models' capability of content understanding on QuAC and CoQA. The experimental results indicate some potential hazards in the benchmark datasets, QuAC and CoQA, for conversational comprehension research. Our analysis also sheds light on both what models may learn and how datasets may bias the models. With deep investigation of the task, it is believed that this work can benefit the future progress of conversation comprehension. The source code is available at https://github.com/MiuLab/CQA-Study.},
archivePrefix = {arXiv},
arxivId = {1909.10743},
author = {Chiang, Ting-Rui and Ye, Hao-Tong and Chen, Yun-Nung},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i05.6257},
eprint = {1909.10743},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chiang, Ye, Chen - 2020 - An Empirical Study of Content Understanding in Conversational Question Answering.pdf:pdf},
issn = {2374-3468},
keywords = {Natural Language Processing},
month = {apr},
number = {05},
pages = {7578--7585},
title = {{An Empirical Study of Content Understanding in Conversational Question Answering}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/6257},
volume = {34},
year = {2020}
}
@article{Akbari2021vatt,
abstract = {We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600,and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training.},
archivePrefix = {arXiv},
arxivId = {2104.11178},
author = {Akbari, Hassan and Yuan, Linagzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
eprint = {2104.11178},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Akbari et al. - 2021 - VATT Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text.pdf:pdf},
month = {apr},
title = {{VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text}},
url = {http://arxiv.org/abs/2104.11178},
year = {2021}
}
@inproceedings{Ochs2018acorformed,
author = {Ochs, M and Blache, P and Montcheuil, G and Pergandi, J-.M. and Bertrand, R and Saubesty, J and Francon, D and Mestre, D},
booktitle = {CLARIN Annual Conference 2018},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ochs et al. - 2018 - The Acorformed Corpus Investigating Multimodality in Human-Human and Human-Virtual Patient Interactions.pdf:pdf},
title = {{The Acorformed Corpus: Investigating Multimodality in Human-Human and Human-Virtual Patient Interactions}},
year = {2018}
}
@article{Lee2020parameter,
abstract = {The recent success of Transformers in the language domain has motivated adapting it to a multimodal setting, where a new visual model is trained in tandem with an already pretrained language model. However, due to the excessive memory requirements from Transformers, existing work typically fixes the language model and train only the vision module, which limits its ability to learn cross-modal information in an end-to-end manner. In this work, we focus on reducing the parameters of multimodal Transformers in the context of audio-visual video representation learning. We alleviate the high memory requirement by sharing the weights of Transformers across layers and modalities; we decompose the Transformer into modality-specific and modality-shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. We show that our approach reduces parameters up to 80$\%$, allowing us to train our model end-to-end from scratch. We also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns with the Transformers. To demonstrate our approach, we pretrain our model on 30-second clips from Kinetics-700 and transfer it to audio-visual classification tasks.},
archivePrefix = {arXiv},
arxivId = {2012.04124},
author = {Lee, Sangho and Yu, Youngjae and Kim, Gunhee and Breuel, Thomas and Kautz, Jan and Song, Yale},
eprint = {2012.04124},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2020 - Parameter Efficient Multimodal Transformers for Video Representation Learning.pdf:pdf},
month = {dec},
title = {{Parameter Efficient Multimodal Transformers for Video Representation Learning}},
url = {http://arxiv.org/abs/2012.04124},
year = {2020}
}
@inproceedings{Dou2021multitalk,
abstract = {We study conversational dialog in which there are many possible responses to a given history. We present the MultiTalk Dataset, a corpus of over 320,000 sentences of written conversational dialog that balances a high branching factor (10) with several conversation turns (6) through selective branch continuation. We make multiple contributions to study dialog generation in the highly branching setting. In order to evaluate a diverse set of generations, we propose a simple scoring algorithm, based on bipartite graph matching, to optimally incorporate a set of diverse references. We study multiple language generation tasks at different levels of predictive conversation depth, using textual attributes induced automatically from pretrained classifiers. Our culminating task is a challenging theory of mind problem, a controllable generation task which requires reasoning about the expected reaction of the listener.},
archivePrefix = {arXiv},
arxivId = {2102.01263},
author = {Dou, Yao and Forbes, Maxwell and Holtzman, Ari and Choi, Yejin},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
eprint = {2102.01263},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dou et al. - 2021 - MultiTalk A Highly-Branching Dialog Testbed for Diverse Conversations.pdf:pdf},
title = {{MultiTalk: A Highly-Branching Dialog Testbed for Diverse Conversations}},
url = {http://arxiv.org/abs/2102.01263},
year = {2021}
}
@article{Shi2020contrastive,
abstract = {Several multi-modality representation learning approaches such as LXMERT and ViLBERT have been proposed recently. Such approaches can achieve superior performance due to the high-level semantic information captured during large-scale multimodal pretraining. However, as ViLBERT and LXMERT adopt visual region regression and classification loss, they often suffer from domain gap and noisy label problems, based on the visual features having been pretrained on the Visual Genome dataset. To overcome these issues, we propose unbiased Contrastive Visual-Linguistic Pretraining (CVLP), which constructs a visual self-supervised loss built upon contrastive learning. We evaluate CVLP on several down-stream tasks, including VQA, GQA and NLVR2 to validate the superiority of contrastive learning on multi-modality representation learning. Our code is available at: https://github.com/ArcherYunDong/CVLP-.},
archivePrefix = {arXiv},
arxivId = {2007.13135},
author = {Shi, Lei and Shuang, Kai and Geng, Shijie and Su, Peng and Jiang, Zhengkai and Gao, Peng and Fu, Zuohui and de Melo, Gerard and Su, Sen},
eprint = {2007.13135},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi et al. - 2020 - Contrastive Visual-Linguistic Pretraining.pdf:pdf},
title = {{Contrastive Visual-Linguistic Pretraining}},
url = {http://arxiv.org/abs/2007.13135},
year = {2020}
}
@article{Arnab2021vivit,
abstract = {We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatio-temporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks. To facilitate further research, we will release code and models.},
archivePrefix = {arXiv},
arxivId = {2103.15691},
author = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'{c}}, Mario and Schmid, Cordelia},
eprint = {2103.15691},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arnab et al. - 2021 - ViViT A Video Vision Transformer.pdf:pdf},
title = {{ViViT: A Video Vision Transformer}},
url = {http://arxiv.org/abs/2103.15691},
year = {2021}
}
@article{Gunasekara2020overview,
abstract = {This paper introduces the Ninth Dialog System Technology Challenge (DSTC-9). This edition of the DSTC focuses on applying end-to-end dialog technologies for four distinct tasks in dialog systems, namely, 1. Task-oriented dialog Modeling with unstructured knowledge access, 2. Multi-domain task-oriented dialog, 3. Interactive evaluation of dialog, and 4. Situated interactive multi-modal dialog. This paper describes the task definition, provided datasets, baselines and evaluation set-up for each track. We also summarize the results of the submitted systems to highlight the overall trends of the state-of-the-art technologies for the tasks.},
archivePrefix = {arXiv},
arxivId = {2011.06486},
author = {Gunasekara, Chulaka and Kim, Seokhwan and D'Haro, Luis Fernando and Rastogi, Abhinav and Chen, Yun-Nung and Eric, Mihail and Hedayatnia, Behnam and Gopalakrishnan, Karthik and Liu, Yang and Huang, Chao-Wei and Hakkani-T{\"{u}}r, Dilek and Li, Jinchao and Zhu, Qi and Luo, Lingxiao and Liden, Lars and Huang, Kaili and Shayandeh, Shahin and Liang, Runze and Peng, Baolin and Zhang, Zheng and Shukla, Swadheen and Huang, Minlie and Gao, Jianfeng and Mehri, Shikib and Feng, Yulan and Gordon, Carla and Alavi, Seyed Hossein and Traum, David and Eskenazi, Maxine and Beirami, Ahmad and Eunjoon, Cho and Crook, Paul A. and De, Ankita and Geramifard, Alborz and Kottur, Satwik and Moon, Seungwhan and Poddar, Shivani and Subba, Rajen},
doi = {10.1016/j.csl.2018.09.004},
eprint = {2011.06486},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gunasekara et al. - 2020 - Overview of the Ninth Dialog System Technology Challenge DSTC9.pdf:pdf},
issn = {08852308},
journal = {Computer Speech & Language},
keywords = {Conversation model,DSTC,Dialogue breakdown,End-to-end dialogue system,Natural Language Generation,Sequence-to-sequence model},
month = {nov},
pages = {1--25},
title = {{Overview of the Ninth Dialog System Technology Challenge: DSTC9}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230818300937 http://arxiv.org/abs/2011.06486},
volume = {55},
year = {2020}
}
@inproceedings{Yu2020towards,
abstract = {With the recent advances of multimodal interactive recommendations, the users are able to express their preference by natural language feedback to the item images, to find the desired items. However, the existing systems either retrieve only one item or require the user to specify (e.g., by click or touch) the commented items from a list of recommendations in each user interaction. As a result, the users are not hands-free and the recommendations may be impractical. We propose a hands-free visual dialog recommender system to interactively recommend a list of items. At each time, the system shows a list of items with visual appearance. The user can comment on the list in natural language, to describe the desired features they further want. With these multimodal data, the system chooses another list of items to recommend. To understand the user preference from these multimodal data, we develop neural network models which identify the described items among the list and further predict the desired attributes. To achieve efficient interactive recommendations, we leverage the inferred user preference and further develop a novel bandit algorithm. Specifically, to avoid the system exploring more than needed, the desired attributes are utilized to reduce the exploration space. More importantly, to achieve sample efficient learning in this hands-free setting, we derive additional samples from the user's relative preference expressed in natural language and design a pairwise logistic loss in bandit learning. Our bandit model is jointly updated by the pairwise logistic loss on the additional samples derived from natural language feedback and the traditional logistic loss. The empirical results show that the probability of finding the desired items by our system is about 3 times as high as that by the traditional interactive recommenders, after a few user interactions.},
author = {Yu, Tong and Shen, Yilin and Jin, Hongxia},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i01.5465},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu, Shen, Jin - 2020 - Towards Hands-Free Visual Dialog Interactive Recommendation.pdf:pdf},
issn = {2374-3468},
keywords = {Applications},
month = {apr},
number = {01},
pages = {1137--1144},
title = {{Towards Hands-Free Visual Dialog Interactive Recommendation}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/5465},
volume = {34},
year = {2020}
}
@inproceedings{Axelsson2019modelling,
abstract = {In dialogue, speakers continuously adapt their speech to accommodate the listener, based on the feedback they receive. In this paper, we explore the modelling of such behaviours in the context of a robot presenting a painting. A Behaviour Tree is used to organise the behaviour on different levels, and allow the robot to adapt its behaviour in real-time; the tree organises engagement, joint attention, turn-taking, feedback and incremental speech processing. An initial implementation of the model is presented, and the system is evaluated in a user study, where the adaptive robot presenter is compared to a non-adaptive version. The adaptive version is found to be more engaging by the users, although no effects are found on the retention of the presented material.},
address = {Stroudsburg, PA, USA},
author = {Axelsson, Nils and Skantze, Gabriel},
booktitle = {Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue},
doi = {10.18653/v1/W19-5940},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Axelsson, Skantze - 2019 - Modelling Adaptive Presentations in Human-Robot Interaction using Behaviour Trees.pdf:pdf},
isbn = {9781950737611},
number = {September},
pages = {345--352},
publisher = {Association for Computational Linguistics},
title = {{Modelling Adaptive Presentations in Human-Robot Interaction using Behaviour Trees}},
url = {https://www.aclweb.org/anthology/W19-5940},
year = {2019}
}
@article{Li2021pretrained,
abstract = {Text generation has become one of the most important yet challenging tasks in natural language processing (NLP). The resurgence of deep learning has greatly advanced this field by neural generation models, especially the paradigm of pretrained language models (PLMs). In this paper, we present an overview of the major advances achieved in the topic of PLMs for text generation. As the preliminaries, we present the general task definition and briefly describe the mainstream architectures of PLMs for text generation. As the core content, we discuss how to adapt existing PLMs to model different input data and satisfy special properties in the generated text. We further summarize several important fine-tuning strategies for text generation. Finally, we present several future directions and conclude this paper. Our survey aims to provide text generation researchers a synthesis and pointer to related research.},
archivePrefix = {arXiv},
arxivId = {2105.10311},
author = {Li, Junyi and Tang, Tianyi and Zhao, Wayne Xin and Wen, Ji-Rong},
eprint = {2105.10311},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2021 - Pretrained Language Models for Text Generation A Survey.pdf:pdf},
month = {may},
title = {{Pretrained Language Models for Text Generation: A Survey}},
url = {http://arxiv.org/abs/2105.10311},
year = {2021}
}
@inproceedings{Cohn2019large,
abstract = {This study tests the effect of cognitive-emotional expression in an Alexa text-to-speech (TTS) voice on users' experience with a social dialog system. We systematically introduced emotionally expressive interjections (e.g., “Wow!”) and filler words (e.g., “um”, “mhmm”) in an Amazon Alexa Prize socialbot, Gunrock. We tested whether these TTS manipulations improved users' ratings of their conversation across thousands of real user interactions (n=5,527). Results showed that interjections and fillers each improved users' holistic ratings, an improvement that further increased if the system used both manipulations. A separate perception experiment corroborated the findings from the user study, with improved social ratings for conversations including interjections; however, no positive effect was observed for fillers, suggesting that the role of the rater in the conversation—as active participant or external listener—is an important factor in assessing social dialogs.},
address = {Stroudsburg, PA, USA},
author = {Cohn, Michelle and Chen, Chun-Yen and Yu, Zhou},
booktitle = {Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue},
doi = {10.18653/v1/W19-5935},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohn, Chen, Yu - 2019 - A Large-Scale User Study of an Alexa Prize Chatbot Effect of TTS Dynamism on Perceived Quality of Social Dialog.pdf:pdf},
isbn = {9781950737611},
number = {September},
pages = {293--306},
publisher = {Association for Computational Linguistics},
title = {{A Large-Scale User Study of an Alexa Prize Chatbot: Effect of TTS Dynamism on Perceived Quality of Social Dialog}},
url = {https://www.aclweb.org/anthology/W19-5935},
year = {2019}
}
@article{Huang2020challenges,
abstract = {There is a resurgent interest in developing intelligent open-domain dialog systems due to the availability of large amounts of conversational data and the recent progress on neural approaches to conversational AI [33]. Unlike traditional task-oriented bots, an open-domain dialog system aims to establish long-term connections with users by satisfying the human need for communication, affection, and social belonging. This article reviews the recent work on neural approaches that are devoted to addressing three challenges in developing such systems: semantics , consistency , and interactiveness . Semantics requires a dialog system to not only understand the content of the dialog but also identify users' emotional and social needs during the conversation. Consistency requires the system to demonstrate a consistent personality to win users' trust and gain their long-term confidence. Interactiveness refers to the system's ability to generate interpersonal responses to achieve particular social goals such as entertainment and conforming. The studies we select to present in this survey are based on our unique views and are by no means complete. Nevertheless, we hope that the discussion will inspire new research in developing more intelligent open-domain dialog systems.},
archivePrefix = {arXiv},
arxivId = {1905.05709},
author = {Huang, Minlie and Zhu, Xiaoyan and Gao, Jianfeng},
doi = {10.1145/3383123},
eprint = {1905.05709},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang, Zhu, Gao - 2020 - Challenges in Building Intelligent Open-domain Dialog Systems.pdf:pdf},
issn = {1046-8188},
journal = {ACM Transactions on Information Systems},
keywords = {Dialog system,chatbot,conversation generation,conversational AI,response generation,social bot},
month = {jun},
number = {3},
pages = {1--32},
title = {{Challenges in Building Intelligent Open-domain Dialog Systems}},
url = {https://dl.acm.org/doi/10.1145/3383123},
volume = {38},
year = {2020}
}
@inproceedings{Roller2021recipes,
abstract = {Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills: providing engaging talking points, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.},
archivePrefix = {arXiv},
arxivId = {2004.13637},
author = {Roller, Stephen and Dinan, Emily and Goyal, Naman and Ju, Da and Williamson, Mary and Liu, Yinhan and Xu, Jing and Ott, Myle and Shuster, Kurt and Smith, Eric M. and Boureau, Y-Lan and Weston, Jason},
booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics},
eprint = {2004.13637},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Roller et al. - 2021 - Recipes for building an open-domain chatbot.pdf:pdf},
isbn = {9781954085022},
pages = {300--325},
title = {{Recipes for building an open-domain chatbot}},
url = {http://arxiv.org/abs/2004.13637},
year = {2021}
}
@inproceedings{Pei2020visual,
abstract = {Recently, various attention-based networks have achieved state-of-art results on image captioning tasks. However, this simple mechanism is insufficient to modelling and reasoning the relationships between the visual regions required for scene understanding. In this research, we propose a visual relational reasoning module to implicit learning semantic and spatial relationships between pairs of relevant visual objects and infers the feature output that is most relevant to the currently generated word. Furthermore, a context gate is introduced to dynamically control the contribution of visual region attention modules and visual relational reasoning module which allows predicting different words according to different type of features (visual or visual relationship). We evaluate our model on the MSCOCO dataset and achieved state-of-the-art results. Qualitative analysis shows that our visual relational reasoning model can dynamically model and reason the most relevant features of different types of generated words and improve the quality of the caption.},
author = {Pei, Haolei and Chen, Qiaohong and Wang, Ji and Sun, Qi and Jia, Yubo},
booktitle = {2020 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN48605.2020.9206815},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pei et al. - 2020 - Visual Relational Reasoning for Image Caption.pdf:pdf},
isbn = {978-1-7281-6926-2},
month = {jul},
pages = {1--8},
publisher = {IEEE},
title = {{Visual Relational Reasoning for Image Caption}},
url = {https://ieeexplore.ieee.org/document/9206815/},
year = {2020}
}
@inproceedings{Ochs2018semi,
abstract = {In this paper, we introduce a two-step corpora-based methodology, starting from a corpus of human-human interactions to construct a semi-autonomous system in order to collect a new corpus of human-machine interaction, a step before the development of a fully autonomous system constructed based on the analysis of the collected corpora. The presented methodology is illustrated in the context of a virtual reality training platform for doctors breaking bad news.},
author = {Ochs, M. and Blache, P. and Montcheuil, G. and Pergandi, J. M. and Saubesty, J. and Francon, D. and Mestre, D.},
booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ochs et al. - 2018 - A Semi-autonomous System for Creating a Human-Machine Interaction Corpus in Virtual Reality Application to the acor.pdf:pdf},
isbn = {9791095546009},
keywords = {Corpus,Health domain,Training,Virtual patient,Virtual reality},
pages = {2952--2957},
title = {{A Semi-autonomous System for Creating a Human-Machine Interaction Corpus in Virtual Reality: Application to the acorformed system for training doctors to break bad news}},
year = {2018}
}
@inproceedings{Kontogiorgos2018multimodal,
abstract = {In this paper we present a corpus of multiparty situated interaction where participants collaborated on moving virtual objects on a large touch screen. A moderator facilitated the discussion and directed the interaction. The corpus contains recordings of a variety of multimodal data, in that we captured speech, eye gaze and gesture data using a multisensory setup (wearable eye trackers, motion capture and audio/video). Furthermore, in the description of the multimodal corpus, we investigate four different types of social gaze: referential gaze, joint attention, mutual gaze and gaze aversion by both perspectives of a speaker and a listener. We annotated the groups' object references during object manipulation tasks and analysed the group's proportional referential eye-gaze with regards to the referent object. When investigating the distributions of gaze during and before referring expressions we could corroborate the differences in time between speakers' and listeners' eye gaze found in earlier studies. This corpus is of particular interest to researchers who are interested in social eye-gaze patterns in turn-taking and referring language in situated multi-party interaction.},
author = {Kontogiorgos, Dimosthenis and Avramova, Vanya and Alexanderson, Simon and Jonell, Patrik and Oertel, Catharine and Beskow, Jonas and Skantze, Gabriel and Gustafson, Joakim},
booktitle = {LREC 2018 - 11th International Conference on Language Resources and Evaluation},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kontogiorgos et al. - 2019 - A multimodal corpus for mutual gaze and joint attention in multiparty situated interaction.pdf:pdf},
isbn = {9791095546009},
keywords = {Joint attention,Multimodal situated interaction,Mutual gaze,Reference resolution,Referential gaze,Social eye-gaze},
pages = {119--127},
title = {{A multimodal corpus for mutual gaze and joint attention in multiparty situated interaction}},
year = {2019}
}
@inproceedings{Aneja2021understanding,
abstract = {Embodied conversational agents have changed the ways we can interact with machines. However, these systems often do not meet users' expectations. A limitation is that the agents are monotonic in behavior and do not adapt to an interlocutor. We present SIVA (a Socially Intelligent Virtual Agent), an expressive, embodied conversational agent that can recognize human behavior during open-ended conversations and automatically align its responses to the conversational and expressive style of the other party. SIVA leverages multimodal inputs to produce rich and perceptually valid responses (lip syncing and facial expressions) during the conversation. We conducted a user study (N=30) in which participants rated SIVA as being more empathetic and believable than the control (agent without style matching). Based on almost 10 hours of interaction, participants who preferred interpersonal involvement evaluated SIVA as significantly more animate than the participants who valued consideration and independence.},
address = {New York, NY, USA},
author = {Aneja, Deepali and Hoegen, Rens and McDuff, Daniel and Czerwinski, Mary},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3411764.3445708},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aneja et al. - 2021 - Understanding Conversational and Expressive Style in a Multimodal Embodied Conversational Agent.pdf:pdf},
isbn = {9781450380966},
keywords = {acm reference format,conversational style,embodied agents,emotional expressions,facial ex-,multi-modality,pressive style,social behavior,social dialogue},
month = {may},
publisher = {ACM},
title = {{Understanding Conversational and Expressive Style in a Multimodal Embodied Conversational Agent}},
url = {https://dl.acm.org/doi/10.1145/3411764.3445708},
year = {2021}
}
@inproceedings{Qian2017exploring,
abstract = {Spoken language understanding (SLU) in dialog systems is generally performed using a natural language understanding (NLU) model based on the hypotheses produced by an automatic speech recognition (ASR) system. However, when new spoken dialog applications are built from scratch in real user environments that often have sub-optimal audio characteristics, ASR performance can suffer due to factors such as the paucity of training data or a mismatch between the training and test data. To address this issue, this paper proposes an ASR-free, end-to-end (E2E) modeling approach to SLU for a cloud-based, modular spoken dialog system (SDS). We evaluate the effectiveness of our approach on crowdsourced data collected from non-native English speakers interacting with a conversational language learning application. Experimental results show that our approach is particularly promising in situations with low ASR accuracy. It can further improve the performance of a sophisticated CNN-based SLU system with more accurate ASR hypotheses by fusing the scores from E2E system, i.e., the overall accuracy of SLU is improved from 85.6% to 86.5%.},
author = {Qian, Yao and Ubale, Rutuja and Ramanaryanan, Vikram and Lange, Patrick and Suendermann-Oeft, David and Evanini, Keelan and Tsuprun, Eugene},
booktitle = {2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
doi = {10.1109/ASRU.2017.8268987},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian et al. - 2017 - Exploring ASR-free end-to-end modeling to improve spoken language understanding in a cloud-based dialog system.pdf:pdf},
isbn = {978-1-5090-4788-8},
keywords = {end-to-end,spoken language understanding},
month = {dec},
pages = {569--576},
publisher = {IEEE},
title = {{Exploring ASR-free end-to-end modeling to improve spoken language understanding in a cloud-based dialog system}},
url = {http://ieeexplore.ieee.org/document/8268987/},
volume = {2018-Janua},
year = {2017}
}
@inproceedings{Kalpakchi2019spacerefnet,
abstract = {Adding interactive capabilities to pedestrian wayfinding systems in the form of spoken dialogue will make them more natural to humans. Such an interactive wayfinding system needs to continuously understand and interpret pedestrian's utterances referring to the spatial context. Achieving this requires the system to identify exophoric referring expressions in the utterances, and link these expressions to the geographic entities in the vicinity. This exophoric spatial reference resolution problem is difficult, as there are often several dozens of candidate referents. We present a neural network-based approach for identifying pedestrian's references (using a network called RefNet) and resolving them to appropriate geographic objects (using a network called SpaceRefNet). Both methods show promising results beating the respective baselines and earlier reported results in the literature.},
address = {Stroudsburg, PA, USA},
author = {Kalpakchi, Dmytro and Boye, Johan},
booktitle = {Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue},
doi = {10.18653/v1/W19-5949},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalpakchi, Boye - 2019 - SpaceRefNet a neural approach to spatial reference resolution in a real city environment.pdf:pdf},
isbn = {9781950737611},
number = {September},
pages = {422--431},
publisher = {Association for Computational Linguistics},
title = {{SpaceRefNet: a neural approach to spatial reference resolution in a real city environment}},
url = {https://www.aclweb.org/anthology/W19-5949},
year = {2019}
}
@inproceedings{Lykartsis2019prediction,
abstract = {In this paper we aim to predict dialogue success and user satisfaction as well as emotion on a turn level. To achieve this, we investigate the use of spectrogram representations, extracted from audio files, in combination with several types of convolutional neural networks. The experiments were performed on the Let's Go V2 database, comprising 5065 audio files and having labels for subjective and objective dialogue turn success, as well as the emotional state of the user. Results show that by using only audio, it is possible to predict turn success with very high accuracy for all three labels (90%). The best performing input representation were 1s long mel-spectrograms in combination with a CNN with a bottleneck architecture. The resulting system has the potential to be used real-time. Our results significantly surpass the state of the art for dialogue success prediction based only on audio.},
address = {Stroudsburg, PA, USA},
author = {Lykartsis, Athanasios and Kotti, Margarita},
booktitle = {Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue},
doi = {10.18653/v1/W19-5939},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lykartsis, Kotti - 2019 - Prediction of User Emotion and Dialogue Success Using Audio Spectrograms and Convolutional Neural Networks.pdf:pdf},
isbn = {9781950737611},
number = {September},
pages = {336--344},
publisher = {Association for Computational Linguistics},
title = {{Prediction of User Emotion and Dialogue Success Using Audio Spectrograms and Convolutional Neural Networks}},
url = {https://www.aclweb.org/anthology/W19-5939},
year = {2019}
}
@inproceedings{Rastogi2020towards,
abstract = {Virtual assistants such as Google Assistant, Alexa and Siri provide a conversational interface to a large number of services and APIs spanning multiple domains. Such systems need to support an ever-increasing number of services with possibly overlapping functionality. Furthermore, some of these services have little to no training data available. Existing public datasets for task-oriented dialogue do not sufficiently capture these challenges since they cover few domains and assume a single static ontology per domain. In this work, we introduce the the Schema-Guided Dialogue (SGD) dataset, containing over 16k multi-domain conversations spanning 16 domains. Our dataset exceeds the existing task-oriented dialogue corpora in scale, while also highlighting the challenges associated with building large-scale virtual assistants. It provides a challenging testbed for a number of tasks including language understanding, slot filling, dialogue state tracking and response generation. Along the same lines, we present a schema-guided paradigm for task-oriented dialogue, in which predictions are made over a dynamic set of intents and slots, provided as input, using their natural language descriptions. This allows a single dialogue system to easily support a large number of services and facilitates simple integration of new services without requiring additional training data. Building upon the proposed paradigm, we release a model for dialogue state tracking capable of zero-shot generalization to new APIs, while remaining competitive in the regular setting.},
archivePrefix = {arXiv},
arxivId = {1909.05855},
author = {Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav and Khaitan, Pranav},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i05.6394},
eprint = {1909.05855},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rastogi et al. - 2020 - Towards Scalable Multi-Domain Conversational Agents The Schema-Guided Dialogue Dataset.pdf:pdf},
issn = {2374-3468},
keywords = {Natural Language Processing},
month = {apr},
number = {05},
pages = {8689--8696},
title = {{Towards Scalable Multi-Domain Conversational Agents: The Schema-Guided Dialogue Dataset}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/6394},
volume = {34},
year = {2020}
}
@inproceedings{Rahimi2020entrainment,
abstract = {Entrainment is the propensity of speakers to begin behaving like one another in conversation. While most entrainment studies have focused on dyadic interactions, researchers have also started to investigate multi-party conversations. In these studies, multi-party entrainment has typically been estimated by averaging the pairs' entrainment values or by averaging individuals' entrainment to the group. While such multi-party measures utilize the strength of dyadic entrainment, they have not yet exploited different aspects of the dynamics of entrainment relations in multi-party groups. In this paper, utilizing an existing pairwise asymmetric entrainment measure, we propose a novel graph-based vector representation of multi-party entrainment that incorporates both strength and dynamics of pairwise entrainment relations. The proposed kernel approach and weakly-supervised representation learning method show promising results at the downstream task of predicting team outcomes. Also, examining the embedding, we found interesting information about the dynamics of the entrainment relations. For example, teams with more influential members have more process conflict.},
author = {Rahimi, Zahra and Litman, Diane},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i05.6393},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rahimi, Litman - 2020 - Entrainment2Vec Embedding Entrainment for Multi-Party Dialogues.pdf:pdf},
issn = {2159-5399},
keywords = {Natural Language Processing},
month = {apr},
number = {05},
pages = {8681--8688},
title = {{Entrainment2Vec: Embedding Entrainment for Multi-Party Dialogues}},
volume = {34},
year = {2020}
}
@inproceedings{Ahuja2019react,
abstract = {Non verbal behaviours such as gestures, facial expressions, body posture, and para-linguistic cues have been shown to complement or clarify verbal messages. Hence to improve telepresence, in form of an avatar, it is important to model these behaviours, especially in dyadic interactions. Creating such personalized avatars not only requires to model intrapersonal dynamics between a avatar's speech and their body pose, but it also needs to model interpersonal dynamics with the interlocutor present in the conversation. In this paper, we introduce a neural architecture named Dyadic Residual-Attention Model (DRAM), which integrates intrapersonal (monadic) and interpersonal (dyadic) dynamics using selective attention to generate sequences of body pose conditioned on audio and body pose of the interlocutor and audio of the human operating the avatar. We evaluate our proposed model on dyadic conversational data consisting of pose and audio of both participants, confirming the importance of adaptive attention between monadic and dyadic dynamics when predicting avatar pose. We also conduct a user study to analyze judgments of human observers. Our results confirm that the generated body pose is more natural, models intrapersonal dynamics and interpersonal dynamics better than non-adaptive monadic/dyadic models.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1910.02181},
author = {Ahuja, Chaitanya and Ma, Shugao and Morency, Louis-Philippe and Sheikh, Yaser},
booktitle = {2019 International Conference on Multimodal Interaction},
doi = {10.1145/3340555.3353725},
eprint = {1910.02181},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ahuja et al. - 2019 - To React or not to React End-to-End Visual Pose Forecasting for Personalized Avatar during Dyadic Conversations.pdf:pdf},
isbn = {9781450368605},
keywords = {Dyadic interactions,Multimodal fusion,Pose forecasting},
month = {oct},
pages = {74--84},
publisher = {ACM},
title = {{To React or not to React: End-to-End Visual Pose Forecasting for Personalized Avatar during Dyadic Conversations}},
url = {https://dl.acm.org/doi/10.1145/3340555.3353725},
year = {2019}
}
@incollection{Yang2020collaborative,
abstract = {This paper investigates the principles of embedding learning to tackle the challenging semi-supervised video object segmentation. Different from previous practices that only explore the embedding learning using pixels from foreground object (s), we consider background should be equally treated and thus propose Collaborative video object segmentation by Foreground-Background Integration (CFBI) approach. Our CFBI implicitly imposes the feature embedding from the target foreground object and its corresponding background to be contrastive, promoting the segmentation results accordingly. With the feature embedding from both foreground and background, our CFBI performs the matching process between the reference and the predicted sequence from both pixel and instance levels, making the CFBI be robust to various object scales. We conduct extensive experiments on three popular benchmarks, i.e., DAVIS 2016, DAVIS 2017, and YouTube-VOS. Our CFBI achieves the performance (J&F) of 89.4%, 81.9%, and 81.4%, respectively, outperforming all the other state-of-the-art methods. Code: https://github.com/z-x-yang/CFBI.},
archivePrefix = {arXiv},
arxivId = {2003.08333},
author = {Yang, Zongxin and Wei, Yunchao and Yang, Yi},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-58558-7_20},
eprint = {2003.08333},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Wei, Yang - 2020 - Collaborative Video Object Segmentation by Foreground-Background Integration.pdf:pdf},
isbn = {9783030585570},
issn = {16113349},
keywords = {Metric learning,Video Object Segmentation},
pages = {332--348},
title = {{Collaborative Video Object Segmentation by Foreground-Background Integration}},
url = {https://link.springer.com/10.1007/978-3-030-58558-7_20},
volume = {12350 LNCS},
year = {2020}
}
@article{Chen2021evaluating,
abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
archivePrefix = {arXiv},
arxivId = {2107.03374},
author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde, Henrique and Kaplan, Jared and Edwards, Harri and Burda, Yura and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, Will and Nichol, Alex and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Carr, Andrew and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
eprint = {2107.03374},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf:pdf},
month = {jul},
title = {{Evaluating Large Language Models Trained on Code}},
url = {http://arxiv.org/abs/2107.03374},
year = {2021}
}
@inproceedings{Inoue2020attentive,
abstract = {We describe an attentive listening system for the autonomous android robot ERICA. The proposed system generates several types of listener responses: backchannels, repeats, elaborating questions, assessments, generic sentimental responses, and generic responses. In this paper, we report a subjective experiment with 20 elderly people. First, we evaluated each system utterance excluding backchannels and generic responses, in an offline manner. It was found that most of the system utterances were linguistically appropriate, and they elicited positive reactions from the subjects. Furthermore, 58.2% of the responses were acknowledged as being appropriate listener responses. We also compared the proposed system with a WOZ system where a human operator was operating the robot. From the subjective evaluation, the proposed system achieved comparable scores in basic skills of attentive listening such as encouragement to talk, focused on the talk, and actively listening. It was also found that there is still a gap between the system and the WOZ for more sophisticated skills such as dialogue understanding, showing interest, and empathy towards the user.},
author = {Inoue, Koji and Lala, Divesh and Yamamoto, Kenta and Nakamura, Shizuka and Takanashi, Katsuya and Kawahara, Tatsuya},
booktitle = {Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Inoue et al. - 2020 - An Attentive Listening System with Android {ERICA} Comparison of Autonomous and {WOZ} Interactions.pdf:pdf},
number = {July},
pages = {118--127},
title = {{An Attentive Listening System with Android {ERICA}: Comparison of Autonomous and {WOZ} Interactions}},
url = {https://pj.ninjal.ac.jp/corpus_%0Ahttps://www.aclweb.org/anthology/2020.sigdial-1.15},
year = {2020}
}
@inproceedings{Kim2020modality,
abstract = {The Visual Dialog task requires a model to exploit both image and conversational context information to generate the next response to the dialogue. However, via manual analysis, we find that a large number of conversational questions can be answered by only looking at the image without any access to the context history, while others still need the conversation context to predict the correct answers. We demonstrate that due to this reason, previous joint-modality (history and image) models over-rely on and are more prone to memorizing the dialogue history (e.g., by extracting certain keywords or patterns in the context information), whereas image-only models are more generalizable (because they cannot memorize or extract keywords from history) and perform substantially better at the primary normalized discounted cumulative gain (NDCG) task metric which allows multiple correct answers. Hence, this observation encourages us to explicitly maintain two models, i.e., an image-only model and an image-history joint model, and combine their complementary abilities for a more balanced multimodal model. We present multiple methods for this integration of the two models, via ensemble and consensus dropout fusion with shared parameters. Empirically, our models achieve strong results on the Visual Dialog challenge 2019 (rank 3 on NDCG and high balance across metrics), and substantially outperform the winner of the Visual Dialog challenge 2018 on most metrics.},
archivePrefix = {arXiv},
arxivId = {2001.06354},
author = {Kim, Hyounghun and Tan, Hao and Bansal, Mohit},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i05.6320},
eprint = {2001.06354},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Tan, Bansal - 2020 - Modality-Balanced Models for Visual Dialogue.pdf:pdf},
issn = {2374-3468},
keywords = {Natural Language Processing},
month = {apr},
number = {05},
pages = {8091--8098},
title = {{Modality-Balanced Models for Visual Dialogue}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/6320},
volume = {34},
year = {2020}
}
@inproceedings{Liang2020moss,
abstract = {A major bottleneck in training end-to-end task-oriented dialog system is the lack of data. To utilize limited training data more efficiently, we propose Modular Supervision Network (MOSS), an encoder-decoder training framework that could incorporate supervision from various intermediate dialog system modules including natural language understanding, dialog state tracking, dialog policy learning and natural language generation. With only 60% of the training data, MOSS-all (i.e., MOSS with supervision from all four dialog modules) outperforms state-of-the-art models on CamRest676. Moreover, introducing modular supervision has even bigger benefits when the dialog task has a more complex dialog state and action space. With only 40% of the training data, MOSS-all outperforms the state-of-the-art model on a complex laptop network trouble shooting dataset, LaptopNetwork, that we introduced. LaptopNetwork consists of conversations between real customers and customer service agents in Chinese. Moreover, MOSS framework can accommodate dialogs that have supervision from different dialog modules at both framework level and model level. Therefore, MOSS is extremely flexible to update in real-world deployment.},
archivePrefix = {arXiv},
arxivId = {1909.05528},
author = {Liang, Weixin and Tian, Youzhi and Chen, Chengcai and Yu, Zhou},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i05.6349},
eprint = {1909.05528},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2020 - MOSS End-to-End Dialog System Framework with Modular Supervision.pdf:pdf},
issn = {2374-3468},
keywords = {Natural Language Processing},
month = {apr},
number = {05},
pages = {8327--8335},
title = {{MOSS: End-to-End Dialog System Framework with Modular Supervision}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/6349},
volume = {34},
year = {2020}
}
@inproceedings{Zhu2020who,
abstract = {Conversation structure is useful for both understanding the nature of conversation dynamics and for providing features for many downstream applications such as summarization of conversations. In this work, we define the problem of conversation structure modeling as identifying the parent utterance(s) to which each utterance in the conversation responds to. Previous work usually took a pair of utterances to decide whether one utterance is the parent of the other. We believe the entire ancestral history is a very important information source to make accurate prediction. Therefore, we design a novel masking mechanism to guide the ancestor flow, and leverage the transformer model to aggregate all ancestors to predict parent utterances. Our experiments are performed on the Reddit dataset (Zhang, Culbertson, and Paritosh 2017) and the Ubuntu IRC dataset (Kummerfeld et al. 2019). In addition, we also report experiments on a new larger corpus from the Reddit platform and release this dataset. We show that the proposed model, that takes into account the ancestral history of the conversation, significantly outperforms several strong baselines including the BERT model on all datasets.},
archivePrefix = {arXiv},
arxivId = {1911.10666},
author = {Zhu, Henghui and Nan, Feng and Wang, Zhiguo and Nallapati, Ramesh and Xiang, Bing},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i05.6524},
eprint = {1911.10666},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu et al. - 2020 - Who Did They Respond to Conversation Structure Modeling Using Masked Hierarchical Transformer.pdf:pdf},
issn = {2374-3468},
keywords = {Natural Language Processing},
month = {apr},
number = {05},
pages = {9741--9748},
title = {{Who Did They Respond to? Conversation Structure Modeling Using Masked Hierarchical Transformer}},
url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/6524},
volume = {34},
year = {2020}
}
@inproceedings{Zou2021topic,
abstract = {In a customer service system, dialogue summarization can boost service efficiency by automatically creating summaries for long spoken dialogues in which customers and agents try to address issues about specific topics. In this work, we focus on topic-oriented dialogue summarization, which generates highly abstractive summaries that preserve the main ideas from dialogues. In spoken dialogues, abundant dialogue noise and common semantics could obscure the underlying informative content, making the general topic modeling approaches difficult to apply. In addition, for customer service, role-specific information matters and is an indispensable part of a summary. To effectively perform topic modeling on dialogues and capture multi-role information, in this work we propose a novel topic-augmented two-stage dialogue summarizer (TDS) jointly with a saliency-aware neural topic model (SATM) for topic-oriented summarization of customer service dialogues. Comprehensive studies on a real-world Chinese customer service dataset demonstrated the superiority of our method against several strong baselines.},
archivePrefix = {arXiv},
arxivId = {2012.07311},
author = {Zou, Yicheng and Zhao, Lujun and Kang, Yangyang and Lin, Jun and Peng, Minlong and Jiang, Zhuoren and Sun, Changlong and Zhang, Qi and Huang, Xuanjing and Liu, Xiaozhong},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
eprint = {2012.07311},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zou et al. - 2021 - Topic-oriented spoken dialogue summarization for customer service with saliency-aware topic modeling.pdf:pdf},
issn = {23318422},
title = {{Topic-oriented spoken dialogue summarization for customer service with saliency-aware topic modeling}},
year = {2021}
}
@inproceedings{Wang2020multi,
abstract = {To simulate human interaction in real life, dialog systems are introduced to generate a response to previous chat utterances. There have been several studies for two-speaker video dialogs in the form of question answering. However, more informative semantic cues might be exploited via a multi-rounds chatting or discussing about the video among multiple speakers. So multi-speakers video dialogs are more applicable in real life. Besides, speakers always chat about a sub-segment of the long video fragment for a period of time. Current video dialog systems require to be directly given the relevant video sub-segment which speakers are chatting about. However, it is always hard to accurately spot the corresponding video sub-segment in practical applications. In this paper, we introduce a novel task of Multi-Speaker Video Dialog with frame-level Temporal Localization (MSVD-TL) to make video dialog systems more applicable. Given a long video fragment and a set of chat history utterances, MSVD-TL targets to predict the following response and localize the relevant video sub-segment in frame level, simultaneously. We develop a new multi-task model with a response prediction module and a frame-level temporal localization module. Besides, we focus on the characteristic of the video dialog generation process and exploit the relation among the video fragment, the chat history, and the following response to refine their representations. We evaluate our approach for both the Multi-Speaker Video Dialog without frame-level temporal localization (MSVD w/o TL) task and the MSVD-TL task. The experimental results further demonstrate that MSVD-TL enhances the applicability of video dialog in real life.},
author = {Wang, Qiang and Jiang, Pin and Guo, Zhiyi and Han, Yahong and Zhao, Zhou},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i07.6901},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2020 - Multi-Speaker Video Dialog with Frame-Level Temporal Localization.pdf:pdf},
issn = {2374-3468},
keywords = {Vision},
month = {apr},
number = {07},
pages = {12200--12207},
title = {{Multi-Speaker Video Dialog with Frame-Level Temporal Localization}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/6901},
volume = {34},
year = {2020}
}
@article{Islam2021exploring,
abstract = {Video captioning is an automated collection of natural language phrases that explains the contents in video frames. Because of the incomparable performance of deep learning in the field of computer vision and natural language processing in recent years, research in this field has been exponentially increased throughout past decades. Numerous approaches, datasets, and measurement metrics have been introduced in the literature, calling for a systematic survey to guide research efforts in this exciting new direction. Through the statistical analysis, this survey paper focuses mostly on state-of-the-art approaches, emphasizing deep learning models, assessing benchmark datasets in several parameters, and classifying the pros and cons of the various evaluation metrics based on the previous works in the deep learning field. This survey shows the most used variants of neural networks for visual and spatio-temporal feature extraction as well as language generation model. The results show that ResNet and VGG as visual feature extractor and 3D convolutional neural network as spatio-temporal feature extractor are mostly used. Besides that, Long Short Term Memory (LSTM) has been mainly used as the language model. However, nowadays, the Gated Recurrent Unit (GRU) and Transformer are slowly replacing LSTM. Regarding dataset usage, so far, MSVD and MSR-VTT are very much dominant due to be part of outstanding results among various captioning models. From 2015 to 2020, with all major datasets, some models such as, Inception-Resnet-v2 + C3D + LSTM, ResNet-101 + I3D + Transformer, ResNet-152 + ResNext-101 (R3D) + (LSTM, GAN) have achieved by far best results in video captioning. Despite rapid advancement, our survey reveals that video captioning research-work still has a lot to develop in accessing the full potential of deep learning for classifying and captioning a large number of activities, as well as creating large datasets covering diversified training video samples.},
author = {Islam, Saiful and Dash, Aurpan and Seum, Ashek and Raj, Amir Hossain and Hossain, Tonmoy and Shah, Faisal Muhammad},
doi = {10.1007/s42979-021-00487-x},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Islam et al. - 2021 - Exploring Video Captioning Techniques A Comprehensive Survey on Deep Learning Methods.pdf:pdf},
isbn = {0123456789},
issn = {2662-995X},
journal = {SN Computer Science},
keywords = {Dataset Comparison,Deep Learning,Evaluation Metrics,Feature Extraction,Spatio-Temporal,Video captioning,dataset comparison,deep,evaluation metrics,feature extraction,spatio-temporal,video captioning},
month = {apr},
number = {2},
publisher = {Springer Singapore},
title = {{Exploring Video Captioning Techniques: A Comprehensive Survey on Deep Learning Methods}},
url = {https://doi.org/10.1007/s42979-021-00487-x http://link.springer.com/10.1007/s42979-021-00487-x},
volume = {2},
year = {2021}
}
@inproceedings{Platonov2020history,
abstract = {A physical blocks world, despite its relative simplicity, requires (in fully interactive form) a rich set of functional capabilities, ranging from vision to natural language understand- ing. In this work we tackle spatial question answering in a holistic way, using a vision system, speech input and output mediated by an animated avatar, a dialogue system that ro- bustly interprets spatial queries, and a con- straint solver that derives answers based on 3-D spatial modeling. The contributions of this work include a semantic parser that maps spatial questions into logical forms consistent with a general approach to meaning represen- tation, a dialogue manager based on a schema representation, and a constraint solver for spa- tial questions that provides answers in agree- ment with human perception. These and other components are integrated into a multi-modal human-computer interaction pipeline.},
archivePrefix = {arXiv},
arxivId = {2005.12501},
author = {Platonov, Georgiy and Kane, Benjamin and Gindi, Aaron and Schubert, Lenhart K.},
booktitle = {Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
eprint = {2005.12501},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Platonov et al. - 2020 - History-Aware Question Answering in a Blocks World Dialogue System.pdf:pdf},
issn = {23318422},
keywords = {Blocks World,Discourse Context,Question Answering,Semantic Parsing,Spatial Reasoning},
number = {July},
pages = {128--131},
title = {{History-Aware Question Answering in a Blocks World Dialogue System ?}},
year = {2020}
}
@inproceedings{Sigurdsson2016hollywood,
abstract = {Computer vision has a great potential to help our daily lives by searching for lost keys, watering flowers or reminding us to take a pill. To succeed with such tasks, computer vision methods need to be trained from real and diverse examples of our daily dynamic scenes. While most of such scenes are not particularly exciting, they typically do not appear on YouTube, in movies or TV broadcasts. So how do we collect sufficiently many diverse but boring samples representing our lives? We propose a novel Hollywood in Homes approach to collect such data. Instead of shooting videos in the lab, we ensure diversity by distributing and crowdsourcing the whole process of video creation from script writing to video recording and annotation. Following this procedure we collect a new dataset, Charades, with hundreds of people recording videos in their own homes, acting out casual everyday activities. The dataset is composed of 9,848 annotated videos with an average length of 30 s, showing activities of 267 people from three continents. Each video is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacted objects. In total, Charades provides 27,847 video descriptions, 66,500 temporally localized intervals for 157 action classes and 41,104 labels for 46 object classes. Using this rich data, we evaluate and provide baseline results for several tasks including action recognition and automatic description generation. We believe that the realism, diversity, and casual nature of this dataset will present unique challenges and new opportunities for computer vision community.},
archivePrefix = {arXiv},
arxivId = {1604.01753},
author = {Sigurdsson, Gunnar A. and Varol, G{\"{u}}l and Wang, Xiaolong and Farhadi, Ali and Laptev, Ivan and Gupta, Abhinav},
booktitle = {Proc. ECCV},
doi = {10.1007/978-3-319-46448-0_31},
eprint = {1604.01753},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sigurdsson et al. - 2016 - Hollywood in Homes Crowdsourcing Data Collection for Activity Understanding.pdf:pdf},
issn = {16113349},
pages = {510--526},
title = {{Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding}},
url = {http://link.springer.com/10.1007/978-3-319-46448-0_31},
volume = {9905 LNCS},
year = {2016}
}
@inproceedings{Meng2020refnet,
abstract = {Existing conversational systems tend to generate generic responses. Recently, Background Based Conversation (BBCs) have been introduced to address this issue. Here, the generated responses are grounded in some background information. The proposed methods for BBCs are able to generate more informative responses, however, they either cannot generate natural responses or have difficulties in locating the right background information. In this paper, we propose a Reference-aware Network (RefNet) to address both issues. Unlike existing methods that generate responses token by token, RefNet incorporates a novel reference decoder that provides an alternative way to learn to directly select a semantic unit (e.g., a span containing complete semantic information) from the background. Experimental results show that RefNet significantly outperforms state-of-the-art methods in terms of both automatic and human evaluations, indicating that RefNet can generate more appropriate and human-like responses.},
archivePrefix = {arXiv},
arxivId = {1908.06449},
author = {Meng, Chuan and Ren, Pengjie and Chen, Zhumin and Monz, Christof and Ma, Jun and {De Rijke}, Maarten},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i05.6370},
eprint = {1908.06449},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meng et al. - 2020 - RefNet A Reference-Aware Network for Background Based Conversation.pdf:pdf},
issn = {2374-3468},
keywords = {Natural Language Processing},
month = {apr},
number = {05},
pages = {8496--8503},
title = {{RefNet: A Reference-Aware Network for Background Based Conversation}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/6370},
volume = {34},
year = {2020}
}
@inproceedings{Okamoto2020towards,
abstract = {State-of-the-art text-to-speech (TTS) systems successfully produce speech with a high degree of intelligibility. But TTS systems still often generate monotonous synthesized speech, unlike natural utterances. Several existing studies have addressed the issue of modeling speaking style variations in TTSs. Unfortunately, scant research has discussed the dialog and entrainment context. In this paper, we address TTS waveform generation toward speech entrainment in human-machine communication and focus on the synchronization of speaking rates that may vary within an utterance, i.e., slowing down to emphasize specific words and distinguish elements to highlight. We assume a dialog system exists and concentrate on its speech processing part. To perform such a task, we develop (1) a multi-task automatic speech recognition (ASR) that listens to the conversation partner and recognizes the content and the speaking rate and (2) a generative adversarial network (GAN)-based TTS that produces the synthesized speech of the response while entraining with the partner's speaking rate. The evaluation is performed on a dialog corpus. Our results reveal that it is possible to entrain the input speech by synchronizing the speaking rate.},
author = {Okamoto, Mayuko and Sakti, Sakriani and Nakamura, Satoshi},
booktitle = {2020 23rd Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)},
doi = {10.1109/O-COCOSDA50338.2020.9295020},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Okamoto, Sakti, Nakamura - 2020 - Towards Speech Entrainment Considering ASR Information in Speaking Rate Variation of TTS Waveform Gene.pdf:pdf},
isbn = {978-1-7281-9896-5},
keywords = {automatic speech recognition,generative adversarial network,speaking-rate variation,speech entrainment,text-to-speech synthesis},
month = {nov},
pages = {139--144},
publisher = {IEEE},
title = {{Towards Speech Entrainment: Considering ASR Information in Speaking Rate Variation of TTS Waveform Generation}},
url = {https://ieeexplore.ieee.org/document/9295020/},
year = {2020}
}
@article{Chiou2021visual,
abstract = {Visual relationship detection aims to reason over relationships among salient objects in images, which has drawn increasing attention over the past few years. Inspired by human reasoning mechanisms, it is believed that external visual commonsense knowledge is beneficial for reasoning visual relationships of objects in images, which is however rarely considered in existing methods. In this paper, we propose a novel approach named Relational Visual-Linguistic Bidirectional Encoder Representations from Transformers (RVL-BERT), which performs relational reasoning with both visual and language commonsense knowledge learned via self-supervised pre-training with multimodal representations. RVL-BERT also uses an effective spatial module and a novel mask attention module to explicitly capture spatial information among the objects. Moreover, our model decouples object detection from visual relationship recognition by taking in object names directly, enabling it to be used on top of any object detection system. We show through quantitative and qualitative experiments that, with the transferred knowledge and novel modules, RVL-BERT achieves competitive results on two challenging visual relationship detection datasets. The source code is available at https://github.com/coldmanck/RVL-BERT.},
archivePrefix = {arXiv},
arxivId = {2009.04965},
author = {Chiou, Meng-Jiun and Zimmermann, Roger and Feng, Jiashi},
doi = {10.1109/ACCESS.2021.3069041},
eprint = {2009.04965},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chiou, Zimmermann, Feng - 2021 - Visual Relationship Detection with Visual-Linguistic Knowledge from Multimodal Representations.pdf:pdf},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {Computer vision,image analysis,multimodal representation,scene graph generation,visual relationship detection},
title = {{Visual Relationship Detection with Visual-Linguistic Knowledge from Multimodal Representations}},
url = {https://ieeexplore.ieee.org/document/9387302/},
volume = {9},
year = {2021}
}
@inproceedings{Serban2016building,
abstract = {We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.},
author = {Serban, Iulian and Sordoni, Alessandro and Bengio, Yoshua and Courville, Aaron and Pineau, Joelle},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Serban et al. - 2016 - Building end-To-end dialogue systems using generative hierarchical neural network models.pdf:pdf},
month = {mar},
number = {1 SE - Special Track: Cognitive Systems},
title = {{Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models}},
url = {https://ojs.aaai.org/index.php/AAAI/article/view/9883},
volume = {30},
year = {2016}
}
@inproceedings{Jiang2020dualvd,
abstract = {Different from Visual Question Answering task that requires to answer only one question about an image, Visual Dialogue involves multiple questions which cover a broad range of visual content that could be related to any objects, relationships or semantics. The key challenge in Visual Dialogue task is thus to learn a more comprehensive and semantic-rich image representation which may have adaptive attentions on the image for variant questions. In this research, we propose a novel model to depict an image from both visual and semantic perspectives. Specifically, the visual view helps capture the appearance-level information, including objects and their relationships, while the semantic view enables the agent to understand high-level visual semantics from the whole image to the local regions. Futhermore, on top of such multi-view image features, we propose a feature selection framework which is able to adaptively capture question-relevant information hierarchically in fine-grained level. The proposed method achieved state-of-the-art results on benchmark Visual Dialogue datasets. More importantly, we can tell which modality (visual or semantic) has more contribution in answering the current question by visualizing the gate values. It gives us insights in understanding of human cognition in Visual Dialogue.},
archivePrefix = {arXiv},
arxivId = {1911.07251},
author = {Jiang, Xiaoze and Yu, Jing and Qin, Zengchang and Zhuang, Yingying and Zhang, Xingxing and Hu, Yue and Wu, Qi},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i07.6769},
eprint = {1911.07251},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2020 - DualVD An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue.pdf:pdf},
issn = {2374-3468},
month = {apr},
number = {07},
pages = {11125--11132},
title = {{DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/6769},
volume = {34},
year = {2020}
}
@inproceedings{Beaver2020towards,
abstract = {As Intelligent Virtual Agents (IVAs) increase in adoption and further emulate human personalities, we are interested in how humans apply relational strategies to them compared to other humans in a service environment. Human-computer data from three live customer service IVAs was collected, and annotators marked all text that was deemed unnecessary to the determination of user intention as well as the presence of multiple intents. After merging the selections of multiple annotators, a second round of annotation determined the classes of relational language present in the unnecessary sections such as Greetings, Backstory, Justification, Gratitude, Rants, or Expressing Emotions. We compare the usage of such language in human-human service interactions. We show that removal of this language from task-based inputs has a positive effect by both an increase in confidence and improvement in responses, as evaluated by humans, demonstrating the need for IVAs to anticipate relational language injection. This work provides a methodology to identify relational segments and a baseline of human performance in this task as well as laying the groundwork for IVAs to reciprocate relational strategies in order to improve their believeability.},
author = {Beaver, Ian and Freeman, Cynthia and Mueen, Abdullah},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i03.5644},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Beaver, Freeman, Mueen - 2020 - Towards Awareness of Human Relational Strategies in Virtual Agents.pdf:pdf},
issn = {2374-3468},
keywords = {Humans and AI},
month = {apr},
number = {03},
pages = {2602--2610},
title = {{Towards Awareness of Human Relational Strategies in Virtual Agents}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/5644},
volume = {34},
year = {2020}
}
@article{Deriu2021survey,
abstract = {In this paper, we survey the methods and concepts developed for the evaluation of dialogue systems. Evaluation, in and of itself, is a crucial part during the development process. Often, dialogue systems are evaluated by means of human evaluations and questionnaires. However, this tends to be very cost- and time-intensive. Thus, much work has been put into finding methods which allow a reduction in involvement of human labour. In this survey, we present the main concepts and methods. For this, we differentiate between the various classes of dialogue systems (task-oriented, conversational, and question-answering dialogue systems). We cover each class by introducing the main technologies developed for the dialogue systems and then present the evaluation methods regarding that class.},
archivePrefix = {arXiv},
arxivId = {1905.04071},
author = {Deriu, Jan and Rodrigo, Alvaro and Otegi, Arantxa and Echegoyen, Guillermo and Rosset, Sophie and Agirre, Eneko and Cieliebak, Mark},
doi = {10.1007/s10462-020-09866-x},
eprint = {1905.04071},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Deriu et al. - 2021 - Survey on evaluation methods for dialogue systems.pdf:pdf},
isbn = {0123456789},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {Chatbots,Conversational AI,Dialogue systems,Discourse model,Evaluation metrics},
month = {jan},
number = {1},
pages = {755--810},
publisher = {Springer Netherlands},
title = {{Survey on evaluation methods for dialogue systems}},
url = {https://doi.org/10.1007/s10462-020-09866-x http://link.springer.com/10.1007/s10462-020-09866-x},
volume = {54},
year = {2021}
}
@article{Firdaus2020,
abstract = {An important skill for effective communication is the ability to express specific sentiment and emotion in a conversation. It is desirable for any robust dialogue system to handle the combined effect of both sentiment and emotion while generating responses to provide a better experience and concurrently increase user satisfaction. Previously, research on either emotion or sentiment controlled dialogue generation has shown impressive performance, but the simultaneous effect of both is still unexplored. The existing dialogue systems are majorly based on unimodal sources, predominantly the text, and thereby cannot utilize the information present in the other sources such as video, audio, image etc. In this work, we present a first large scale benchmark Sentiment Emotion aware MultimodalDialogue (SEMD) dataset for the task of sentiment and emotion controlled dialogue generation. The SEMD dataset consists of55k conversations from 10 TV shows having text, audio and video information. To utilize multimodal information we propose multimodal attention based conditional variational autoencoder (M-CVAE) that outperforms several sophisticated multimodal baselines. Quantitative and qualitative analysis show that multimodality along with contextual information plays an essential role in generating coherent and diverse responses for any given emotion and sentiment.},
author = {Firdaus, Mauajama and Chauhan, Hardik and Ekbal, Asif and Bhattacharyya, Pushpak},
doi = {10.1109/TAFFC.2020.3015491},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Firdaus et al. - 2020 - EmoSen Generating Sentiment and Emotion Controlled Responses in a Multimodal Dialogue System.pdf:pdf},
issn = {1949-3045},
journal = {IEEE Transactions on Affective Computing},
keywords = {Artificial intelligence,Buildings,Conversational AI,Emotion-aware NLG,Generative adversarial networks,History,Multmodality,Natural Language Generation,Sentiment-aware NLG,TV,Task analysis,Visualization},
number = {8},
title = {{EmoSen: Generating Sentiment and Emotion Controlled Responses in a Multimodal Dialogue System}},
url = {https://ieeexplore.ieee.org/document/9165162/},
volume = {14},
year = {2020}
}
@article{Bain2021frozen,
abstract = {Our objective in this work is video-text retrieval - in particular a joint embedding that enables efficient text-to-video retrieval. The challenges in this area include the design of the visual architecture and the nature of the training data, in that the available large scale video-text training datasets, such as HowTo100M, are noisy and hence competitive performance is achieved only at scale through large amounts of compute. We address both these challenges in this paper. We propose an end-to-end trainable model that is designed to take advantage of both large-scale image and video captioning datasets. Our model is an adaptation and extension of the recent ViT and Timesformer architectures, and consists of attention in both space and time. The model is flexible and can be trained on both image and video text datasets, either independently or in conjunction. It is trained with a curriculum learning schedule that begins by treating images as 'frozen' snapshots of video, and then gradually learns to attend to increasing temporal context when trained on video datasets. We also provide a new video-text pretraining dataset WebVid-2M, comprised of over two million videos with weak captions scraped from the internet. Despite training on datasets that are an order of magnitude smaller, we show that this approach yields state-of-the-art results on standard downstream video-retrieval benchmarks including MSR-VTT, MSVD, DiDeMo and LSMDC.},
archivePrefix = {arXiv},
arxivId = {2104.00650},
author = {Bain, Max and Nagrani, Arsha and Varol, G{\"{u}}l and Zisserman, Andrew},
eprint = {2104.00650},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bain et al. - 2021 - Frozen in Time A Joint Video and Image Encoder for End-to-End Retrieval.pdf:pdf},
month = {apr},
title = {{Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval}},
url = {http://arxiv.org/abs/2104.00650},
year = {2021}
}
@article{Girdhar2021anticipative,
abstract = {We propose Anticipative Video Transformer (AVT), an end-to-end attention-based video modeling architecture that attends to the previously observed video in order to anticipate future actions. We train the model jointly to predict the next action in a video sequence, while also learning frame feature encoders that are predictive of successive future frames' features. Compared to existing temporal aggregation strategies, AVT has the advantage of both maintaining the sequential progression of observed actions while still capturing long-range dependencies--both critical for the anticipation task. Through extensive experiments, we show that AVT obtains the best reported performance on four popular action anticipation benchmarks: EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads, including outperforming all submissions to the EpicKitchens-100 CVPR'21 challenge.},
archivePrefix = {arXiv},
arxivId = {2106.02036},
author = {Girdhar, Rohit and Grauman, Kristen},
eprint = {2106.02036},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Girdhar, Grauman - 2021 - Anticipative Video Transformer.pdf:pdf},
month = {jun},
title = {{Anticipative Video Transformer}},
url = {http://arxiv.org/abs/2106.02036},
year = {2021}
}
@inproceedings{Takanobu2020is,
abstract = {There is a growing interest in developing goal-oriented dialog systems which serve users in accomplishing complex tasks through multi-turn conversations. Although many methods are devised to evaluate and improve the performance of individual dialog components, there is a lack of comprehensive empirical study on how different components contribute to the overall performance of a dialog system. In this paper, we perform a system-wise evaluation and present an empirical analysis on different types of dialog systems which are composed of different modules in different settings. Our results show that (1) a pipeline dialog system trained using fine-grained supervision signals at different component levels often obtains better performance than the systems that use joint or end-to-end models trained on coarse-grained labels, (2) component-wise, single-turn evaluation results are not always consistent with the overall performance of a dialog system, and (3) despite the discrepancy between simulators and human users, simulated evaluation is still a valid alternative to the costly human evaluation especially in the early stage of development.},
archivePrefix = {arXiv},
arxivId = {2005.07362},
author = {Takanobu, Ryuichi and Zhu, Qi and Li, Jinchao and Peng, Baolin and Gao, Jianfeng and Huang, Minlie},
booktitle = {Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
eprint = {2005.07362},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Takanobu et al. - 2020 - Is your goal-oriented dialog model performing really well Empirical analysis of system-wise evaluation.pdf:pdf},
issn = {23318422},
number = {July},
pages = {297--310},
title = {{Is your goal-oriented dialog model performing really well? Empirical analysis of system-wise evaluation}},
year = {2020}
}
@inproceedings{Partovi2019influence,
abstract = {We describe a longitudinal user study conducted in the context of a Spoken Dialogue System for a household robot, where we examined the influence of time displacement and situational risk on users' preferred responses. To this effect, we employed a corpus of spoken requests that asked a robot to fetch or move objects in a room. In the first stage of our study, participants selected among four response types to these requests under two risk conditions: low and high. After some time, the same participants rated several responses to the previous requests — these responses were instantiated from the four response types. Our results show that participants did not rate highly their own response types; moreover, they rated their own response types similarly to different ones. This suggests that, at least in this context, people's preferences at a particular point in time may not reflect their general attitudes, and that various reasonable response types may be equally acceptable. Our study also reveals that situational risk influences the acceptability of some response types.},
address = {Stroudsburg, PA, USA},
author = {Partovi, Andisheh and Zukerman, Ingrid},
booktitle = {Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue},
doi = {10.18653/v1/W19-5936},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Partovi, Zukerman - 2019 - Influence of Time and Risk on Response Acceptability in a Simple Spoken Dialogue System.pdf:pdf},
isbn = {9781950737611},
number = {September},
pages = {307--319},
publisher = {Association for Computational Linguistics},
title = {{Influence of Time and Risk on Response Acceptability in a Simple Spoken Dialogue System}},
url = {https://www.aclweb.org/anthology/W19-5936},
year = {2019}
}
@inproceedings{Mostafazadeh2017image,
abstract = {The popularity of image sharing on social media and the engagement it creates between users reflects the important role that visual context plays in everyday conversations. We present a novel task, Image-Grounded Conversations (IGC), in which natural-sounding conversations are generated about a shared image. To benchmark progress, we introduce a new multiple-reference dataset of crowd-sourced, event-centric conversations on images. IGC falls on the continuum between chit-chat and goal-directed conversation models, where visual grounding constrains the topic of conversation to event-driven utterances. Experiments with models trained on social media data show that the combination of visual and textual context enhances the quality of generated conversational turns. In human evaluation, the gap between human performance and that of both neural and retrieval architectures suggests that multi-modal IGC presents an interesting challenge for dialogue research.},
archivePrefix = {arXiv},
arxivId = {1701.08251},
author = {Mostafazadeh, Nasrin and Brockett, Chris and Dolan, Bill and Galley, Michel and Gao, Jianfeng and Spithourakis, Georgios P. and Vanderwende, Lucy},
booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing},
eprint = {1701.08251},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mostafazadeh et al. - 2017 - Image-Grounded Conversations Multimodal Context for Natural Question and Response Generation.pdf:pdf},
pages = {462--472},
title = {{Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation}},
url = {https://www.aclweb.org/anthology/I17-1047},
year = {2017}
}
@inproceedings{Narayan2019collaborative,
abstract = {We wish to develop interactive agents that can communicate with humans to collaboratively solve tasks in grounded scenarios. Since computer games allow us to simulate such tasks without the need for physical robots, we define a Minecraft-based collaborative building task in which one player (A, the Architect) is shown a target structure and needs to instruct the other player (B, the Builder) to build this structure. Both players interact via a chat interface. A can observe B but cannot place blocks. We present the Minecraft Dialogue Corpus, a collection of 509 conversations and game logs. As a first step towards our goal of developing fully interactive agents for this task, we consider the subtask of Architect utterance generation, and show how challenging it is.},
address = {Stroudsburg, PA, USA},
author = {Narayan-Chen, Anjali and Jayannavar, Prashant and Hockenmaier, Julia},
booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
doi = {10.18653/v1/P19-1537},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narayan-Chen, Jayannavar, Hockenmaier - 2019 - Collaborative Dialogue in Minecraft.pdf:pdf},
isbn = {9781950737482},
pages = {5405--5415},
publisher = {Association for Computational Linguistics},
title = {{Collaborative Dialogue in Minecraft}},
url = {https://www.aclweb.org/anthology/P19-1537},
year = {2019}
}
@inproceedings{Zhang2020filling,
abstract = {The phenomenon of ellipsis is prevalent in social conversations. Ellipsis increases the difficulty of a series of downstream language understanding tasks, such as dialog act prediction and semantic role labeling. We propose to resolve ellipsis through automatic sentence completion to improve language understanding. However, automatic ellipsis completion can result in output which does not accurately reflect user intent. To address this issue, we propose a method which considers both the original utterance that has ellipsis and the automatically completed utterance in dialog act and semantic role labeling tasks. Specifically, we first complete user utterances to resolve ellipsis using an end-to-end pointer network model. We then train a prediction model using both utterances containing ellipsis and our automatically completed utterances. Finally, we combine the prediction results from these two utterances using a selection model that is guided by expert knowledge. Our approach improves dialog act prediction and semantic role labeling by 1.3% and 2.5% in F1 score respectively in social conversations. We also present an open-domain human-machine conversation dataset with manually completed user utterances and annotated semantic role labeling after manual completion.},
archivePrefix = {arXiv},
arxivId = {1911.10776},
author = {Zhang, Xiyuan and Li, Chengxi and Yu, Dian and Davidson, Samuel and Yu, Zhou},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i05.6505},
eprint = {1911.10776},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2020 - Filling Conversation Ellipsis for Better Social Dialog Understanding.pdf:pdf},
issn = {2374-3468},
keywords = {Natural Language Processing},
month = {apr},
number = {05},
pages = {9587--9595},
title = {{Filling Conversation Ellipsis for Better Social Dialog Understanding}},
url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/6505},
volume = {34},
year = {2020}
}
@inproceedings{Ghazarian2020predictive,
abstract = {User engagement is a critical metric for evaluating the quality of open-domain dialogue systems. Prior work has focused on conversation-level engagement by using heuristically constructed features such as the number of turns and the total time of the conversation. In this paper, we investigate the possibility and efficacy of estimating utterance-level engagement and define a novel metric, predictive engagement, for automatic evaluation of open-domain dialogue systems. Our experiments demonstrate that (1) human annotators have high agreement on assessing utterance-level engagement scores; (2) conversation-level engagement scores can be predicted from properly aggregated utterance-level engagement scores. Furthermore, we show that the utterance-level engagement scores can be learned from data. These scores can be incorporated into automatic evaluation metrics for open-domain dialogue systems to improve the correlation with human judgements. This suggests that predictive engagement can be used as a real-time feedback for training better dialogue models.},
archivePrefix = {arXiv},
arxivId = {1911.01456},
author = {Ghazarian, Sarik and Weischedel, Ralph and Galstyan, Aram and Peng, Nanyun},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i05.6283},
eprint = {1911.01456},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghazarian et al. - 2020 - Predictive Engagement An Efficient Metric for Automatic Evaluation of Open-Domain Dialogue Systems.pdf:pdf},
issn = {2374-3468},
keywords = {Natural Language Processing},
month = {apr},
number = {05},
pages = {7789--7796},
title = {{Predictive Engagement: An Efficient Metric for Automatic Evaluation of Open-Domain Dialogue Systems}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/6283},
volume = {34},
year = {2020}
}
@inproceedings{Andreas2016neural,
abstract = {Visual question answering is fundamentally compositional in nature - a question like where is the dog? shares substructure with questions like what color is the dog? and where is the cat? This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning neural module networks, which compose collections of jointly-trained neural 'modules' into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.},
archivePrefix = {arXiv},
arxivId = {1511.02799},
author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
booktitle = {Proc. CVPR},
doi = {10.1109/CVPR.2016.12},
eprint = {1511.02799},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andreas et al. - 2016 - Neural Module Networks.pdf:pdf},
pages = {39--48},
title = {{Neural Module Networks}},
year = {2016}
}
@inproceedings{Kamiya2014hl,
author = {神谷, 翔大 and 橋本, 佳 and 大浦, 圭一郎 and 南角, 吉彦 and 徳田, 恵一},
booktitle = {日本音響学会2014年秋季研究発表会講演論文集},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/神谷 et al. - 2014 - HL型アクセント推定と音響モデリングを統合したHMM音声合成の検討.pdf:pdf},
pages = {237--238},
title = {{H/L型アクセント推定と音響モデリングを統合したHMM音声合成の検討}},
year = {2014}
}
@inproceedings{Jia2021ddrel,
abstract = {Interpersonal language style shifting in dialogues is an interesting and almost instinctive ability of human. Understanding interpersonal relationship from language content is also a crucial step toward further understanding dialogues. Previous work mainly focuses on relation extraction between named entities in texts. In this paper, we propose the task of relation classification of interlocutors based on their dialogues. We crawled movie scripts from IMSDb, and annotated the relation labels for each session according to 13 pre-defined relationships. The annotated dataset DDRel consists of 6300 dyadic dialogue sessions between 694 pair of speakers with 53,126 utterances in total. We also construct session-level and pair-level relation classification tasks with widely-accepted baselines. The experimental results show that this task is challenging for existing models and the dataset will be useful for future research.},
archivePrefix = {arXiv},
arxivId = {2012.02553},
author = {Jia, Qi and Huang, Hongru and Zhu, Kenny Q.},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
eprint = {2012.02553},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jia, Huang, Zhu - 2021 - DDRel A new dataset for interpersonal relation classification in dyadic dialogues.pdf:pdf},
issn = {23318422},
title = {{DDRel: A new dataset for interpersonal relation classification in dyadic dialogues}},
year = {2021}
}
@inproceedings{Yang2021ubar,
abstract = {This paper presents our task-oriented dialog system UBAR 1 which models task-oriented dialogs on a dialog session level. Specifically, UBAR is acquired by fine-tuning the large pre-trained unidirectional language model GPT-2 on the sequence of the entire dialog session which is composed of user utterance, belief state, database result, system act, and system response of every dialog turn. Additionally, UBAR is evaluated in a more realistic setting, where its dialog context has access to user utterances and all content it generated such as belief states, system acts, and system responses. Experimental results on the MultiWOZ datasets show that UBAR achieves state-of-the-art performances in multiple settings, improving the combined score of response generation, policy optimization, and end-to-end modeling by 4.7, 3.5, and 9.4 points respectively. Thorough analyses demonstrate that the session-level training sequence formulation and the generated dialog context are essential for UBAR to operate as a fully end-to-end task-oriented dialog system in real life. We also examine the transfer ability of UBAR to new domains with limited data and provide visualization and a case study to illustrate the advantages of UBAR in modeling on a dialog session level.2},
archivePrefix = {arXiv},
arxivId = {2012.03539},
author = {Yang, Yunyi and Li, Yunhao and Quan, Xiaojun},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
eprint = {2012.03539},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Li, Quan - 2021 - UBAR Towards fully end-to-end task-oriented dialog systems with GPT-2.pdf:pdf},
issn = {23318422},
title = {{UBAR: Towards fully end-to-end task-oriented dialog systems with GPT-2}},
year = {2021}
}
@article{Inoue2018engagement,
abstract = {Engagement represents how much a user is interested in and willing to continue the current dialogue. Engagement recognition will provide an important clue for dialogue systems to generate adaptive behaviors for the user. This paper addresses engagement recognition based on multimodal listener behaviors of backchannels, laughing, head nodding, and eye gaze. In the annotation of engagement, the ground-truth data often differs from one annotator to another due to the subjectivity of the perception of engagement. To deal with this, we assume that each annotator has a latent character that affects his/her perception of engagement. We propose a hierarchical Bayesian model that estimates both engagement and the character of each annotator as latent variables. Furthermore, we integrate the engagement recognition model with automatic detection of the listener behaviors to realize online engagement recognition. Experimental results show that the proposed model improves recognition accuracy compared with other methods which do not consider the character such as majority voting. We also achieve online engagement recognition without degrading accuracy.},
author = {Inoue, Koji and Lala, Divesh and Takanashi, Katsuya and Kawahara, Tatsuya},
doi = {10.1017/ATSIP.2018.11},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Inoue et al. - 2018 - Engagement recognition by a latent character model based on multimodal listener behaviors in spoken dialogue.pdf:pdf},
issn = {2048-7703},
journal = {APSIPA Transactions on Signal and Information Processing},
keywords = {Dialogue,Engagement,Latent variable model,Listener behaviors,Multimodal},
month = {sep},
number = {2018},
title = {{Engagement recognition by a latent character model based on multimodal listener behaviors in spoken dialogue}},
url = {https://www.cambridge.org/core/product/identifier/S2048770318000112/type/journal_article},
volume = {7},
year = {2018}
}
@inproceedings{Fares2020towards,
abstract = {One of the key challenges in designing Embodied Conversational Agents (ECA) is to produce human-like gestural and visual prosody expressivity. Another major challenge is to maintain the interlocutor's attention by adapting the agent's behavior to the interlocutor's multimodal behavior. This paper outlines my PhD research plan that aims to develop convincing expressive and natural behavior in ECAs and to explore and model the mechanisms that govern human-agent multimodal interaction. Additionally, I describe in this paper my first PhD milestone which focuses on developing an end-to-end LSTM Neural Network model for upper-face gestures generation. The main task consists of building a model that can produce expressive and coherent upper-face gestures while considering multiple modalities: speech audio, text, and action units.},
address = {New York, NY, USA},
author = {Fares, Mireille},
booktitle = {Proceedings of the 2020 International Conference on Multimodal Interaction},
doi = {10.1145/3382507.3421155},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fares - 2020 - Towards Multimodal Human-Like Characteristics and Expressive Visual Prosody in Virtual Agents.pdf:pdf},
isbn = {9781450375818},
keywords = {embodied conversational agents,multi-modality,neural networks,speech,upper-face expressivity,visual prosody},
month = {oct},
pages = {743--747},
publisher = {ACM},
title = {{Towards Multimodal Human-Like Characteristics and Expressive Visual Prosody in Virtual Agents}},
url = {https://dl.acm.org/doi/10.1145/3382507.3421155},
year = {2020}
}
@inproceedings{Pang2020visual,
abstract = {GuessWhat?! is a visual dialogue task between a guesser and an oracle. The guesser aims to locate an object supposed by the oracle oneself in an image by asking a sequence of Yes/No questions. Asking proper questions with the progress of dialogue is vital for achieving successful final guess. As a result, the progress of dialogue should be properly represented and tracked. Previous models for question generation pay less attention on the representation and tracking of dialogue states, and therefore are prone to asking low quality questions such as repeated questions. This paper proposes visual dialogue state tracking (VDST) based method for question generation. A visual dialogue state is defined as the distribution on objects in the image as well as representations of objects. Representations of objects are updated with the change of the distribution on objects. An object-difference based attention is used to decode new question. The distribution on objects is updated by comparing the question-answer pair and objects. Experimental results on GuessWhat?! dataset show that our model significantly outperforms existing methods and achieves new state-of-the-art performance. It is also noticeable that our model reduces the rate of repeated questions from more than 50% to 21.9% compared with previous state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1911.07928},
author = {Pang, Wei and Wang, Xiaojie},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i07.6856},
eprint = {1911.07928},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pang, Wang - 2020 - Visual Dialogue State Tracking for Question Generation.pdf:pdf},
issn = {2374-3468},
keywords = {Vision},
month = {apr},
number = {07},
pages = {11831--11838},
title = {{Visual Dialogue State Tracking for Question Generation}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/6856},
volume = {34},
year = {2020}
}
@inproceedings{Alloatti2019real,
abstract = {Real life scenarios are often left untouched by the newest advances in research. They usually require the resolution of some specific task applied to a restricted domain, all the while providing small amounts of data to begin with. In this study we apply one of the newest innovations in Deep Learning to a task of text classification. The goal is to create a question answering system in Italian that provides information about a specific subject, e-invoicing and digital billing. Italy recently introduced a new legislation about e-invoicing and people have some legit doubts, therefore a large share of professionals could benefit from this tool. We gathered few pairs of question and answers; afterwards, we expanded the data, using it as a training corpus for BERT language model. Through a separate test corpus we evaluated the accuracy of the answer provided. Values show that the automatic system alone performs surprisingly well. The demo interface is hosted on Telegram, which makes the system immediately available to test.},
address = {Stroudsburg, PA, USA},
author = {Alloatti, Francesca and {Di Caro}, Luigi and Sportelli, Gianpiero},
booktitle = {Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue},
doi = {10.18653/v1/W19-5930},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alloatti, Di Caro, Sportelli - 2019 - Real Life Application of a Question Answering System Using BERT Language Model.pdf:pdf},
isbn = {9781950737611},
number = {September},
pages = {250--253},
publisher = {Association for Computational Linguistics},
title = {{Real Life Application of a Question Answering System Using BERT Language Model}},
url = {https://www.aclweb.org/anthology/W19-5930},
year = {2019}
}
@inproceedings{Li2020end,
abstract = {End-to-end task-oriented dialog models have achieved promising performance on collaborative tasks where users willingly coordinate with the system to complete a given task. While in non-collaborative settings, for example, negotiation and persuasion, users and systems do not share a common goal. As a result, compared to collaborate tasks, people use social content to build rapport and trust in these non-collaborative settings in order to advance their goals. To handle social content, we introduce a hierarchical intent annotation scheme, which can be generalized to different non-collaborative dialog tasks. Building upon TransferTransfo (Wolf et al. 2019), we propose an end-to-end neural network model to generate diverse coherent responses. Our model utilizes intent and semantic slots as the intermediate sentence representation to guide the generation process. In addition, we design a filter to select appropriate responses based on whether these intermediate representations fit the designed task and conversation constraints. Our non-collaborative dialog model guides users to complete the task while simultaneously keeps them engaged. We test our approach on our newly proposed AntiScam dataset and an existing PersuasionForGood dataset. Both automatic and human evaluations suggest that our model outperforms multiple baselines in these two non-collaborative tasks.},
author = {Li, Yu and Qian, Kun and Shi, Weiyan and Yu, Zhou},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i05.6345},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2020 - End-to-End Trainable Non-Collaborative Dialog System.pdf:pdf},
issn = {2374-3468},
month = {apr},
number = {05},
pages = {8293--8302},
title = {{End-to-End Trainable Non-Collaborative Dialog System}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/6345},
volume = {34},
year = {2020}
}
@article{Yan2021adaspeech3,
abstract = {While recent text to speech (TTS) models perform very well in synthesizing reading-style (e.g., audiobook) speech, it is still challenging to synthesize spontaneous-style speech (e.g., podcast or conversation), mainly because of two reasons: 1) the lack of training data for spontaneous speech; 2) the difficulty in modeling the filled pauses (um and uh) and diverse rhythms in spontaneous speech. In this paper, we develop AdaSpeech 3, an adaptive TTS system that fine-tunes a well-trained reading-style TTS model for spontaneous-style speech. Specifically, 1) to insert filled pauses (FP) in the text sequence appropriately, we introduce an FP predictor to the TTS model; 2) to model the varying rhythms, we introduce a duration predictor based on mixture of experts (MoE), which contains three experts responsible for the generation of fast, medium and slow speech respectively, and fine-tune it as well as the pitch predictor for rhythm adaptation; 3) to adapt to other speaker timbre, we fine-tune some parameters in the decoder with few speech data. To address the challenge of lack of training data, we mine a spontaneous speech dataset to support our research this work and facilitate future research on spontaneous TTS. Experiments show that AdaSpeech 3 synthesizes speech with natural FP and rhythms in spontaneous styles, and achieves much better MOS and SMOS scores than previous adaptive TTS systems.},
archivePrefix = {arXiv},
arxivId = {2107.02530},
author = {Yan, Yuzi and Tan, Xu and Li, Bohan and Zhang, Guangyan and Qin, Tao and Zhao, Sheng and Shen, Yuan and Zhang, Wei-qiang and Liu, Tie-yan},
eprint = {2107.02530},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yan et al. - 2021 - AdaSpeech 3 Adaptive Text to Speech for Spontaneous Style.pdf:pdf},
month = {jul},
title = {{AdaSpeech 3: Adaptive Text to Speech for Spontaneous Style}},
url = {http://arxiv.org/abs/2107.02530},
year = {2021}
}
@inproceedings{Liu2021filling,
abstract = {A multi-turn dialogue is composed of multiple utterances from two or more different speaker roles. Thus utterance- and speaker-aware clues are supposed to be well captured in models. However, in the existing retrieval-based multi-turn dialogue modeling, the pre-trained language models (PrLMs) as encoder represent the dialogues coarsely by taking the pairwise dialogue history and candidate response as a whole, the hierarchical information on either utterance interrelation or speaker roles coupled in such representations is not well addressed. In this work, we propose a novel model to fill such a gap by modeling the effective utterance-aware and speaker-aware representations entailed in a dialogue history. In detail, we decouple the contextualized word representations by masking mechanisms in Transformer-based PrLM, making each word only focus on the words in current utterance, other utterances, two speaker roles (i.e., utterances of sender and utterances of receiver), respectively. Experimental results show that our method boosts the strong ELECTRA baseline substantially in four public benchmark datasets, and achieves various new state-of-the-art performance over previous methods. A series of ablation studies are conducted to demonstrate the effectiveness of our method.},
archivePrefix = {arXiv},
arxivId = {2009.06504},
author = {Liu, Longxiang and Zhang, Zhuosheng and Zhao, Hai and Zhou, Xi and Zhou, Xiang},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
eprint = {2009.06504},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2021 - Filling the gap of utterance-aware and speaker-aware representation for multi-turn dialogue.pdf:pdf},
issn = {23318422},
title = {{Filling the gap of utterance-aware and speaker-aware representation for multi-turn dialogue}},
year = {2021}
}
@article{Le2021vgnmn,
abstract = {Neural module networks (NMN) have achieved success in image-grounded tasks such as Visual Question Answering (VQA) on synthetic images. However, very limited work on NMN has been studied in the video-grounded language tasks. These tasks extend the complexity of traditional visual tasks with the additional visual temporal variance. Motivated by recent NMN approaches on image-grounded tasks, we introduce Video-grounded Neural Module Network (VGNMN) to model the information retrieval process in video-grounded language tasks as a pipeline of neural modules. VGNMN first decomposes all language components to explicitly resolve any entity references and detect corresponding action-based inputs from the question. The detected entities and actions are used as parameters to instantiate neural module networks and extract visual cues from the video. Our experiments show that VGNMN can achieve promising performance on two video-grounded language tasks: video QA and video-grounded dialogues.},
archivePrefix = {arXiv},
arxivId = {2104.07921},
author = {Le, Hung and Chen, Nancy F. and Hoi, Steven C. H.},
eprint = {2104.07921},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Chen, Hoi - 2021 - VGNMN Video-grounded Neural Module Network to Video-Grounded Language Tasks.pdf:pdf},
month = {apr},
title = {{VGNMN: Video-grounded Neural Module Network to Video-Grounded Language Tasks}},
journal={arXiv preprint arXiv:2104.07921},
url = {http://arxiv.org/abs/2104.07921},
year = {2021}
}
@article{Li2021bridging,
abstract = {Audio-Visual Scene-Aware Dialog (AVSD) is a task to generate responses when chatting about a given video, which is organized as a track of the Dialog System Technology Challenge (DSTC8). There are two challenges in this task: 1) making effective interaction among different modalities; 2) better understanding dialogues and generating informative responses. To tackle the challenges, we propose a universal multimodal transformer and introduce the multi-task learning method to learn joint representations among different modalities as well as generate informative and fluent responses by leveraging the pre-trained language model. Our method extends the natural language generation pre-trained model to multimodal dialogue generation task, which allows fine-tuning language models to capture information across both visual and textual modalities. Our system achieves the best performance in the objective evaluation in both DSTC7-AVSD and DSTC8-AVSD dataset and achieves an impressive 98.4\% of the human performance based on human ratings in the DSTC8-AVSD challenge.},
archivePrefix = {arXiv},
arxivId = {2002.00163},
author = {Li, Zekang and Li, Zongjia and Zhang, Jinchao and Feng, Yang and Zhou, Jie},
doi = {10.1109/TASLP.2021.3065823},
eprint = {2002.00163},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2021 - Bridging Text and Video A Universal Multimodal Transformer for Video-Audio Scene-Aware Dialog.pdf:pdf},
issn = {2329-9290},
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
keywords = {Dialogue System,Feature extraction,History,Multimodal,Natural Language Processing,Pattern recognition,Social networking (online),Speech processing,Task analysis,Video Understanding,Visualization},
title = {{Bridging Text and Video: A Universal Multimodal Transformer for Video-Audio Scene-Aware Dialog}},
url = {https://ieeexplore.ieee.org/document/9376902/},
pages = {2476--2483},
volume = {29},
year = {2021}
}
@article{Cho2021unifying,
abstract = {Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent task-specific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on questions that have rare answers. Also, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, achieving similar performance to separately optimized single-task models. Our code is publicly available at: https://github.com/j-min/VL-T5},
archivePrefix = {arXiv},
arxivId = {2102.02779},
author = {Cho, Jaemin and Lei, Jie and Tan, Hao and Bansal, Mohit},
eprint = {2102.02779},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cho et al. - 2021 - Unifying Vision-and-Language Tasks via Text Generation.pdf:pdf},
month = {feb},
number = {2019},
title = {{Unifying Vision-and-Language Tasks via Text Generation}},
url = {http://arxiv.org/abs/2102.02779},
year = {2021}
}
@inproceedings{Alamri2020audio,
abstract = {Dialog systems need to understand dynamic visual scenes in order to have conversations with users about the objects and events around them. In this Audio Visual Scene-aware Dialog (AVSD) track for Natural Language Generation (NLG), we challenge a new research target, a dialog system that can have conversations with users about the objects and events around them, which lies at the intersection of multiple avenues of research in natural language processing, computer vision, and audio processing. We introduce a new dataset of dialogs about videos of human behaviors. Each dialog is a typed conversation that consists of a sequence of 10 question-and-answer (QA) pairs between two parties. In total, we collected dialogs on 11, 156 videos. In this overview, we describe the task design and data sets, and review the submitted systems and applied techniques for conversation modeling. The AVSD track for Natural Language Generation (NLG) received 31 system submission from a total of 9 teams, and evaluated them based on several objective measures such as BLEU, METEOR, CIDEr and a human-rating based subjective measure. Finally, we discuss technical achievements and remaining problems related to this challenge.},
author = {Alamri, Huda and Hori, Chiori and Marks, Tim K and Batra, Dhruv and Parikh, Devi},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alamri et al. - 2020 - Audio Visual Scene-aware dialog ( AVSD ) Track for Natural Language Generation in DSTC8.pdf:pdf},
title = {{Audio Visual Scene-aware dialog ( AVSD ) Track for Natural Language Generation in DSTC8}},
url = {http://allenai.org/plato/charades/},
year = {2020}
}
@article{Lee2020dstc8,
abstract = {Audio Visual Scene-aware Dialog (AVSD) is the task of generating a response for a question with a given scene, video, audio, and the history of previous turns in the dialog. Existing systems for this task employ the transformers or recurrent neural network-based architecture with the encoder-decoder framework. Even though these techniques show superior performance for this task, they have significant limitations: the model easily overfits only to memorize the grammatical patterns; the model follows the prior distribution of the vocabularies in a dataset. To alleviate the problems, we propose a Multimodal Semantic Transformer Network. It employs a transformer-based architecture with an attention-based word embedding layer that generates words by querying word embeddings. With this design, our model keeps considering the meaning of the words at the generation stage. The empirical results demonstrate the superiority of our proposed model that outperforms most of the previous works for the AVSD task.},
archivePrefix = {arXiv},
arxivId = {2004.08299},
author = {Lee, Hwanhee and Yoon, Seunghyun and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Jung, Kyomin},
eprint = {2004.08299},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2020 - DSTC8-AVSD Multimodal Semantic Transformer Network with Retrieval Style Word Generator.pdf:pdf},
title = {{DSTC8-AVSD: Multimodal Semantic Transformer Network with Retrieval Style Word Generator}},
url = {http://arxiv.org/abs/2004.08299},
year = {2020}
}
@inproceedings{Bertasius2021is,
abstract = {We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named "TimeSformer," adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that "divided attention," where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: https://github.com/facebookresearch/TimeSformer.},
archivePrefix = {arXiv},
arxivId = {2102.05095},
author = {Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
eprint = {2102.05095},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertasius, Wang, Torresani - 2021 - Is Space-Time Attention All You Need for Video Understanding.pdf:pdf},
month = {feb},
booktitle = {Proc. ICML},
pages = {813--824},
title = {{Is Space-Time Attention All You Need for Video Understanding?}},
url = {http://arxiv.org/abs/2102.05095},
year = {2021}
}
@inproceedings{Hori2019end,
abstract = {In order for machines interacting with the real world to have conversations with users about the objects and events around them, they need to understand dynamic audiovisual scenes. The recent revolution of neural network models allows us to combine various modules into a single end-to-end differentiable network. As a result, Audio Visual Scene-Aware Dialog (AVSD) systems for real-world applications can be developed by integrating state-of-the-art technologies from multiple research areas, including end-to-end dialog technologies, visual question answering (VQA) technologies, and video description technologies. In this paper, we introduce a new data set of dialogs about videos of human behaviors, as well as an end-to-end Audio Visual Scene-Aware Dialog (AVSD) model, trained using this new data set, that generates responses in a dialog about a video. By using features that were developed for multimodal attention-based video description, our system improves the quality of generated dialog about dynamic video scenes.},
archivePrefix = {arXiv},
arxivId = {1806.08409},
author = {Hori, Chiori and Alamri, Huda and Wang, Jue and Wichern, Gordon and Hori, Takaaki and Cherian, Anoop and Marks, Tim K. and Cartillier, Vincent and Lopes, Raphael Gontijo and Das, Abhishek and Essa, Irfan and Batra, Dhruv and Parikh, Devi},
booktitle = {Proc. ICASSP},
doi = {10.1109/ICASSP.2019.8682583},
eprint = {1806.08409},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hori et al. - 2019 - End-to-end Audio Visual Scene-aware Dialog Using Multimodal Attention-based Video Features.pdf:pdf},
keywords = {Audio visual scene-aware dialog,End-to-end modeling,Video description,Visual QA},
pages = {2352--2356},
title = {{End-to-end Audio Visual Scene-aware Dialog Using Multimodal Attention-based Video Features}},
url = {https://ieeexplore.ieee.org/document/8682583/},
year = {2019}
}
@inproceedings{Pasunuru2019dstc7,
abstract = {Scene-aware dialogue systems are designed to have conversations about surrounding objects and events. We approach this challenge by building an end-to-end multimodal dialogue system with video (non-audio) and chat history as the context with novel ways of grounding through effective alignment and cross-attention approaches. For this, we use the Audio Visual Scene-Aware Dialog (AVSD) dataset to evaluate the performance of our models and also study the importance of each of the modality and component to the overall performance of our multimodal dialogue models. This achieves the third-rank system in the competition. Further, we also discuss the various other approaches that we tried to improve the performance of our models, e.g., reinforcement learning, contextual embeddings, pointer-generator copy models, and external data.},
author = {Pasunuru, Ramakanth and Bansal, Mohit},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pasunuru, Bansal - 2019 - DSTC7-AVSD Scene-Aware Video-Dialogue Systems with Dual Attention.pdf:pdf},
title = {{DSTC7-AVSD: Scene-Aware Video-Dialogue Systems with Dual Attention}},
url = {www.aaai.org},
year = {2019}
}
@inproceedings{Hori2019joint,
author = {Hori, Chiori and Cherian, Anoop and Marks, Tim K and Hori, Takaaki},
booktitle = {Proc. INTERSPEECH},
doi = {10.21437/Interspeech.2019-3143},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hori et al. - 2019 - Joint Student-Teacher Learning for Audio-Visual Scene-Aware Dialog.pdf:pdf},
keywords = {[Electronic Manuscript]},
month = {sep},
pages = {1886--1890},
title = {{Joint Student-Teacher Learning for Audio-Visual Scene-Aware Dialog}},
url = {http://www.isca-speech.org/archive/Interspeech_2019/abstracts/3143.html},
year = {2019}
}
@article{Gupta2021towards,
abstract = {A special purpose learning system assumes knowledge of admissible tasks at design time. Adapting such a system to unforeseen tasks requires architecture manipulation such as adding an output head for each new task or dataset. In this work, we propose a task-agnostic vision-language system that accepts an image and a natural language task description and outputs bounding boxes, confidences, and text. The system supports a wide range of vision tasks such as classification, localization, question answering, captioning, and more. We evaluate the system's ability to learn multiple skills simultaneously, to perform tasks with novel skill-concept combinations, and to learn new skills efficiently and without forgetting.},
archivePrefix = {arXiv},
arxivId = {2104.00743},
author = {Gupta, Tanmay and Kamath, Amita and Kembhavi, Aniruddha and Hoiem, Derek},
eprint = {2104.00743},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gupta et al. - 2021 - Towards General Purpose Vision Systems.pdf:pdf},
month = {apr},
title = {{Towards General Purpose Vision Systems}},
url = {http://arxiv.org/abs/2104.00743},
year = {2021}
}
@inproceedings{Geng2021dynamic,
abstract = {Given an input video, its associated audio, and a brief caption, the audio-visual scene aware dialog (AVSD) task requires an agent to indulge in a question-answer dialog with a human about the audio-visual content. This task thus poses a challenging multi-modal representation learning and reasoning scenario, advancements into which could influence several human-machine interaction applications. To solve this task, we introduce a semantics-controlled multi-modal shuffled Transformer reasoning framework, consisting of a sequence of Transformer modules, each taking a modality as input and producing representations conditioned on the input question. Our proposed Transformer variant uses a shuffling scheme on their multi-head outputs, demonstrating better regularization. To encode fine-grained visual information, we present a novel dynamic scene graph representation learning pipeline that consists of an intra-frame reasoning layer producing spatio-semantic graph representations for every frame, and an inter-frame aggregation module capturing temporal cues. Our entire pipeline is trained end-to-end. We present experiments on the benchmark AVSD dataset, both on answer generation and selection tasks. Our results demonstrate state-of-the-art performances on all evaluation metrics.},
archivePrefix = {arXiv},
arxivId = {2007.03848},
author = {Geng, Shijie and Gao, Peng and Chatterjee, Moitreya and Hori, Chiori and Roux, Jonathan Le and Zhang, Yongfeng and Li, Hongsheng and Cherian, Anoop},
booktitle = {Proc. AAAI},
eprint = {2007.03848},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Geng et al. - 2021 - Dynamic Graph Representation Learning for Video Dialog via Multi-Modal Shuffled Transformers(2).pdf:pdf},
keywords = {Computer Vision: Language and Vision,Computer Vision: Video Understanding & Activity An,Speech & Natural Language Processing: Language Gro,Speech & Natural Language Processing: Question Ans},
pages = {1415--1423},
title = {{Dynamic Graph Representation Learning for Video Dialog via Multi-Modal Shuffled Transformers}},
url = {http://arxiv.org/abs/2007.03848},
year = {2021}
}
@inproceedings{Li2020hero,
abstract = {We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities.},
address = {Stroudsburg, PA, USA},
archivePrefix = {arXiv},
arxivId = {2005.00200},
author = {Li, Linjie and Chen, Yen-Chun and Cheng, Yu and Gan, Zhe and Yu, Licheng and Liu, Jingjing},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.18653/v1/2020.emnlp-main.161},
eprint = {2005.00200},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2020 - HERO Hierarchical Encoder for VideoLanguage Omni-representation Pre-training.pdf:pdf},
pages = {2046--2065},
publisher = {Association for Computational Linguistics},
title = {{HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training}},
url = {https://www.aclweb.org/anthology/2020.emnlp-main.161},
year = {2020}
}
@article{Luo2020univl,
abstract = {With the recent success of the pre-training technique for NLP and image-linguistic tasks, some video-linguistic pre-training works are gradually developed to improve video-text related downstream tasks. However, most of the existing multimodal models are pre-trained for understanding tasks, leading to a pretrain-finetune discrepancy for generation tasks. This paper proposes UniVL: a Unified Video and Language pre-training model for both multimodal understanding and generation. It comprises four components, including two single-modal encoders, a cross encoder, and a decoder with the Transformer backbone. Five objectives, including video-text joint, conditioned masked language model (CMLM), conditioned masked frame model (CMFM), video-text alignment, and language reconstruction, are designed to train each of the components. We further develop two pre-training strategies, stage by stage pre-training (StagedP) and enhanced video representation (EnhancedV), to make the training process of the UniVL more effective. The pre-train is carried out on a sizeable instructional video dataset HowTo100M. Experimental results demonstrate that the UniVL can learn strong video-text representation and achieves state-of-the-art results on five downstream tasks.},
archivePrefix = {arXiv},
arxivId = {2002.06353},
author = {Luo, Huaishao and Ji, Lei and Shi, Botian and Huang, Haoyang and Duan, Nan and Li, Tianrui and Li, Jason and Bharti, Taroon and Zhou, Ming},
eprint = {2002.06353},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo et al. - 2020 - UniVL A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation.pdf:pdf},
title = {{UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation}},
url = {http://arxiv.org/abs/2002.06353},
year = {2020}
}
@article{Chen2021visualgpt,
abstract = {The ability to quickly learn from a small quantity oftraining data widens the range of machine learning applications. In this paper, we propose a data-efficient image captioning model, VisualGPT, which leverages the linguistic knowledge from a large pretrained language model(LM). A crucial challenge is to balance between the use of visual information in the image and prior linguistic knowledge acquired from pretraining. We designed a novel self-resurrecting encoder-decoder attention mechanism to quickly adapt the pretrained LM as the language decoder ona small amount of in-domain training data. The proposed self-resurrecting activation unit produces sparse activations but has reduced susceptibility to zero gradients. We train the proposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual Captions training data. Under these conditions, we outperform the best baseline model by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual Captions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray, a medical report generation dataset. To the best of our knowledge, this is the first work that improves data efficiency of image captioning by utilizing LM pretrained on unimodal data. Our code is available at: https://github.com/Vision-CAIR/VisualGPT.},
archivePrefix = {arXiv},
arxivId = {2102.10407},
author = {Chen, Jun and Guo, Han and Yi, Kai and Li, Boyang and Elhoseiny, Mohamed},
eprint = {2102.10407},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2021 - VisualGPT Data-efficient Adaptation of Pretrained Language Models for Image Captioning.pdf:pdf},
month = {feb},
title = {{VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning}},
url = {http://arxiv.org/abs/2102.10407},
year = {2021}
}
@article{Sharir2021image,
abstract = {Leading methods in the domain of action recognition try to distill information from both the spatial and temporal dimensions of an input video. Methods that reach State of the Art (SotA) accuracy, usually make use of 3D convolution layers as a way to abstract the temporal information from video frames. The use of such convolutions requires sampling short clips from the input video, where each clip is a collection of closely sampled frames. Since each short clip covers a small fraction of an input video, multiple clips are sampled at inference in order to cover the whole temporal length of the video. This leads to increased computational load and is impractical for real-world applications. We address the computational bottleneck by significantly reducing the number of frames required for inference. Our approach relies on a temporal transformer that applies global attention over video frames, and thus better exploits the salient information in each frame. Therefore our approach is very input efficient, and can achieve SotA results (on Kinetics dataset) with a fraction of the data (frames per video), computation and latency. Specifically on Kinetics-400, we reach $80.5$ top-1 accuracy with $\times 30$ less frames per video, and $\times 40$ faster inference than the current leading method. Code is available at: https://github.com/Alibaba-MIIL/STAM},
archivePrefix = {arXiv},
arxivId = {2103.13915},
author = {Sharir, Gilad and Noy, Asaf and Zelnik-Manor, Lihi},
eprint = {2103.13915},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sharir, Noy, Zelnik-Manor - 2021 - An Image is Worth 16x16 Words, What is a Video Worth.pdf:pdf},
month = {mar},
title = {{An Image is Worth 16x16 Words, What is a Video Worth?}},
url = {http://arxiv.org/abs/2103.13915},
year = {2021}
}
@inproceedings{Le2019multimodal,
abstract = {Developing Video-Grounded Dialogue Systems (VGDS), where a dialogue is conducted based on visual and audio aspects of a given video, is significantly more challenging than traditional image or text-grounded dialogue systems because (1) feature space of videos span across multiple picture frames, making it difficult to obtain semantic information; and (2) a dialogue agent must perceive and process information from different modalities (audio, video, caption, etc.) to obtain a comprehensive understanding. Most existing work is based on RNNs and sequence-to-sequence architectures, which are not very effective for capturing complex long-term dependencies (like in videos). To overcome this, we propose Multimodal Transformer Networks (MTN) to encode videos and incorporate information from different modalities. We also propose query-aware attention through an auto-encoder to extract query-aware features from non-text modalities. We develop a training procedure to simulate token-level decoding to improve the quality of generated responses during inference. We get state of the art performance on Dialogue System Technology Challenge 7 (DSTC7). Our model also generalizes to another multimodal visual-grounded dialogue task, and obtains promising performance.},
address = {Stroudsburg, PA, USA},
archivePrefix = {arXiv},
arxivId = {1907.01166},
author = {Le, Hung and Sahoo, Doyen and Chen, Nancy and Hoi, Steven},
booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
doi = {10.18653/v1/P19-1564},
eprint = {1907.01166},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le et al. - 2019 - Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems.pdf:pdf},
isbn = {9781950737482},
pages = {5612--5623},
publisher = {Association for Computational Linguistics},
title = {{Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems}},
url = {https://www.aclweb.org/anthology/P19-1564},
year = {2019}
}
@inproceedings{Chen2020pretraining,
author = {Chen, Junkun and He, Huang and Bao, Siqi and Wang, Fan},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2020 - Pre-training Assisted Scene-aware Dialogue Generation.pdf:pdf},
title = {{Pre-training Assisted Scene-aware Dialogue Generation}},
booktitle = {DSTC8 at AAAI2020 workshop},
year = {2020}
}
@article{Le2020multimodal,
abstract = {Audio-Visual Scene-Aware Dialog (AVSD) is an extension from Video Question Answering (QA) whereby the dialogue agent is required to generate natural language responses to address user queries and carry on conversations. This is a challenging task as it consists of video features of multiple modalities, including text, visual, and audio features. The agent also needs to learn semantic dependencies among user utterances and system responses to make coherent conversations with humans. In this work, we describe our submission to the AVSD track of the 8th Dialogue System Technology Challenge. We adopt dot-product attention to combine text and non-text features of input video. We further enhance the generation capability of the dialogue agent by adopting pointer networks to point to tokens from multiple source sequences in each generation step. Our systems achieve high performance in automatic metrics and obtain 5th and 6th place in human evaluation among all submissions.},
archivePrefix = {arXiv},
arxivId = {2002.10695},
author = {Le, Hung and Chen, Nancy F.},
eprint = {2002.10695},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Chen - 2020 - Multimodal Transformer with Pointer Network for the DSTC8 AVSD Challenge.pdf:pdf},
title = {{Multimodal Transformer with Pointer Network for the DSTC8 AVSD Challenge}},
url = {http://arxiv.org/abs/2002.10695},
year = {2020}
}
@inproceedings{Xie2020audio,
abstract = {The audio visual scene-aware dialog (AVSD) task, proposed as one of the tracks in the Eighth Dialog System Technology Challenge (DSTC8), is a multimodal dialog task which aims to automatically generate a response to an input question about the content of a video clip in the context of a given dialog. In this paper, we propose for this task a number of models that are based on dynamic memory networks (DMNs). Compared to the baseline model released by the AVSD organizers, our DMN-based AVSD model with single modality achieves performance improvements of more than 4.2% in the BLEU-4 score and 18.1% in the CIDEr score, demonstrating the effectiveness of DMNs for encoding long-term context information in dialog tasks. We also present a multimodal variant of the DMN-based model which incorporates all modalities.},
author = {Xie, Huiyuan and Iacobacci, Ignacio},
booktitle = {DSTC8 Workshop at AAAI-20},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie, Iacobacci - 2020 - Audio Visual Scene-Aware Dialog System Using Dynamic Memory Networks.pdf:pdf},
number = {February},
title = {{Audio Visual Scene-Aware Dialog System Using Dynamic Memory Networks}},
url = {www.aaai.org},
year = {2020}
}
@inproceedings{Kumar2019context,
abstract = {We are witnessing a confluence of vision, speech and dialog system technologies that are enabling the IVAs to learn audio-visual groundings of utterances and have conversations with users about the objects, activities and events surrounding them. Recent progress in visual grounding techniques and Audio Understanding are enabling machines to understand shared semantic concepts and listen to the various sensory events in the environment. With audio and visual grounding methods, end-to-end multimodal SDS are trained to meaningfully communicate with us in natural language about the real dynamic audio-visual sensory world around us. In this work, we explore the role of `topics' as the context of the conversation along with multimodal attention into such an end-to-end audio-visual scene-aware dialog system architecture. We also incorporate an end-to-end audio classification ConvNet, AclNet, into our models. We develop and test our approaches on the Audio Visual Scene-Aware Dialog (AVSD) dataset released as a part of the DSTC7. We present the analysis of our experiments and show that some of our model variations outperform the baseline system released for AVSD.},
archivePrefix = {arXiv},
arxivId = {1912.10132},
author = {Kumar, Shachi H and Okur, Eda and Sahay, Saurav and Huang, Jonathan and Nachman, Lama},
eprint = {1912.10132},
file = {:C\:/Users/Yoshihiro Yamazaki/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumar et al. - 2019 - Context, Attention and Audio Feature Explorations for Audio Visual Scene-Aware Dialog.pdf:pdf},
title = {{Context, Attention and Audio Feature Explorations for Audio Visual Scene-Aware Dialog}},
url = {http://arxiv.org/abs/1912.10132},
year = {2019}
}
