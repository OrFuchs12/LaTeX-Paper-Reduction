\begin{thebibliography}{116}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Aguilar et~al.(2020)Aguilar, Ling, Zhang
  et~al.}]{aguilar2020knowledge}
Aguilar, G.; Ling, Y.; Zhang, Y.; et~al. 2020.
\newblock Knowledge Distillation from Internal Representations.
\newblock In \emph{{AAAI}}.

\bibitem[{Bai et~al.(2021)Bai, Zhang, Hou et~al.}]{binarybert}
Bai, H.; Zhang, W.; Hou, L.; et~al. 2021.
\newblock BinaryBERT: Pushing the Limit of {BERT} Quantization.
\newblock In \emph{{ACL-IJCNLP}}.

\bibitem[{Beck and Teboulle(2003)}]{beck2003mirror}
Beck, A.; and Teboulle, M. 2003.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock \emph{Operations Research Letters}.

\bibitem[{Bender et~al.(2021)Bender, Gebru, McMillan{-}Major, and
  Shmitchell}]{parrot}
Bender, E.~M.; Gebru, T.; McMillan{-}Major, A.; and Shmitchell, S. 2021.
\newblock On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?
\newblock In \emph{FAccT}.

\bibitem[{Bengio, L{\'e}onard, and Courville(2013)}]{bengio2013estimating}
Bengio, Y.; L{\'e}onard, N.; and Courville, A. 2013.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{arXiv preprint arXiv:1308.3432}.

\bibitem[{Chen et~al.(2021)Chen, Yu, Dhillon, and Hsieh}]{chen2021drone}
Chen, P.-H.; Yu, H.-F.; Dhillon, I.; and Hsieh, C.-J. 2021.
\newblock DRONE: Data-aware Low-rank Compression for Large NLP Models.
\newblock In \emph{NeurIPS}.

\bibitem[{Chen et~al.(2020)Chen, Frankle, Chang et~al.}]{chen2020lottery}
Chen, T.; Frankle, J.; Chang, S.; et~al. 2020.
\newblock The Lottery Ticket Hypothesis for Pre-trained {BERT} Networks.
\newblock In \emph{NeurIPS}.

\bibitem[{Dabre and Fujita(2019)}]{dabre2019recurrent}
Dabre, R.; and Fujita, A. 2019.
\newblock Recurrent Stacking of Layers for Compact Neural Machine Translation
  Models.
\newblock In \emph{{AAAI}}.

\bibitem[{Dehghani et~al.(2019)Dehghani, Gouws, Vinyals
  et~al.}]{dehghani2019universal}
Dehghani, M.; Gouws, S.; Vinyals, O.; et~al. 2019.
\newblock Universal Transformers.
\newblock In \emph{{ICLR}}.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova}]{bert}
Devlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.
\newblock {BERT:} Pre-training of Deep Bidirectional Transformers for Language
  Understanding.
\newblock In \emph{{NAACL-HLT}}.

\bibitem[{Dong et~al.(2019)Dong, Yao, Gholami et~al.}]{hawq}
Dong, Z.; Yao, Z.; Gholami, A.; et~al. 2019.
\newblock {HAWQ:} Hessian AWare Quantization of Neural Networks With
  Mixed-Precision.
\newblock In \emph{{ICCV}}.

\bibitem[{Du et~al.(2021)Du, Mukherjee, Cheng et~al.}]{du2021compressed}
Du, M.; Mukherjee, S.; Cheng, Y.; et~al. 2021.
\newblock What do Compressed Large Language Models Forget? Robustness
  Challenges in Model Compression.
\newblock \emph{arXiv preprint arXiv:2110.08419}.

\bibitem[{Edalati et~al.(2022)Edalati, Tahaei, Rashid et~al.}]{kroneckergpt}
Edalati, A.; Tahaei, M.~S.; Rashid, A.; et~al. 2022.
\newblock Kronecker Decomposition for {GPT} Compression.
\newblock In \emph{{ACL}}.

\bibitem[{Fan, Grave, and Joulin(2020)}]{drophead}
Fan, A.; Grave, E.; and Joulin, A. 2020.
\newblock Reducing Transformer Depth on Demand with Structured Dropout.
\newblock In \emph{{ICLR}}.

\bibitem[{Finn, Abbeel, and Levine(2017)}]{maml}
Finn, C.; Abbeel, P.; and Levine, S. 2017.
\newblock Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.
\newblock In \emph{{ICML}}, Proceedings of Machine Learning Research.

\bibitem[{Frankle and Carbin(2019)}]{lottery}
Frankle, J.; and Carbin, M. 2019.
\newblock The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural
  Networks.
\newblock In \emph{{ICLR}}.

\bibitem[{Geng et~al.(2021)Geng, Gao, Fu, and Zhang}]{geng2021romebert}
Geng, S.; Gao, P.; Fu, Z.; and Zhang, Y. 2021.
\newblock Romebert: Robust training of multi-exit bert.
\newblock \emph{arXiv preprint arXiv:2101.09755}.

\bibitem[{Gordon, Duh, and Andrews(2020)}]{gordon2020compressing}
Gordon, M.~A.; Duh, K.; and Andrews, N. 2020.
\newblock Compressing {BERT:} Studying the Effects of Weight Pruning on
  Transfer Learning.
\newblock In \emph{RepL4NLP@ACL}.

\bibitem[{Goyal et~al.(2020)Goyal, Choudhury, Raje et~al.}]{powerbert}
Goyal, S.; Choudhury, A.~R.; Raje, S.; et~al. 2020.
\newblock PoWER-BERT: Accelerating {BERT} Inference via Progressive Word-vector
  Elimination.
\newblock In \emph{{ICML}}.

\bibitem[{Grachev, Ignatov, and Savchenko(2017)}]{grachev2017neural}
Grachev, A.~M.; Ignatov, D.~I.; and Savchenko, A.~V. 2017.
\newblock Neural Networks Compression for Language Modeling.
\newblock In \emph{PReMI}.

\bibitem[{Guan et~al.(2022)Guan, Li, Leng et~al.}]{transkimmer}
Guan, Y.; Li, Z.; Leng, J.; et~al. 2022.
\newblock Transkimmer: Transformer Learns to Layer-wise Skim.
\newblock In \emph{{ACL}}.

\bibitem[{Guo, Rush, and Kim(2021)}]{guo2021parameter}
Guo, D.; Rush, A.~M.; and Kim, Y. 2021.
\newblock Parameter-Efficient Transfer Learning with Diff Pruning.
\newblock In \emph{{ACL-IJCNLP}}.

\bibitem[{Han, Mao, and Dally(2016)}]{han2016deep}
Han, S.; Mao, H.; and Dally, W.~J. 2016.
\newblock Deep Compression: Compressing Deep Neural Network with Pruning,
  Trained Quantization and Huffman Coding.
\newblock In \emph{{ICLR}}.

\bibitem[{Han et~al.(2021)Han, Zhang, Ding et~al.}]{han2021pre}
Han, X.; Zhang, Z.; Ding, N.; et~al. 2021.
\newblock Pre-trained models: Past, present and future.
\newblock \emph{AI Open}.

\bibitem[{Henderson et~al.(2020)Henderson, Hu, Romoff
  et~al.}]{henderson2020towards}
Henderson, P.; Hu, J.; Romoff, J.; et~al. 2020.
\newblock Towards the systematic reporting of the energy and carbon footprints
  of machine learning.
\newblock \emph{Journal of Machine Learning Research}.

\bibitem[{Hinton et~al.(2015)Hinton, Vinyals, Dean
  et~al.}]{hinton2015distilling}
Hinton, G.; Vinyals, O.; Dean, J.; et~al. 2015.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}.

\bibitem[{Hou et~al.(2020)Hou, Huang, Shang et~al.}]{dynabert}
Hou, L.; Huang, Z.; Shang, L.; et~al. 2020.
\newblock DynaBERT: Dynamic {BERT} with Adaptive Width and Depth.
\newblock In \emph{NeurIPS}.

\bibitem[{Huang et~al.(2022)Huang, Xu, Yen et~al.}]{huang2022sparse}
Huang, S.; Xu, D.; Yen, I.~E.; et~al. 2022.
\newblock Sparse Progressive Distillation: Resolving Overfitting under
  Pretrain-and-Finetune Paradigm.
\newblock In \emph{{ACL}}.

\bibitem[{Jacob et~al.(2018)Jacob, Kligys, Chen et~al.}]{jacob2018quantization}
Jacob, B.; Kligys, S.; Chen, B.; et~al. 2018.
\newblock Quantization and Training of Neural Networks for Efficient
  Integer-Arithmetic-Only Inference.
\newblock In \emph{{CVPR}}.

\bibitem[{Jiao et~al.(2020)Jiao, Yin, Shang et~al.}]{tinybert}
Jiao, X.; Yin, Y.; Shang, L.; et~al. 2020.
\newblock TinyBERT: Distilling {BERT} for Natural Language Understanding.
\newblock In \emph{{EMNLP} (Findings)}.

\bibitem[{Kaya, Hong, and Dumitras(2019)}]{shallowdeep}
Kaya, Y.; Hong, S.; and Dumitras, T. 2019.
\newblock Shallow-Deep Networks: Understanding and Mitigating Network
  Overthinking.
\newblock In \emph{{ICML}}.

\bibitem[{Kim and Cho(2021)}]{lat}
Kim, G.; and Cho, K. 2021.
\newblock Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime
  with Search.
\newblock In \emph{{ACL-IJCNLP}}.

\bibitem[{Kim et~al.(2021)Kim, Gholami, Yao et~al.}]{ibert}
Kim, S.; Gholami, A.; Yao, Z.; et~al. 2021.
\newblock {I-BERT:} Integer-only {BERT} Quantization.
\newblock In \emph{{ICML}}.

\bibitem[{Kim et~al.(2022)Kim, Shen, Thorsley et~al.}]{ltp}
Kim, S.; Shen, S.; Thorsley, D.; et~al. 2022.
\newblock Learned Token Pruning for Transformers.
\newblock In \emph{{KDD}}.

\bibitem[{Kim and Awadalla(2020)}]{kim2020fastformers}
Kim, Y.~J.; and Awadalla, H.~H. 2020.
\newblock Fastformers: Highly efficient transformer models for natural language
  understanding.
\newblock \emph{arXiv preprint arXiv:2010.13382}.

\bibitem[{Lacoste et~al.(2019)Lacoste, Luccioni, Schmidt, and
  Dandres}]{lacoste2019quantifying}
Lacoste, A.; Luccioni, A.; Schmidt, V.; and Dandres, T. 2019.
\newblock Quantifying the carbon emissions of machine learning.
\newblock \emph{arXiv preprint arXiv:1910.09700}.

\bibitem[{Lagunas et~al.(2021)Lagunas, Charlaix, Sanh, and
  Rush}]{lagunas2021block}
Lagunas, F.; Charlaix, E.; Sanh, V.; and Rush, A.~M. 2021.
\newblock Block Pruning For Faster Transformers.
\newblock In \emph{{EMNLP}}.

\bibitem[{Lan et~al.(2020)Lan, Chen, Goodman et~al.}]{albert}
Lan, Z.; Chen, M.; Goodman, S.; et~al. 2020.
\newblock {ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language
  Representations.
\newblock In \emph{{ICLR}}.

\bibitem[{Lathauwer(2008)}]{btd}
Lathauwer, L.~D. 2008.
\newblock Decompositions of a Higher-Order Tensor in Block Terms - Part {II:}
  Definitions and Uniqueness.
\newblock \emph{{SIAM} J. Matrix Anal. Appl.}

\bibitem[{Le~Scao et~al.(2022)Le~Scao, Fan, Akiki et~al.}]{scao2022bloom}
Le~Scao, T.; Fan, A.; Akiki, C.; et~al. 2022.
\newblock BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.
\newblock \emph{arXiv preprint arXiv:2211.05100}.

\bibitem[{LeCun, Denker, and Solla(1989)}]{obd}
LeCun, Y.; Denker, J.~S.; and Solla, S.~A. 1989.
\newblock Optimal Brain Damage.
\newblock In \emph{{NIPS}}.

\bibitem[{Li et~al.(2020)Li, Wallace, Shen et~al.}]{li2020train}
Li, Z.; Wallace, E.; Shen, S.; et~al. 2020.
\newblock Train Large, Then Compress: Rethinking Model Size for Efficient
  Training and Inference of Transformers.
\newblock In \emph{{ICML}}.

\bibitem[{Liang et~al.(2021)Liang, Hao, Shen et~al.}]{mixkd}
Liang, K.~J.; Hao, W.; Shen, D.; et~al. 2021.
\newblock MixKD: Towards Efficient Distillation of Large-scale Language Models.
\newblock In \emph{{ICLR}}.

\bibitem[{Liao et~al.(2021)Liao, Zhang, Ren et~al.}]{liao2021global}
Liao, K.; Zhang, Y.; Ren, X.; et~al. 2021.
\newblock A Global Past-Future Early Exit Method for Accelerating Inference of
  Pre-trained Language Models.
\newblock In \emph{{NAACL-HLT}}.

\bibitem[{Lin et~al.(2020)Lin, Liu, Yang et~al.}]{snip}
Lin, Z.; Liu, J.~Z.; Yang, Z.; et~al. 2020.
\newblock Pruning Redundant Mappings in Transformer Models via
  Spectral-Normalized Identity Prior.
\newblock In \emph{{EMNLP} (Findings)}.

\bibitem[{Liu et~al.(2022)Liu, Tao, Feng, and Zhao}]{liu2022multi}
Liu, C.; Tao, C.; Feng, J.; and Zhao, D. 2022.
\newblock Multi-Granularity Structural Knowledge Distillation for Language
  Model Compression.
\newblock In \emph{{ACL}}.

\bibitem[{Liu, Simonyan, and Yang(2019)}]{darts}
Liu, H.; Simonyan, K.; and Yang, Y. 2019.
\newblock {DARTS:} Differentiable Architecture Search.
\newblock In \emph{{ICLR}}.

\bibitem[{Liu et~al.(2020)Liu, Zhou, Wang et~al.}]{fastbert}
Liu, W.; Zhou, P.; Wang, Z.; et~al. 2020.
\newblock FastBERT: a Self-distilling {BERT} with Adaptive Inference Time.
\newblock In \emph{{ACL}}.

\bibitem[{Liu et~al.(2021)Liu, Sun, He et~al.}]{liu2021towards}
Liu, X.; Sun, T.; He, J.; et~al. 2021.
\newblock Towards Efficient NLP: A Standard Evaluation and A Strong Baseline.
\newblock \emph{arXiv preprint arXiv:2110.07038}.

\bibitem[{Louizos, Welling, and Kingma(2018)}]{l0}
Louizos, C.; Welling, M.; and Kingma, D.~P. 2018.
\newblock Learning Sparse Neural Networks through L{\_}0 Regularization.
\newblock In \emph{{ICLR}}.

\bibitem[{Ma et~al.(2022)Ma, Wang, Yu, and Zhang}]{raptilemeta}
Ma, X.; Wang, J.; Yu, L.; and Zhang, X. 2022.
\newblock Knowledge Distillation with Reptile Meta-Learning for Pretrained
  Language Model Compression.
\newblock In \emph{{COLING}}.

\bibitem[{Ma et~al.(2019)Ma, Zhang, Zhang et~al.}]{ma2019tensorized}
Ma, X.; Zhang, P.; Zhang, S.; et~al. 2019.
\newblock A Tensorized Transformer for Language Modeling.
\newblock In \emph{NeurIPS}.

\bibitem[{Michel, Levy, and Neubig(2019)}]{16heads}
Michel, P.; Levy, O.; and Neubig, G. 2019.
\newblock Are Sixteen Heads Really Better than One?
\newblock In \emph{NeurIPS}.

\bibitem[{Min et~al.(2020)Min, Boyd{-}Graber, Alberti et~al.}]{efficientqa}
Min, S.; Boyd{-}Graber, J.~L.; Alberti, C.; et~al. 2020.
\newblock NeurIPS 2020 EfficientQA Competition: Systems, Analyses and Lessons
  Learned.
\newblock In \emph{NeurIPS (Competition and Demos)}.

\bibitem[{Mishra et~al.(2021)Mishra, Latorre, Pool
  et~al.}]{mishra2021accelerating}
Mishra, A.; Latorre, J.~A.; Pool, J.; et~al. 2021.
\newblock Accelerating sparse deep neural networks.
\newblock \emph{arXiv preprint arXiv:2104.08378}.

\bibitem[{Nakkiran et~al.(2020)Nakkiran, Kaplun, Bansal
  et~al.}]{nakkiran2020deep}
Nakkiran, P.; Kaplun, G.; Bansal, Y.; et~al. 2020.
\newblock Deep Double Descent: Where Bigger Models and More Data Hurt.
\newblock In \emph{{ICLR}}.

\bibitem[{Narang et~al.(2017)Narang, Diamos, Sengupta, and
  Elsen}]{narang2017exploring}
Narang, S.; Diamos, G.; Sengupta, S.; and Elsen, E. 2017.
\newblock Exploring Sparsity in Recurrent Neural Networks.
\newblock In \emph{{ICLR}}.

\bibitem[{Narang, Undersander, and Diamos(2017)}]{narang2017block}
Narang, S.; Undersander, E.; and Diamos, G. 2017.
\newblock Block-sparse recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1711.02782}.

\bibitem[{Noach and Goldberg(2020)}]{noach2020compressing}
Noach, M.~B.; and Goldberg, Y. 2020.
\newblock Compressing Pre-trained Language Models by Matrix Decomposition.
\newblock In \emph{{AACL-IJCNLP}}.

\bibitem[{Park et~al.(2015)Park, Kim, Kim et~al.}]{park2015big}
Park, E.; Kim, D.; Kim, S.; et~al. 2015.
\newblock Big/little deep neural network for ultra low power inference.
\newblock In \emph{{CODES+ISSS}}.

\bibitem[{Peters et~al.(2018)Peters, Neumann, Iyyer et~al.}]{elmo}
Peters, M.~E.; Neumann, M.; Iyyer, M.; et~al. 2018.
\newblock Deep Contextualized Word Representations.
\newblock In \emph{{NAACL-HLT}}.

\bibitem[{Prasanna, Rogers, and Rumshisky(2020)}]{prasanna2020when}
Prasanna, S.; Rogers, A.; and Rumshisky, A. 2020.
\newblock When {BERT} Plays the Lottery, All Tickets Are Winning.
\newblock In \emph{{EMNLP}}.

\bibitem[{Prato, Charlaix, and Rezagholizadeh(2020)}]{prato2020fully}
Prato, G.; Charlaix, E.; and Rezagholizadeh, M. 2020.
\newblock Fully Quantized Transformer for Machine Translation.
\newblock In \emph{{EMNLP} (Findings)}.

\bibitem[{Qiu et~al.(2020)Qiu, Sun, Xu et~al.}]{qiu2020pre}
Qiu, X.; Sun, T.; Xu, Y.; et~al. 2020.
\newblock Pre-trained models for natural language processing: A survey.
\newblock \emph{Science China Technological Sciences}.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts et~al.}]{t5}
Raffel, C.; Shazeer, N.; Roberts, A.; et~al. 2020.
\newblock Exploring the Limits of Transfer Learning with a Unified Text-to-Text
  Transformer.
\newblock \emph{J. Mach. Learn. Res.}

\bibitem[{Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang}]{squad}
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.
\newblock SQuAD: 100, 000+ Questions for Machine Comprehension of Text.
\newblock In \emph{{EMNLP}}.

\bibitem[{Reid, Marrese{-}Taylor, and Matsuo(2021)}]{reid2021subformer}
Reid, M.; Marrese{-}Taylor, E.; and Matsuo, Y. 2021.
\newblock Subformer: Exploring Weight Sharing for Parameter Efficiency in
  Generative Transformers.
\newblock In \emph{{EMNLP} (Findings)}.

\bibitem[{Rothe, Narayan, and Severyn(2020)}]{rothe2020leveraging}
Rothe, S.; Narayan, S.; and Severyn, A. 2020.
\newblock Leveraging Pre-trained Checkpoints for Sequence Generation Tasks.
\newblock \emph{{TACL}}.

\bibitem[{Sainath et~al.(2013)Sainath, Kingsbury, Sindhwani
  et~al.}]{sainath2013low}
Sainath, T.~N.; Kingsbury, B.; Sindhwani, V.; et~al. 2013.
\newblock Low-rank matrix factorization for Deep Neural Network training with
  high-dimensional output targets.
\newblock In \emph{{ICASSP}}.

\bibitem[{Sanh et~al.(2019)Sanh, Debut, Chaumond, and
  Wolf}]{sanh2019distilbert}
Sanh, V.; Debut, L.; Chaumond, J.; and Wolf, T. 2019.
\newblock DistilBERT, a distilled version of BERT: smaller, faster, cheaper and
  lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}.

\bibitem[{Sanh, Wolf, and Rush(2020)}]{sanh2020movement}
Sanh, V.; Wolf, T.; and Rush, A.~M. 2020.
\newblock Movement Pruning: Adaptive Sparsity by Fine-Tuning.
\newblock In \emph{NeurIPS}.

\bibitem[{Schuster et~al.(2021)Schuster, Fisch, Jaakkola, and
  Barzilay}]{schuster2021consistent}
Schuster, T.; Fisch, A.; Jaakkola, T.~S.; and Barzilay, R. 2021.
\newblock Consistent accelerated inference via confident adaptive transformers.
\newblock \emph{arXiv preprint arXiv:2104.08803}.

\bibitem[{Schwartz et~al.(2020)Schwartz, Stanovsky, Swayamdipta
  et~al.}]{schwartz2020right}
Schwartz, R.; Stanovsky, G.; Swayamdipta, S.; et~al. 2020.
\newblock The Right Tool for the Job: Matching Model and Instance Complexities.
\newblock In \emph{{ACL}}.

\bibitem[{See, Luong, and Manning(2016)}]{see2016}
See, A.; Luong, M.; and Manning, C.~D. 2016.
\newblock Compression of Neural Machine Translation Models via Pruning.
\newblock In \emph{CoNLL}.

\bibitem[{Shen et~al.(2020)Shen, Dong, Ye et~al.}]{qbert}
Shen, S.; Dong, Z.; Ye, J.; et~al. 2020.
\newblock {Q-BERT:} Hessian Based Ultra Low Precision Quantization of {BERT}.
\newblock In \emph{{AAAI}}.

\bibitem[{Shi et~al.(2020)Shi, Song, Zhou et~al.}]{shi2020learning}
Shi, W.; Song, Y.; Zhou, H.; et~al. 2020.
\newblock Learning from deep model via exploring local targets.
\newblock \emph{OpenReview preprint}.

\bibitem[{Stanton et~al.(2021)Stanton, Izmailov, Kirichenko
  et~al.}]{stanton2021does}
Stanton, S.; Izmailov, P.; Kirichenko, P.; et~al. 2021.
\newblock Does Knowledge Distillation Really Work?
\newblock \emph{arXiv preprint arXiv:2106.05945}.

\bibitem[{Su et~al.(2018)Su, Zhang, Chen et~al.}]{su2018is}
Su, D.; Zhang, H.; Chen, H.; et~al. 2018.
\newblock Is Robustness the Cost of Accuracy? - {A} Comprehensive Study on the
  Robustness of 18 Deep Image Classification Models.
\newblock In \emph{{ECCV}}.

\bibitem[{Sun et~al.(2019)Sun, Cheng, Gan, and Liu}]{pkd}
Sun, S.; Cheng, Y.; Gan, Z.; and Liu, J. 2019.
\newblock Patient Knowledge Distillation for {BERT} Model Compression.
\newblock In \emph{{EMNLP-IJCNLP}}.

\bibitem[{Sun et~al.(2021)Sun, Zhou, Liu et~al.}]{sun2021early}
Sun, T.; Zhou, Y.; Liu, X.; et~al. 2021.
\newblock Early Exiting with Ensemble Internal Classifiers.
\newblock \emph{arXiv preprint arXiv:2105.13792}.

\bibitem[{Sun et~al.(2020)Sun, Yu, Song et~al.}]{mobilebert}
Sun, Z.; Yu, H.; Song, X.; et~al. 2020.
\newblock MobileBERT: a Compact Task-Agnostic {BERT} for Resource-Limited
  Devices.
\newblock In \emph{{ACL}}.

\bibitem[{Tahaei et~al.(2021)Tahaei, Charlaix, Nia
  et~al.}]{tahaei2021kroneckerbert}
Tahaei, M.~S.; Charlaix, E.; Nia, V.~P.; et~al. 2021.
\newblock Kroneckerbert: Learning kronecker decomposition for pre-trained
  language models via knowledge distillation.
\newblock \emph{arXiv preprint arXiv:2109.06243}.

\bibitem[{Takase and Kiyono(2021)}]{takase2021lessons}
Takase, S.; and Kiyono, S. 2021.
\newblock Lessons on parameter sharing across layers in transformers.
\newblock \emph{arXiv preprint arXiv:2104.06022}.

\bibitem[{Tang et~al.(2019)Tang, Lu, Liu et~al.}]{tang2019distilling}
Tang, R.; Lu, Y.; Liu, L.; et~al. 2019.
\newblock Distilling task-specific knowledge from bert into simple neural
  networks.
\newblock \emph{arXiv preprint arXiv:1903.12136}.

\bibitem[{Tao et~al.(2022)Tao, Hou, Zhang et~al.}]{tao2022compression}
Tao, C.; Hou, L.; Zhang, W.; et~al. 2022.
\newblock Compression of Generative Pre-trained Language Models via
  Quantization.
\newblock In \emph{{ACL}}.

\bibitem[{Teerapittayanon, McDanel, and Kung(2016)}]{branchynet}
Teerapittayanon, S.; McDanel, B.; and Kung, H.~T. 2016.
\newblock BranchyNet: Fast inference via early exiting from deep neural
  networks.
\newblock In \emph{{ICPR}}. {IEEE}.

\bibitem[{Turc et~al.(2019)Turc, Chang, Lee, and Toutanova}]{turc2019well}
Turc, I.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
\newblock Well-read students learn better: On the importance of pre-training
  compact models.
\newblock \emph{arXiv preprint arXiv:1908.08962}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar et~al.}]{transformer}
Vaswani, A.; Shazeer, N.; Parmar, N.; et~al. 2017.
\newblock Attention is All you Need.
\newblock In \emph{{NIPS}}.

\bibitem[{Voita et~al.(2019)Voita, Talbot, Moiseev et~al.}]{voita2019analyzing}
Voita, E.; Talbot, D.; Moiseev, F.; et~al. 2019.
\newblock Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy
  Lifting, the Rest Can Be Pruned.
\newblock In \emph{{ACL}}.

\bibitem[{Wang et~al.(2019{\natexlab{a}})Wang, Pruksachatkun, Nangia
  et~al.}]{superglue}
Wang, A.; Pruksachatkun, Y.; Nangia, N.; et~al. 2019{\natexlab{a}}.
\newblock SuperGLUE: {A} Stickier Benchmark for General-Purpose Language
  Understanding Systems.
\newblock In \emph{NeurIPS}.

\bibitem[{Wang et~al.(2019{\natexlab{b}})Wang, Singh, Michael et~al.}]{glue}
Wang, A.; Singh, A.; Michael, J.; et~al. 2019{\natexlab{b}}.
\newblock {GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
  Language Understanding.
\newblock In \emph{{ICLR}}.

\bibitem[{Wang and Wolf(2020)}]{wang2020overview}
Wang, A.; and Wolf, T. 2020.
\newblock Overview of the SustaiNLP 2020 Shared Task.
\newblock In \emph{Proceedings of SustaiNLP: Workshop on Simple and Efficient
  Natural Language Processing}.

\bibitem[{Wang et~al.(2022)Wang, Chen, Chen et~al.}]{skipbert}
Wang, J.; Chen, K.; Chen, G.; et~al. 2022.
\newblock SkipBERT: Efficient Inference with Shallow Layer Skipping.
\newblock In \emph{{ACL}}.

\bibitem[{Wang et~al.(2021)Wang, Bao, Huang et~al.}]{minilmv2}
Wang, W.; Bao, H.; Huang, S.; et~al. 2021.
\newblock MiniLMv2: Multi-Head Self-Attention Relation Distillation for
  Compressing Pretrained Transformers.
\newblock In \emph{{ACL-IJCNLP} (Findings)}.

\bibitem[{Wang et~al.(2020{\natexlab{a}})Wang, Wei, Dong et~al.}]{minilm}
Wang, W.; Wei, F.; Dong, L.; et~al. 2020{\natexlab{a}}.
\newblock MiniLM: Deep Self-Attention Distillation for Task-Agnostic
  Compression of Pre-Trained Transformers.
\newblock In \emph{NeurIPS}.

\bibitem[{Wang et~al.(2020{\natexlab{b}})Wang, Wang, Li, and Tu}]{wang2020on}
Wang, Y.; Wang, L.; Li, V. O.~K.; and Tu, Z. 2020{\natexlab{b}}.
\newblock On the Sparsity of Neural Machine Translation Models.
\newblock In \emph{{EMNLP}}.

\bibitem[{Winata et~al.(2019)Winata, Madotto, Shin
  et~al.}]{winata2019effectiveness}
Winata, G.~I.; Madotto, A.; Shin, J.; et~al. 2019.
\newblock On the effectiveness of low-rank matrix factorization for lstm model
  compression.
\newblock \emph{arXiv preprint arXiv:1908.09982}.

\bibitem[{Wu, Wu, and Huang(2021)}]{wu2021one}
Wu, C.; Wu, F.; and Huang, Y. 2021.
\newblock One Teacher is Enough? Pre-trained Language Model Distillation from
  Multiple Teachers.
\newblock In \emph{{ACL-IJCNLP} (Findings)}.

\bibitem[{Xia, Zhong, and Chen(2022)}]{cofipruning}
Xia, M.; Zhong, Z.; and Chen, D. 2022.
\newblock Structured Pruning Learns Compact and Accurate Models.
\newblock In \emph{{ACL}}.

\bibitem[{Xia et~al.(2019)Xia, He, Tan et~al.}]{xia2019tied}
Xia, Y.; He, T.; Tan, X.; et~al. 2019.
\newblock Tied Transformers: Neural Machine Translation with Shared Encoder and
  Decoder.
\newblock In \emph{{AAAI}}.

\bibitem[{Xin et~al.(2020)Xin, Tang, Lee et~al.}]{deebert}
Xin, J.; Tang, R.; Lee, J.; et~al. 2020.
\newblock DeeBERT: Dynamic Early Exiting for Accelerating {BERT} Inference.
\newblock In \emph{{ACL}}.

\bibitem[{Xin et~al.(2021)Xin, Tang, Yu, and Lin}]{xin2021berxit}
Xin, J.; Tang, R.; Yu, Y.; and Lin, J. 2021.
\newblock BERxiT: Early Exiting for {BERT} with Better Fine-Tuning and
  Extension to Regression.
\newblock In \emph{{EACL}}.

\bibitem[{Xu et~al.(2020)Xu, Zhou, Ge et~al.}]{bot}
Xu, C.; Zhou, W.; Ge, T.; et~al. 2020.
\newblock BERT-of-Theseus: Compressing {BERT} by Progressive Module Replacing.
\newblock In \emph{{EMNLP}}.

\bibitem[{Xu et~al.(2021{\natexlab{a}})Xu, Zhou, Ge et~al.}]{xu2021beyond}
Xu, C.; Zhou, W.; Ge, T.; et~al. 2021{\natexlab{a}}.
\newblock Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of
  {BERT} Compression.
\newblock In \emph{{EMNLP}}.

\bibitem[{Xu et~al.(2021{\natexlab{b}})Xu, Zhou, Fu et~al.}]{xu2021survey}
Xu, J.; Zhou, W.; Fu, Z.; et~al. 2021{\natexlab{b}}.
\newblock A Survey on Green Deep Learning.
\newblock \emph{arXiv preprint arXiv:2111.05193}.

\bibitem[{Ye et~al.(2021)Ye, Lin, Huang, and Sun}]{trbert}
Ye, D.; Lin, Y.; Huang, Y.; and Sun, M. 2021.
\newblock {TR-BERT:} Dynamic Token Reduction for Accelerating {BERT} Inference.
\newblock In \emph{{NAACL-HLT}}.

\bibitem[{Yuan and Lin(2006)}]{yuan2006model}
Yuan, M.; and Lin, Y. 2006.
\newblock Model selection and estimation in regression with grouped variables.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}.

\bibitem[{Zadeh et~al.(2020)Zadeh, Edo, Awad et~al.}]{gobo}
Zadeh, A.~H.; Edo, I.; Awad, O.~M.; et~al. 2020.
\newblock {GOBO:} Quantizing Attention-Based {NLP} Models for Low Latency and
  Energy Efficient Inference.
\newblock In \emph{{MICRO}}.

\bibitem[{Zafrir et~al.(2019)Zafrir, Boudoukh, Izsak, and
  Wasserblat}]{zafrir2019q8bert}
Zafrir, O.; Boudoukh, G.; Izsak, P.; and Wasserblat, M. 2019.
\newblock Q8bert: Quantized 8bit bert.
\newblock \emph{arXiv preprint arXiv:1910.06188}.

\bibitem[{Zhang et~al.(2018)Zhang, Ciss{\'{e}}, Dauphin, and
  Lopez{-}Paz}]{mixup}
Zhang, H.; Ciss{\'{e}}, M.; Dauphin, Y.~N.; and Lopez{-}Paz, D. 2018.
\newblock mixup: Beyond Empirical Risk Minimization.
\newblock In \emph{{ICLR}}.

\bibitem[{Zhang and Stadie(2020)}]{zhang2020one}
Zhang, M.~S.; and Stadie, B.~C. 2020.
\newblock One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum
  Evaluation.
\newblock In \emph{{ICLR}}.

\bibitem[{Zhang et~al.(2020)Zhang, Hou, Yin et~al.}]{ternarybert}
Zhang, W.; Hou, L.; Yin, Y.; et~al. 2020.
\newblock TernaryBERT: Distillation-aware Ultra-low Bit {BERT}.
\newblock In \emph{{EMNLP}}.

\bibitem[{Zhang et~al.(2022)Zhang, Zhu, Zhang et~al.}]{pceebert}
Zhang, Z.; Zhu, W.; Zhang, J.; et~al. 2022.
\newblock {PCEE-BERT:} Accelerating {BERT} Inference via Patient and Confident
  Early Exiting.
\newblock In \emph{{NAACL-HLT} (Findings)}.

\bibitem[{Zhou et~al.(2020)Zhou, Xu, Ge et~al.}]{pabee}
Zhou, W.; Xu, C.; Ge, T.; et~al. 2020.
\newblock {BERT} Loses Patience: Fast and Robust Inference with Early Exit.
\newblock In \emph{NeurIPS}.

\bibitem[{Zhou, Xu, and McAuley(2022)}]{zhou2021meta}
Zhou, W.; Xu, C.; and McAuley, J.~J. 2022.
\newblock {BERT} Learns to Teach: Knowledge Distillation with Meta Learning.
\newblock In \emph{{ACL}}.

\bibitem[{Zhu(2021)}]{leebert}
Zhu, W. 2021.
\newblock LeeBERT: Learned Early Exit for {BERT} with cross-level optimization.
\newblock In \emph{{ACL-IJCNLP}}.

\end{thebibliography}
