@inproceedings{transformer,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               others},
  title     = {Attention is All you Need},
  booktitle = {{NIPS}},
  year      = {2017}
}

@inproceedings{elmo,
  author    = {Matthew E. Peters and
               Mark Neumann and
               Mohit Iyyer and
               others},
  title     = {Deep Contextualized Word Representations},
  booktitle = {{NAACL-HLT}},
  year      = {2018}
}

@inproceedings{glue,
  author    = {Alex Wang and
               Amanpreet Singh and
               Julian Michael and
               others},
  title     = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
               Language Understanding},
  booktitle = {{ICLR}},
  year      = {2019}
}

@inproceedings{superglue,
  author    = {Alex Wang and
               Yada Pruksachatkun and
               Nikita Nangia and
               others},
  title     = {SuperGLUE: {A} Stickier Benchmark for General-Purpose Language Understanding
               Systems},
  booktitle = {NeurIPS},
  year      = {2019}
}

@inproceedings{squad,
  author    = {Pranav Rajpurkar and
               Jian Zhang and
               Konstantin Lopyrev and
               Percy Liang},
  title     = {SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  booktitle = {{EMNLP}},
  year      = {2016}
}

@inproceedings{wang2020overview,
  title={Overview of the SustaiNLP 2020 Shared Task},
  author={Wang, Alex and Wolf, Thomas},
  booktitle={Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing},
  year={2020}
}

@inproceedings{gpt3,
  author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               others},
  title     = {Language Models are Few-Shot Learners},
  booktitle = {NeurIPS},
  year      = {2020}
}

@inproceedings{efficientqa,
  author    = {Sewon Min and
               Jordan L. Boyd{-}Graber and
               Chris Alberti and
               others},
  title     = {NeurIPS 2020 EfficientQA Competition: Systems, Analyses and Lessons
               Learned},
  booktitle = {NeurIPS (Competition and Demos)},
  year      = {2020}
}

@article{qiu2020pre,
  title={Pre-trained models for natural language processing: A survey},
  author={Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and others},
  journal={Science China Technological Sciences},
  year={2020},
}

@article{han2021pre,
  title={Pre-trained models: Past, present and future},
  author={Han, Xu and Zhang, Zhengyan and Ding, Ning and others},
  journal={AI Open},
  year={2021},
}

@article{xu2021survey,
  title={A Survey on Green Deep Learning},
  author={Xu, Jingjing and Zhou, Wangchunshu and Fu, Zhiyi and others},
  journal={arXiv preprint arXiv:2111.05193},
  year={2021}
}

@article{liu2021towards,
  title={Towards Efficient NLP: A Standard Evaluation and A Strong Baseline},
  author={Liu, Xiangyang and Sun, Tianxiang and He, Junliang and others},
  journal={arXiv preprint arXiv:2110.07038},
  year={2021}
}

@inproceedings{albert,
  author    = {Zhenzhong Lan and
               Mingda Chen and
               Sebastian Goodman and
               others},
  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language
               Representations},
  booktitle = {{ICLR}},
  year      = {2020}
}

@inproceedings{bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  booktitle = {{NAACL-HLT}},
  year      = {2019}
}

@article{tay2020efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={arXiv preprint arXiv:2009.06732},
  year={2020}
}

@article{lacoste2019quantifying,
  title={Quantifying the carbon emissions of machine learning},
  author={Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
  journal={arXiv preprint arXiv:1910.09700},
  year={2019}
}

@article{henderson2020towards,
  title={Towards the systematic reporting of the energy and carbon footprints of machine learning},
  author={Henderson, Peter and Hu, Jieru and Romoff, Joshua and others},
  journal={Journal of Machine Learning Research},
  year={2020}
}

@inproceedings{xu2021beyond,
  author    = {Canwen Xu and
               Wangchunshu Zhou and
               Tao Ge and
               others},
  title     = {Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of {BERT}
               Compression},
  booktitle = {{EMNLP}},
  year      = {2021}
}

@article{stanton2021does,
  title={Does Knowledge Distillation Really Work?},
  author={Stanton, Samuel and Izmailov, Pavel and Kirichenko, Polina and others},
  journal={arXiv preprint arXiv:2106.05945},
  year={2021}
}

@inproceedings{su2018is,
  author    = {Dong Su and
               Huan Zhang and
               Hongge Chen and
               others},
  title     = {Is Robustness the Cost of Accuracy? - {A} Comprehensive Study on the
               Robustness of 18 Deep Image Classification Models},
  booktitle = {{ECCV}},
  year      = {2018}
}

@article{du2021compressed,
  title={What do Compressed Large Language Models Forget? Robustness Challenges in Model Compression},
  author={Du, Mengnan and Mukherjee, Subhabrata and Cheng, Yu and others},
  journal={arXiv preprint arXiv:2110.08419},
  year={2021}
}

@article{sun2021early,
  title={Early Exiting with Ensemble Internal Classifiers},
  author={Sun, Tianxiang and Zhou, Yunhua and Liu, Xiangyang and others},
  journal={arXiv preprint arXiv:2105.13792},
  year={2021}
}

@inproceedings{pabee,
  author    = {Wangchunshu Zhou and
               Canwen Xu and
               Tao Ge and
               others},
  title     = {{BERT} Loses Patience: Fast and Robust Inference with Early Exit},
  booktitle = {NeurIPS},
  year      = {2020}
}

@inproceedings{li2020train,
  author    = {Zhuohan Li and
               Eric Wallace and
               Sheng Shen and
               others},
  title     = {Train Large, Then Compress: Rethinking Model Size for Efficient Training
               and Inference of Transformers},
  booktitle = {{ICML}},
  year      = {2020}
}

Encoder-Decoder weight sharing

@article{rothe2020leveraging,
  author    = {Sascha Rothe and
               Shashi Narayan and
               Aliaksei Severyn},
  title     = {Leveraging Pre-trained Checkpoints for Sequence Generation Tasks},
  journal  = {{TACL}},
  year      = {2020}
}

@inproceedings{xia2019tied,
  author    = {Yingce Xia and
               Tianyu He and
               Xu Tan and
               others},
  title     = {Tied Transformers: Neural Machine Translation with Shared Encoder
               and Decoder},
  booktitle = {{AAAI}},
  year      = {2019}
}

Full-layer Weight Sharing

@inproceedings{dabre2019recurrent,
  author    = {Raj Dabre and
               Atsushi Fujita},
  title     = {Recurrent Stacking of Layers for Compact Neural Machine Translation
               Models},
  booktitle = {{AAAI}},
  year      = {2019}
}

@inproceedings{dehghani2019universal,
  author    = {Mostafa Dehghani and
               Stephan Gouws and
               Oriol Vinyals and
               others},
  title     = {Universal Transformers},
  booktitle = {{ICLR}},
  year      = {2019}
}

@inproceedings{lan2020albert,
  author    = {Zhenzhong Lan and
               Mingda Chen and
               Sebastian Goodman and
               others},
  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language
               Representations},
  booktitle = {{ICLR}},
  year      = {2020}
}

@article{takase2021lessons,
  title={Lessons on parameter sharing across layers in transformers},
  author={Takase, Sho and Kiyono, Shun},
  journal={arXiv preprint arXiv:2104.06022},
  year={2021}
}

@inproceedings{reid2021subformer,
  author    = {Machel Reid and
               Edison Marrese{-}Taylor and
               Yutaka Matsuo},
  title     = {Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative
               Transformers},
  booktitle = {{EMNLP} (Findings)},
  year      = {2021}
}

Cross-lingual

@inproceedings{duong2015low,
  author    = {Long Duong and
               Trevor Cohn and
               Steven Bird and
               Paul Cook},
  title     = {Low Resource Dependency Parsing: Cross-lingual Parameter Sharing in
               a Neural Network Parser},
  booktitle = {{ACL}},
  year      = {2015}
}

@inproceedings{firat2016multi,
  author    = {Orhan Firat and
               Kyunghyun Cho and
               Yoshua Bengio},
  title     = {Multi-Way, Multilingual Neural Machine Translation with a Shared Attention
               Mechanism},
  booktitle = {{HLT-NAACL}},
  year      = {2016}
}

@inproceedings{upadhyay2016cross,
  author    = {Shyam Upadhyay and
               Manaal Faruqui and
               Chris Dyer and
               Dan Roth},
  title     = {Cross-lingual Models of Word Embeddings: An Empirical Comparison},
  booktitle = {{ACL}},
  year      = {2016}
}

@inproceedings{blackwood2018multilingual,
  author    = {Graeme W. Blackwood and
               Miguel Ballesteros and
               Todd Ward},
  title     = {Multilingual Neural Machine Translation with Task-Specific Attention},
  booktitle = {{COLING}},
  year      = {2018}
}

@inproceedings{xlm,
  author    = {Alexis Conneau and
               Guillaume Lample},
  title     = {Cross-lingual Language Model Pretraining},
  booktitle = {NeurIPS},
  year      = {2019}
}

@inproceedings{xlm-r,
  author    = {Alexis Conneau and
               Kartikay Khandelwal and
               Naman Goyal and
               others},
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  booktitle = {{ACL}},
  year      = {2020}
}

@inproceedings{evaluating2020aaai,
  author    = {Aditya Siddhant and
               Melvin Johnson and
               Henry Tsai and
               others},
  title     = {Evaluating the Cross-Lingual Effectiveness of Massively Multilingual
               Neural Machine Translation},
  booktitle = {{AAAI}},
  year      = {2020}
}


Cross-task

@inproceedings{sogaard2016deep,
  author    = {Anders S{\o}gaard and
               Yoav Goldberg},
  title     = {Deep multi-task learning with low level tasks supervised at lower
               layers},
  booktitle = {{ACL}},
  year      = {2016}
}

@inproceedings{hashimoto2017joint,
  author    = {Kazuma Hashimoto and
               Caiming Xiong and
               others},
  title     = {A Joint Many-Task Model: Growing a Neural Network for Multiple {NLP}
               Tasks},
  booktitle = {{EMNLP}},
  year      = {2017}
}

@inproceedings{mtdnn,
  author    = {Xiaodong Liu and
               Pengcheng He and
               Weizhu Chen and
               Jianfeng Gao},
  title     = {Multi-Task Deep Neural Networks for Natural Language Understanding},
  booktitle = {{ACL}},
  year      = {2019}
}

@article{t5,
  author    = {Colin Raffel and
               Noam Shazeer and
               Adam Roberts and
               others},
  title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
               Transformer},
  journal   = {J. Mach. Learn. Res.},
  year      = {2020}
}

@inproceedings{xu2020unihanlm,
  author    = {Canwen Xu and
               Tao Ge and
               Chenliang Li and
               Furu Wei},
  title     = {UnihanLM: Coarse-to-Fine Chinese-Japanese Language Model Pretraining
               with the Unihan Database},
  booktitle = {{AACL-IJCNLP}},
  year      = {2020}
}

Low-rank Decomposition

@inproceedings{sainath2013low,
  author    = {Tara N. Sainath and
               Brian Kingsbury and
               Vikas Sindhwani and
               others},
  title     = {Low-rank matrix factorization for Deep Neural Network training with
               high-dimensional output targets},
  booktitle = {{ICASSP}},
  year      = {2013}
}

Low-rank Decomposition in NLP

@inproceedings{grachev2017neural,
  author    = {Artem M. Grachev and
               Dmitry I. Ignatov and
               Andrey V. Savchenko},
  title     = {Neural Networks Compression for Language Modeling},
  booktitle = {PReMI},
  year      = {2017}
}

@article{winata2019effectiveness,
  title={On the effectiveness of low-rank matrix factorization for lstm model compression},
  author={Winata, Genta Indra and Madotto, Andrea and Shin, Jamin and others},
  journal={arXiv preprint arXiv:1908.09982},
  year={2019}
}

@inproceedings{ma2019tensorized,
  author    = {Xindian Ma and
               Peng Zhang and
               Shuai Zhang and
               others},
  title     = {A Tensorized Transformer for Language Modeling},
  booktitle = {NeurIPS},
  year      = {2019}
}

@inproceedings{noach2020compressing,
  author    = {Matan Ben Noach and
               Yoav Goldberg},
  title     = {Compressing Pre-trained Language Models by Matrix Decomposition},
  booktitle = {{AACL-IJCNLP}},
  year      = {2020}
}

lan2020albert, reid2021subformer

@inproceedings{chen2021drone,
  title={DRONE: Data-aware Low-rank Compression for Large NLP Models},
  author={Chen, Pei-Hung and Yu, Hsiang-Fu and Dhillon, Inderjit and Hsieh, Cho-Jui},
  booktitle = {NeurIPS},
  year      = {2021}
}

@article{btd,
  author    = {Lieven De Lathauwer},
  title     = {Decompositions of a Higher-Order Tensor in Block Terms - Part {II:}
               Definitions and Uniqueness},
  journal   = {{SIAM} J. Matrix Anal. Appl.},
  year      = {2008}
}

Hybrid

@inproceedings{mao2020ladabert,
  author    = {Yihuan Mao and
               Yujing Wang and
               Chufan Wu and
               others},
  title     = {LadaBERT: Lightweight Adaptation of {BERT} through Hybrid Model Compression},
  booktitle = {{COLING}},
  year      = {2020}
}

Pruning

@article{mishra2021accelerating,
  title={Accelerating sparse deep neural networks},
  author={Mishra, Asit and Latorre, Jorge Albericio and Pool, Jeff and others},
  journal={arXiv preprint arXiv:2104.08378},
  year={2021}
}

@inproceedings{see2016,
  author    = {Abigail See and
               Minh{-}Thang Luong and
               Christopher D. Manning},
  title     = {Compression of Neural Machine Translation Models via Pruning},
  booktitle = {CoNLL},
  year      = {2016}
}

@inproceedings{han2016eie,
  author    = {Song Han and
               Xingyu Liu and
               Huizi Mao and
               others},
  title     = {{EIE:} Efficient Inference Engine on Compressed Deep Neural Network},
  booktitle = {{ISCA}},
  year      = {2016}
}

@inproceedings{han2016deep,
  author    = {Song Han and
               Huizi Mao and
               William J. Dally},
  title     = {Deep Compression: Compressing Deep Neural Network with Pruning, Trained
               Quantization and Huffman Coding},
  booktitle = {{ICLR}},
  year      = {2016}
}

@inproceedings{narang2017exploring,
  author    = {Sharan Narang and
               Greg Diamos and
               Shubho Sengupta and
               Erich Elsen},
  title     = {Exploring Sparsity in Recurrent Neural Networks},
  booktitle = {{ICLR}},
  year      = {2017}
}

@article{narang2017block,
  title={Block-sparse recurrent neural networks},
  author={Narang, Sharan and Undersander, Eric and Diamos, Gregory},
  journal={arXiv preprint arXiv:1711.02782},
  year={2017}
}

@article{yuan2006model,
  title={Model selection and estimation in regression with grouped variables},
  author={Yuan, Ming and Lin, Yi},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year={2006},
  publisher={Wiley Online Library}
}

@inproceedings{lottery,
  author    = {Jonathan Frankle and
               Michael Carbin},
  title     = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  booktitle = {{ICLR}},
  year      = {2019}
}

@inproceedings{gordon2020compressing,
  author    = {Mitchell A. Gordon and
               Kevin Duh and
               Nicholas Andrews},
  title     = {Compressing {BERT:} Studying the Effects of Weight Pruning on Transfer
               Learning},
  booktitle = {RepL4NLP@ACL},
  year      = {2020}
}

@inproceedings{guo2021parameter,
  author    = {Demi Guo and
               Alexander M. Rush and
               Yoon Kim},
  title     = {Parameter-Efficient Transfer Learning with Diff Pruning},
  booktitle = {{ACL-IJCNLP}},
  year      = {2021}
}

@inproceedings{sanh2020movement,
  author    = {Victor Sanh and
               Thomas Wolf and
               Alexander M. Rush},
  title     = {Movement Pruning: Adaptive Sparsity by Fine-Tuning},
  booktitle = {NeurIPS},
  year      = {2020}
}

@inproceedings{prasanna2020when,
  author    = {Sai Prasanna and
               Anna Rogers and
               Anna Rumshisky},
  title     = {When {BERT} Plays the Lottery, All Tickets Are Winning},
  booktitle = {{EMNLP}},
  year      = {2020}
}

@inproceedings{chen2020lottery,
  author    = {Tianlong Chen and
               Jonathan Frankle and
               Shiyu Chang and
               others},
  title     = {The Lottery Ticket Hypothesis for Pre-trained {BERT} Networks},
  booktitle = {NeurIPS},
  year      = {2020}
}

@inproceedings{nakkiran2020deep,
  author    = {Preetum Nakkiran and
               Gal Kaplun and
               Yamini Bansal and
               others},
  title     = {Deep Double Descent: Where Bigger Models and More Data Hurt},
  booktitle = {{ICLR}},
  year      = {2020}
}

@inproceedings{l0,
  author    = {Christos Louizos and
               Max Welling and
               Diederik P. Kingma},
  title     = {Learning Sparse Neural Networks through L{\_}0 Regularization},
  booktitle = {{ICLR}},
  year      = {2018}
}

@inproceedings{obd,
  author    = {Yann LeCun and
               John S. Denker and
               Sara A. Solla},
  title     = {Optimal Brain Damage},
  booktitle = {{NIPS}},
  year      = {1989}
}

@inproceedings{zhang2020one,
  author    = {Matthew Shunshi Zhang and
               Bradly C. Stadie},
  title     = {One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum
               Evaluation},
  booktitle = {{ICLR}},
  year      = {2020}
}

@article{guo2019reweighted,
  title={Reweighted proximal pruning for large-scale language representation},
  author={Guo, Fu-Ming and Liu, Sijia and Mungall, Finlay S and others},
  journal={arXiv preprint arXiv:1909.12486},
  year={2019}
}

@inproceedings{16heads,
  author    = {Paul Michel and
               Omer Levy and
               Graham Neubig},
  title     = {Are Sixteen Heads Really Better than One?},
  booktitle = {NeurIPS},
  year      = {2019}
}

@inproceedings{voita2019analyzing,
  author    = {Elena Voita and
               David Talbot and
               Fedor Moiseev and
               others},
  title     = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy
               Lifting, the Rest Can Be Pruned},
  booktitle = {{ACL}},
  year      = {2019}
}

@inproceedings{drophead,
  author    = {Angela Fan and
               Edouard Grave and
               Armand Joulin},
  title     = {Reducing Transformer Depth on Demand with Structured Dropout},
  booktitle = {{ICLR}},
  year      = {2020}
}


@inproceedings{lagunas2021block,
  author    = {Fran{\c{c}}ois Lagunas and
               Ella Charlaix and
               Victor Sanh and
               Alexander M. Rush},
  title     = {Block Pruning For Faster Transformers},
  booktitle = {{EMNLP}},
  year      = {2021}
}

rejuvenation:

@inproceedings{wang2020on,
  author    = {Yong Wang and
               Longyue Wang and
               Victor O. K. Li and
               Zhaopeng Tu},
  title     = {On the Sparsity of Neural Machine Translation Models},
  booktitle = {{EMNLP}},
  year      = {2020}
}

Quantization

@inproceedings{alom2018effective,
  author    = {Md. Zahangir Alom and
               Adam T. Moody and
               Naoya Maruyama and
               others},
  title     = {Effective Quantization Approaches for Recurrent Neural Networks},
  booktitle = {{IJCNN}},
  year      = {2018}
}

@article{bhandare2019efficient,
  title={Efficient 8-bit quantization of transformer neural machine language translation model},
  author={Bhandare, Aishwarya and Sripathi, Vamsi and Karkada, Deepthi and others},
  journal={arXiv preprint arXiv:1906.00532},
  year={2019}
}

 @inproceedings{prato2020fully,
  author    = {Gabriele Prato and
               Ella Charlaix and
               Mehdi Rezagholizadeh},
  title     = {Fully Quantized Transformer for Machine Translation},
  booktitle = {{EMNLP} (Findings)},
  year      = {2020}
}

@article{zafrir2019q8bert,
  title={Q8bert: Quantized 8bit bert},
  author={Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
  journal={arXiv preprint arXiv:1910.06188},
  year={2019}
}


@inproceedings{qbert,
  author    = {Sheng Shen and
               Zhen Dong and
               Jiayu Ye and
               others},
  title     = {{Q-BERT:} Hessian Based Ultra Low Precision Quantization of {BERT}},
  booktitle = {{AAAI}},
  year      = {2020}
}

@inproceedings{ternarybert,
  author    = {Wei Zhang and
               Lu Hou and
               Yichun Yin and
               others},
  title     = {TernaryBERT: Distillation-aware Ultra-low Bit {BERT}},
  booktitle = {{EMNLP}},
  year      = {2020}
}

@inproceedings{binarybert,
  author    = {Haoli Bai and
               Wei Zhang and
               Lu Hou and
               others},
  title     = {BinaryBERT: Pushing the Limit of {BERT} Quantization},
  booktitle = {{ACL-IJCNLP}},
  year      = {2021}
}

@inproceedings{ibert,
  author    = {Sehoon Kim and
               Amir Gholami and
               Zhewei Yao and
               others},
  title     = {{I-BERT:} Integer-only {BERT} Quantization},
  booktitle = {{ICML}},
  year      = {2021}
}


@inproceedings{jacob2018quantization,
  author    = {Benoit Jacob and
               Skirmantas Kligys and
               Bo Chen and
               others},
  title     = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only
               Inference},
  booktitle = {{CVPR}},
  year      = {2018}
}

@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}

@inproceedings{hawq,
  author    = {Zhen Dong and
               Zhewei Yao and
               Amir Gholami and
               others},
  title     = {{HAWQ:} Hessian AWare Quantization of Neural Networks With Mixed-Precision},
  booktitle = {{ICCV}},
  year      = {2019}
}

@inproceedings{gobo,
  author    = {Ali Hadi Zadeh and
               Isak Edo and
               Omar Mohamed Awad and
               others},
  title     = {{GOBO:} Quantizing Attention-Based {NLP} Models for Low Latency and
               Energy Efficient Inference},
  booktitle = {{MICRO}},
  year      = {2020}
}

KD

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{tang2019distilling,
  title={Distilling task-specific knowledge from bert into simple neural networks},
  author={Tang, Raphael and Lu, Yao and Liu, Linqing and others},
  journal={arXiv preprint arXiv:1903.12136},
  year={2019}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@inproceedings{pkd,
  author    = {Siqi Sun and
               Yu Cheng and
               Zhe Gan and
               Jingjing Liu},
  title     = {Patient Knowledge Distillation for {BERT} Model Compression},
  booktitle = {{EMNLP-IJCNLP}},
  year      = {2019}
}

@article{turc2019well,
  title={Well-read students learn better: On the importance of pre-training compact models},
  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1908.08962},
  year={2019}
}

@inproceedings{aguilar2020knowledge,
  author    = {Gustavo Aguilar and
               Yuan Ling and
               Yu Zhang and
               others},
  title     = {Knowledge Distillation from Internal Representations},
  booktitle = {{AAAI}},
  year      = {2020}
}

@inproceedings{zhou2021meta,
  author    = {Wangchunshu Zhou and
               Canwen Xu and
               Julian J. McAuley},
  title     = {{BERT} Learns to Teach: Knowledge Distillation with Meta Learning},
  booktitle = {{ACL}},
  year      = {2022}
}

@inproceedings{wu2021one,
  author    = {Chuhan Wu and
               Fangzhao Wu and
               Yongfeng Huang},
  title     = {One Teacher is Enough? Pre-trained Language Model Distillation from
               Multiple Teachers},
  booktitle = {{ACL-IJCNLP} (Findings)},
  year      = {2021}
}

@inproceedings{tinybert,
  author    = {Xiaoqi Jiao and
               Yichun Yin and
               Lifeng Shang and
               others},
  title     = {TinyBERT: Distilling {BERT} for Natural Language Understanding},
  booktitle = {{EMNLP} (Findings)},
  year      = {2020}
}

@inproceedings{mobilebert,
  author    = {Zhiqing Sun and
               Hongkun Yu and
               Xiaodan Song and
               others},
  title     = {MobileBERT: a Compact Task-Agnostic {BERT} for Resource-Limited Devices},
  booktitle = {{ACL}},
  year      = {2020}
}

@inproceedings{minilm,
  author    = {Wenhui Wang and
               Furu Wei and
               Li Dong and
               others},
  title     = {MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression
               of Pre-Trained Transformers},
  booktitle = {NeurIPS},
  year      = {2020}
}

@inproceedings{minilmv2,
  author    = {Wenhui Wang and
               Hangbo Bao and
               Shaohan Huang and
               others},
  title     = {MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing
               Pretrained Transformers},
  booktitle = {{ACL-IJCNLP} (Findings)},
  year      = {2021}
}

@inproceedings{bot,
  author    = {Canwen Xu and
               Wangchunshu Zhou and
               Tao Ge and
               others},
  title     = {BERT-of-Theseus: Compressing {BERT} by Progressive Module Replacing},
  booktitle = {{EMNLP}},
  year      = {2020}
}

@inproceedings{mixkd,
  author    = {Kevin J. Liang and
               Weituo Hao and
               Dinghan Shen and
               others},
  title     = {MixKD: Towards Efficient Distillation of Large-scale Language Models},
  booktitle = {{ICLR}},
  year      = {2021}
}

@article{shi2020learning,
  title={Learning from deep model via exploring local targets},
  author={Shi, Wenxian and Song, Yuxuan and Zhou, Hao and others},
  journal={OpenReview preprint},
  year={2020}
}

@article{beck2003mirror,
  title={Mirror descent and nonlinear projected subgradient methods for convex optimization},
  author={Beck, Amir and Teboulle, Marc},
  journal={Operations Research Letters},
  year={2003}
}

@inproceedings{mixup,
  author    = {Hongyi Zhang and
               Moustapha Ciss{\'{e}} and
               Yann N. Dauphin and
               David Lopez{-}Paz},
  title     = {mixup: Beyond Empirical Risk Minimization},
  booktitle = {{ICLR}},
  year      = {2018}
}

@inproceedings{parrot,
  author    = {Emily M. Bender and
               Timnit Gebru and
               Angelina McMillan{-}Major and
               Shmargaret Shmitchell},
  title     = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  booktitle = {FAccT},
  year      = {2021}
}

@article{kim2020fastformers,
  title={Fastformers: Highly efficient transformer models for natural language understanding},
  author={Kim, Young Jin and Awadalla, Hany Hassan},
  journal={arXiv preprint arXiv:2010.13382},
  year={2020}
}

@inproceedings{darts,
  author    = {Hanxiao Liu and
               Karen Simonyan and
               Yiming Yang},
  title     = {{DARTS:} Differentiable Architecture Search},
  booktitle = {{ICLR}},
  year      = {2019}
}

@inproceedings{maml,
  author    = {Chelsea Finn and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  booktitle = {{ICML}},
  series    = {Proceedings of Machine Learning Research},
  year      = {2017}
}

@inproceedings{cofipruning,
  author    = {Mengzhou Xia and
               Zexuan Zhong and
               Danqi Chen},
  title     = {Structured Pruning Learns Compact and Accurate Models},
  booktitle = {{ACL}},
  year      = {2022}
}

@inproceedings{tao2022compression,
  author    = {Chaofan Tao and
               Lu Hou and
               Wei Zhang and
               others},
  title     = {Compression of Generative Pre-trained Language Models via Quantization},
  booktitle = {{ACL}},
  year      = {2022}
}

@inproceedings{liu2022multi,
  author    = {Chang Liu and
               Chongyang Tao and
               Jiazhan Feng and
               Dongyan Zhao},
  title     = {Multi-Granularity Structural Knowledge Distillation for Language Model
               Compression},
  booktitle = {{ACL}},
  year      = {2022}
}

@inproceedings{park2015big,
  author    = {Eunhyeok Park and
               Dongyoung Kim and
               Soobeom Kim and
               others},
  title     = {Big/little deep neural network for ultra low power inference},
  booktitle = {{CODES+ISSS}},
  year      = {2015}
}

@inproceedings{branchynet,
  author    = {Surat Teerapittayanon and
               Bradley McDanel and
               H. T. Kung},
  title     = {BranchyNet: Fast inference via early exiting from deep neural networks},
  booktitle = {{ICPR}},
  publisher = {{IEEE}},
  year      = {2016}
}

@inproceedings{shallowdeep,
  author    = {Yigitcan Kaya and
               Sanghyun Hong and
               Tudor Dumitras},
  title     = {Shallow-Deep Networks: Understanding and Mitigating Network Overthinking},
  booktitle = {{ICML}},
  year      = {2019}
}

@inproceedings{deebert,
  author    = {Ji Xin and
               Raphael Tang and
               Jaejun Lee and
               others},
  title     = {DeeBERT: Dynamic Early Exiting for Accelerating {BERT} Inference},
  booktitle = {{ACL}},
  year      = {2020}
}

@inproceedings{schwartz2020right,
  author    = {Roy Schwartz and
               Gabriel Stanovsky and
               Swabha Swayamdipta and
               others},
  title     = {The Right Tool for the Job: Matching Model and Instance Complexities},
  booktitle = {{ACL}},
  year      = {2020}
}

@inproceedings{fastbert,
  author    = {Weijie Liu and
               Peng Zhou and
               Zhiruo Wang and
               others},
  title     = {FastBERT: a Self-distilling {BERT} with Adaptive Inference Time},
  booktitle = {{ACL}},
  year      = {2020}
}

@article{geng2021romebert,
  title={Romebert: Robust training of multi-exit bert},
  author={Shijie Geng and
               Peng Gao and
               Zuohui Fu and
               Yongfeng Zhang},
  journal={arXiv preprint arXiv:2101.09755},
  year={2021}
}

@inproceedings{leebert,
  author    = {Wei Zhu},
  title     = {LeeBERT: Learned Early Exit for {BERT} with cross-level optimization},
  booktitle = {{ACL-IJCNLP}},
  year      = {2021}
}

@inproceedings{liao2021global,
  author    = {Kaiyuan Liao and
               Yi Zhang and
               Xuancheng Ren and
               others},
  title     = {A Global Past-Future Early Exit Method for Accelerating Inference
               of Pre-trained Language Models},
  booktitle = {{NAACL-HLT}},
  year      = {2021}
}

@inproceedings{pceebert,
  author    = {Zhen Zhang and
               Wei Zhu and
               Jinfan Zhang and
               others},
  title     = {{PCEE-BERT:} Accelerating {BERT} Inference via Patient and Confident
               Early Exiting},
  booktitle = {{NAACL-HLT} (Findings)},
  year      = {2022}
}

@inproceedings{xin2021berxit,
  author    = {Ji Xin and
               Raphael Tang and
               Yaoliang Yu and
               Jimmy Lin},
  title     = {BERxiT: Early Exiting for {BERT} with Better Fine-Tuning and Extension
               to Regression},
  booktitle = {{EACL}},
  year      = {2021}
}

@article{schuster2021consistent,
  title={Consistent accelerated inference via confident adaptive transformers},
  author={Tal Schuster and
               Adam Fisch and
               Tommi S. Jaakkola and
               Regina Barzilay},
  journal={arXiv preprint arXiv:2104.08803},
  year={2021}
}

@inproceedings{snip,
  author    = {Zi Lin and
               Jeremiah Z. Liu and
               Zi Yang and
               others},
  title     = {Pruning Redundant Mappings in Transformer Models via Spectral-Normalized
               Identity Prior},
  booktitle = {{EMNLP} (Findings)},
  year      = {2020}
}

@inproceedings{raptilemeta,
  author    = {Xinge Ma and
               Jin Wang and
               Liang{-}Chih Yu and
               Xuejie Zhang},
  title     = {Knowledge Distillation with Reptile Meta-Learning for Pretrained Language
               Model Compression},
  booktitle = {{COLING}},
  year      = {2022}
}

@inproceedings{huang2022sparse,
  author    = {Shaoyi Huang and
               Dongkuan Xu and
               Ian En{-}Hsu Yen and
               others},
  title     = {Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune
               Paradigm},
  booktitle = {{ACL}},
  year      = {2022}
}

@inproceedings{powerbert,
  author    = {Saurabh Goyal and
               Anamitra Roy Choudhury and
               Saurabh Raje and
               others},
  title     = {PoWER-BERT: Accelerating {BERT} Inference via Progressive Word-vector
               Elimination},
  booktitle = {{ICML}},
  year      = {2020}
}

@inproceedings{dynabert,
  author    = {Lu Hou and
               Zhiqi Huang and
               Lifeng Shang and
               others},
  title     = {DynaBERT: Dynamic {BERT} with Adaptive Width and Depth},
  booktitle = {NeurIPS},
  year      = {2020}
}

@article{tahaei2021kroneckerbert,
  title={Kroneckerbert: Learning kronecker decomposition for pre-trained language models via knowledge distillation},
  author={Marzieh S. Tahaei and
               Ella Charlaix and
               Vahid Partovi Nia and others},
  journal={arXiv preprint arXiv:2109.06243},
  year={2021}
}

@inproceedings{kroneckergpt,
  author    = {Ali Edalati and
               Marzieh S. Tahaei and
               Ahmad Rashid and
               others},
  title     = {Kronecker Decomposition for {GPT} Compression},
  booktitle = {{ACL}},
  year      = {2022}
}

@inproceedings{trbert,
  author    = {Deming Ye and
               Yankai Lin and
               Yufei Huang and
               Maosong Sun},
  title     = {{TR-BERT:} Dynamic Token Reduction for Accelerating {BERT} Inference},
  booktitle = {{NAACL-HLT}},
  year      = {2021}
}

@inproceedings{transkimmer,
  author    = {Yue Guan and
               Zhengyi Li and
               Jingwen Leng and
               others},
  title     = {Transkimmer: Transformer Learns to Layer-wise Skim},
  booktitle = {{ACL}},
  year      = {2022}
}

@inproceedings{lat,
  author    = {Gyuwan Kim and
               Kyunghyun Cho},
  title     = {Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime
               with Search},
  booktitle = {{ACL-IJCNLP}},
  year      = {2021}
}

@inproceedings{ltp,
  author    = {Sehoon Kim and
               Sheng Shen and
               David Thorsley and
               others},
  title     = {Learned Token Pruning for Transformers},
  booktitle = {{KDD}},
  year      = {2022}
}

@article{scao2022bloom,
  title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author={Le Scao, Teven and Fan, Angela and Akiki, Christopher and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@inproceedings{skipbert,
  author    = {Jue Wang and
               Ke Chen and
               Gang Chen and
               others},
  title     = {SkipBERT: Efficient Inference with Shallow Layer Skipping},
  booktitle = {{ACL}},
  year      = {2022}
}
