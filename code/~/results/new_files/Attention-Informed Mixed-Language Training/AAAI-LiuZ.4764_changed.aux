\relax 
\citation{zhong2018global}
\citation{wu2019transferable}
\citation{chen2018xl}
\citation{schuster2019cross}
\citation{conneau2017word}
\citation{lample2019cross}
\citation{devlin2019bert}
\citation{joulin2018loss}
\citation{mrkvsic2017neural}
\citation{mrkvsic2017neural}
\citation{zhong2018global}
\citation{zhong2018global}
\citation{liu2016attention}
\citation{liu2016attention}
\citation{goo2018slot}
\citation{goo2018slot}
\citation{mrkvsic2017semantic}
\citation{schuster2019cross}
\citation{mrkvsic2017semantic}
\citation{mrkvsic2017semantic}
\citation{mrkvsic2017neural}
\citation{schuster2019cross}
\citation{schuster2019cross}
\citation{conneau2017word}
\citation{conneau2017word}
\citation{devlin2019bert}
\citation{lample2019cross}
\citation{devlin2019bert}
\citation{lample2019cross}
\citation{ni2017weakly}
\citation{pan2017cross}
\citation{kim2017cross}
\citation{zhang2016ten}
\citation{chen2018xl}
\citation{upadhyay2018almost}
\citation{liu2019zero}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:MLT}{{1}{2}{Illustration of the mixed-language training (MLT) approach and zero-shot transfer. \textbf  {EN} denotes an English text, \textbf  {IT} denotes an Italian text, and \textbf  {CS} denotes a code-switching text (i.e., a mixed-language sentence). In the training step, code-switching sentence generator will replace the task-related word with its corresponding translation in the target language to generate code-switching sentences. In the zero-shot transfer step, we leverage cross-lingual word embeddings and directly adapt the trained attention model to the target language.}{}{}}
\citation{conneau2017word}
\citation{joulin2018loss}
\citation{chen2018xl}
\citation{chen2018xl}
\newlabel{fig:models}{{2}{3}{Dialogue State Tracking Model \textbf  {(left)} and Natural Language Understanding Model \textbf  {(right)}. For each model, we apply an attention layer to learn important task-related words.}{}{}}
\citation{felbo2017using}
\citation{lample2016neural}
\citation{chen2018xl}
\citation{chen2018xl}
\citation{chen2018xl}
\citation{chen2018xl}
\citation{chen2018xl}
\citation{chen2018xl}
\citation{chen2018xl}
\citation{chen2018xl}
\citation{lample2019cross}
\citation{lample2019cross}
\citation{dehghani2018universal}
\citation{mrkvsic2017semantic}
\citation{mrkvsic2017semantic}
\newlabel{eq5}{{4}{4}{}{}{}}
\newlabel{fig:subword2word}{{3}{4}{Illustration of how we leverage a \textit  {transformer encoder} to incorporate subword embeddings into word-level representations. The parameters in the \textit  {transformer encoder} are shared for all subword embeddings.}{}{}}
\citation{schuster2019cross}
\citation{schuster2019cross}
\citation{schuster2019cross}
\citation{upadhyay2018almost}
\citation{schuster2019cross}
\citation{upadhyay2018almost}
\citation{conneau2017word}
\citation{joulin2018loss}
\citation{lample2019cross}
\citation{devlin2019bert}
\citation{chen2018xl}
\citation{chen2018xl}
\citation{chen2018xl}
\citation{chen2018xl}
\citation{chen2018xl}
\citation{chen2018xl}
\newlabel{table:dst}{{1}{5}{Zero-shot results for the target languages on \textbf  {Multilingual WOZ 2.0}. \textbf  {MLT}$_A$ denotes our approach (attention-informed MLT), which utilizes the same number of word pairs as \textbf  {MLT}$_O$ (90 word pairs). $^\ddagger $ denotes the results of XL-NBT. Note that, we realize that the goal accuracy in \def \def {##}## 1##2{##1}\unhbox \voidb@x \def -1000{1000}\def (\nobreak  \hskip 0in{##})##2{{##1}}\let \reserved@d =[\def \par }{}{}}
\citation{upadhyay2018almost}
\citation{upadhyay2018almost}
\citation{bojanowski2017enriching}
\citation{schuster2019cross}
\citation{schuster2019cross}
\citation{yu2018multilingual}
\citation{schuster2019cross}
\citation{schuster2019cross}
\citation{schuster2019cross}
\citation{schuster2019cross}
\newlabel{table:nlu}{{2}{6}{Results on multilingual NLU dataset\nobreakspace  {}\def 1##2{\def {##1}##1 ##2}\unhbox \voidb@x \def -1000{-1000}\def (\nobreak  \hskip 0in{##})##2{(\nobreak  \hskip 0in{##1})}\let \reserved@d =[\def \par }{}{}}
\bibstyle{aaai}
\bibdata{aaai2020}
\newlabel{fig:intent-es}{{4a}{7}{\relax }{}{}}
\newlabel{sub@fig:intent-es}{{a}{7}{\relax }{}{}}
\newlabel{fig:slot-es}{{4b}{7}{\relax }{}{}}
\newlabel{sub@fig:slot-es}{{b}{7}{\relax }{}{}}
\newlabel{fig:intent-es-unseen}{{4c}{7}{\relax }{}{}}
\newlabel{sub@fig:intent-es-unseen}{{c}{7}{\relax }{}{}}
\newlabel{fig:slot-es-unseen}{{4d}{7}{\relax }{}{}}
\newlabel{sub@fig:slot-es-unseen}{{d}{7}{\relax }{}{}}
\newlabel{fig:dynamics}{{4}{7}{The dynamics of the NLU task: intent and slot-filling results with different numbers of word pairs on Spanish test data using RCSLS. The words are decided according to the frequency in the source language (English) training set. We evaluate on all test data for \textbf  {(a)} and \textbf  {(b)}. For \textbf  {(c)} and \textbf  {(d)}, we only evaluate on filtered test data that do not contain any word pairs.}{}{}}
\newlabel{fig:bar}{{\caption@xref {fig:bar}{ on input line 381}}{7}{}{}{}}
\newlabel{sub@fig:bar}{{}{7}{}{}{}}
\newlabel{fig:attention}{{5}{7}{Attentions on words in both training and testing phases. A darker color shows a higher attention score and importance.}{}{}}
\gdef \@abspage@last{8}
