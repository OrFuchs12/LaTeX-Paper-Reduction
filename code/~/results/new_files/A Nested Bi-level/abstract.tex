Model-Agnostic Meta-Learning (MAML), a popular gradient-based meta-learning framework, assumes that the contribution of each task or instance to the meta-learner is equal. Hence, it fails to address the domain shift between base and novel classes in few-shot learning. In this work, we propose a novel robust meta-learning algorithm,  \sysname{}, which learns to assign weights to training tasks or instances. We consider weights as hyper-parameters and iteratively optimize them using a small set of validation tasks set in a \biopt{} optimization approach (in contrast to the standard bi-level optimization in MAML). We then apply \sysname{} in the meta-training stage, which involves (1) several tasks sampled from a distribution different from the meta-test task distribution, or (2) some data samples with noisy labels. Extensive experiments on synthetic and real-world datasets demonstrate that \sysname{} efficiently mitigates the effects of "unwanted" tasks or instances, leading to significant improvement over the state-of-the-art robust meta-learning methods.

%Model-Agnostic Meta-Learning (MAML) is a popular gradient-based meta-learning framework that tries to find an optimal initialization to minimize the expected loss across all possible tasks during meta-training. However, it inherently assumes that each task's or instance's contribution to the meta-learner is equal for learning. Therefore, it fails to address the problem of different domains between base and novel classes in few-shot learning. In this work, we propose a novel and robust meta-learning algorithm, namely \sysname{}, which learns to assign weights to training tasks or instances. We consider these weights to be hyper-parameters. Hence, we iteratively optimize the weights using a small set of validation tasks in a \emph{bi-bi-level} optimization approach (in contrast to the standard bi-level optimization in MAML). We then apply \sysname{} in two scenarios: in the meta-training stage, which involves (1) several tasks sampled from a distribution that is different from that in the meta-testing stage, or (2) some data samples with noisy labels. Extensive experiments on synthetic and real-world datasets demonstrate that our framework efficiently mitigates the effects of "unwanted" tasks or instances, leading to a significant improvement over the state-of-the-art robust meta-learning methods.

%, and propose an online approximation
%and an online approximation in a \emph{bi-bi-level} optimization framework, in contrast to the standard bi-level optimization in MAML. 

%Therefore, we investigate a practical evaluation setting to demonstrate the scalability of our \sysname{} in two scenarios: (1) out-of-distribution tasks and (2) noisy labels in the meta-training stage.