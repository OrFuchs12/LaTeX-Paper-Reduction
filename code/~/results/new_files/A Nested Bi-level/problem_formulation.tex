\subsection{Notations}
In the setting of meta-learning for few-shot learning, there is a set of meta-training tasks $\{\mathcal{T}_i\}_{i=1}^M$ sampled from the probability distribution 
$p_{tr}(\mathcal{T})$. Each few-shot learning task $\mathcal{T}_i$ has an associated dataset $\mathcal{D}_i$ containing two disjoint sets $\{\mathcal{D}^{S}_{i},\mathcal{D}^{Q}_{i}\}$, where the superscripts $S$ and $Q$ denote support set and query set respectively. The query sets take the form $\mathcal{D}^{Q}_{i} = \{\boldsymbol{x}_i^k, {y}_i^k\}_{k=1}^K$ and similarly for $\mathcal{D}^{S}_{i}$. Meta-validation tasks are denoted in a similar manner: $\{\mathcal{T}^{\mathcal{V}}_{j}= \{\mathcal{V}_{j}^{S}, \mathcal{V}_{j}^{Q}\} \}_{j = 1} ^ {N}$
% We are interested in learning the models of the form $h : \mathcal{X}\mapsto\mathcal{Y}$.  
Let the loss function be denoted as $\mathcal{L}(\boldsymbol{\phi}, \mathcal{D})$ with $\boldsymbol{\phi}$  denoting model parameters and $\mathcal{D}$ denoting the dataset, and $\ell(\boldsymbol{\theta}, d)$ with model parameters $\boldsymbol{\theta}$ on the data-point $d$. For example, $\mathcal{L}(\phi, \mathcal{D}_i^{Q})$ denotes the loss of the $i^{th}$ training task query set $\mathcal{D}_i^{Q}$ for given model parameters $\phi \in \boldsymbol{\Phi}\equiv \mathbb{R}^d$, where $\boldsymbol{\phi}:=\mathcal{A}lg(\boldsymbol{\theta}, \mathcal{D}^S)$ and $\boldsymbol{\theta}\in \boldsymbol{\Theta} \equiv \mathbb{R}^d$ is the meta-parameter. $\mathcal{A}lg(\cdot)$ corresponds to a learning algorithm.
% The functional form of $\mathcal{A}lg(\cdot)$ for a single gradient step is given section 3.2.

% We weigh each datapoint (instance) in the query set of task $\mathcal{T}_i$ with a weight $w_{ik}\in \mathbb{R}$, where $k=1,...,K$ denotes the datapoint index. 
% Instance weights are optimized using a held-out set of meta-validation tasks $\{\mathcal{T}_j ^{\mathcal{V}}\}_{j=1}^N$ drawn from $p_{val}(\mathcal{T})$. Similar to meta-training tasks, each task $\mathcal{T}^{\mathcal{V}}_j$ in the meta-validation set contains a support set $\mathcal{V}_j^S$ and a query set $\mathcal{V}_j^Q$. 
% Let $\ell(\boldsymbol{\theta}, d)$ be the loss on the query data-point $d$ corresponding to model parameters $\boldsymbol{\theta}$. 

For notation convenience, we write $\mathcal{L}_i(\phi) := \mathcal{L}(\phi, \mathcal{D}_i^{Q})$; $\mathcal{L}_{V_j}(\phi):=\mathcal{L}(\phi, \mathcal{V}_j^{Q})$;  $\widehat{\mathcal{L}}_{V_j}(\boldsymbol{\phi}) := \mathcal{L}(\boldsymbol{\phi}, \mathcal{V}_j^{S})$. We denote scalars by lower case italic letters, vectors by lower case boldface letters, and matrices by capital italic letters throughout the paper. A table of notations with corresponding explanations is given in Appendix~\ref{app:notations}.

% For notation convenience we introduce the following terms. Let $\mathcal{L}_i(\phi) := \mathcal{L}(\phi, \mathcal{D}_i^{Q})$ where $\mathcal{L}_i(\phi)$ is the loss of the $i^{th}$ training task query set $\mathcal{D}_i^{Q}$ for given model parameters $\phi$. Let $\mathcal{L}_{V_j}(\phi):=\mathcal{L}(\phi, \mathcal{V}_j^{Q})$ where $\mathcal{L}_{V_j}(\phi)$ is the loss of the $j^{th}$ validation task query set $\mathcal{V}_j^{Q}$ for given model parameters $\phi$. Let $\widehat{\mathcal{L}}_{V_j}(\boldsymbol{\phi}) := \mathcal{L}(\boldsymbol{\phi}, \mathcal{V}_j^{S})$ where $\widehat{\mathcal{L}}_{V_j}(\phi)$ is the loss calculated on the $j^{th}$ validation task support set $\mathcal{V}_j^{S}$ for given model parameters $\phi$.

% The goal for task $\mathcal{T}_i$ is to learn the model-specific parameter $\phi_i$ using support set $\mathcal{D}_{i}^{S}$ such that we can minimize the query set loss $\mathcal{L}(\phi_i, \mathcal{D}_{i}^{Q})$.

\subsection{Model-Agnostic Meta-Learning}
The goal of MAML~\cite{finn2017model} is to obtain the optimal initial parameters that minimize the meta-training objective:
\begin{equation}
\begin{aligned}
    \label{meta-objective}
    \overbrace{\boldsymbol{\theta}^*_{ML}= \argmin_{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\mathcal{F}(\boldsymbol{\theta})}}^{outer-level}\text{\hspace{1.7cm}}\\
    \text{where, }\mathcal{F}(\boldsymbol{\theta}) = \frac{1}{M}\sum\nolimits_{i=1}^{M}\mathcal{L}(\overbrace{\mathcal{A}lg(\boldsymbol{\theta}, \mathcal{D}_{i}^{S})}^{inner-level}, \mathcal{D}_{i}^{Q})\text{\hspace{0.5cm}}
\end{aligned}
\end{equation}
This is a bi-level optimization problem, where we construe that $\mathcal{A}lg(\boldsymbol{\theta}, \mathcal{D}_{i}^{S})$ explicitly or implicitly optimizes the inner-level task-specific adaptation. The outer-level corresponds to the meta-training objective of generalizing well (i.e. low test error) on the query set of each task after adaptation. 
 
Since $\mathcal{A}lg(\boldsymbol{\theta}, \mathcal{D}_{i}^{S})$ corresponds to single or multiple gradient descent steps. In case of a single gradient descent, $\mathcal{A}lg(\boldsymbol{\theta}, \mathcal{D}_{i}^{S})$ can be perceived as follwing:
\begin{equation}
\label{param-adaptation}  \mathcal{A}lg(\boldsymbol{\theta}, \mathcal{D}_{i}^{S}) = \boldsymbol{\theta} - \alpha \nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}, \mathcal{D}_{i}^{S})
\end{equation}
where $\alpha $ is a learning rate.
% hyper-parameter, which is a scalar or can also be a vector ~\citep{li2017meta}. 
% Works like ~\cite{behl2019alpha} show an online learning rate hyper-parameter $\alpha$ adaptation method that removes the need for fine-tuning it. 
As shown above, the meta-training objective assumes equal weights to each task for generalization, which may not be ideal in the case of adversaries in the training tasks set.