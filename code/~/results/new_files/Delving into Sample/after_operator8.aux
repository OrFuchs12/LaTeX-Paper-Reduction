\relax 
\bibstyle{aaai22}
\citation{hu2018squeeze,simonyan2014very,he2016deep}
\citation{5206848,10.1007/978-3-319-10602-1_48}
\citation{zhang2016understanding}
\citation{he2009learning}
\citation{zhang2017range,liu2019large,kang2019decoupling,shu2019meta,tan2020equalization}
\citation{cao2020heteroskedastic}
\providecommand \oddpage@label [2]{}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:costcurve}{{1}{1}{ Average training loss along with variance of clean and noisy samples. Though noisy and clean samples become indistinguishable from transient loss over training, the trend of their loss curves are dramatically different in the long run. Such difference could provide valuable priors to distinguish the two types of biased data and assign proper sample weights accordingly through meta-learning, making it feasible to embrace biased training data with both corrupted labels and class imbalance for model training.}{}{}}
\citation{shu2019meta}
\citation{kumar2010self,pi2016self,hendrycks2018using,ma2018dimensionality}
\citation{shu2019meta}
\citation{kumar2010self,jiang2014easy,jiang2014self}
\citation{bengio2009curriculum}
\citation{pi2016self}
\citation{brooks2011support}
\citation{van2015learning}
\citation{masnadi2008design}
\citation{hendrycks2018using}
\citation{reed2014training}
\citation{han2018co}
\citation{ma2018dimensionality}
\citation{goldberger2016training}
\citation{huang2019o2u}
\citation{huang2016learning,wang2017learning}
\citation{mahajan2018exploring,mikolov2013distributed}
\citation{freund1997decision,lin2017focal,malisiewicz2011ensemble,dong2017class}
\citation{wang2017learning,cui2018large}
\citation{finn2017model,antoniou2018train,li2017meta,shu2018small,ravi2016optimization}
\citation{wu2018learning}
\citation{jiang2018mentornet}
\citation{ren2018learning}
\citation{shu2019meta}
\citation{shu2019meta}
\citation{shu2019meta}
\newlabel{sec:revisit}{{}{3}{}{}{}}
\newlabel{eq:eq1}{{1}{3}{}{}{}}
\newlabel{eq:theta}{{3}{3}{}{}{}}
\newlabel{eq:fl}{{4}{3}{}{}{}}
\newlabel{eq:ug}{{5}{3}{}{}{}}
\newlabel{eq:uf}{{6}{3}{}{}{}}
\newlabel{sec:os}{{}{3}{}{}{}}
\citation{cao2020heteroskedastic}
\citation{Xu2021FaMUS}
\citation{smith2017cyclical}
\citation{huang2019o2u}
\newlabel{fig:framework}{{2}{4}{Overall workflow of the proposed probe-and-allocate training strategy: the probing stage trains a classifier on the entire biased dataset to collect training loss curve for each sample; the allocating stage first re-weights the loss curves by integrating the loss curve and class embedding through a newly designed CurveNet, and then generates parameters of the classifier for different types of biased data through meta-learning. }{}{}}
\newlabel{eq:mean}{{7}{4}{}{}{}}
\newlabel{eq:norm}{{8}{4}{}{}{}}
\newlabel{fig:CurveNet}{{3}{4}{The network architecture of Curvenet, which takes normalized loss curve and class label as input and outputs a proper weight for each sample adaptively. }{}{}}
\newlabel{eq:ug_s}{{9}{4}{}{}{}}
\newlabel{eq:ug_sK}{{10}{4}{}{}{}}
\newlabel{eq:fl_our}{{11}{4}{}{}{}}
\citation{krizhevsky2009learning}
\citation{xiao2015learning}
\citation{lee2018cleannet}
\citation{lin2017focal}
\citation{cui2019class}
\citation{cao2019learning}
\citation{han2018co}
\citation{huang2019o2u}
\citation{reed2014training}
\citation{goldberger2016training}
\citation{li2019learning}
\citation{lee2018cleannet}
\citation{zhang2019metacleaner}
\citation{han2019deep}
\citation{zhang2020distilling}
\citation{shu2019meta}
\citation{cao2020heteroskedastic}
\newlabel{fig:imb_noise_0}{{4}{5}{ The number of all samples (solid line) and noisy samples (shadow below) of each class in CIFAR-100 with varying imbalance factors and noise rates. }{}{}}
\newlabel{eq:uf_our}{{12}{5}{}{}{}}
\newlabel{sec:dataset}{{}{5}{}{}{}}
\newlabel{sec:implementaion}{{}{5}{}{}{}}
\newlabel{sec:vis_cifar10}{{}{5}{}{}{}}
\citation{shu2019meta}
\newlabel{tab:cifar10_res}{{1}{6}{Performance comparisons on CIFAR10 and CIFAR100 with varying noise rates and imbalance factors. The best results are highlighted in \textbf  {bold}.}{}{}}
\newlabel{tab:cifar10_res_d}{{2}{6}{Performance comparisons on CIFAR10 with varying noise rates or imbalance factors. }{}{}}
\newlabel{tab:cifar100_res_d}{{3}{6}{Performance comparisons on CIFAR100 with varying noise rates or imbalance factors. }{}{}}
\newlabel{tab:clothing}{{4}{6}{Performance comparisons on Clothing1M.}{}{}}
\newlabel{tab:food101}{{5}{6}{Performance comparisons on Food-101N.}{}{}}
\newlabel{fig:side:weightinclass}{{5}{7}{ The weight of clean and noisy samples of all classes on CIFAR-10 with imbalance factor 20 and noise rate 0.4.}{}{}}
\newlabel{fig:side:confusion}{{6}{7}{ Confusion matrices for the CE Loss model and our model on CIFAR-100 with imbalance factor 20 and noise rate 0.6.}{}{}}
\newlabel{tab:cifar10_wrn}{{6}{7}{ Test accuracy of our model with WRN-16-8 and DRW on CIFAR-10. }{}{}}
\newlabel{tab:cifar10_size}{{7}{7}{ Test accuracy of our model with different sacles of meta data on CIFAR-10. }{}{}}
\newlabel{tab:cifar10_skiplayer}{{8}{7}{ Test accuracy and training time of our method with different skip layers on CIFAR-10. }{}{}}
\newlabel{tab:cifar10_P}{{9}{7}{Test accuracy of our model with different embedding dimension P on CIFAR-10.}{}{}}
\newlabel{sec:conclusion}{{}{7}{}{}{}}
\bibdata{aaai22}
\newlabel{tab:cifar10_s}{{10}{8}{Test accuracy of our model with different S removed loss values on CIFAR-10.}{}{}}
\gdef \@abspage@last{8}
