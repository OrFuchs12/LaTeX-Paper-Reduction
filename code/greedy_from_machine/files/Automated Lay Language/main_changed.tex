\def\year{2021}\relax
%File: formatting-instructions-latex-2021.tex
%release 2021.1
\PassOptionsToPackage{table}{xcolor}
\documentclass[letterpaper, table]{article} % DO NOT CHANGE THIS
\pdfoutput=1
\usepackage{aaai21}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{multirow}
% \usepackage{hyperref}
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
\usepackage{tabularx}
\usepackage{listings}
% \usepackage[table]{xcolor}
\usepackage{booktabs}
\usepackage[switch]{lineno}
\usepackage[dvipsnames]{xcolor}

\newcommand{\draftonly}[1]{#1}
\newcommand{\draftcomment}[3]{\draftonly{\textcolor{#2}{[#3]{$_{\textsc{\tiny #1}}$}}}}
\newcommand{\todo}[1]{\draftcomment{TODO}{red}{#1}}
\newcommand{\yizhong}[1]{\draftcomment{Yizhong}{blue}{#1}}

%\nocopyright
%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.


% /Title ()
% Put your actual complete title (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case
% Leave the space between \Title and the beginning parenthesis alone
% /Author ()
% Put your actual complete list of authors (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case.
% Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands,
% remove them.

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
%  -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
%  -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai21.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash



\title{Automated Lay Language Summarization of Biomedical Scientific Reviews}

\author {
    % Authors
        Yue Guo,\thanks{These authors contributed equally to this work.}\textsuperscript{\rm 1}
        Wei Qiu, \footnotemark[1]\textsuperscript{\rm 2} Yizhong Wang, \textsuperscript{\rm 2}
        Trevor Cohen\textsuperscript{\rm 1}\\
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1} Biomedical and Health Informatics, University of Washington  \\
    \textsuperscript{\rm 2} Paul G. Allen School of Computer Science, University of Washington\\
    % email address must be in roman text type, not monospace or sans serif
    yguo50@uw.edu, \{wqiu0528, yizhongw\}@cs.washington.edu, cohenta@uw.edu
}

%%% Submissions may consist of up to 7 pages of technical content plus up to two additional pages solely for references, an ethics statement if applicable (see below)
\begin{document}
% \linenumbers
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\maketitle


\begin{abstract}
Health literacy has emerged as a crucial factor in making appropriate health decisions and ensuring treatment outcomes. However, medical jargon and the complex structure of professional language in this domain make health information especially hard to interpret. Thus, there is an urgent unmet need for automated methods to enhance the accessibility of the biomedical literature to the general population. This problem can be framed as a type of translation problem between the language of healthcare professionals, and that of the general public.  In this paper,
we introduce the novel task of automated generation of lay language summaries of biomedical scientific reviews, and construct a dataset to support the development and evaluation of automated methods through which to enhance the accessibility of the biomedical literature.  We conduct analyses of the various challenges in performing this task, including not only summarization of the key points but also explanation of background knowledge and simplification of professional language. We experiment with state-of-the-art summarization models as well as several data augmentation techniques, and evaluate their performance using both automated metrics and human assessment. Results indicate that automatically generated summaries produced using contemporary neural architectures can achieve promising quality and readability as compared with reference summaries developed for the lay public by experts (best ROUGE-L of 50.24 and Flesch-Kincaid readability score of 13.30). We also discuss the limitations of the current effort, providing insights and directions for future work.
\end{abstract}

\section{Introduction}
The ability to understand scientific concepts, content, and research in the medical domain is defined as health literacy \cite{parker1999health}, which is crucial to making appropriate health decisions and ensuring treatment outcomes. The development of the internet has enabled the general population to access health information, greatly expanding the volume of available health education materials. However, challenges arise in reading and understanding these materials because of the inability to identify credible resources \cite{howes2004evidence}, unfamiliarity with medical jargon \cite{korsch1968gaps}, and the complex structure of professional language \cite{friedman2002two}. Furthermore, knowledge in the health domain evolves over time, presenting laypeople with the additional challenge of discerning the most up-to-date information. The COVID-19 pandemic has cast a spotlight on the challenges in the general public's ability to obtain, interpret, and apply knowledge to guide their health-related behavior. These challenges are exemplified by difficulties in interpreting articles from the biomedical literature, for those without specific training in this domain. Our project aims to bridge the gap between the increasing availability of health information and the difficulty the public has understanding it. To do so, we confront the task of rendering the biomedical literature comprehensible to the lay public, which can be framed as a type of translation problem: from the language of healthcare professionals to plain language.

A systematic review, such as those in the widely-used Cochrane Database of Systematic Reviews\footnote{http:///www.cochranelibrary.com} (CDSR), is a type of biomedical scientific publication that synthesizes current empirical evidence for a research question, to identify the strongest evidence to inform decision making while minimizing bias. Of importance for the current research, reviews in the CDSR include lay language summaries, written by review authors or Cochrane staff. In this paper, we introduce the novel task of automated generation of lay language summaries of biomedical scientific reviews. We introduce a dataset constructed by extracting 7,805 high-quality abstract pairs that consist of both abstracts intended for professional readers, and plain language versions written by domain experts. The \textit{source} in the training dataset is the healthcare professional version of an abstract, with an average length of 714 words. The \textit{target} is the corresponding plain language version, with an average length of 371 words.

This dataset is the first corpus developed to facilitate development of methods to improve document-level understandability of biomedical narratives for the general public, and covering a broad range healthcare topics. The closest parallel in the literature may be the manually annotated dataset, MSD, which was developed recently to improve bi-directional communication between clinicians and patients \cite{cao2020expertise}. MSD is focused on communication of clinical topics at the sentence level, and the accompanying work approached the problem using text style transfer and simplification algorithms. However, constraining the task to sentence level prohibits methods from considering the broader context in which a sentence occurs.
While the text summarization community has developed various corpora
\cite{allahyari2017text} for document-level summarization tasks, current resources for the biomedical scientific domain are limited \cite{Moradi2019TextSI}. Furthermore, the proposed plain language summarization task imposes additional challenges, such as terminology explanation and sentence structure simplification, that are not required for general domain summarization (see Table \ref{transformation_case_study} for an analysis of five additional task components).
One important goal of our work is to meet the need for a dataset to support research into the task of generating summaries of biomedical  professional literature that are comprehensible to the lay public.


To approach this task, we implemented several state-of-the art extractive and abstractive summarization models and evaluated them on the collected CDSR dataset. On account of the limited size of this dataset, we also applied intermediate pre-training (both out-of-domain on-task, and in-domain off-task) to the best abstractive model. We evaluated the utility of pre-training this model on CNN/DM \cite{nallapati2016abstractive}, a much larger general domain summarization dataset. To provide the model with more domain-specific biomedical language, we pre-trained it on an unlabeled biomedical corpus of 300K abstracts from the PubMed database.

Standard automated metrics of summarization and readability were adopted to evaluate model performance. In addition, we used ratings of human evaluators to assess the generated summaries from several perspectives. The results suggest that the best-performing model can generate lay language summaries with promising quality and readability.

Our main contributions can be summarized as follows:
\begin{itemize}
  \item We introduce the novel task of automated generation of lay language summaries of biomedical scientific reviews.
  \item We construct a dataset of 7,805 summaries and a qualitative analysis of the NLP challenges inherent in this task.
  \item We evaluate performance of state-of-the-art summarization models leveraging neural machine translation architectures, with and without data augmentation techniques, on this task.
  \item We conduct automated and human evaluation from multiple perspectives, showing that machine generated summaries can achieve promising quality and readability as compared with reference summaries developed for the lay public by domain experts.\footnote{We release our code at \url{https://github.com/qiuweipku/Plain_language_summarization}}
\end{itemize}

% \begin{table*}[t!]
% \centering
% \small
% %\begin{tabular}{m{1.5cm}<{\centering}m{4.5cm}<{\centering}m{4.5cm}<{\centering}m{4.5cm}<{\centering}}
% \begin{tabular}{@{}m{1.40cm}@{\hspace{0.15cm}} m{3.95cm}@{\hspace{0.12cm}} m{3.95cm}@{\hspace{0.12cm}} m{3.95cm}@{\hspace{0.12cm}} m{3.95cm} @{}}
% \toprule
% \textbf{Category}  & \textbf{Source}   & \textbf{Target}  & \textbf{BART+CNN/DM+PubMed} & \textbf{BART+PubMed}  \\ \midrule
% Removing Unnecessary Details
% & \ldots \textbf{A complete case analysis (i.e. participants who completed the study) among trials investigating CDAD (31 trials, 8672 participants)} suggests that  \ldots \cite{goldenberg2017probiotics}
% & \ldots Our results suggest that when  \ldots
% & \ldots We found that probiotics  \ldots
% & \ldots \textbf{A complete case analysis (i.e. participants who completed the study) among trials investigating CDAD (31 trials, 8672 participants)} suggests that  \ldots
% \\ \midrule
% Relevant Background Explanation
% %& \ldots \textcolor{purple}{Cervical length measured by TVU is a relatively new screening test, and has been associated with better prediction of} \textcolor{teal}{PTB} than previously available tests. \ldots \cite{berghella2019cervical}
% %& \ldots \textcolor{purple}{During pregnancy, a short cervical length is associated with a risk} \textcolor{teal}{of spontaneous preterm birth}. The shorter the cervical length, the greater the risk. Therefore, measuring cervical length by ultrasound can help predict spontaneous preterm birth. \ldots
% %& \ldots \textcolor{teal}{Preterm birth (PTB) is the premature rupture of the membranes (PROM) of the placenta before 34 weeks of pregnancy. It is a major cause of neonatal morbidity and mortality worldwide. Preterm labour is characterised by contractions of the cervix (cervix) before 37 weeks' gestation, and is associated with an increased risk of preterm births.} The cervix is the area between the vagina and the perineum, which is usually measured by ultrasound.  \ldots
% %& \ldots \textcolor{teal}{Preterm birth (PTB) is the premature rupture of membranes (PROM) of the membranes (the membranes that line the cervix) before 37 weeks of pregnancy. It is a major cause of death and illness for women and their babies. Preterm labour is the onset of labour before 34 weeks' gestation, and can be lifethreatening for the mother and her baby.} \textcolor{purple}{Cervical length, measured by ultrasound, has been associated with better prediction of PTB than previously available tests}. \ldots
% & Hepatitis C is a major cause of liverrelated morbidity and mortality.  \ldots \cite{brok2010ribavirin}
% & \textbf{Globally about 170 million people are chronically infected with hepatitis C virus.} \ldots
% & Hepatitis C is a major cause of liverrelated morbidity and mortality.  \ldots
% & Hepatitis C is a major cause of liverrelated morbidity and mortality.  \ldots
% \\ \midrule
% Jargon Explanation
% & \ldots We considered all \textbf{randomised controlled trials (RCTs)} comparing EVLA, \ldots \cite{paravastu2016endovenous}
% & \ldots We found three \textbf{randomised controlled trials (clinical studies where people are randomly put into one of two or more treatment groups)} that \ldots
% & \ldots We found three \textbf{randomised controlled trials (clinical studies where people are randomly put into one of two or more treatment groups)} that \ldots
% & \ldots We found three \textbf{randomised controlled trials (clinical studies where people are randomly put into one of two or more treatment groups)} that \ldots
%  \\ \midrule
% Converting to Interrogative Sentence
% & \ldots Non-resection versus resection for an asymptomatic primary tumour in patients with unresectable stage IV colorectal cancer \ldots \cite{cirocchi2012non}
% & \ldots Should the primary cancer be surgically removed in asymptomatic patients with unresectable stage IV colorectal cancer? \ldots
% & \ldots Primary tumour resection versus no resection in asymptomatic patients with unresectable stage IV colorectal cancer \ldots
% & Primary tumour resection in asymptomatic patients with unresectable stage IV colorectal cancer who are treated with palliative chemo/radiotherapy \ldots
% \\ \midrule
% Sentence Structure Simplification
% & \ldots Abnormal blood flow patterns in fetal circulation \textbf{detected by} Doppler ultrasound may \textbf{indicate} \ldots \cite{alfirevic2017fetal}
% & \ldots Doppler ultrasound \textbf{detects} changes in the pattern of blood flow through the baby's circulation. \textbf{These changes may identify} \ldots
% & \ldots Abnormal blood flow patterns in the blood vessels of the unborn baby may \textbf{indicate}  \ldots
% & \ldots The aim of the review was to find out if using Doppler ultrasonography ( ultrasound of the baby's heart and blood vessels) \ldots \\
% \bottomrule
% \end{tabular}
% \caption{Typical transformation phenomena from \textit{source} to \textit{target}, and the corresponding generated summaries using two best-performing models. We only show part of the long text here for brevity.}
% \label{transformation_case_study}

% \end{table*}


\begin{table*}[t!]
\centering
\tiny
%\begin{tabular}{m{1.5cm}<{\centering}m{4.5cm}<{\centering}m{4.5cm}<{\centering}m{4.5cm}<{\centering}}
\begin{tabular}{@{}m{1.4cm}<{\centering}m{3.55cm}<{\centering}m{3.55cm}<{\centering}m{3.55cm}<{\centering}m{3.55cm}<{\centering} @{}}
\toprule
\textbf{Category}  & \textbf{Source}   & \textbf{Target}  & \textbf{BART+CNN/DM+PubMed} & \textbf{BART+PubMed}  \\ \midrule
Removing Unnecessary Details
& \ldots \textcolor{magenta}{A complete case analysis (i.e. participants who completed the study) among trials investigating CDAD (31 trials, 8672 participants)} suggests that probiotics reduce the risk of CDAD by 60\%. \textcolor{magenta}{The incidence of CDAD was 1.5\% (70/4525) in the probiotic group compared to 4.0\% (164/4147) in the placebo or no treatment control group (RR 0.40, 95\% CI 0.30 to 0.52; GRADE = moderate)}. \ldots \cite{goldenberg2017probiotics}
& \ldots Our results suggest that when probiotics are given with antibiotics the risk of developing CDAD is reduced by 60\% on average. \ldots
& \ldots We found that probiotics reduce the risk of developing CDAD by 60\% \textcolor{magenta}{(1.5\% (70/4525) versus 4.0\% (164/4147) in the placebo or no treatment control group; GRADE = moderate)}. \ldots
& \ldots \textcolor{magenta}{A complete case analysis (i.e. participants who completed the study) among trials investigating CDAD (31 trials, 8672 participants)} suggests that probiotics reduce the risk of developing CDAD by 60\%. \textcolor{magenta}{A total of 1.5\% (70/4525) of participants in the probiotic group developed CDAD compared to 4.0\% (164/4147) in the placebo or no treatment control group (moderate quality evidence)}. \ldots
\\ \midrule
Relevant Background Explanation
%& \ldots \textcolor{purple}{Cervical length measured by TVU is a relatively new screening test, and has been associated with better prediction of} \textcolor{teal}{PTB} than previously available tests. \ldots \cite{berghella2019cervical}
%& \ldots \textcolor{purple}{During pregnancy, a short cervical length is associated with a risk} \textcolor{teal}{of spontaneous preterm birth}. The shorter the cervical length, the greater the risk. Therefore, measuring cervical length by ultrasound can help predict spontaneous preterm birth. \ldots
%& \ldots \textcolor{teal}{Preterm birth (PTB) is the premature rupture of the membranes (PROM) of the placenta before 34 weeks of pregnancy. It is a major cause of neonatal morbidity and mortality worldwide. Preterm labour is characterised by contractions of the cervix (cervix) before 37 weeks' gestation, and is associated with an increased risk of preterm births.} The cervix is the area between the vagina and the perineum, which is usually measured by ultrasound.  \ldots
%& \ldots \textcolor{teal}{Preterm birth (PTB) is the premature rupture of membranes (PROM) of the membranes (the membranes that line the cervix) before 37 weeks of pregnancy. It is a major cause of death and illness for women and their babies. Preterm labour is the onset of labour before 34 weeks' gestation, and can be lifethreatening for the mother and her baby.} \textcolor{purple}{Cervical length, measured by ultrasound, has been associated with better prediction of PTB than previously available tests}. \ldots
& Hepatitis C is a major cause of liverrelated morbidity and mortality. Standard therapy is ribavirin plus pegylated interferon to achieve undetectable level of virus in the blood, but the effect on clinical outcomes is controversial. \ldots \cite{brok2010ribavirin}
& \textcolor{teal}{Globally about 170 million people are chronically infected with hepatitis C virus. Hepatitis C is a bloodborne virus and routes of transmission include intravenous drug use, mothertoinfant transmission, unsafe medical practices, highrisk sexual behavior, and blood transfusion. Chronic hepatitis C is in most patients a benign viral infection, but a minority of patients develop liver cirrhosis and may suffer from complications due to cirrhosis or die. It is known that treatment with the drug interferon clears hepatitis C virus from the blood in about 15\% of patients}. \ldots
& Hepatitis C is a major cause of liverrelated morbidity and mortality. Standard therapy is ribavirin plus pegylated interferons to achieve undetectable level of virus in the blood, but the effect on clinical outcomes is controversial. \ldots
& Hepatitis C is a major cause of liverrelated morbidity and mortality. Standard therapy is ribavirin plus pegylated interferons to achieve undetectable level of virus in the blood, but the effect on clinical outcomes is controversial. \ldots
\\ \midrule
Jargon Explanation
& \ldots We considered all \textcolor{violet}{randomised controlled trials (RCTs)} comparing EVLA, endovenous RFA or UGFS with conventional surgery in the treatment of SSV varices for inclusion. \ldots \cite{paravastu2016endovenous}
& \ldots We found three \textcolor{violet}{randomised controlled trials (clinical studies where people are randomly put into one of two or more treatment groups)} that compared endovenous lasers (EVLA) with surgery.  \ldots
& \ldots We found three \textcolor{violet}{randomised controlled trials (clinical studies where people are randomly put into one of two or more treatment groups)} that compared endovenous lasers (EVLA) with surgery. \ldots
& \ldots We found three \textcolor{violet}{randomised controlled trials (clinical studies where people are randomly put into one of two or more treatment groups)} that compared endvenous laser Ablation with surgery. \ldots
 \\ \midrule
Converting to Interrogative Sentence
& \ldots Non-resection versus resection for an asymptomatic primary tumour in patients with unresectable stage IV colorectal cancer \ldots \cite{cirocchi2012non}
& \ldots Should the primary cancer be surgically removed in asymptomatic patients with unresectable stage IV colorectal cancer? \ldots
& \ldots Primary tumour resection versus no resection in asymptomatic patients with unresectable stage IV colorectal cancer who are treated with palliative chemo/radiotherapy
& Primary tumour resection in asymptomatic patients with unresectable stage IV colorectal cancer who are treated with palliative chemo/radiotherapy \ldots
\\ \midrule
Sentence Structure Simplification
& \ldots Abnormal blood flow patterns in fetal circulation \textcolor{orange}{detected by} Doppler ultrasound may \textcolor{cyan}{indicate poor fetal prognosis}. \ldots \cite{alfirevic2017fetal}
& \ldots Doppler ultrasound \textcolor{orange}{detects} changes in the pattern of blood flow through the baby's circulation. \textcolor{cyan}{These changes may identify babies who have problems.} \ldots
& \ldots Abnormal blood flow patterns in the blood vessels of the unborn baby may \textcolor{cyan}{indicate poor outcomes for the baby.}  \ldots
& \ldots The aim of the review was to find out if using Doppler ultrasonography ( ultrasound of the baby's heart and blood vessels) during pregnancy can improve outcomes for babies. \ldots \\
\bottomrule
\end{tabular}
\caption{Typical transformation phenomena from \textit{source} to \textit{target}, and the corresponding generated summaries using two best-performing models. We only show part of the long text here for brevity.}
\label{transformation_case_study}

\end{table*}

% \begin{table*}[t!]
% \centering
% \small
% %\begin{tabular}{m{1.5cm}<{\centering}m{4.5cm}<{\centering}m{4.5cm}<{\centering}m{4.5cm}<{\centering}}
% \begin{tabular}{@{}m{1.4cm}<{\centering}m{3.55cm}<{\centering}m{3.55cm}<{\centering}m{3.55cm}<{\centering}m{3.55cm}<{\centering} @{}}
% \toprule
% \textbf{Category}  & \textbf{Source}   & \textbf{Target}  & \textbf{BART+CNN/DM+PubMed} & \textbf{BART+PubMed}  \\ \midrule
% Removing Unnecessary Details
% & \ldots \textbf{A complete case analysis (i.e. participants who completed the study) among trials investigating CDAD (31 trials, 8672 participants)} suggests that  \ldots \cite{goldenberg2017probiotics}
% & \ldots Our results suggest that when  \ldots
% & \ldots We found that probiotics  \ldots
% & \ldots \textbf{A complete case analysis (i.e. participants who completed the study) among trials investigating CDAD (31 trials, 8672 participants)} suggests that  \ldots
% \\ \midrule
% Relevant Background Explanation
% %& \ldots \textcolor{purple}{Cervical length measured by TVU is a relatively new screening test, and has been associated with better prediction of} \textcolor{teal}{PTB} than previously available tests. \ldots \cite{berghella2019cervical}
% %& \ldots \textcolor{purple}{During pregnancy, a short cervical length is associated with a risk} \textcolor{teal}{of spontaneous preterm birth}. The shorter the cervical length, the greater the risk. Therefore, measuring cervical length by ultrasound can help predict spontaneous preterm birth. \ldots
% %& \ldots \textcolor{teal}{Preterm birth (PTB) is the premature rupture of the membranes (PROM) of the placenta before 34 weeks of pregnancy. It is a major cause of neonatal morbidity and mortality worldwide. Preterm labour is characterised by contractions of the cervix (cervix) before 37 weeks' gestation, and is associated with an increased risk of preterm births.} The cervix is the area between the vagina and the perineum, which is usually measured by ultrasound.  \ldots
% %& \ldots \textcolor{teal}{Preterm birth (PTB) is the premature rupture of membranes (PROM) of the membranes (the membranes that line the cervix) before 37 weeks of pregnancy. It is a major cause of death and illness for women and their babies. Preterm labour is the onset of labour before 34 weeks' gestation, and can be lifethreatening for the mother and her baby.} \textcolor{purple}{Cervical length, measured by ultrasound, has been associated with better prediction of PTB than previously available tests}. \ldots
% & Hepatitis C is a major cause of liverrelated morbidity and mortality.  \ldots \cite{brok2010ribavirin}
% & \textbf{Globally about 170 million people are chronically infected with hepatitis C virus.} \ldots
% & Hepatitis C is a major cause of liverrelated morbidity and mortality.  \ldots
% & Hepatitis C is a major cause of liverrelated morbidity and mortality.  \ldots
% \\ \midrule
% Jargon Explanation
% & \ldots We considered all \textbf{randomised controlled trials (RCTs)} comparing EVLA, \ldots \cite{paravastu2016endovenous}
% & \ldots We found three \textbf{randomised controlled trials (clinical studies where people are randomly put into one of two or more treatment groups)} that \ldots
% & \ldots We found three \textbf{randomised controlled trials (clinical studies where people are randomly put into one of two or more treatment groups)} that \ldots
% & \ldots We found three \textbf{randomised controlled trials (clinical studies where people are randomly put into one of two or more treatment groups)} that \ldots
%  \\ \midrule
% Converting to Interrogative Sentence
% & \ldots Non-resection versus resection for an asymptomatic primary tumour in patients with unresectable stage IV colorectal cancer \ldots \cite{cirocchi2012non}
% & \ldots Should the primary cancer be surgically removed in asymptomatic patients with unresectable stage IV colorectal cancer? \ldots
% & \ldots Primary tumour resection versus no resection in asymptomatic patients with unresectable stage IV colorectal cancer \ldots
% & Primary tumour resection in asymptomatic patients with unresectable stage IV colorectal cancer who are treated with palliative chemo/radiotherapy \ldots
% \\ \midrule
% Sentence Structure Simplification
% & \ldots Abnormal blood flow patterns in fetal circulation \textbf{detected by} Doppler ultrasound may \textbf{indicate} \ldots \cite{alfirevic2017fetal}
% & \ldots Doppler ultrasound \textbf{detects} changes in the pattern of blood flow through the baby's circulation. \textbf{These changes may identify} \ldots
% & \ldots Abnormal blood flow patterns in the blood vessels of the unborn baby may \textbf{indicate}  \ldots
% & \ldots The aim of the review was to find out if using Doppler ultrasonography ( ultrasound of the baby's heart and blood vessels) \ldots \\
% \bottomrule
% \end{tabular}
% \caption{Typical transformation phenomena from \textit{source} to \textit{target}, and the corresponding generated summaries using two best-performing models. We only show part of the long text here for brevity.}
% \label{transformation_case_study}

% \end{table*}

%Even though individuals with a higher level of general literacy are able to apply their skills in a new and unfamiliar context, medical jargon makes health information especially difficult to interpret  general literacy does not imply health literacy. Recent studies show that individuals with low health literacy were significantly more likely to report difficulty in self-reported access to care, have low rates of health insurance coverage, and result in higher mortality, compared to those individuals with adequate health literacy. The challenge of COVID19 has cast a spotlight on the challenges in the ability of the general public to obtain, interpret and apply information to guide their health-related behavior. One such challenge involves the difficulty in interpreting biomedical articles, for those without specific training in this domain. Our project aims to bridge the gap between the increasing availability of health information, and the difficulty the public has understanding it. Therefore, we frame the question as a combination problem of translation and summarizaiton between the language of health experts, and that of the general public.



%Although numerous interventions that aimed to improve the health literacy have been designed and applied, such as clear health education materials, patient-center communications techniques, reinforcement, etc \cite{sudore2009interventions}, lack of consensus on the evaluation matrix for health literacy materials making it even difficult to assess the quality and effectiveness of each component. Also, the plain language and clear education materials are usually written by health practitioners or health professionals, which lead to restricted volume and variety of the topics due to the large effort to do this. A cost-effective automated digital solution should be invented to replace the human labor.

%There is ongoing concern that the quality of generated plain language materials used for improving health literacy is not good as human ones. The corpus in medical domain are available for question answering on electronic records \cite{pampari2018emrqa}, medical entity recognition \cite{neveol2014quaero}, and drug-related adverse event extraction \cite{gurulingappa2012development}. A manually annotated dataset MSD was developed recently to improve bi-directional communication between clinicians and patients \cite{cao2020expertise}, which it concentrates on the medical topics in sentence level, and solves the issue by applying text style transfer and text simplification algorithms. However, the sentence level simplification unable to capture the complex strategies in plain language paragraph or documentary transformation.

%I'm not sure what the previous sentence is intending to say, and may be paraphrasing it incorrectly. I don't see how the next sentence follows from it (why would we introduce this task because the reviews contain a search date?).

% While the CNN/DM dataset \cite{nallapati2016abstractive} does provide a document-level training and evaluation set for summarization tasks, it is derived from news articles. There are fundamental differences between the content of this general domain text and that of biomedical scientific documents. These differences impose additional component tasks, such as medical terminology explanation and style transfer, that are not required for general domain summarization. Therefore, one goal of our work is to meet the urgent need for a dataset to support research into the the task of generating biomedical summaries of professional literature that are comprehensible to the lay public.

% Move this sentence to the last paragraph, since it discusses the characteristics of our task, and this paragraph will mainly focus on our models.
% We further analyzed this task's challenges, including removing unnecessary details, relevant background explanations, jargon explanation, converting to interrogative sentences and sentence structure simplification.

\section{Related Work}
\subsection{Scientific Document Summarization}
Automated summarization of scientific documents has been a long-standing research topic \cite{Paice1980TheAG,teufel2002summarization}. Advances have been achieved through the development of high-quality datasets and evaluation tasks, including but not limited to abstract generation of academic papers \cite{Cohan2018ADA}, citation sentence generation \cite{Luu2020CitationTG} or extreme
% is ``extreme summarization'' a term the authors defined? It sounds like a sporting event. Yes.
summarization of the entire document \cite{Cachola2020TLDRES}. In the medical field, \citet{Sarkar2011UsingML} explored extractive summarization of medical news articles.
The Text Analysis Conference (TAC) 2014 Biomedical Summarization track introduced several subtasks to evaluate citation-based summaries.
Our proposed task differs from this prior work in two ways: 1) our target summaries are in plain language, so the overall task requires other capabilities beyond summarization;
2) the target summaries in our dataset are considerably longer (see Table \ref{data_description}), which poses further challenges.

Existing summarization methods can be broadly categorized into extractive and abstractive approaches \cite{Andr2007ASO}. The former \cite{Erkan2004LexRankGC, Cheng2016NeuralSB} select sentences from the original text, while the latter \cite{rush2015abstractive, nallapati2016abstractive} can generate summaries using words that are not found in the original document. Our task is abstractive by nature. For scientific document summarization, various features (e.g. citation networks) can be used to improve performance. While augmentation of this sort is beyond the scope of the current work, we refer the interested reader to \citet{Altmami2020AutomaticSO} and \citet{ Moradi2019TextSI} for a  comprehensive overview of the relevant approaches.
\subsection{Text Simplification}
% Automatic text simplification is the process of transferring difficult phrases into common text and replacing complicated sentences into shorter ones, which convey the same message but easier to understand.
Text simplification \cite{Shardlow2014ASO} modifies the content or structure of a text to make it easier to understand. Unlike summarization, text simplification approximates the meaning of the original sentence without necessarily shortening it.
Simplification techniques have been used in the biomedical domain for generating patient-centered radiology reports \cite{qenam2017text}, name entity recognition \cite{habibi2017deep}, preprocessing for biomedical interaction recognition \cite{baumgartner2008concept}, syntactic parsing \cite{jonnalagadda2010towards}, and simplification of medical journal text \cite{jonnalagadda2010towards}. For lexical simplification, WordNet \cite{miller1995wordnet}, the UMLS \cite{bodenreider2004unified} and Wiktionary \cite{zesch2008extracting} are widely used as synonym resources to find and replace medical terms. Toward the goal of producing summaries of abstracts that are understandable to a lay audience, our task requires simplification at the document rather than the single sentence level, and combines this with summarization to achieve both shorter and more readily
understandable summaries.
\subsection{Pre-training and Transfer Learning}
Large pre-trained neural networks have led to recent advances in performance across a broad range of NLP tasks \cite{peters2018elmo, Radford2018ImprovingLU,Devlin2019BERTPO} - especially when the available labeled data are limited \cite{brown2020language}, or there is a shift in domains involved \cite{Hendrycks2020PretrainedTI}. Due to the complexity of our task and the relatively small size of our data, we posit that pre-training is a prerequisite to strong performance. Furthermore, recent work shows that adaptive pre-training with domain-relevant unlabeled data \cite{Gururangan2020DontSP} or task-relevant labeled data \cite{Pruksachatkun2020IntermediateTaskTL} can further improve the performance of pre-trained models. The intermediate pre-training strategies we evaluated in our experiments are inspired by these findings, and we encourage efforts in finding additional useful data to further improve performance on this task.
\subsection{Biomedical Domain Summarization}
The most common document types used for summarization tasks in the biomedical domain are clinical notes, with the aim to reduce information overload for health practitioners \cite{pivovarov2015automated, feblowitz2011summarization, molla2011development}. This aim differs from our main objective: generating lay language summaries of biomedical scientific reviews for health consumers. Another related area of work concerns information retrieval from the internet, where the goal is to help consumers find (rather than interpret) health information \cite{goeuriot2020overview}.

% Consequently, we evaluated the effects on performance of an intermediate pre-training step using both task-relevant general domain labeled data (CNN/DM) and domain-specific unlabeled data (PubMed), both of which have proved effective in other NLP tasks \cite{Gururangan2020DontSP, Pruksachatkun2020IntermediateTaskTL}.

% However, general terms that describe medical phenomena are often vague and difficult to understand. Although medical concepts can be detected and replaced by the synonym resources, each of the resources was created from different original documentary, e.g. Consumer Health Dictionary is terms that are frequently used in MedlinePlus, which will lead to missing terms. Another issue with lexical simplification is abbreviations.

%This looks as though it belongs in the methods (as you are talking about what you did, rather than about the problem domain in general).



\section{Dataset Description}
\subsection{The CDSR}
The Cochrane Database of Systematic Reviews (CDSR) includes high-quality systematic reviews \cite{uman2011systematic} in various health care domains that facilitate evidence-based medical decision making. For a systematic review, two independent reviewers will review eligible peer-reviewed papers, registered clinical trials, conference papers, or ``grey literature''\footnote{This can be loosely defined as literature that is disseminated outside the usual publishing channels.}; search for evidence on a clearly formulated question; extract data from the studies; and grade the quality of available data. According to the hierarchy of scientific evidence \cite{murad2016new}, systematic review is the most robust evidence supporting an argument. Of particular importance for the current work, a plain language version of each abstract accompanies its professional language counterpart. Of note, plain language summaries have been required from authors submitting a review since 2015. Prior to this, they were written by Cochrane staff with specialized training.
\subsection{Dataset construction}
We extracted 7,805 abstracts (\textit{source}), paired with their plain language versions (\textit{target}) from CDSR reviews available up to March 29, 2020. The original data is downloadable via the official API \footnote{\url{https://www.cochranelibrary.com/cdsr/reviews}}. We only retained examples with source length between 300 to 1,000 words, and target length between 100 to 700 words. This resulted in a set of 5,195 source-target pairs which constitutes our training set, a further 500 abstract pairs as the validation set, and 1000 more as the test set. %We will release our scripts for downloading and preprocessing the data upon acceptance.
\subsection{Data analysis}
\label{Data_analysis}

\begin{table*}
%\small
\centering
% \begin{tabular}{m{1.5cm}<{}|m{0.8cm}<{\centering}m{0.8cm}<{\centering}|m{0.8cm}<{\centering}m{0.8cm}<{\centering}|m{0.8cm}<{\centering}m{0.8cm}<{\centering}}
\begin{tabular}{@{} l rr rr rr @{}}
\toprule
% \multicolumn{1}{|c|}{\multirow{2}{*}{Statistic}}
& \multicolumn{2}{c}{\bf Train} & \multicolumn{2}{c}{\bf Validation} & \multicolumn{2}{c}{\bf Test} \\
\cmidrule(lr){2-3}
\cmidrule(lr){4-5}
\cmidrule(lr){6-7}
\multicolumn{1}{c}{}                           & \textbf{Source}       & \textbf{Target}
& \textbf{Source}       & \textbf{Target}
& \textbf{Source}       & \textbf{Target}      \\
\midrule
\# abstracts                              & 5,195        & 5,195       & 500            & 500            & 1,000       & 1,000       \\
Average length (words)                      & 714          & 374         & 713            & 368            & 727         & 378         \\
Vocabulary size                                  & 57,685       & 34,175       & 16,574         & 10,596         & 23,938      & 15,407      \\ \midrule
Flesch-Kincaid & 14.68 & 13.25 & 14.93 & 13.57 & 14.70 & 13.23\\
Gunning & 14.57 & 13.54 & 14.69 & 13.79 & 14.57 & 13.49\\
Coleman-Liau & 15.40 & 14.37 & 15.57 & 14.51 & 15.39 & 14.43 \\
\bottomrule

\end{tabular}
\caption{Dataset statistics across the different splits.}
\label{data_description}

\end{table*}

% \begin{table}[htb]
% \small
% \centering
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
%                 & \multicolumn{2}{c|}{Train} & \multicolumn{2}{c|}{Validation} & \multicolumn{2}{c|}{Test} \\ \hline
%                 & Source       & Target      & Source         & Target         & Source      & Target      \\ \hline
% \# abstracts    & 5,195        & 5,195       & 500            & 500            & 1,000       & 1,000       \\ \hline
% Avg. length & 714          & 374         & 713            & 368            & 727         & 378         \\ \hline
% Vocabulary size & 57,685       & 34,175      & 16,574         & 10,596         & 23,938      & 15,407      \\ \hline
% \end{tabular}
% \caption{}
% \label{data_description}
% \end{table}

Table \ref{data_description} shows the characteristics of the dataset. Methods of calculating the readability score are detailed in the Evaluation Metrics Section\ref{evaluation_metrics}. The readability of both source and target texts are at undergraduate level (e.g. 13th grade = 1st year, 15th grade = 3rd year). Notably, the average lengths of abstracts from the source sets are larger than those from the target sets, and the target sets have lower readability scores (i.e. more readable) on average. Since the length of the abstracts and the readability scores from different subset splits are similar, the dataset splits can be considered to be comparable.

%In order to establish how experts translate scientific biomedical abstracts into plain language versions that target the general population, we identified five common phenomena on the basis of our observations when constructing the current dataset. These typical categories and examples are depicted in Table \ref{transformation_case_study}. More comprehensive guidelines for writing Cochrane plain language summaries can be found in \citet{mcilwain2014standards}.

% In order to establish how experts translate scientific biomedical abstracts into plain language versions that target the general population, we identified five common phenomena on the basis of our observations when constructing the current dataset, and categorized them using the  Federal Plain Language Guidelines \cite{plain2011federal}. Categories and examples are depicted in Table \ref{transformation_case_study}.

In order to understand how experts translate scientific biomedical abstracts into plain language versions that target the general population, we identified five typical transformation phenomena on the basis of our observations when constructing the current dataset.\footnote{More comprehensive guidelines for writing Cochrane plain language summaries can be found in \citet{mcilwain2014standards}.} These transformation categories with examples are presented in Table \ref{transformation_case_study}.

The most common transformation to make the paragraph more straightforward is to remove unnecessary details. Although some details such as experimental settings, control experiments or quantitative results are informative for professionals and may indicate the quality of a scientific review, such information
% Although the quality and number of biomedical literature databases considered can indicate the quality of a systematic review, this information
may confuse laypeople and obscure the key findings from the review.
% Also, the target version often eliminates scientific evaluation metrics (e.g., Odds ratios and Risk Ratios) and confidence intervals.
The critical message for laypeople is the general association between an intervention and a health condition, rather than the precise details of the scientific evidence used to support this conclusion.

Explaining relevant background information, including the prevalence, mortality, risk factors and outcome of a condition, enables readers to establish whether or not the topic under discussion meets their information needs. Jargon (or even some standard medical terms) presents another challenge that prevents laypeople from referring to peer-reviewed papers for answers to their health-related questions. Providing definitions for technical terms (such as ``randomized control trials'' in Table \ref{transformation_case_study}) can make professionally-authored text more understandable to a lay audience. Restating the sentence, especially title or headings, in an interrogative sentence makes the scientific content more engaging, and highlights the clinical question under consideration in the review. Sometimes, it is difficult for laypeople to understand the importance of the study question.

Finally, many cases in our dataset require sentence structure simplification. Rephrasing lengthy, convoluted sentences as shorter ones can divide complex information into smaller, easier-to-process units. We also identified other less frequent transformations, such as avoiding passive voice, using ``must'' to indicate requirements, and minimizing abbreviations. As it would be intractable to exhaustively identify and categorize all of the transformation types in our dataset, we provide only the most commonly encountered ones in this paper.
% Sentence structure simplification includes many other techniques, such as avoiding using passive voice, using "must" to indicate requirements, minimizing abbreviations, and so forth.



% \begin{table*}[]
% \centering
% \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
% \hline
% % \multicolumn{1}{|c|}{\multirow{2}{*}{Statistic}}
% & \multicolumn{3}{c|}{Train} & \multicolumn{3}{c|}{Validation} & \multicolumn{3}{c|}{Test} \\ \cline{2-10}
% \multicolumn{1}{|c|}{}                           & Source  & Target  & Ratio  & Source    & Target    & Ratio   & Source  & Target  & Ratio \\ \hline
% Flesch reading ease score                        & 32.3    & 40.6    & 0.26   & 31.0      & 39.2      & 0.26    & 32.3    & 40.5    & 0.25  \\ \hline
% Flesch-kincaid grade level                       & 14.7    & 13.3    & 0.10   & 14.9      & 13.6      & 0.09    & 14.7    & 13.2    & 0.10  \\ \hline
% Gunning fog index                                & 15.4    & 14.4    & 0.07   & 15.6      & 14.5      & 0.07    & 14.6    & 13.5    & 0.08  \\ \hline
% \end{tabular}
% \caption{\yizhong{How did you get the ratio? It doesn't look very intuitive. Also, Can we merge this table with Table 1?}}
% \label{reading_score}
% \end{table*}

\section{Evaluation Metrics}
\label{evaluation_metrics}

The various phenomena in our task present a challenge for comprehensive and fair evaluation. Therefore, we adopt several automatic evaluation metrics, as well as human evaluation to assess different aspects of model performance. % Our task involves both summarization and acquisition of health domain knowledge, which can not be captured by a single evaluation metric. Therefore, we evaluated the models for summarization accuracy and readability using automated metrics, and conducted an evaluation using human judgment to further assess summarization quality.
\subsection{Automated evaluation}
\subsubsection{Summarization evaluation}
We first use ROUGE \cite{lin2004rouge} to evaluate the summarization performance.
% ROUGE compares a model-generated summary against a human-generated reference summary.
ROUGE-n measures overlap of n-grams between the model-generated summary and the human-generated reference summary, and ROUGE-L measures the longest matching sequence of words using the longest common subsequence.
% The F1-sc ore of ROUGE is the harmonic mean of the ROUGE precision and recall.
In this task, we report the F1 scores of ROUGE-1, ROUGE-2, and ROUGE-L as the summarization performance measures. ROUGE scores were computed using \texttt{pyrouge}\footnote{https://pypi.org/project/pyrouge/}.
\subsubsection{Readability evaluation} Other than how much information is retained in the summary, we are also interested in assessing the ease with which a reader can understand a passage, defined as readability. We use three standard metrics to evaluate readability: Flesch-Kincaid grade level \cite{kincaid1975derivation}, Gunning fog index \cite{gunning1952technique}, and Coleman-Liau index \cite{coleman1975computer}.
These scores are computed using \texttt{textstat}\footnote{https://pypi.org/project/textstat/}, and their formulae are as follows:

\begin{itemize}
    \item \textbf{Flesch-Kincaid grade level}:
    $$0.39 \left (\frac{\mbox{total words}}{\mbox{total sentences}}\right ) + 11.8 \left (\frac{\mbox{total syllables}}{\mbox{total words}} \right ) - 15.59,$$
    \item \textbf{Gunning fog index}:
    $$0.4\left[\left(\frac{\mbox{words}}{\mbox{sentences}}\right ) +100\left (\frac{\mbox{complex words}}{\mbox{words}}\right)\right],$$
    where complex words are those words with three or more syllables.
    % A gunning fog index of 14 requires the reading level of a college sophomore.
    \item \textbf{Coleman-Liau index}:
    $$0.0588L - 0.296S - 15.8,$$
    where $L$ is the average number of letters per 100 words and $S$ is the average number of sentences per 100 words.
\end{itemize}
    These readability evaluation metrics all estimate the years of education generally required to understand the text. Lower scores indicate that the text is easier to read.
    % ; higher scores mark the more difficult text to read.
    More specifically, scores of 13-16 correspond to college-level reading ability in the United States education system. Table \ref{data_description} shows the readability scores of the source and target sets of our dataset. Although all the scores indicate a college level of education is needed to read even the target summary, we do see a stable difference between the scores of the source and target. This indicates that these scores are useful for reflecting the different level of readability for text in our dataset.
\subsection{Human evaluation}
While we have adopted the most commonly used metrics for assessing summarization and simplification performance, many aspects of the generated text, such as fluency, grammaticality, and coherence, are not captured by them. Of particular importance, factual correctness of the generated text is crucial in the medical domain. To consider these desirable properties, we developed a method for further assessment of summary quality by human evaluators.


Specifically, we presented an evaluator with two biomedical abstracts followed by four questions. Evaluators were recruited if they: (1) were able to read and write in English; and (2) had at least 12 years education (as the education level required for the training dataset we preprocessed is college level). Evaluators were excluded if they (1) had participated in medical training or shadowing in a hospital; or (2) had completed advanced (graduate level) biology courses. These criteria were selected to ensure that our evaluators were representative of the college-educated lay public. Participants were recruited by convenience sampling, and the study was considered exempt upon institutional IRB review. The estimated time for completion of the human evaluation was 30 minutes for each participant. No compensation was provided for participating in this study.  We recruited 8 human raters.
The average age of these evaluators was 23.5 years old. Four of them were female, and all had more than 12 years of formal education. Each of the abstract/summary pairs was assigned to two independent evaluators.

Two versions of each biomedical abstract were presented: SOURCE refers to the original professional language version, and SUMMARY refers to the version to be evaluated. This was either the professionally written target, or the version generated by our best-performing automated summarization model, BART pre-trained on both CNN/DM and PubMed. Evaluators were blinded to the authorship of the summary (BART vs. human expert). Two biomedical abstracts (A and B) were randomly selected from the test set. Evaluators were required to read through the two pairs of abstracts and compare the SUMMARY to SOURCE considering the following aspects on a 1-5 Likert scale (1 - very poor; 5 - very good):
\begin{itemize}
    \item \textbf{Grammaticality} Do you think the SUMMARY is grammatically correct?
    \item \textbf{Meaning preservation} Does the SUMMARY provide all the useful information you think is important from the source?
    \item \textbf{Understandability} Is the SUMMARY easier to understand than the source?
    \item \textbf{Correctness of key information} How do you judge the overall quality of the SUMMARY in terms of its correctness of the key information compared to the source?
\end{itemize}

  %The system-generated abstracts were the produced by the BART model pre-trained on both CNN/DM and PubMed, and each of the abstract/summary pairs was assigned to 2 independent evaluators. %Of note, evaluators were not informed as to the authorship of the summaries (BART vs. human expert).





\section{Experiments}
\begin{table*}[thb]
%\small
\centering
\begin{tabular}{@{} l ccc ccc @{}}
\toprule
\textbf{Model}                  & \textbf{ROUGE-1}         & \textbf{ROUGE-2}         & \textbf{ROUGE-L}    & \textbf{Flesch-Kincaid} & \textbf{Gunning} & \textbf{Coleman-Liau}   \\ \midrule
Oracle extractive & \textbf{53.56}$_{\pm0.58}$           & \textbf{25.54} $_{\pm0.78}$         & \textbf{49.56}$_{\pm0.65}$   & 14.85 &  13.45  & 16.13      \\
BERT extractive              & 26.60$_{\pm0.51}$          & 11.11$_{\pm0.41}$          & 24.59$_{\pm0.47}$    & \textbf{13.44}  & \textbf{13.26}  &  \textbf{14.40}      \\ \midrule
Pointer generator                & 38.33$_{\pm0.61}$          & 14.11$_{\pm0.46}$          & 35.81$_{\pm0.60}$     &  16.36  &  15.86  &  15.90     \\
BART                 & 52.53$_{\pm0.51}$          & 21.83$_{\pm0.52}$          & 49.75$_{\pm0.52}$          &  13.59  &  14.16  &  14.45 \\
BART+CNN/DM             & 52.46$_{\pm0.48}$          & 21.84$_{\pm0.50}$          & 49.70$_{\pm0.50}$     & 13.73  &  14.33  &  14.60      \\
BART+PubMed            & 52.66$_{\pm0.48}$          & 21.73$_{\pm0.48}$          & 49.97$_{\pm0.51}$     &  \textbf{13.30}  &  \textbf{13.80}  &  \textbf{14.28}     \\
BART+CNN/DM+PubMed   & \textbf{53.02}$_{\pm0.48}$ & \textbf{22.06}$_{\pm0.49}$ & \textbf{50.24}$_{\pm0.49}$  &  13.60  &  14.11  &  14.41 \\
 \bottomrule
\end{tabular}
\caption{Test set performance evaluated by ROUGE and readability score. BART model pretrained on CNN/DM and PubMed is the best-performing model based on ROUGE, while BART model pretrained on PubMed is the best one based on readability score (Best model performance is in bold). $x_\pm$ indicates 95\% interval: $[x-, x+]$}
\label{rouge_readability}

\end{table*}

% \begin{table}
% \small
% \centering
% \begin{tabular}{l|c|c|c}
% \hline
% Model                  & Rouge-1         & Rouge-2         & Rouge-L         \\ \hline
% BERTEXT+C              & 0.2660          & 0.1111          & 0.2459          \\
% PTGEN+C                & 0.3833          & 0.1411          & 0.3581          \\
% BART+C                 & 0.5253          & 0.2183          & 0.4975          \\
% BART+CD+C              & 0.5246          & 0.2184          & 0.4970           \\
% BART+P+C               & 0.5266          & 0.2173          & 0.4997          \\
% \textbf{BART+CD+P+C}   & \textbf{0.5302} & \textbf{0.2206} & \textbf{0.5024} \\ \hline
% Extractive Upper Bound & 0.5440           & 0.2501          & 0.4777          \\ \hline
% \end{tabular}
% \caption{}
% \label{rouge}
% \end{table}

% Oracale:
% medical_plain_language/fairseq/result/test_extracted_abstract_R2.txt.pyrouge
% 1 ROUGE-1 Average_F: 0.53563 (95%-conf.int. 0.52966 - 0.54144)
% 1 ROUGE-2 Average_F: 0.25538 (95%-conf.int. 0.24811 - 0.26319)
% 1 ROUGE-L Average_F: 0.49555 (95%-conf.int. 0.48911 - 0.50189)

% BERT extractive:
% medical_plain_language/PreSumm/result/test_7500.pyrouge
% 1 ROUGE-1 Average_F: 0.26597 (95%-conf.int. 0.26083 - 0.27122)
% 1 ROUGE-2 Average_F: 0.11105 (95%-conf.int. 0.10699 - 0.11492)
% 1 ROUGE-L Average_F: 0.24590 (95%-conf.int. 0.24123 - 0.25071)


% Pointer generator:
% medical_plain_language/neural-summ-cnndm-pytorch/result/neural-summ-cnndm-pytorch_cochrane.test.hypo.pyrouge
% 1 ROUGE-1 Average_F: 0.38334 (95%-conf.int. 0.37723 - 0.38956)
% 1 ROUGE-2 Average_F: 0.14106 (95%-conf.int. 0.13651 - 0.14611)
% 1 ROUGE-L Average_F: 0.35811 (95%-conf.int. 0.35210 - 0.36413)

% BART:
% medical_plain_language/fairseq/result/paper_pretrained_bart_large_cochrane.test.hypo.pyrouge
% 1 ROUGE-1 Average_F: 0.52532 (95%-conf.int. 0.52020 - 0.53038)
% 1 ROUGE-2 Average_F: 0.21825 (95%-conf.int. 0.21305 - 0.22342)
% 1 ROUGE-L Average_F: 0.49747 (95%-conf.int. 0.49234 - 0.50258)

% BART+CNN/DM:
% medical_plain_language/fairseq/result/paper_pretrained_bart_large_cnndm_cochrane.test.hypo.pyrouge


% BART+Pubmed:
% medical_plain_language/fairseq/result/cochrane_pubmed_small_paper_pretrained_bart_large.test.hypo.pyrouge

% BART+CNN/DM+Pubmed:
% medical_plain_language/fairseq/result/cochrane_pubmed_small_paper_pretrained_bart_large_cnndm.test.hypo.pyrouge
\subsection{Methods}
Summarization methods can be broadly categorized into extractive and abstractive approaches. The extractive approach
creates summaries by selecting the most important sentences in a document,
% formulates the summarization problem as a binary classification task for each sentence with labels indicating whether a sentence should be included in the summary,
while the abstractive approach usually employs sequence-to-sequence models to generate summaries that may contain new phrases not included in the source document. We experimented with several state-of-the-art extractive and abstractive methods to check the feasibility and difficulty of the plain language summarization task.
\subsubsection{Extractive methods}
We applied two extractive methods -- \textit{Oracle extractive} and \textit{BERT extractive} \cite{liu2019text} -- to the CDSR dataset. \textit{Oracle extractive} can be viewed as an upper bound for extractive models. It creates an oracle summary by selecting the set of sentences in the document that generates the highest ROUGE-2 score with respect to the gold standard summary. Since oracle extractive summarization takes the gold standard summary into consideration, it can't be applied summarization tasks in practice. \textit{BERT extractive} is the state-of-the-art extractive method for text summarization. \textit{BERT} \cite{devlin2018bert} is a bidirectional unsupervised language representation derived by pre-training a Transformer architecture on a unlabeled text corpus for reconstruction. Several inter-sentence Transformer layers are then stacked on top of BERT outputs, to capture document-level features for extracting summaries. A sigmoid classifier is added as the output layer for extractive summarization. The oracle summary in the \textit{Oracle Extractive} method are used as supervision for training the \textit{BERT Extractive} model.
\subsubsection{Abstractive methods}
We experimented with two abstractive models, \textit{pointer-generator} \cite{see2017get} and \textit{BART} \cite{lewis2019bart}, for our task. \textit{Pointer-generator} was a commonly used abstractive model before pretraining dominated the field. It enhances the standard sequence-to-sequence model with a pointer network that allows both copying words from the source and generating words from a fixed vocabulary. \textit{BART} is a state-of-the-art summarization model based on a large transformer sequence-to-sequence architecture. It is pre-trained on large corpora by corrupting text with an arbitrary noising function, and learning a model to reconstruct the original text.
% It uses a standard Tranformer-based neural machine translation architecture, which can be seen as a generalized pre-training scheme.
As a sequence-to-sequence model, BART can be directly fine tuned for abstractive summarization task.
\subsubsection{Intermediate pre-training}
\label{sec:inter-pretraining}
To compensate for the limited training data, we added intermediate pre-training steps for the BART model before finetuning.
% it for our plain language summarization task.
We first experimented with adding labeled data for summarization task in other domains. We adopted the CNN/DM dataset \cite{nallapati2016abstractive}, which contains about 287K document-summary pairs, and BART is among the best-performing systems for this task. %For simplicity, we directly used the publicly released version of BART that is finetuned on CNN/DM\footnote{https://github.com/pytorch/fairseq/tree/master/examples/bart}.
% adopted the publicly released BART model \footnote{https://github.com/pytorch/fairseq/tree/master/examples/bart}, which was additionally trained on CNN/DM summarization dataset \cite{nallapati2016abstractive}. The CNN/DM summarization dataset contains about 287K document-summary pairs, and BART is among the best-performing systems for this task.
Secondly, we tried to pre-train BART with an unlabeled biomedical corpus to expose the model to medical domain-specific language. We used the PMC articles dataset \footnote{https://www.kaggle.com/cvltmao/pmc-articles}
which contains 300K PubMed abstracts. Following the BART paper, we corrupted these documents using several transformations, including text substitution and sentence shuffling. BART was then trained on the corrupted abstracts to reconstruct the original PubMed abstracts. Lastly, we combined these two strategies to train BART on CNN/DM and PubMed sequentially before finetuning it on our dataset.
%Can you provide a footnote citation linking to the authors' release?
%What was the sequence of the PubMed-BART pretraining for the model trained on both of these sets? First PubMed then CNN/DM, I would wager - this isn't clear from the paragraph above.
\subsubsection{Training details}
All experiments were run using a single NVIDIA Tesla V-100 GPU. All models were developed using \texttt{PyTorch}.
We used \texttt{neural-summ-cnndm-pytorch}\footnote{https://github.com/lipiji/neural-summ-cnndm-pytorch/} to implement the pointer-generator model. The batch size was set to 4. Other hyper-parameters were set to default values. We built the BERT extractive model using code released by the authors.\footnote{https://github.com/nlpyang/presumm}
The learning rate was set to $2 \times 10^{-3}$ and the batch size 140. Other hyper-parameters were set to default values. We used the Fairseq \footnote{https://github.com/pytorch/fairseq} BART implementation. All BART models were trained using the Adam optimizer. The learning rate was set to $3 \times 10^{-5}$, and learning decay was applied. The minimum length of the generated summaries was set to 100, and the maximum length was set to 700.
\subsection{Results}
\subsubsection{Automated evaluation}
% \begin{table*}
% \centering
% \small
% \begin{tabular}{l|ccc|ccc}
% \toprule
%                   & \multicolumn{3}{c|}{Abstract A} & \multicolumn{3}{c}{Abstract B} \\
% \midrule
% Perspectives & Target  & Generated  & Ratio  & Target  & Generated  & Ratio  \\
% \midrule
% Grammaticality            & 4.25    & 4.5        & 0.06    & 3.5     & 4          & 0.14    \\
% Meaning Preservation       & 3.75    & 4.75       & 0.27    & 3.5     & 4.5        & 0.29    \\
% Understandability  & 1.25    & 1.5        & 0.2     & 2.25    & 2.5        & 0.11    \\
% Correctness of key information        & 3.5     & 4.5        & 0.29    & 4       & 4          & 0      \\
% \bottomrule
% \end{tabular}
% \caption{Human evaluation score. Generated abstracts from BART+CNN/DM+PubMed model have better scores in grammar, meaning preservation, and correctness of key information. (Ratio = (Generated - Target) / Target)}
% \label{result_human_evaluation}

% \end{table*}


\begin{table*}
\centering
%\small
\begin{tabular}{@{} l cc cc @{}}
\toprule
                  & \multicolumn{2}{c}{\textbf{Abstract A}} & \multicolumn{2}{c}{\textbf{Abstract B}} \\
\cmidrule(lr){2-3}
\cmidrule(lr){4-5}
\textbf{Perspectives} & \textbf{Target}  & \textbf{Generated} & \textbf{Target}  & \textbf{Generated} \\
\midrule
Grammaticality            & 4.25    & 4.50        &3.50     & 4.00 \\
Meaning Preservation       & 3.75    & 4.75       & 3.50     & 4.50 \\
Understandability  & 3.75    & 3.50        & 2.75    & 2.50     \\
Correctness of Key Information        & 3.50     & 4.50    & 4.00       & 4.00 \\
\bottomrule
\end{tabular}
\caption{Human evaluation scores of the expert-generated summaries (\textit{Target}) and the model-generated summaries (\textit{Generated}) for two abstracts from the test set. Generated abstracts from BART+CNN/DM+PubMed model have better scores in grammaticality, meaning preservation, and correctness of key information.}
\label{result_human_evaluation}

\end{table*}
ROUGE and readability results on the CDSR test set are shown in Table \ref{rouge_readability}. We compare the seven methods described above: Oracle extractive, BERT extractive, pointer-generator, BART, BART pre-trained on CNN/DM, BART pre-trained on Pubmed abstracts, and BART pre-trained on both CNN/DM and PubMed abstracts.

The oracle extractive method, as an upper bound for the extractive approach, produces the best ROUGE-1 and ROUGE-2 scores. However, it obtains approximately the same level of readability as the source text in our test set (Table \ref{data_description})
, which indicates that selecting the reference sentences will only result in a summary that is as difficult to read as the original abstract.
% This indicates that although the oracle extractive approach can achieve relatively strong performance as estimated by ROUGE, the selected sentences are as difficult to read as the original abstract, which does not suggest a practical solution for generating lay language summaries. In addition, the oracle extractive method cannot be applied in practice, since no target exist in an applied setting.
In contrast, the BERT-based extractive model achieves better readability scores while performing worst in terms of ROUGE scores. This demonstrates that, in practice, training the model to extract the correct content from the original abstract might be difficult, even though the model learns to extract shorter and easier sentences.
% In contrast, the other extractive model based on BERT has the lowest Gunning fog index and Coleman-Liau readability index scores, but the worst ROUGE scores. This demonstrates that the BERT extractive model retains the least content in comparison with other models. To generate lay language summaries, it is desirable to both retain key content, and make this information easy to understand. Our results suggest that the BERT extract approach is poorly suited to this task.

Among the 5 abstractive models, the pointer-generator model performs significantly worse in both ROUGE and readability, emphasizing the importance of pre-training for our task.
BART-based models achieve surprisingly good performance in terms of both summarization and readability, suggesting contemporary NLP models have the potential to perform the task, and to help the general public access professional medical information. Additionally, BART pre-trained on CNN/DM and PubMed abstracts achieves the best performance in ROUGE, and BART pre-trained only on PubMed abstracts obtained the lowest readability.
This demonstrates the usefulness of either adding task-relevant labeled data or domain-specific unlabeled data. However, our strategies for adding such data are quite straightforward, and we lacked resources to do hyperparameter search for the relatively expensive pre-training procedure. Therefore, we only see marginal improvement compared with the BART model. We will aggregate more relevant data, and develop better pre-training strategies to improve the performance in future work.
% These two models demonstrate the effectiveness of domain-specific pre-training with additional summarization pre-training.
% These results also demonstrate that abstractive models with pre-training on a biomedical corpus are promising solutions for the generation of lay language biomedical summarizes, as they retain key content (as demonstrated by ROUGE) while reducing readability requirements (as demonstrated by a drop in readability requirements of $\approx$ 1.4 years of education relative to the original documents).

\subsubsection{Human evaluation}
Table \ref{result_human_evaluation} shows the human evaluation results. Intriguingly, human evaluators rated the model-generated summaries with comparable or even higher scores for all the four aspects, and for both abstract A and B. The average Kendalls coefficient \cite{sen1968estimates} for the two biomedical abstracts among all evaluators' inter-rater aggrement is 0.62. Kendalls coefficient ranges from -1 to 1, indicating low to high association. Considering the subjectivity of the rating task, this number indicated high human agreement for the tasks.
% Compared to the target (an expert-generated summary), evaluators rated the BART-derived summaries with higher scores in grammar, meaning preservation, and correctness of key information aspects. However, they found the generated abstracts were more challenging to understand, with scores of between 0.2 and 0.11 lower than the target abstracts of A and B.
%While further studies with larger numbers of human annotators and more articles would be required to demonstrate this conclusively, the current study provides preliminary evidence of the acceptability of automatically-generated plain language summaries to human readers without specific biomedical expertise.
While larger scale study is required,
this work provides preliminary evidence that automatically-generated plain language summaries
are readable and interpretable to non-expert human readers.
\subsection{Qualitative analysis}
We present the output of our best two models in the last two columns of Table \ref{transformation_case_study}.
%Examination of the model-generated summaries in comparison with the target text
This provides evidence that the best-performing models can address some transformations, and generate grammatical and meaningful outputs.
Specifically, out of the five listed phenomena, we observed that model-generated summaries could achieve three transformation types to some extent, including removing unnecessary details, jargon explanation and sentence structure simplification.
Some capabilities the model demonstrated are encouraging for future research.
For example, it learned to explain the term RCT from similar examples in the training data.

% Examination of the resulting summaries in comparison with their source text provides evidence that the best-performing of our models can also address the transformations required to approximate the expert-authored summaries in the CDSR database. For the 5 phenomena described in Table \ref{transformation_case_study}, the abstracts generated from the two best models achieved transformations in each of these categories, aside from transformation to an interrogative sentence. Intriguingly, although the illustrated example background explanation for our best models are exactly same as the source (i.e. extractive rather than abstractive), we did find other examples in which our abstractive models appeared to learn relevant background knowledge and medical term explanations from \textit{both} the training set \textit{and} the pre-training corpus, and apply them in summaries generated from the test set.
%If we did find examples of generation relevant background explanations, why do we say ``aside from ...... generation of background explanations''?
%An example might be helpful to support the last sentence if there is space.


%For example (Table \ref{transformation_case_study}, row 2), the BART model pre-trained on PubMed (last column) retains the main message that there is an association between cervical length and spontaneous preterm birth (PTB) while explaining what PTB is. In contrast, the BART model pre-trained on CNN/DM and PubMed (column 4) has the explanation, but omits the association. However, the BART model pre-trained on PubMed alone generally keeps more information (best exemplified in the first row of the table), which indicates that this model may not be as effective at removing unecessary details.
On the downside, the  models are still struggle with some difficult transformations, such as relevant background explanation. This ability is harder to learn, and our dataset might not contain the required background knowledge.
Therefore, external knowledge might be also useful.
Furthermore, we also see risks in using the current abstractive models to generate reliable information for the public. For example, in the example of sentence structure simplification, \textit{BART+PubMed} changed the meaning of the original sentence: the source sentence claims an association between the pattern of blood flow with poor prognosis, while the generated sentence focuses on the Doppler ultrasonography. \textit{BART+CNN/DM+PubMed}
performs better in this case.
% model shows better quality in this case.

% Furthermore, the sentence structure simplification in the model that pre-trained on PubMed alone (BART+PubMed) changes the meaning of the sentence. The corresponding sentence in the source claims an association between Doppler ultrasound and the pattern of blood flow. However, the generated summary from this model omits this claim entirely. Therefore, the examples shown in this table are consistent with the ROUGE results in their suggestion that BART model pre-trained on both CNN/DM and PubMed offers advantages as a strategy to generate lay language biomedical summaries.











\section{Discussion}
Automated lay language summarization of biomedical scientific reviews requires both summarization and the acquisition of domain knowledge.
% Prior to the current work,
Previously, available datasets
% for automated lay language transformation
were constructed at sentence level. However, sentence-level simplification or transformation does not require the complex strategies used by experts when rendering biomedical literature understandable to a lay audience. Therefore, we consider the
% development and dissemination of a
document-level dataset as an important outcome of our work,  which
can be useful for future research on this topic.
%will advance the science of automated lay language summarization in the biomedical domain.
%Among the models,
Abstractive models are more practical than extractive ones,
since extractive summaries
are written
% will by definition be written
in the same professional language as their source documents.
The best performing model is BART pre-trained on both CNN/DM and PubMed abstracts, which preserves key information (based on ROUGE) while dropping the reading requirements a year or two (based on readability scores).

%For the automated lay language summarization of biomedical scientific reviews task, human evaluation is necessary.
Human evaluation is necessary for our task.
There is a considerable gap between the automatic evaluation merics
% among ROUGE, readability scores,
and human judgement.
Despite being widely used to evaluate summarization systems, ROUGE is not practical for our task because it can neither capture the required transformation phenomena nor assess difficulty in understanding.
%Regarding readability scores,
Similarly, lower readability scores do not imply understandability.
%That the score calculated by the readability formula is low does not necessarily indicate that the target audience will understand the text.
Readability scores consider only the surface forms,
% number of words, the length of the sentence, and the syllable number,
without considering the complexity introduced by medical abbreviations and domain-specific concepts.
Human evaluation is the most robust method to evaluate the performance.
However, aside from the small number of participants,
the survey questions need a formal validity.
% a limitation of the current human evaluation is the need to establish the formal validity of the survey questions.
Further studies are required to find that BART-derived summaries were more appealing to human raters on several fronts hold when more abstracts and human raters are involved.






\section{Conclusion}
We propose a novel plain language summarization task at the document level and construct a dataset to support training and evaluation. The dataset is of high quality, and the task is challenging due to typical transformation phenomena in this domain. We tried both extractive and abstractive summarization models, and obtained best performance with a BART model pre-trained further on CNN/DM and PubMed, as evaluated by automated metrics. Human evaluation suggests the automatically generated summaries may be at least as acceptable as their professionally authored counterparts.




\bibstyle{aaai21}
Unde commodi consequatur eaque nisi laboriosam, amet eius illo aspernatur dolorem quam officiis ex, unde deserunt quisquam maiores molestias quas provident ratione culpa quidem enim?Reprehenderit earum consequuntur asperiores tempora, dolor quo nobis consequuntur commodi recusandae incidunt maiores.Voluptatum officiis vero maiores assumenda earum quidem quae at, repellat enim et ipsa ad, itaque voluptate dolorum adipisci, saepe ducimus soluta eaque quasi atque fuga, at tenetur fuga minus adipisci facilis repellendus doloremque placeat?Dolor vero nobis quaerat doloremque exercitationem, nam animi error inventore dolores iusto optio velit eveniet, sunt corporis repudiandae quam nihil ea quis dolores tenetur expedita saepe, veniam repellendus ullam necessitatibus fugit reiciendis alias eaque ipsa iste, unde perspiciatis excepturi.Facilis magni amet placeat neque ratione sunt inventore iure quasi vero porro, voluptas explicabo eius, eum repellat provident deserunt soluta corporis saepe accusamus earum explicabo sint,
\bibliography{main}
\end{document}