Dear Roni Stern,

We regret to inform you that your paper, "Bounded-Suboptimal Resilient Pattern Databases" (42), has not been accepted for presentation
at the 30th AAAI Conference on Artificial Intelligence (AAAI-16). The competition was very strong this year,
with a record-number 2132 submissions and only 549 papers accepted, for an acceptance rate of 26%.
Attached to this message are the reviews of your submission, which we hope will explain the result and
provide useful feedback on your work.

If you indicated on submission that you would like your paper to be considered by a AAAI-16 workshop,
then your manuscript and the reviews will be made available to the chairs of the workshop that you
specified. We expect that workshop chairs will contact the authors of submissions selected for presentations
in their workshops around 23 November.

We understand that every paper is the result of the careful thought and hard work of its authors. Due to the
large number of high-quality submissions, we were forced to reject many interesting and deserving papers.
Every AAAI-16 submission received at least two (and 99% at least three) reviews, provided by over 1000
members of the program committee (all PhD researchers with extensive AI publication experience) as well
as additional auxiliary reviewers under supervision of PC members. The review process for each paper was
overseen by one of over 100 senior program committee members or special track co-chairs, who monitored
the reviews of the papers and initiated discussion after the author feedback period. Reviewers read and
discussed the author response, and in many cases additional reviews were collected. Discussions were often
very detailed and, in most cases, led to resolution of issues brought up by the reviewers. For the main
technical track, senior program committee members wrote a meta-review summarizing the
recommendation and providing further explanation where required. Finally, the program co-chairs and
track chairs decided the papers to include in the program.

Whereas the program committee expended tremendous effort in an attempt to be as fair and accurate as
possible, we recognize that the conference reviewing process is inherently limited. Evaluation is subject to
random and subjective factors, and no doubt many decisions could have justifiably gone the other way. We
wish you well in finding an appropriate publication forum for your work.

We also hope that despite this submission outcome you will be able to join us at the conference in Phoenix.
In addition to the outstanding technical program, AAAI-16 continues the tradition of a series of prominent
invited speakers and panels, an extensive demo program, state-of-the-field presentations by senior
members of the AAAI community, a "What's Hot" track highlighting the best work from sister conferences
and competitions, and other special events. For more details on all AAAI-16 events, please see
http://www.aaai.org/Conferences/AAAI/aaai16.php.

Sincerely,

Dale Schuurmans and Michael Wellman
AAAI-16 Program Co-Chairs


----------------------- REVIEW 1 ---------------------
PAPER: 42
TITLE: Bounded-Suboptimal Resilient Pattern Databases
AUTHORS: Levi Lelis, Richard Valenzano, Roni Stern and Gabriel Nazar

Significance: 1 (minimal or no contribution)
Soundness: 3 (correct)
Scholarship: 3 (excellent coverage of related work)
Clarity: 3 (crystal clear)
Breadth of Interest: 2 (interest limited to specialty area)
SUMMARY RATING: -3 (---)
CONFIDENCE: 2 (reasonably confident)

----------- SUMMARIZE THE MAIN CONTRIBUTION OF THE PAPER -----------
This paper presents an approach to IDA*-based search when an initially admissible heuristic, given as a memory-based heuristic such as a pattern database (PDB), gets corrupted after it is loaded into the Dynamic RAM (DRAM). According to the authors, DRAM corruption can be caused by reasons such as exposing a physical device to radiation, and reducing the refresh power of DRAM with the purpose of saving energy.

A. A drawback that I see with this paper is related to the significance of the problem addressed. For example, in motivating the work, the authors argue that energy has become a major constraint in many computer systems running in clusters and server farms. Furthermore, one way of saving power is to reduce the refresh power of DRAMs, which could cause memory errors (i.e., bits flipped in DRAM). Thus, the implication here is that the operators and the users of computer clusters and server farms would be willing to accept a tradeoff where some energy is saved but the data stored on the computers will become corrupt. I doubt very strongly that such a tradeoff would be considered as a viable option. I for one would not accept it as an operator or as a user. I do not view the correctness of the RAM data as something that we could give up on in exchange for some energy savings.

B. There are some obvious alternatives to the approach discussed in this paper, and I believe that such alternatives should be considered more seriously, despite the fact that the authors rule them out very quickly. Specifically, I am talking about Weighted IDA*, mentioned in passing as a method that can "return solutions that are arbitrarily suboptimal when guided by PDBs stored in unreliable memory". This statement is true but on the other hand one can use Weighted IDA* just in the normal way, as follows. Have the standard error-correction code (ECC) in place, which admittedly occupies some RAM and takes some computation. Then run Weighted IDA*, with the weight set, for example, to the same suboptimality bound as in method described in this submission (i.e., a factor of 3). Assume that using a weight greater than 1 results in a speed-up, as normally expected. Then it is possible in principle that the faster computation can actually save power due to a smaller CPU time requ!
 ired. The speedup could possibly exceed the slowdown caused by the ECC computations, as well as the slowdown caused by the reduced size of the RAM available for the PDB (as ECC requires some bits for its own use). Unfortunately the paper does not steer into this direction at all.

C. There appears to be an inconsistency between Theorems 2 and 3. Theorem 2 says M(g(n) + h*(n)). That is, the M factor applies to h*(n) as well, not just to g(n). On the other hand, in Theorem 3, the constant factor 3 applies only to g(n), not to h*(n). I don't know if this is just a small typo or a more serious error, but either way the authors need to address it.

The technical contribution is reasonable but it does not stand out as a very deep contribution. This is not a bad thing on its own but, but the drawbacks summarized earlier remain drag the paper down.

The paper is really easy to follow, and one can say that the authors have invested a proper amount of time in making the text so clear and so accessible to a reader.

In the rebuttal, please consider replying to points A, B and C (a few words will do for the last one).

----------- COMMENTS FOR THE AUTHORS -----------
Please see the previous section.

POST REBUTTAL COMMENTS:

I thank the authors for their response. The response was useful as it further clarified part of the questions, but I still consider that some major improvement is necessary with the paper. The motivation remains unclear to me, and I still cannot see a clear case where this approach is better than alternatives available.

I hope that the following additional insights will help the authors to improve their work. My suggestion is to work on making sure that concerns such as the ones below are not left without an answer.

I took a brief look at (Meza et al 2015) and here are a few insights. They show that some error correcting codes (ECCs) can correct one wrong bit per 64-bit block and can also detect two wrong bits per 64-bit block. The memory overhead for the necessary metadata is 12.5\%, so not so big. Checking for errors and detecting errors has no significant CPU overhead, but correcting errors induces an overhead. In other words, no significant CPU overhead unless errors do occur. So the question is how frequently errors pop up. In the Facebook study, about 2\% of all servers report correctable errors in a month. The uncorrectable errors are two orders of magnitude less frequent. So, yes, I agree that "DRAM errors are a real concern" but ECC seems to be well posed to address such errors, especially given the low frequency of the errors. Furthermore, ECC is a domain-independent technique that protects everything in RAM, not just a heuristic lookup table.

In my review, point B mentioned a very standard approach, such as weighted search with unaltered RAM management (e.g., don't reduce refresh rate). Such an approach can actually be quite energy-friendly, given that weighted search can finish more quickly than unweighted search, with a corresponding reduction in the energy consumption. A comparison to that alternative is necessary in my opinion.


----------------------- REVIEW 2 ---------------------
PAPER: 42
TITLE: Bounded-Suboptimal Resilient Pattern Databases
AUTHORS: Levi Lelis, Richard Valenzano, Roni Stern and Gabriel Nazar

Significance: 2 (modest or incremental contribution)
Soundness: 2 (minor inconsistencies or small fixable errors)
Scholarship: 2 (relevant literature cited but could expand)
Clarity: 2 (more or less readable)
Breadth of Interest: 3 (some interest beyond specialty area)
SUMMARY RATING: 1 (+ (weak accept))
CONFIDENCE: 2 (reasonably confident)

----------- SUMMARIZE THE MAIN CONTRIBUTION OF THE PAPER -----------
The paper presents an approach to correct heuristic functions that
have been corrupted because of, e.g., low energy of DRAMs or because
of radiation. The proposed correction method guarantees an upper bound
on the costs of the found solutions with IDA*. The method is
experimentally evaluated on common search benchmarks.

----------- COMMENTS FOR THE AUTHORS -----------
I like the topic addressed in the paper: Finding methods to
automatically correct inconsistencies of heuristics is an interesting
and promising research direction.

I am a bit skeptical about the general storyline of the paper, though,
which is motivated quite a lot from a practical point of view. For
example, the authors argue that heuristics might be corrupted because
of low energy of DRAMs or by radiation. However, the reader might
wonder how often it actually occurs in practice that a PDB heuristic
(created in a rather domain-specific way) has been corrupted because
of, e.g., radiation, while solving a (rather academic) search
benchmark like the sliding tile puzzle. From my point of view, the
core problem that is addressed in the paper, i.e., automatically
detecting and correcting inconsistent heuristics, is interesting in
its own right. In particular, as already discussed by the authors, the
applied heuristic for the 24-puzzle in the experiments is already
inconsistent by definition, and the correction method still works
because obviously the source of the inconsistency does not matter.

The notion of consistency that is introduced in definition 1 appears
to be stronger than the commonly used notion of consistency, in the
sense that heuristic values are neither allowed to decrease nor
increase more than the cost of the applied transition. What kind of
non-trivial heuristics do satisfy this criterion, apart from the PDB
heuristics used that have been created with domain abstraction?

The correction methods seem to be quite related to the (bidirectional)
pathmax technique (BPMX). What is the relationship to BPMX, and could
BPMX also be used for your purpose? A discussion about this would be
appreciated.

In summary, the general motivation from a practical point of view is
not entirely convincing, but I generally like the topic and the
proposed techniques. Please address the questions from above (which
kind of heuristics satisfy your consistency definition, and what is
the relationship to BPMX?) in the rebuttal. Currently, I rate the
paper slightly above the borderline.


Other comments:

page 1: typo: "for examples"

page 3: Theorem 1: "h(n) is admissible": admissibility is defined for
functions, not for numbers.

page 3: Below theorem 1: "...an overhead of b^(U+1) per node, which is
not feasible for any non-trivial U": if b>=2

page 3: "as mentioned before, U is hard to compute": where has it been
mentioned before?

page 3: The first sentence in subsection "Theoretical Foundation" is
unclear.

page 3: typo: "h(init)" instead of "h(n_{init})"

page 4: example of PMCD: "CMCD correction would set h(n_1) correctly
as 4": CMCD should be PMCD (CMCD is introduced below); furthermore,
why it is correct to set h(n_1) to 4? The distance of n_1 is 5. The
same statement appears later when CMCD is introduced.

page 5: When describing the heuristics that have been applied in the
experiments, it would be useful to have a short discussion that (and
why) these heuristics satisfy definition 1.

references: page numbers are missing in "Hafer et al." ("pp.-");
proceeding names should be consistent (e.g., "ICAPS" vs "International
Conference on Automated Planning and Scheduling")

Update after rebuttal:
----------------------
Thanks for your response. Regarding the
motivation of the paper, the authors might alterantively want to
consider a more "abstract" presentation, i.e., a presentation focussed
on the core heuristic improvement technique without a concrete
practical motivation. In this context, a discussion on related
techniques like (bidirectional) pathmax will be helpful.


----------------------- REVIEW 3 ---------------------
PAPER: 42
TITLE: Bounded-Suboptimal Resilient Pattern Databases
AUTHORS: Levi Lelis, Richard Valenzano, Roni Stern and Gabriel Nazar

Significance: 2 (modest or incremental contribution)
Soundness: 3 (correct)
Scholarship: 2 (relevant literature cited but could expand)
Clarity: 2 (more or less readable)
Breadth of Interest: 2 (interest limited to specialty area)
SUMMARY RATING: -2 (--)
CONFIDENCE: 2 (reasonably confident)

----------- SUMMARIZE THE MAIN CONTRIBUTION OF THE PAPER -----------
The paper presents an approach to correcting h-values provided by a
PDB heuristic that is stored in memory that may yield read errors.

----------- COMMENTS FOR THE AUTHORS -----------
The main issue with this paper is that it is assumed that the PDB is
admissible (first page and Theorem 3). Given this, it is possible to
run IDA* using pathmax to correct the heuristic of states whose
h-value is inconsistent. This could be less informative than detecting
and correcting corruptions with the proposed methods, but IDA*+pathmax
could be much more efficient. In fact, Table 1 shows that IDA* used
with the corrupted h-values is quite competitive. On the other hand,
wIDA* or wRBFS (w<3) could be used along with pathmax (to compute the
h values of the successors) and obtain solutions quickly with the
bound presented in Theorem 3. In addition, one could run wIDA* wRBFS
with pathmax using a decreasing weight schedule in the time limit that
is used in the experimental section (10 min and 1hr)

In sum, I think the paper does not provide sufficient experimental
evidence in support of the approach proposed since it is plausible
that state-of-the-art approaches may be more efficient. I would like
to know the opinion of the authors regarding this. In its current
version I vote for rejection.


-------------------------  METAREVIEW  ------------------------
PAPER: 42
TITLE: Bounded-Suboptimal Resilient Pattern Databases

Meta review:

Reviewers appreciated your work and specifically mentioned the clarity of the paper. But they all agreed that the paper suffers from some disadvateges which prevents it from being published in AAAI at this point.

All reviewers had concerns with the practical aspects/motivation of the problem. The reviewers also pointed out variants of known algorithms that the authors did not compare to or did not convince that their method is better than these methods.

We encourage you to improve your paper along these lines for future submissions.