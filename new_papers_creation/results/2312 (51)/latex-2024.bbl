\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen, Chu, Clark, Shafey, Huang, Meier-Hellstern, Mishra, Moreira, Omernick, Robinson, Ruder, Tay, Xiao, Xu, Zhang, Abrego, Ahn, Austin, Barham, Botha, Bradbury, Brahma, Brooks, Catasta, Cheng, Cherry, Choquette-Choo, Chowdhery, Crepy, Dave, Dehghani, Dev, Devlin, Díaz, Du, Dyer, Feinberg, Feng, Fienber, Freitag, Garcia, Gehrmann, Gonzalez, Gur-Ari, Hand, Hashemi, Hou, Howland, Hu, Hui, Hurwitz, Isard, Ittycheriah, Jagielski, Jia, Kenealy, Krikun, Kudugunta, Lan, Lee, Lee, Li, Li, Li, Li, Li, Lim, Lin, Liu, Liu, Maggioni, Mahendru, Maynez, Misra, Moussalem, Nado, Nham, Ni, Nystrom, Parrish, Pellat, Polacek, Polozov, Pope, Qiao, Reif, Richter, Riley, Ros, Roy, Saeta, Samuel, Shelby, Slone, Smilkov, So, Sohn, Tokumine, Valter, Vasudevan, Vodrahalli, Wang, Wang, Wang, Wang, Wieting, Wu, Xu, Xu, Xue, Yin, Yu, Zhang, Zheng, Zheng, Zhou, Zhou, Petrov, and Wu}]{anil2023palm}
Anil, R.; Dai, A.~M.; Firat, O.; Johnson, M.; Lepikhin, D.; Passos, A.; Shakeri, S.; Taropa, E.; Bailey, P.; Chen, Z.; Chu, E.; Clark, J.~H.; Shafey, L.~E.; Huang, Y.; Meier-Hellstern, K.; Mishra, G.; Moreira, E.; Omernick, M.; Robinson, K.; Ruder, S.; Tay, Y.; Xiao, K.; Xu, Y.; Zhang, Y.; Abrego, G.~H.; Ahn, J.; Austin, J.; Barham, P.; Botha, J.; Bradbury, J.; Brahma, S.; Brooks, K.; Catasta, M.; Cheng, Y.; Cherry, C.; Choquette-Choo, C.~A.; Chowdhery, A.; Crepy, C.; Dave, S.; Dehghani, M.; Dev, S.; Devlin, J.; Díaz, M.; Du, N.; Dyer, E.; Feinberg, V.; Feng, F.; Fienber, V.; Freitag, M.; Garcia, X.; Gehrmann, S.; Gonzalez, L.; Gur-Ari, G.; Hand, S.; Hashemi, H.; Hou, L.; Howland, J.; Hu, A.; Hui, J.; Hurwitz, J.; Isard, M.; Ittycheriah, A.; Jagielski, M.; Jia, W.; Kenealy, K.; Krikun, M.; Kudugunta, S.; Lan, C.; Lee, K.; Lee, B.; Li, E.; Li, M.; Li, W.; Li, Y.; Li, J.; Lim, H.; Lin, H.; Liu, Z.; Liu, F.; Maggioni, M.; Mahendru, A.; Maynez, J.; Misra, V.; Moussalem, M.; Nado, Z.; Nham, J.; Ni, E.; Nystrom,
  A.; Parrish, A.; Pellat, M.; Polacek, M.; Polozov, A.; Pope, R.; Qiao, S.; Reif, E.; Richter, B.; Riley, P.; Ros, A.~C.; Roy, A.; Saeta, B.; Samuel, R.; Shelby, R.; Slone, A.; Smilkov, D.; So, D.~R.; Sohn, D.; Tokumine, S.; Valter, D.; Vasudevan, V.; Vodrahalli, K.; Wang, X.; Wang, P.; Wang, Z.; Wang, T.; Wieting, J.; Wu, Y.; Xu, K.; Xu, Y.; Xue, L.; Yin, P.; Yu, J.; Zhang, Q.; Zheng, S.; Zheng, C.; Zhou, W.; Zhou, D.; Petrov, S.; and Wu, Y. 2023.
\newblock PaLM 2 Technical Report.
\newblock arXiv:2305.10403.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.~D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33: 1877--1901.

\bibitem[{Carlini et~al.(2022)Carlini, Ippolito, Jagielski, Lee, Tramer, and Zhang}]{carlini2022quantifying}
Carlini, N.; Ippolito, D.; Jagielski, M.; Lee, K.; Tramer, F.; and Zhang, C. 2022.
\newblock Quantifying memorization across neural language models.
\newblock \emph{arXiv preprint arXiv:2202.07646}.

\bibitem[{Carlini et~al.(2021)Carlini, Tramer, Wallace, Jagielski, Herbert-Voss, Lee, Roberts, Brown, Song, Erlingsson et~al.}]{carlini2021extracting}
Carlini, N.; Tramer, F.; Wallace, E.; Jagielski, M.; Herbert-Voss, A.; Lee, K.; Roberts, A.; Brown, T.; Song, D.; Erlingsson, U.; et~al. 2021.
\newblock Extracting training data from large language models.
\newblock In \emph{30th USENIX Security Symposium (USENIX Security 21)}, 2633--2650.

\bibitem[{Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing}]{vicuna2023}
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J.~E.; Stoica, I.; and Xing, E.~P. 2023.
\newblock Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality.

\bibitem[{Choi et~al.(2018)Choi, He, Iyyer, Yatskar, Yih, Choi, Liang, and Zettlemoyer}]{choi2018quac}
Choi, E.; He, H.; Iyyer, M.; Yatskar, M.; Yih, W.-t.; Choi, Y.; Liang, P.; and Zettlemoyer, L. 2018.
\newblock QuAC: Question answering in context.
\newblock \emph{arXiv preprint arXiv:1808.07036}.

\bibitem[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel}]{chowdhery2022palm}
Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H.~W.; Sutton, C.; Gehrmann, S.; Schuh, P.; Shi, K.; Tsvyashchenko, S.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y.; Shazeer, N.; Prabhakaran, V.; Reif, E.; Du, N.; Hutchinson, B.; Pope, R.; Bradbury, J.; Austin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.; Levskaya, A.; Ghemawat, S.; Dev, S.; Michalewski, H.; Garcia, X.; Misra, V.; Robinson, K.; Fedus, L.; Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.; Spiridonov, A.; Sepassi, R.; Dohan, D.; Agrawal, S.; Omernick, M.; Dai, A.~M.; Pillai, T.~S.; Pellat, M.; Lewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.; Wang, X.; Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; Wei, J.; Meier-Hellstern, K.; Eck, D.; Dean, J.; Petrov, S.; and Fiedel, N. 2022.
\newblock PaLM: Scaling Language Modeling with Pathways.
\newblock arXiv:2204.02311.

\bibitem[{Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova}]{clark2019boolq}
Clark, C.; Lee, K.; Chang, M.-W.; Kwiatkowski, T.; Collins, M.; and Toutanova, K. 2019.
\newblock BoolQ: Exploring the surprising difficulty of natural yes/no questions.
\newblock \emph{arXiv preprint arXiv:1905.10044}.

\bibitem[{Dickson(2023)}]{dickson2023}
Dickson, B. 2023.
\newblock Why data contamination is a big issue for LLMs.
\newblock Accessed: 2023-07-28.

\bibitem[{Dodge et~al.(2021)Dodge, Sap, Marasovi{\'c}, Agnew, Ilharco, Groeneveld, Mitchell, and Gardner}]{dodge2021documenting}
Dodge, J.; Sap, M.; Marasovi{\'c}, A.; Agnew, W.; Ilharco, G.; Groeneveld, D.; Mitchell, M.; and Gardner, M. 2021.
\newblock Documenting large webtext corpora: A case study on the colossal clean crawled corpus.
\newblock \emph{arXiv preprint arXiv:2104.08758}.

\bibitem[{Grootendorst(2020)}]{grootendorst2020keybert}
Grootendorst, M. 2020.
\newblock KeyBERT: Minimal keyword extraction with BERT.

\bibitem[{Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2021measuring}
Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2021.
\newblock Measuring Massive Multitask Language Understanding.
\newblock arXiv:2009.03300.

\bibitem[{Jacovi et~al.(2023)Jacovi, Caciularu, Goldman, and Goldberg}]{jacovi2023stop}
Jacovi, A.; Caciularu, A.; Goldman, O.; and Goldberg, Y. 2023.
\newblock Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks.
\newblock arXiv:2305.10160.

\bibitem[{Kreutzer et~al.(2022)Kreutzer, Caswell, Wang, Wahab, van Esch, Ulzii-Orshikh, Tapo, Subramani, Sokolov, Sikasote, Setyawan, Sarin, Samb, Sagot, Rivera, Rios, Papadimitriou, Osei, Suarez, Orife, Ogueji, Rubungo, Nguyen, M{\"u}ller, M{\"u}ller, Muhammad, Muhammad, Mnyakeni, Mirzakhalov, Matangira, Leong, Lawson, Kudugunta, Jernite, Jenny, Firat, Dossou, Dlamini, de~Silva, {\c{C}}abuk~Ball{\i}, Biderman, Battisti, Baruwa, Bapna, Baljekar, Azime, Awokoya, Ataman, Ahia, Ahia, Agrawal, and Adeyemi}]{kreutzer-etal-2022-quality}
Kreutzer, J.; Caswell, I.; Wang, L.; Wahab, A.; van Esch, D.; Ulzii-Orshikh, N.; Tapo, A.; Subramani, N.; Sokolov, A.; Sikasote, C.; Setyawan, M.; Sarin, S.; Samb, S.; Sagot, B.; Rivera, C.; Rios, A.; Papadimitriou, I.; Osei, S.; Suarez, P.~O.; Orife, I.; Ogueji, K.; Rubungo, A.~N.; Nguyen, T.~Q.; M{\"u}ller, M.; M{\"u}ller, A.; Muhammad, S.~H.; Muhammad, N.; Mnyakeni, A.; Mirzakhalov, J.; Matangira, T.; Leong, C.; Lawson, N.; Kudugunta, S.; Jernite, Y.; Jenny, M.; Firat, O.; Dossou, B. F.~P.; Dlamini, S.; de~Silva, N.; {\c{C}}abuk~Ball{\i}, S.; Biderman, S.; Battisti, A.; Baruwa, A.; Bapna, A.; Baljekar, P.; Azime, I.~A.; Awokoya, A.; Ataman, D.; Ahia, O.; Ahia, O.; Agrawal, S.; and Adeyemi, M. 2022.
\newblock Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10: 50--72.

\bibitem[{Li(2023{\natexlab{a}})}]{li2023estimating}
Li, Y. 2023{\natexlab{a}}.
\newblock Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation.
\newblock arXiv:2309.10677.

\bibitem[{Li(2023{\natexlab{b}})}]{li2023open}
Li, Y. 2023{\natexlab{b}}.
\newblock An Open Source Data Contamination Report for Large Language Models.
\newblock arXiv:2310.17589.

\bibitem[{Li(2023{\natexlab{c}})}]{li2023unlocking}
Li, Y. 2023{\natexlab{c}}.
\newblock Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering.
\newblock arXiv:2304.12102.

\bibitem[{Li et~al.(2023)Li, Dong, Lin, and Guerin}]{li2023compressing}
Li, Y.; Dong, B.; Lin, C.; and Guerin, F. 2023.
\newblock Compressing Context to Enhance Inference Efficiency of Large Language Models.
\newblock arXiv:2310.06201.

\bibitem[{Liu, Zhang, and Liang(2023)}]{liu2023evaluating}
Liu, N.~F.; Zhang, T.; and Liang, P. 2023.
\newblock Evaluating Verifiability in Generative Search Engines.
\newblock arXiv:2304.09848.

\bibitem[{Marie(2023)}]{marie2023}
Marie, B. 2023.
\newblock The Decontaminated Evaluation of GPT-4.
\newblock Accessed: 2023-07-28.

\bibitem[{Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher}]{merity2016pointer}
Merity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2016.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}.

\bibitem[{Narayanan and Kapoor(2023)}]{narayanan_kapoor2023}
Narayanan, A.; and Kapoor, S. 2023.
\newblock GPT-4 and professional benchmarks: the wrong answer to the wrong question.
\newblock Accessed: 2023-07-28.

\bibitem[{OpenAI(2023)}]{openai2023gpt4}
OpenAI. 2023.
\newblock GPT-4 Technical Report.
\newblock arXiv:2303.08774.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training}
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et~al. 2022.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35: 27730--27744.

\bibitem[{Rajpurkar, Jia, and Liang(2018)}]{rajpurkar2018know}
Rajpurkar, P.; Jia, R.; and Liang, P. 2018.
\newblock Know what you don't know: Unanswerable questions for SQuAD.
\newblock \emph{arXiv preprint arXiv:1806.03822}.

\bibitem[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama}
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozi{\`e}re, B.; Goyal, N.; Hambro, E.; Azhar, F.; et~al. 2023.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}.

\bibitem[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le}]{wei2021finetuned}
Wei, J.; Bosma, M.; Zhao, V.~Y.; Guu, K.; Yu, A.~W.; Lester, B.; Du, N.; Dai, A.~M.; and Le, Q.~V. 2021.
\newblock Finetuned language models are zero-shot learners.
\newblock \emph{arXiv preprint arXiv:2109.01652}.

\bibitem[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt}
Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X.~V.; et~al. 2022.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}.

\bibitem[{Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing et~al.}]{zheng2023judging}
Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et~al. 2023.
\newblock Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.
\newblock \emph{arXiv preprint arXiv:2306.05685}.

\end{thebibliography}
