\relax 
\bibstyle{aaai24}
\citation{markman1979realizing}
\citation{scheirer2013open}
\citation{bendale2016towards}
\citation{zhou2021learning}
\citation{xu2023contrastive}
\citation{yang2022convolutional}
\citation{oza2019c2ae}
\citation{xu2023contrastive}
\citation{vaze2022openset}
\citation{guo2021conditional}
\citation{huang2023class}
\citation{scheirer2013open}
\citation{hendrycks2016a}
\citation{bendale2016towards}
\citation{zhou2021learning}
\citation{xu2023contrastive}
\citation{oza2019c2ae}
\citation{chen2020learning}
\citation{sun2020open}
\citation{guo2021conditional}
\citation{yang2022convolutional}
\citation{neal2018open}
\citation{ian2014gan}
\citation{chen2022adversarial}
\citation{kong2022opengan}
\citation{moon2022difficulty}
\citation{liu2023opentext}
\citation{scheirer2013open}
\newlabel{eq:base_target}{{1}{2}{}{}{}}
\newlabel{eq:emp_risk}{{2}{2}{}{}{}}
\newlabel{eq:prob_unknown}{{3}{2}{}{}{}}
\newlabel{eq:rej_unknown}{{4}{2}{}{}{}}
\citation{cover2012elements}
\citation{geirhos2020shortcut}
\citation{wang2018deep}
\citation{cover2012elements}
\citation{huang2023class}
\citation{kong2022opengan}
\citation{shazeer2017moe}
\newlabel{eq:potential_risk}{{5}{3}{}{}{}}
\newlabel{eq:cond_entro}{{6}{3}{}{}{}}
\newlabel{eq:defin_fd}{{7}{3}{}{}{}}
\newlabel{eq:defin_fs}{{8}{3}{}{}{}}
\newlabel{thm:1}{{1}{3}{}{}{}}
\newlabel{eq:cond_entropy}{{9}{3}{}{}{}}
\newlabel{eq:mutual_info}{{10}{3}{}{}{}}
\newlabel{eq:info_geq}{{11}{3}{}{}{}}
\citation{zhou2016learning}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Fig_arch}{{1}{4}{Illustration of the proposed MEDAF method. MEDAF consists of a multi-expert feature extractor to explore diverse representations by constraining the learned attention map of each expert to be mutually different. Then a gating network adaptively generates weights to integrate expert-independent predictions.}{}{}}
\newlabel{eq:gap}{{12}{4}{}{}{}}
\newlabel{eq:cam_relu}{{13}{4}{}{}{}}
\newlabel{eq:diver_loss}{{14}{4}{}{}{}}
\citation{moon2022difficulty}
\citation{he2016deep}
\citation{moon2022difficulty}
\citation{krizhevsky09learning}
\citation{netzer2011reading}
\citation{pouransari2014tiny}
\citation{moon2022difficulty,neal2018open}
\citation{chen2020learning}
\newlabel{table_auroc}{{1}{5}{Comparison of different methods on unknown detection tasks using AUROC. All results are the average value, and the best performance values are in bold.}{}{}}
\newlabel{eq:gating_lg}{{15}{5}{}{}{}}
\newlabel{eq:loss_full}{{16}{5}{}{}{}}
\newlabel{eq:cross_perb}{{17}{5}{}{}{}}
\newlabel{eq:score_ft}{{18}{5}{}{}{}}
\newlabel{eq:score_func}{{19}{5}{}{}{}}
\citation{yoshihashi2019classification}
\citation{yu2015lsun}
\citation{deng2009imagenet}
\newlabel{table_ood}{{2}{6}{The performance of multiple methods on out-of-distribution task. Regarding CIFAR10, CIFAR100 and SVHN are considered as near- and far- out-of-distribution datasets, respectively.}{}{}}
\newlabel{table_f1}{{3}{6}{The macro-F1 results on the CIFAR-10 with various unknown datasets.}{}{}}
\newlabel{fig:Fig_expert}{{2}{6}{Performance with different expert numbers on multiple datasets, with (a) recording closed-set accuracy and (b) recording AUROC.}{}{}}
\newlabel{table_attention}{{4}{6}{The ablation results on loss term and gating network. From top to down, each row represents the results of using a single expert's prediction, adding diverse attention loss, and using average prediction, using weights generated by the gating network.}{}{}}
\citation{russakovsky2015imagenet}
\citation{xu2023contrastive}
\newlabel{table_imgn1k}{{5}{7}{Comparison on ImageNet-1k on different metrics.}{}{}}
\newlabel{fig:Fig_gflops}{{3}{7}{OSR performance against computational cost.}{}{}}
\newlabel{table_close_acc}{{6}{7}{Closed-set classification performance comparison.}{}{}}
\newlabel{fig:Fig_cam}{{4}{7}{Visulizations on CAMs of baseline and MEDAF.}{}{}}
\bibdata{ref}
\gdef \@abspage@last{8}
