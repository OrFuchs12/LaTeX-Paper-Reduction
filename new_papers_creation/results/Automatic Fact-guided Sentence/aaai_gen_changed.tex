\def\year{2020}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai20}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{graphicx}  % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS

\usepackage{latexsym}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{xcolor}
% \usepackage{float} # NOT ALLOWED
% \usepackage[draft]{hyperref} # NOT ALLOWED
% \usepackage{color}


\DeclareMathOperator*{\argmax}{argmax}

\usepackage{url}

\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\renewcommand{\eqref}[1]{Eq.~\ref{#1}}
\newcommand{\appref}[1]{Appendix \ref{#1}}
% \crefformat{section}{\S#2#1#3} % see manual of cleveref, section 8.2.1
% \crefformat{subsection}{\S#2#1#3}
% \crefformat{subsubsection}{\S#2#1#3}

\DeclareMathOperator{\rel}{\boldsymbol{rel}}
\DeclareMathOperator{\mask}{mask}
\DeclareMathOperator{\atten}{atten}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\mm}{\boldsymbol{m}}
\DeclareMathOperator{\ev}{\boldsymbol{ev}}
\DeclareMathOperator{\A}{\boldsymbol{Agr}}
\DeclareMathOperator{\D}{\boldsymbol{Dis}}
\DeclareMathOperator{\N}{\boldsymbol{N}}

% \usepackage[notes=true, done=false]{dtrt}
% \newcommand{\darsh}[1]{(\textbf{\#\# DARSH: #1 \#\#})}
% \newcommand{\tal}[1]{(\textbf{\#\# TAL: #1 \#\#})}




%\nocopyright
%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
% /Title ()
% Put your actual complete title (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case
% Leave the space between \Title and the beginning parenthesis alone
% /Author ()
% Put your actual complete list of authors (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case.
% Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands,
% remove them.

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{caption} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \natbib} -- This package is specifically forbidden -- use the following workaround:
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
%  -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
%  -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai20.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%
\setlength\titlebox{2.5in} % If your paper contains an overfull \vbox too high warning at the beginning of the document, use this
% command to correct it. You may not alter the value below 2.5 in

\title{Automatic Fact-Guided Sentence Modification}

%Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\author{Darsh J Shah,* ~~ Tal Schuster,*  ~~ Regina Barzilay\\
  \mbox{}
Computer Science and Artificial Intelligence Lab\\
Massachusetts Institute of Technology \\
% CSAIL, MIT \\
\{darsh, tals, regina\}@csail.mit.edu
}
 \begin{document}


\maketitle
\let\svthefootnote\thefootnote
\let\thefootnote\relax\footnote{\textbf{*} Order decided by a coin toss.}
\addtocounter{footnote}{-1}
\let\thefootnote\svthefootnote
\begin{abstract}
Online encyclopediae like Wikipedia contain large amounts of text that need frequent corrections and updates. The new information may contradict existing content in encyclopediae. In this paper, we focus on rewriting such dynamically changing articles. This is a challenging constrained generation task, as the output must be consistent with the new information and fit into the rest of the existing document. To this end, we propose a two-step solution: (1) We identify and remove the contradicting components in a target text for a given claim, using a neutralizing stance model; (2) We expand the remaining text to be consistent with the given claim, using a novel two-encoder sequence-to-sequence model with copy attention. Applied to a Wikipedia fact update dataset, our method successfully generates updated sentences for new claims, achieving the highest SARI score. Furthermore, we demonstrate that generating synthetic data through such rewritten sentences can successfully augment the FEVER fact-checking training dataset, leading to a relative error reduction of 13\%.\footnote{ Code: (1) \url{https://github.com/TalSchuster/TokenMasker}\\ (2) \url{https://github.com/darsh10/split_encoder_pointer_summarizer}}


%Online encyclopediae like Wikipedia contain large amounts of text that need frequent corrections and updates. Moreover, new information may contradict existing content in encyclopediae. In this paper, we focus on rewriting such dynamically changing articles. This is a challenging constrained generation task, as the output must be consistent with the new information and fit into the rest of the existing document without contradiction. To this end, we propose a two-step solution: (1) We identify and remove the contradicting components in a target text for a given claim, using a neutralizing entailment model; (2) We expand the remaining text to be consistent with the given claim, using a novel 2-encoder sequence-to-sequence model with copy attention. Applied to a Wikipedia fact-update dataset, our method successfully generates updated sentences for new claims, achieving the highest SARI scores compared to all examined baselines. Furthermore, we demonstrate that generating synthetic data through such rewritten sentences can successfully augment the FEVER fact-checking dataset, leading to a relative error reduction of 13\%.\footnote{All our code and data is submitted and will be released upon acceptance.}
%In this paper, we propose a new task in the domains of fact-updates and fact-checking --- automatic factual modification.
%Many \darsh{wouldn't all knowledge books have text information ?} knowledge books include textual information that requires frequent corrections and updates.\darsh{We don't change many sentences. We make one change at a time.} Given a new fact, the content of many sentences might needed to be update in order to follow the new information
%\darsh{this seems more like an introduction; it can go there}
%could affect the content of many sentences, requiring them to be modified in order to be consistent with the current information.
%\tal{so you can try something else. The previous beginning is not great. For a aaai conf we should focus on the model, not the task. Also, can you say that a task has applications to another task? I think we should motiviate the need and say that we present a model to....}

%\tal{check the abstract of NSMN - https://arxiv.org/pdf/1811.07039.pdf. Not saying we should do exactly the same. But for them it worked}
%\darsh{I don't know how much we can compare against them. They are describing their system for a shared task. We are first presenting the task.}
%\tal{we are presenting a method}
%\darsh{I haven't seen any of Regina's papers having such a long abstract. }
%\tal{again, I'm not saying to do that. But we should focus on the method/ approach. Not present a task imo. Especially for a aaai conf. e.g.: https://arxiv.org/pdf/1903.10122.pdf}
%\darsh{we can add more technical detail, but not increase the size too much}

%We propose a new task of automatic fact-guided editing, with applications to fact-updates and fact-checking. In this setting, \done\tal{In this setting...} a guiding claim \done\tal{"guiding claim"?} \done\darsh{I am fine with removing fact-update, just felt that it helps emphasize the good task even at the cost of the flow of the phrase} provides contrasting information to existing knowledge, requiring a rewrite that accommodates the \done\tal{"that accomoddates"?}  new information.
%Encyclopediae contain large amounts of text needing frequent corrections and updates. \darsh{Given a new fact, we would need to modify the text to generate the corresponding text}%Modifications are imminent when presented with a relevant text containing the new information to be updated.
%\darsh{two contradictions together}
%In this paper, we automate this process, wherein a guiding claim provides contrasting information to existing knowledge, requiring a rewrite --- a challenging constrained generation task necessitating the output to be consistent with a provided reference. %Such functionality would be useful for knowledge-book fact-updates and in modifying legal documents to conform to specific constraints.\tal{not sure that the prev sentence is good for abstract. maybe focus on what we do in the paper, i.e. Wikipedia updates} This could even help augment datasets for fact verification which are currently manually compiled.\tal{the next sentence doesn't flow. Should come after the challenges. maybe move the "This presents..." sentence to here.}
%To this end, we propose a two-step solution: (1) We apply a neural interpretability style entailment model --- to a pair of claim and target text, identifying the contradicting components in the text and removing them; (2) We expand this residual text to be consistent with the given claim, using a novel 2-encoder sequence to sequence model with copy attention. Applied to a Wikipedia fact-update dataset, our method successfully generates updated sentences for new claims, achieving the highest SARI scores, compared to all examined baselines. Similarly, we demonstrate that generating synthetic data through such rewritten sentences can successfully augment the FEVER fact-checking dataset, leading to a relative error reduction of 13\%.\footnote{All our code and data is submitted and will be released upon acceptance.}
%In this paper, we propose a new task in the domain of fact-checking --- fake evidence generation. Given a claim and refuting evidence, we learn how to rewrite the evidence to support the claim. This presents a challenging constrained generation task requiring the output to be consistent with a provided reference. From the practical viewpoint, this could help in augmenting datasets for fact verification which are currently manually compiled. To this end, we propose a two-step solution: (1) For a provided claim and evidence, we identify the contradicting components in the evidence and remove them; (2) We expand this residual evidence to be consistent with the given claim. Applied to the FEVER claim verification dataset, our method successfully generates supporting evidence for the fabricated claims, achieving the highest scores on human evaluation, compared to competitive baselines. We also demonstrate that those rewritten sentences can successfully augment the dataset, leading to a relative error reduction of 9.1\%.\footnote{All our code and data is submitted and will be released upon acceptance.}

%Finally, we show that our method can be used to augment a deficient training dataset, reducing the relative error by 8.2\% on a symmetric test set.


%In this paper, we consider the task of modifying a text evidence with respect to a reference claim. Given a fabricated claim, refuted by an evidence, we aim to generate an altered supporting evidence for it. Currently, fact checking datasets are asymmetric, missing supporting evidence for such false claims.To sidestep the lack of supervision, we propose a two step solution. First, we identify the contradicting components in the evidence and remove them. Next, we use this residual evidence and the claim to generate a new sentence that supports the claim. Applied on the FEVER claim verification dataset, our method successfully generates supporting evidence for 91\% of the fabricated claims, measured by a pretrained classifier.Finally, we show that our method can be used to augment the original biased dataset, reducing the relative error by 8.2\% on an unbiased test set.
\end{abstract}
\section{Introduction}
\label{sec:introduction}
Online text resources like Wikipedia contain millions of articles that must be continually updated. Some updates involve expansions of existing articles, while others modify the content. In this work, we are interested in the latter scenario where the modification contradicts the current articles. Such changes are common in online sources and often cover a broad spectrum of subjects ranging from the changing of dates for events to modifications of the relationship between entities. In these cases, simple solutions like negating the original text or concatenating it with the new information would not apply. In this work, our goal is to automate these updates. Specifically, given a claim and an outdated sentence from an article, we rewrite the sentence to be consistent with the given claim while preserving non-contradicting content.

\begin{figure}[t]
  \centering
\includegraphics[width=0.47\textwidth]{obama.pdf}
\caption{Our fact-guided update pipeline. Given a claim which refutes incorrect information, a masker is applied to remove the contradicting parts from the original text while preserving the rest of the context. Then, the residual neutral text and claim are fused to create an updated text that is consistent with the claim.
}\label{fig:Obama_example}
\end{figure}


Consider the Wikipedia update scenario depicted in Figure \ref{fig:Obama_example}. The claim, informing that \textit{23 of 43} minority stakeholdings are significant, contradicts the old information in the Wikipedia sentence, requiring modification. Directly learning a model for this task would demand supervision, i.e.\ demonstrated updates with the corresponding claims. For Wikipedia, however, the underlying claims which drive the changes are not easily accessible. Therefore, we need to utilize other available sources of supervision.

In order to make the corresponding update, we develop a two step solution: (1) Identify and remove the contradicting segments of the text (in this case, \textit{28 of their 42 minority stakeholdings}); (2) Rewrite the residual sentence to include the updated information (e.g.\ fraction of significant stakeholdings) while also preserving the rest of the content.

For the first step, we utilize a neutrality stance classifier as indirect supervision to identify the polarizing spans in the target sentence. We consider a sentence span as polarizing if its absence increases the neutrality of the claim-sentence pair. To identify and mask such sentence spans, we introduce an interpretability-inspired ~\cite{lei-rational} neural architecture to effectively explore the space of possible spans. We formulate our objective in a way that the masking is minimal, thus preserving the context of the sentence.

For the second step, we introduce a novel, two-encoder decoder architecture, where two encoders fuse the claim and the residual sentence with a more refined control over their interaction.


We apply our method to two tasks: automatic fact-guided modifications and data augmentation for fact-checking. On the first task, our method is able to generate corrected Wikipedia sentences guided by unstructured textual claims. Evaluation on Wikipedia modifications demonstrates that our model's outputs were the most successful in making the requisite updates, compared to strong baselines. On the FEVER fact-checking dataset, our model is able to successfully generate new claim-evidence supporting pairs, starting with claim-evidence refuting pairs --- intended to reduce the bias in the dataset. Using these outputs to augment the dataset, we attain a 13\% decrease in relative error on an unbiased evaluation set.
% \darsh{maybe u would like to do it in a different way}\tal{this paragrpah needs some work. I'll try to think how to do it}
%
%Online text resources like Wikipedia contain millions of articles that must be continually updated. Some updates involve expansions of existing articles, while others modify the content. In this work, we are interested in the latter scenario where the modification contradicts the input articles. Such changes are common in online sources and often cover a broad spectrum of subjects ranging from the changing of dates for events to modifications of the relationship between entities. In these cases, simple solutions like negating the original text or concatenating it with the new information would not apply. In this work, our goal is to automate these updates. Specifically, given a claim and an outdated sentence of the article, we rewrite the sentence to be consistent with the given claim, while preserving the sentence in the context of the article.

%\begin{figure}[t]
%  \centering
%\includegraphics[width=0.5\textwidth]{figures/obama.pdf}
%\caption{An illustration of our model application. Given a fact-update claim which refutes incorrect information, a masker is applied to remove the contradicting parts from the original text while preserving the rest of the context. Then, the residual neutral text and claim are fused to create an updated text that is consistent with the claim.
%}\label{fig:Obama_example}
%\end{figure}

%\tal{move to before last paragprah and edit accordignly}
%Consider the Wikipedia update scenario depicted in Figure \ref{fig:Obama_example}. The claim, informing that \textit{23 of 43} minority stakeholdings are significant, contradicts the old information in the Wikipedia sentence, requiring modification. Directly learning a model for this task would demand supervision i.e. demonstrated updates with the corresponding claims. For Wikipedia, however, the underlying fact updates which drive the changes are not easily accessible. Therefore, we need to utilize other available sources of supervision.

%In order to make the corresponding update, we develop a two step solution: (1) Identify and remove the contradicting segments of the text, in this case, \textit{28 of their 42 minority stakeholdings}; (2) Rewrite the residual sentence to include the updated fraction of significant stakeholdings --- \textit{23 of 43} --- while also preserving the rest of the content. For the first step, we utilize a neutrality stance classifier as indirect supervision, to explicitly identify the polarizing spans in the target sentence. We consider a sentence span as polarizing if its absence increases the neutrality of the claim-sentence pair. To identify and mask such sentence spans, we use an interpretability-inspired~\cite{lei-rational} neural architecture to effectively explore the space of possible spans. We formulate our objective in a way that the masking is minimal, thus preserving the context of the sentence. In the second step, we rewrite the residual text to be consistent with the given claim (see Figure \ref{fig:Obama_example}). We introduce a novel, two-encoder decoder architecture, where two encoders fuse the claim and the residual sentence for a more refined control over their interaction.
%Our proposed solution consists of two steps: (1) identifying and removing the specific segments of existing text which contradict the given fact; (2) rewriting the sentence to include the new information while preserving the rest of the content.

%Analogously, typical work on fact-checking focuses on the detection task~\cite{fakenews-review}. This requires expensive manually compiled datasets ~\cite{thorne-vlachos-2018-automated}, that might contain unnecessary biases ~\cite{schuster2019towards}. Even for such problems, our complementary task of justification generation for new claims can allow for dataset augmentation and an alleviation of the existing bias. In consolidation, our goal is to rewrite a provided sentence to support the guiding claim. Specifically, the task involves two aspects: (1) identifying the specific segments of existing text which contradict the new information; (2) maximally preserving the rest of the text so that the modification fits the source document.


%Most of the current work on fact-checking focuses on the detection task~\cite{fakenews-review,thorne-vlachos-2018-automated}. In this paper, we look into the complementary aspect of this problem, fake evidence generation. The goal is to rewrite a provided evidence to support a given claim. Specifically, this task involves two aspects: (1) identifying the specific segments of the evidence that contradict the claim and modifying it accordingly; (2) maximally preserving the rest of the evidence sentence so that it fits in the source document. Making progress in this area will ultimately benefit the detection task since understanding the relation between claim and evidence is key for both. Furthermore, the generated sentences can be used to augment manually compiled datasets for fact verification.% to alleviate potential bias.

%Figure \ref{fig:Obama_example} shows an example of a fact-update claim and original knowledge-book information where the marriage year of \textit{Barack} and \textit{Michelle Obama} needs correcting. We identify the span in the text responsible for the contradiction, remove it and copy \textit{1992} from the claim. In this way, the claim guides us towards a correct update.
%Table \ref{tab:examples} shows two examples of input claims and evidence, along with the rewritten evidence. The first case demonstrates a scenario where the year of marriage of \textit{Barack} and \textit{Michelle Obama} has been fabricated. We identify the span in the evidence responsible for this, remove it and copy \textit{1980} from the claim. In the second example, when \textit{New Girl} is falsely being called a movie, we need to remove the phrase in the evidence acknowledging it to be a show. Using the semantics of the inputs, we need to generate a phrase like \textit{``horror film series''} which supports the claim and maintains the structure of the evidence.
%Our model is able to identify the phrase responsible for this, and to modify it in the generation phase. In the second example, the show \textit{New Girl} is being called a movie. In order to preserve the structure of the sentence, our model generates the phrase of the same length \textit{``horror film series''}, which also satisfies the false claim.

%\begin{table}[t]
%    \centering
%	\begin{tabular}{p{0.2cm}p{7cm}}
%	\toprule
%  $C$   &  \textit{Michelle Obama has been married to Barack Obama since 1980.} \\
%  $E$  & \textit{Barack and Michelle \textbf{\textcolor{red}{married in 1992}} and have two daughters.} \\
%   $E_{\emptyset}$ & \textit{Barack and Michelle \_ in \_ and have two \_ .} \\
%   $E^+$ & \textit{Barack and Michelle \textbf{\textcolor{blue}{married in 1980}} and have two daughters.} \\
%	\midrule
%  $C$   &  \textit{New Girl is a movie.} \\
%  $E$  & \textit{New Girl is an American \textbf{\textcolor{red}{sitcom television series}} that premiered on Fox on September 20 , 2011.} \\
%   $E_{\emptyset}$ & \textit{It reached number one again in 1991 for another five weeks when the same version was re-released , eventually becoming the UK's third-best-\_ \_ of all time.} \\
%   $E^+$ & \textit{New Girl is an American \textbf{\textcolor{blue}{horror film series}} that premiered on Fox on September 20 , 2011.} \\
%  \bottomrule
%\end{tabular}
%    \caption{Examples of claim ($C$) and evidence ($E$) with a refute relation from the FEVER dataset. The contradicting spans in the evidence are marked in red. The new, model-generated evidence ($E^+$) supports the original claim, with the generation marked in blue.}
%    \label{tab:examples}
%\end{table}



% From a technical viewpoint,


%From the technical viewpoint, this constrained generation task is challenging. Simple solutions like concatenating the target sentence and the guiding claim are likely to be both contradictory in parts, as well as contextually redundant. Existing text rewriting approaches, like sentence fusion, do not explicitly handle the presence of contradictory elements of input sentences which are inherent in our task. Finally, there is no existing corpora which provides such pairs for supervised rewriting.

%\done\tal{this sentence repeats the paragraph "our proposed".. maybe here talk more explicitly (For the first step of our solution....)}

%Directly learning a model for this task would require supervision --- sufficient paired instances of claim, incorrect sentence and corrected sentence for training. For Wikipedia, however, the underlying fact updates which drive the model are not easily accessible. Therefore, to circumvent the lack of supervision, we construct a two step solution.
  %\tal{I've removed this sentence. It doesn't make much sense in the new fact-update storyline}

 %This generation task attends an unrestricted set of information aspects, making it challenging. \done\tal{not sure I understand "unknown set of information aspects"} Simple solutions like concatenating the original sentence and claim are likely to be both contradictory in parts, as well as contextually redundant. Existing text rewriting approaches, like sentence fusion, do not explicitly handle the presence of contradictory elements of input sentences which are inherent in our task. Finally, there is no existing corpora that provides such tuples for supervised rewriting.
% The output is generated by a sequence-to-sequence model, which is trained in an auto-encoder style objective.

%We apply our method to two tasks: automatic fact-guided modifications and data augmentation for fact-checking. On the automatic fact-update task, our method is able to generate corrected Wikipedia sentences guided by human claims. Evaluation against real Wikipedia sentences demonstrates that our model's outputs were the most successful in making the requisite updates, compared to strong baselines. On the FEVER fact-checking dataset, our model is able to successfully generate new claim-evidence supporting pairs, starting with claim-evidence refuting pairs --- intended to reduce the bias in the dataset. Using these outputs to augment the dataset, we attain a 13\% decrease in relative error on an unbiased evaluation set.
%We evaluate our method on two tasks. On an automatic fact-update evaluation set , introduced by us, our method is able to generate new Wikipedia sentences guided by human claims. Comparison with actual Wikipedia sentences, demonstrates that our models' outputs were the most successful in making updates, compared to baselines. Alternatively, on the FEVER fact-checking dataset, our model is able to successfully generate new claim-evidence supporting pairs. Using these outputs to augment the dataset, we get upto a 13\% drop in relative error on an unbiased symmetric evaluation set.
%We evaluate our method using the FEVER dataset~\cite{fever}, which contains human-generated claims based on sentences from Wikipedia. Our method is able to generate supporting evidence sentences for all the 41850 false claims. A qualitative human evaluation confirms that our models' outputs scored the highest in supporting false claims, compared to baselines. Using the generated outputs to augment the dataset, we get upto a 13\% drop in relative error on a test set where each claim has a refuting and supporting evidence sentence.
\section{Related Work}
\label{sec:related_work}
\paragraph{Text Rewriting}

There have been several recent advancements in the field of text rewriting, including style transfer
~\cite{shen2017style,zhang2018style,chen-etal-2018-learning} and sentence fusion ~\cite{barzilay2005sentence,narayan-etal-2017-split,geva2019discofuse}. Unlike previous approaches, our sentence modification task addresses potential contradictions between two sources of information.

Our work is fairly related to the approach of \cite{li-etal-2018-delete}, which separates the task of sentiment transfer into deleting strong markers of sentiment in a sentence and retrieving markers of the target label to generate a sentence with the opposite sentiment. In contrast to such work, where the requisite modification is along a fixed aspect (e.g.\ sentiment), in our setting, an arbitrary input sentence (the claim) dictates the space of desired modifications. Therefore, in order to succeed at our task, a system should understand the varying degree of polarization in the spans of the outdated sentence against the claim before modifying the sentence to be consistent with the claim.%a system should reason and exploit the degree of polarization of various components of the evidence with the claim.

\paragraph{Wikipedia Edits}
Wikipedia edit history has been analyzed for insights into the kinds of modifications made
\cite{daxenberger-gurevych-2013-automatically,yang-etal-2017-identifying-semantic,faruqui-etal-2018-wikiatomicedits}. The edit history has also been used for text generation tasks such as sentence compression and simplification
\cite{yatskar-etal-2010-sake}, paraphrasing \cite{max-wisniewski-2010-mining} and writing assistance
\cite{cahill-etal-2013-robust}. In this work, we are interested in the novel task of automating the editing process with the guidance of a textual claim.

\paragraph{Fact Verification Datasets}

The growing interest in automatic fake news detection led to the development of several fact verification datasets
~\cite{vlachos-riedel-2014-fact,wang-2017-liar,rashkin-etal-2017-truth,fever}. FEVER, the largest fact-checking dataset, contains 185K human written fake and real claims, generated by crowd-workers, in context of sentences from Wikipedia articles.
This dataset contains biases that allow a model to identify many of the false claims without any evidence~\cite{schuster2019towards}.
This bias affects the generalization capabilities of models trained on such data.
In this work, we show that our automatic modification method can be used to augment a fact-checking dataset and to improve the inference of models trained on it.




\paragraph{Data Augmentation}
Methods for data augmentation are commonly used in computer vision~\cite{perez2017effectiveness}. There have been recent successes in NLP where augmentation techniques such as paraphrasing and word replacement were applied to text classification ~\cite{kobayashi2018contextual,wu2018conditional}. Adversarial examples in NLI with syntactic modifications can also be considered as methods of data augmentation ~\cite{iyyer-etal-2018-adversarial,zhang2019paws}.
 In this work, we create constrained modifications, based on a reference claim, to augment data for our task at hand. Our additions are specifically aimed towards reducing the bias in the training data, by having a false claim appear in both ``Agrees'' and ``Disagrees'' classes.


%\paragraph{Text Rewriting}
% \begin{enumerate}
%     \item Style transfer
%     \item Fusion, summarization
% \end{enumerate}
%There have been several recent advancements in the field of text rewriting, including style transfer
%~\cite{shen2017style,hu2017toward,NIPS2018_7959,NIPS2018_7757,zhang2018style,chen-etal-2018-learning} and sentence fusion ~\cite{barzilay2005sentence,narayan-etal-2017-split,botha-etal-2018-learning,geva2019discofuse}.
%~\cite{shen2017style,zhang2018style,chen-etal-2018-learning} and sentence fusion ~\cite{barzilay2005sentence,narayan-etal-2017-split,geva2019discofuse}. Unlike previous approaches, our sentence modification task addresses potential contradictions between two sources of information.
%In this paper, we introduce a method to perform a constrained fusion that addresses potential contradicting information between the two sources.
%\done\tal{instead: Unlike previous approaches, our sentence modification task requires to address potential contradictions between two sources of information.}

%Our work is fairly related to the approach of \cite{li-etal-2018-delete}, which separates the task of sentiment transfer into deleting strong markers of sentiment in a sentence and retrieving markers of the target label to generate a sentence with the opposite sentiment. In contrast to such work, where the requisite modification is along a fixed aspect (e.g.\ sentiment), in our setting, an arbitrary input sentence (the claim) dictates the space of desired modifications. Therefore, in order to succeed at our task, a system should understand the varying degree of polarization in the spans of the outdated sentence against the claim, before modifying the sentence to be consistent with the claim.%a system should reason and exploit the degree of polarization of various components of the evidence with the claim.

%\paragraph{Wikipedia Edits}
%Wikipedia edit history have been analyzed for insights into the kinds of modifications made %\cite{daxenberger-gurevych-2012-corpus,daxenberger-gurevych-2013-automatically,yang-etal-2017-identifying-semantic}
%\cite{daxenberger-gurevych-2013-automatically,yang-etal-2017-identifying-semantic,faruqui-etal-2018-wikiatomicedits}. The edit history has also been used for text generation tasks such as sentence compression and simplification %\cite{yamangil-nelken-2008-mining,yatskar-etal-2010-sake}
%\cite{yatskar-etal-2010-sake}, paraphrasing \cite{max-wisniewski-2010-mining} and writing assistance %\cite{zesch-2012-measuring,cahill-etal-2013-robust}.
%\cite{cahill-etal-2013-robust}. In this work, we are interested in the novel task of automating the editing process, guided by a claim.

%\paragraph{Fact Verification Datasets}
% 1. many datsets, fever the largest.
% 2. fever is biased
% 3. models are inspired by the nli task (where the datasets are also biased)

%The growing interest in automatic fake news detection led to the development of several fact verification datasets %~\cite{vlachos-riedel-2014-fact,ferreira-vlachos-2016-emergent,wang-2017-liar,rashkin-etal-2017-truth,fever}.
%~\cite{vlachos-riedel-2014-fact,wang-2017-liar,rashkin-etal-2017-truth,fever}.%The largest currently available dataset is FEVER, constructed by \cite{fever}.
% FEVER, the largest fact-checking dataset, contains 185K human written fake and real claims, generated by crowd-workers, incontext of sentences from Wikipedia articles.
%This dataset contains biases that allow a model to identify many of the false claims without any evidence~\cite{schuster2019towards}.
% This is similar to other reasoning datasets~\cite{mccoy2019right}, where a classifier that is aware of only one of the sentences can perform surprisingly well %\cite{poliak2018hypothesis,gururangan2018annotation}\cite{poliak2018hypothesis}.
%This bias affects the generalization capabilities of models trained on such data.
%In this work, we show that our automatic modification method can be used to augment fact-checking datasets and to improve the inference of models trained on them.
% We address this issue by automatically augmenting the training with cases that are currently absent --- supported false claims.



%\paragraph{Data Augmentation}
%Methods for data augmentation are commonly used in computer vision~\cite{perez2017effectiveness}. There have been recent successes in NLP where augmentation techniques such as paraphrasing and word replacement were applied to text classification ~\cite{kobayashi2018contextual,wu2018conditional}. Adversarial examples in NLI with syntactic modifications can also be considered as methods of data augmentation ~\cite{iyyer-etal-2018-adversarial,zhang2019paws}.
% In this work, we create constrained modifications, based on a reference claim, to augment data for our task at hand. Our additions are specifically aimed towards reducing the bias in the training data, by having a false claim appear in both ``refutes'' and ``supports'' classes.




\section{Model}
\paragraph{Problem Statement}
\label{sec:prob_state}

We assume access to a corpus $\mathcal{D}$ of claims and knowledge-book sentences. Specifically, $\mathcal{D} = \{\{C_1 ,..., C_n\}, \{S_1, ..., S_m\}\}$, where $C$ is a short factual sentence (claim), and $S$ is a sentence from Wikipedia. Each pair of claim and Wikipedia sentence has a relation $\rel(S, C)$, of either agree ($\A$), disagree ($\D$) or neutral ($\N$). In this corpus, a Wikipedia sentence $S$ is defined as outdated with respect to $C$ if $\rel(S,C)=\D$ and updated if $\rel(S,C)=\A$. The neutral relation holds for pairs in which the sentence doesn't contain specific information about the claim.


Our goal is to automatically update a given sentence $S$, which is outdated with respect to a $C$. Specifically, given a claim and a pair for which $\rel(S, C) = \D$, our objective is to apply minimal modifications to $S$ such that the relation of the modified sentence $S^+$ will be: $\rel(S^+, C) = \A$. In addition, $S^+$ should be structurally similar to $S$.
%Our goal is to modify an evidence, such that for a false claim, the pair's relation changes. Specifically, given a false claim and a pair for which $\rel(E, C) = \textit{REF}$, our objective is to apply minimal modifications to $S$ such that the relation of the modified evidence $S^+$ will be: $\rel(S^+, C) = \textit{SUP}$. In addition, $S^+$ should be syntactically and structurally similar to $S$.



\paragraph{Framework}


% Our dataset has fact-update claims paired with either contradicting or consistent sentences, leaving no direct supervision for the task of updating old sentences using guiding claims.

Currently, to the best of our knowledge, there is no large dataset for fact-guided modifications. Instead, we utilize a large dataset with pairs of claims and sentences that are labeled to be consistent, inconsistent or neutral.
In order to compensate the lack of direct supervision, we develop a two-step solution. First, using a pretrained fact-checking classifier for indirect supervision, we identify the polarizing spans of the outdated sentence and mask them to get a $S^{\emptyset}$ such that $\rel(S^{\emptyset},C) = \N$. Then, we fuse this pair to generate the updated sentence which is consistent with the claim. This is done with a sequence-to-sequence model trained with consistent pairs through an auto-encoder style objective. The two steps are trained independently to simplify optimization (see \figref{fig:pipeline}).


\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\textwidth]{mask_gen.pdf}
\caption{Illustrating the flow of the masker module.
}\label{fig:mask_gen}
\end{figure*}
\subsection{Masker: Eliminate Polarizing Spans} \label{sec:mask_gen}

In this section we describe the module to identify the polarizing spans within a Wikipedia sentence. Masking these spans ensures that the residual sentence-claim pairs attain a neutral relation. Here, neutrality is determined by a classifier trained on claim and Wikipedia sentence pairs as described below. Using this classifier, the masking module is trained to identify the polarizing spans by maximizing the neutrality of the residual-sentence and claim pairs. In order to preserve the context of the original sentence, we include optimization constraints to ensure minimal deletions. This approach is similar to neural rationale-based models~\cite{lei-rational}, where a module tries to identify the spans of the input that justify the model's prediction.


\paragraph{Neutrality Masker} Given a knowledge-book sentence ($S$) and a claim ($C$), the masker's goal is to create $S^{\emptyset}$ such that $\rel(S^{\emptyset},C) = \N$. For the original sentence with $l$ tokens, $S = \{x_i\}_{i=1}^{l}$, the output is a mask $m\in[0,1]^l$. The neutral sentence $S^{\emptyset}$ is constructed as:
\begin{equation}
  {S^\emptyset_i}=\begin{cases}
    x_i, & \text{if $m_i=0$}\\
    \star, & \text{otherwise}
  \end{cases}
\end{equation}
where $\star$ is a special token.\footnote{The special token is treated as an out-of-vocabulary token for the following models.} The details of the masker architecture are stated below and depicted in \figref{fig:mask_gen}.





\paragraph{Encoding}
We encode $S$ with a sequence encoder to get $e_i = f(x; \boldsymbol{w}_f)_i$.
Since the neutrality of the sentence needs to be measured with respect to a claim, we also encode the claim and enhance $S$'s representations with that of $C$ using attention mechanism. Formally, we compute
\begin{equation}
	z_i = e_i + \sum_{j=1}^n a_{i,j}\cdot c_j ,
\end{equation}
where $c_j$ are the encoded representations of the claim and $a_{i,j}$ are the parameterized bilinear attention ~\cite{kim2018bilinear} weights computed by:
\begin{equation}
	a_{i,j} = \softmax_j(\atten(e_i, c_j)),
\end{equation}

\begin{equation}
	\atten(e_i, c_j) = e_i W c_j^T + b.
\end{equation}
Finally, the aggregated representations are used as input to a sequence encoder $g(\cdot; \boldsymbol{w}_g)$.


\paragraph{Masking}

The encoded sentence is used to predict a per token masking probability:
\begin{equation}
	p(m_i = 1) = \sigma(g(z; \boldsymbol{w}_g)_i).
\label{eq:mask_prob}
\end{equation}

Then, the mask is applied to achieve the residual sentence:
\begin{equation}
    S^{\emptyset} = S \circ (1-m),
\end{equation}
where $\circ$ denotes element-wise multiplication. During training, we perform soft deletions over the token embeddings and add the out-of-vocabulary embedding in place. During inference, the values of $m$ are rounded to create a discrete mask.



\paragraph{Training}
A pretrained fact-checking neutrality classifier's prediction $\rel(S,C)$ is used to guide the training of the masker. In order to encourage maximal retention of the context, we utilize a regularization term to minimize the fraction of the masked words. The joint objective is to minimize:
\begin{equation}
    \small{
    \mathcal{L}(S,C,m)\!= - \log \left(p(\rel(S^\emptyset,C){=}\N)\right) + \frac{\lambda}{l} \sum_{i=1}^{l} m_i.
    }
\label{eq:mask_gen_loss}
\end{equation}


\paragraph{Fact-checking Neutrality Classifier} \label{sec:neutral_classifier}
 Our fact-checking classifier is pretrained on agreeing and disagreeing $(S,C)$ pairs from $\mathcal{D}$, in addition to neutral examples constructed through negative sampling. For each claim we construct a \textit{neutral} pair by sampling a random sentence from the same paragraph of the polarizing sentence, making it contextually close to the claim, but unlikely to polarize it. We pretrain the classifier on these examples and fix its parameters during the training of the masker.



\paragraph{Optional Syntactic Regularization}


Currently the model is trained with distant supervision, so, we pre-compute a valid neutrality mask as additional signal, when possible. To this end, we parse the original sentences using a constituency parser and iterate over continuous syntactic phrases by increasing length. For each sentence, the shortest successful neutrality mask (if any) is selected as a target mask.\footnote{If there are several successful masks of the same length, we use the one with the highest neutrality score.} In the event of successfully finding such a mask, the masking module is regularized to emulate the target mask by adding the following term to \eqref{eq:mask_gen_loss}:
\begin{equation}
    \frac{1}{l} || m - m' ||^2,
    \label{eq:boots}
\end{equation}
where $m'$ is the target mask.

Empirically, we find that the model can perform well even without this regularization, but it can help to stabilize the training. Additional details and analysis are available in the appendix.
% \newline \newline

\subsection{Two-encoder Pointer Generator: Constructing a Fact-updated Sentence} \label{sec:pointer_gen}
In this section we describe our method to generate an output which agrees with the claim. If the earlier masking step is done perfectly, the merging boils down to a simple fusion task. However, in certain cases, especially ones with a strong contradiction, our minimal deletion constraint might leave us with some residual contradictions in $S^{\emptyset}$. Thus, we develop a model which can control the amount of information to consider from either input.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\textwidth]{inference_pipeline_wtrain.pdf}
\caption{A summary of our pipeline. Given a sentence that is inconsistent with a claim, a masker is applied to mask out the contradicting parts from the original text while preserving the rest of the content. Then, the residual neutral text and claim are fused to create an updated text that is consistent with the claim. The Masker and the Two-Encoder Generator are trained separately.
 \label{fig:pipeline}}
\end{figure*}

%\paragraph{2-Encoder Pointer Generator}
We extend the pointer-generator model of \cite{pointer-generator} to enable multiple encoders. While sequence-to-sequence models support the encoding of multiple sentences by simply concatenating them, our use of a per input encoder allows the decoder to better control the use of each source. This is especially of interest to our task, where the context of the claim must be translated to the output while ignoring contradicting spans from the outdated Wikipedia sentence.

Next, we describe the details of our generator's architecture. Here, we use one encoder for the outdated sentence and one encoder for the claim. In order to reduce the size of the model, we share the parameters of the two encoders. The model can be similarly extended to any number of encoders.


% \begin{figure}[t]
% \centering
% \includegraphics[width=0.5\textwidth]{figures/split_encoder.png}
% \caption{The 2-encoder pointer generator model on the input \textit{Fake Claim: "George H. W. Bush retired from all politics in 1980 ."} and the \textit{"Residual Evidence: Bush left office in \_ ."}}, in between its generation of \textit{"Bush left "}.\label{fig:split_encoder_model}
% \end{figure}
% \paragraph{Encoder-Decoder architecture}

\paragraph{Encoding}
At each time step $t$, the decoder output $h^t$, is a function of a weighted combination of the two encoders' context representations $r^{t}$, the decoder output in the previous step $h^{t-1}$ and the representation of the word output at the end of the previous step $emb(y^{t-1})$:

\begin{equation}
    h^t = \textit{RNN}([r^t, emb(y^{t-1})], h^{t-1}).
    \label{eq:decoder}
\end{equation}

As the decoder should decide at each time step which encoder to attend more, we introduce an encoder weight $\alpha$. The shared encoder context representation $r^t$ is based on their individual representations $r^t_1$ and $r^t_2$:

\begin{align*}
    \alpha = \sigma(u^{T}_{enc}[r^t_{1}, r^t_{2}]), \\
    r^t = \alpha \cdot r^t_{1} + (1-\alpha)r^t_{2}.
    \tag{10}
\end{align*}

The context representation $r_i^t$ ($i{\in}\{1,2\}$) is the attention score over the encoder representation $r_i$ for a particular decoder state $h^{t-1}$:

\begin{align*}
    z_j^{t} &= u^{T}\tanh(r_{i,j} + h^{t-1}), \\
    a_{i}^t &= \softmax(z^t), \\
    r_i^t &= \sum_{j}a_{i,j}^{t}{r_{i,j}}.
    \tag{11} \label{eq:rep}
\end{align*}


\paragraph{Decoding}
Following standard copy mechanism, predicting the next word $y^t$, involves deciding whether to \textit{generate} ($p_{gen}$) or \textit{copy}, based on the decoder input $x^{t} = [r^t, emb(y^{t-1})]$, the decoder state $h^{t}$ and context vector $r^{t}$:


\begin{equation*}
    p_{gen} = \sigma(v^{T}_{x}x^{t} + v^{T}_{h}h^{t} + v^{T}_{r}r^{t}).
    \label{eq:generate_copy}
    \tag{12}
\end{equation*}

In case of copying, we need an additional gating mechanism to select between the two sources:

\begin{equation*}
    p_{enc1} = \sigma(u^{T}_{x}x^{t} + u^{T}_{h}h^{t} + u^{T}_{r}r^{t}).
    \label{eq:encoder_voc}
    \tag{13}
\end{equation*}


When generating a new word, the probability over words from the vocabulary is computed by:

\begin{equation*}
    P_{vocab} = \softmax(V^{T}[h^t,r^t]).
    \label{eq:vocab}
    \tag{14}
\end{equation*}

The final output of the decoder at each time step is then computed by:

\begin{align*}
    P(w) &= p_{gen}P_{vocab}(w)  + \\
    & (1-p_{gen})(p_{enc1})\sum_{j:w_{j}=w} a_{1,j}^{t} + \\
    & (1-p_{gen})(1 - p_{enc1})\sum_{j:w_{j}=w} a_{2,j}^{t}, \\
     y^t = & \operatorname{argmax}_w P(w).
     \tag{15} \label{eq:pointer_out}
\end{align*}
where $a^t$ are the input sequence attention scores from \eqref{eq:rep}.
% The generated output sequence of length $m$, is \{$y^1, y^2 ... y^m$\}, where $y^m$ is the end-of-sequence token.

\paragraph{Training}
% Since we have the supervision of supporting pairs with only fact-update claims and supporting Wikipedia sentences, the generator module is trained only on the such pairs. Our model is trained to reconstruct the original sentence using $C$ and the residual $S^{\emptyset}$ produced by the masker module.

Since we have no training data for claim guided sentence updates, we train the generator module to reconstruct a sentence $S$ to be consistent with an agreeing claim $C$. The training input is the residual up-to-date neutral sentence $S^{\emptyset}$ and the guiding claim $C$.

During inference, we utilize only guiding claims and residual outdated sentences $S^{\emptyset}$ to create $S^+$. While generating the updated sentences $S^+$, we would like to preserve as much context as possible from the contradicting sentence, while ensuring the correct relation with the claim. Therefore, for each case, if the later goal is not achieved, we gradually increase the focus on the claim by increasing $\alpha$ and $p_{enc1}$ values until the output $S^+$ satisfies $\rel(S^+,C)=\A$, or until a predefined maximum weight.

% apart from using the model parameters as is, we might need to increase the decoder's relative focus on $C$, especially when $S^{\emptyset}$ has some residual contradictions. We achieve this by gradually increasing $\alpha$ and $p_{enc1}$ values, so that the output $S^+$ can satisfy $\rel(S^+,C)=\textit{SUP}$.%, determined by a fact-checking classifier.

%As discussed above, in order to prevent contradictions while maintaining the context, our task requires a per case control over the amount of context preserved from the evidence. Suppose we encode the claim with the first encoder, increasing the values of $z$ and $p_{enc1}$ will attend more to the claim, neglecting information from the evidence. Therefore, during inference, we gradually increase their values from $0.5$ to $1$ until the output $S^+$ satisfies $\rel(S^+,C)=\textit{SUP}$, determined by a pretrained classifier.

\section{Experimental Setup}
\label{sec:experiments}
We evaluate our model on two tasks: (1) Automatic fact updates of Wikipedia sentences, where we update outdated wikipedia sentences using guiding fact claims; and (2) Generation of synthetic claim-evidence pairs to augment an existing biased fact-checking dataset in order to improve the performance of trained classifiers on an unbiased dataset.

\subsection{Datasets}


\paragraph{Training Data from FEVER}
We use FEVER \cite{fever}, the largest available Wikipedia based fact-checking dataset to train our models for both of our tasks. This dataset contains claim-evidence pairs where the claim is a short factual sentence and the evidence is a relevant sentence retrieved from Wikipedia.
We use these pairs as our claim-setnence samples and use the ``refutes'', ``not enough information'', ``supports'' labels of that dataset as our $\D, \N, \A$ relations, respectively.



\paragraph{Evaluation Data for Automatic Fact Updates}
We evaluate the automatic fact updates task on an evaluation set based on part of the symmetric dataset from ~\cite{schuster2019towards} and the fact-based cases from a Wikipedia updates dataset \cite{yang-etal-2017-identifying-semantic}. For the symmetric dataset, we use the modified Wikipedia sentences with their guiding claims to generate the true Wikipedia sentence. For the cases from the updates dataset, we have human annotators write a guiding claim for each update and use it, together with the outdated sentence, to generate the updated Wikipedia sentence.
% have annotators write a guiding claim for each edit example.
% generate the corresponding fact-update claims to update the outdated text.
Overall we have a total of 201 tuples of fact update claims, outdated sentences and updated sentences.






\paragraph{Evaluation Data for Augmentation}


To measure the proficiency of our generated outputs for data augmentation, we use the unbiased FEVER-based evaluation set of \cite{schuster2019towards}.
As shown by \cite{schuster2019towards}, the claims in the FEVER dataset contain give-away phrases that can make FEVER-trained models overly rely on them, resulting in decreased performance when evaluated on unbiased datasets.

The classifiers trained on our augmented dataset are evaluated on the unbiased symmetric dataset of \cite{schuster2019towards}. This dataset (version 0.2) contains 531 claim-evidence pairs for validation and 534 claim-evidence pairs for testing.


In addition, we extend the symmetric test set by creating additional FEVER-based pairs.
We hired crowd-workers on Amazon Mechanical Turk and asked them to simulate the process of generating synthetic training pairs. Specifically, for a ``refutes'' claim-evidence FEVER pair, the workers were asked to generate a modified supporting evidence while preserving as much information as possible from the original evidence. We collected responses of workers for 500 refuting pairs from the FEVER training set.
This process extends the symmetric test set (\textsc{+TURK}) by 1000 cases --- 500 ``refutes'' pairs, and corresponding 500 ``supports'' pairs generated by turkers.


\begin{table*}[t]
\small
\centering
% \resizebox{\columnwidth}{!}{
% \scalebox{0.9}{
\begin{tabular}{llrrrrccc}
\toprule
& & \multicolumn{4}{c}{Automatic Evaluation} & & \multicolumn{2}{c}{Human's Scores}\\
\cmidrule(lr){3-6}\cmidrule(lr){8-9}
&\textsc{model}              & \textsc{SARI} & \textsc{Keep} & \textsc{Add} & \textsc{Del} & & \textsc{Grammar} & \textsc{Agreement} \\
\midrule
\multicolumn{2}{l}{\textit{\textbf{Fact updates}}:} &\\
% Orig.\ $E$ & \textbf{58} & \textbf{75}      & 0      & \textbf{100}    &   -      \\
&Split-no-Copy & 15.1 & 36.9 & 1.9 & 49.5 && - & -\\
&Paraphrase & 15.9 & 18.7 & 4.2 & 50.7 & & 3.75 & 3.65 \\
&Claim Ext.    & 12.9 & 22.6       & 1.9      & 50.4 & & 1.75 & 2.65     \\
%Concat             &   26.3   &    \textbf{73.3}   &     &  1.4        &     4.2       \\
&M. Concat     & 26.5 & \textbf{61.7}      & 6.7      & 44.9  & & 3.28 & 2.75 \\
&Ours & \textbf{31.5} & 45.4      & \textbf{13.2}      & \textbf{52.1}  & & \textbf{3.85} & \textbf{4.00} \\
%Ours               & \textbf{39.7} & \textbf{63.3}      & \textbf{10.8}     & & 45.2 & 3.78 & 3.33     \\
% \midrule
&Human &\multicolumn{4}{r}{\rule[0.09cm]{4cm}{0.01cm}} & & 4.80 & 4.70 \\
\midrule

\multicolumn{2}{l}{\textit{\textbf{Data augmentation}}:} &\\
% Orig.\ $E$ & \textbf{58} & \textbf{75}      & 0      & \textbf{100}    &   -      \\
&Paraphrase & 18.2 & 12.5 & 10.6 & 45.7 & & 4.12 & 3.92 \\
&Claim Ext.    & 12.2 & 9.8       & 4.0      & 46.4  & & 1.58 & 2.84    \\
%Concat             &   26.3   &    \textbf{73.3}       & &  1.4        &     4.2       \\
&M. Concat     & 22.1 & \textbf{71.6}      & 6.8      & 22.3  & & \textbf{4.45} & 2.05  \\
&Ours               & \textbf{34.4} & 33.0      & \textbf{26.0}     & \textbf{47.5}   & & 4.14 & \textbf{3.98}  \\
% \hline
&Human &\multicolumn{4}{r}{\rule[0.09cm]{4cm}{0.01cm}} & & 4.69 & 4.15 \\


\bottomrule
\end{tabular}
% }
% }
\caption{Human evaluation results for our model's outputs for the fact update task (top) and for the data augmentation task (bottom).
The left part of the table shows the geometric SARI score with the three \textsc{F1} scores that construct it.
The right part shows the human's scores in a 1-5 Likert scale on grammatically of the output sentence and on agreement with the given claim.}\label{tab:symmetric_auto_res}

\end{table*}


\subsection{Implementation Details}

\paragraph{Masker}
We implemented the masker using the AllenNLP framework \cite{Gardner2017AllenNLP}. For a neutrality classifier, we train an ESIM model \cite{chen2017enhanced} to classify a relation of $\A$, $\D$ or $\N$. To train this classifier, we use the $\A$ and $\D$ pairs from the FEVER dataset and for each claim we add a neutral sentence which is sampled from the sentences in the same document as the polarizing one. The classifier and masker are trained with GloVe~\cite{pennington-etal-2014-glove} word embeddings. We use BiLSTM \cite{sak2014long} encoders with hidden dimensions of 100 and share the parameters of the claim and original sentence encoders. The model is trained for up to 100 epochs with a patience value of 10, where the stopping condition is defined as the highest delta between accuracy and deletion size on the development set ($\Delta$ in \tabref{tab:mask_res}).%\footnote{Code: \url{https://github.com/TalSchuster/TokenMasker}}

For syntactic guidance, we use the constituency parser of \cite{stern-etal-2017-minimal} and consider continuous spans of length 2 to 10 as masking candidates (without combinations). By doing so, we obtain valid neutrality masks for 38\% of the $\A$ and $\D$ pairs from the FEVER training dataset. These masks are used for \eqref{eq:boots}.

\paragraph{Two-Encoder Pointer Generator}
We implemented our proposed multi-sequence-to-sequence model, based on the pointer-generator framework.%\footnote{\url{https://github.com/atulkum/pointer_summarizer}}
We use a one layer BiLSTM for encoding and decoding with a hidden dimension of 256. The parameters of the two encoders are shared. The model is trained with batches of size 64 for a total of 50K steps.%\footnote{ \url{https://github.com/darsh10/split_encoder_pointer_summarizer}}
% with an Adam optimizer and a learning rate of 0.001.

\paragraph{BERT Fact-Checking Classifier}
We use a BERT \cite{devlin2018bert} classifier, which takes in as input a (claim-evidence) pair separated by a special token, to predict out of 3 labels ($\A$, $\D$ or $\N$). The model is fine-tuned for 3 epochs, which is sufficient to perform well on the task.

\paragraph{Evidence Regeneration}
Since we are interested in using the generated supporting pairs for data augmentation, we add machine generated cases to the $\A$ set of the dataset. Adding machine generated sentences to only one of the labels in the data can be ineffective. Therefore, we balance this by regenerating paraphrased refuting evidence for the false claims. This is then added along with all models' outputs for a balanced augmentation.





\subsection{Baselines}
We consider the following baselines for constructing a fact-guided updated sentence:

\begin{itemize}
\item \textbf{Copy Claim} The sentence of the claim is copied and used as the updated sentence for itself (used only for data augmentation).

\item \textbf{Paraphrase} The claim is paraphrased using the back-translation method of \cite{wieting-gimpel-2018-paranmt}\footnote{\url{https://github.com/vsuthichai/paraphraser}}, and the output is used as the updated sentence.

\item \textbf{Claim Extension [Claim Ext.]} A pointer-generator network is trained to generate the updated sentence from an input claim alone. The model is trained on FEVER's agreeing pairs and applied on the to-be-updated claims during inference.

\item \textbf{Masked Concatenation [M. Concat]} Instead of our Two-Encoder Generator, we use a pointer-generator network.
The residual sentence (output from the masker module) and the claim are concatenated and used as input.

\item \textbf{Split Encoder without Copy [Split-no-Copy]} Our Two-Encoder Generator, without the copy mechanism. The original text and contradicting claim are passed through each of the encoders.



\end{itemize}
\section{Results}
\label{sec:results}
We report the performance of the model outputs for automatic fact-updates by comparing them to the corresponding correct wikipedia sentences. We also have crowd workers score the outputs on grammar and for agreeing with the claim. Additionally, we report the results on a fact-checking classifier using model outputs from the FEVER training set as data augmentation.

\begin{table}[t]
\centering
\begin{tabular}{lccc}
\toprule
\textsc{Model} & \textsc{Dev} & \textsc{Test}   & \textsc{+Turk}\\
\midrule
No Augmentation & 62.7 & 66.1 & 77.0\\
\midrule
Paraphrase & 60.8 & 64.6 & 77.4 \\
Copy Claim    &     62.1     &  63.6 & 77.4    \\
Claim Ext.  &     62.5              &  65.0 & 76.8\\
%Concat             &  59.9       &   59.9 & \\
M. Concat     &    60.1   & 63.7  & 78.5\\
\midrule
Ours & \textbf{63.8} & \textbf{67.8} & \textbf{80.0} \\
\bottomrule
\end{tabular}
% }
\caption{Classifiers' accuracy on the symmetric \textsc{Dev} and \textsc{Test} splits. The right column (\textsc{+Turk}) shows the accuracy on the \textsc{Test} set extended to include the 500 responses of turkers for the simulated process and the refuted pairs that they originated from. The BERT classifiers were trained on the FEVER training dataset augmented by outputs of the different methods.}
\label{tab:data_augmentation3}
\end{table}



\paragraph{Fact Updates}
Following recent text simplification work, we use the SARI~\cite{xu-etal-2016-optimizing} method. The SARI method takes 3 inputs: (i) original sentence, (ii) human written updated sentence and (iii) model output. It measures the similarity of the machine generated and human reference sentences based on the deletions, additions and kept n-grams\footnote{We use the default up to 4-grams setting.} with respect to the original sentence.\footnote{Following \cite{geva2019discofuse} we use the F1 measure for all three sets, including deletions. The final \textsc{SARI} score is the geometric mean of the \textsc{Add}, \textsc{Del} and \textsc{Keep} score.}
For human evaluation of the model's outputs, 20\% of the evaluation dataset was used.
Crowd-workers were provided with the model outputs and the corresponding supposably consistent claims. They were instructed to score the model outputs from 1 to 5 (1 being the poorest and 5 the highest), on grammaticality and agreement with the claim.


Table \ref{tab:symmetric_auto_res} reports the automatic and human evaluation results. Our model gets the highest \textsc{SARI} score, showing that it is the closest to humans in modifying the text for the corresponding tasks. Humans also score our outputs the highest for consistency with the claim, an essential criterion of our task. In addition, the outputs are more grammaticality sound compared to those from other methods.

Examining the gold answers, we notice that many of them include very minimal and local modifications, keeping much of the original sentence.  The M.\ Concat model keeps most of the original sentence as is, even at the cost of being inconsistent with the claim. This corresponds to a high \textsc{Keep} score but a lower \textsc{SARI} score overall, and a low human score on supporting the claim. Claim Ext.\ and Paraphrase do not maintain the structure of the original sentence, and perform poorly on \textsc{Keep}, leading to a low \textsc{SARI} score. The Split-no-Copy model has the same low \textsc{ADD} score as Claim Ext.\ since instead of copying the accurate information from the claim, it generates other tokens.
% on account of having no copy mechanism doesn't learn to rely on the words of the claim, but instead generates unrelated tokens.




\paragraph{Data Augmentation} \label{res:aug}
For 41850 $\D$ pairs in the FEVER training data, our method generates synthetic evidence sentences leading to 41850 $\A$ pairs. We train the BERT fact-checking classifier with this augmented data and report the performance on the symmetric dataset in Table \ref{tab:data_augmentation3}. In addition, we repeat the human evaluation process on the generated augmentation pairs and report it in Table \ref{tab:symmetric_auto_res}.







Our method's outputs are effective for augmentation, outperforming a classifier trained only on the original biased training data by an absolute 1.7\% on the \textsc{Test} set and an absolute 3.0\% on the \textsc{+Turk} set.
The outputs of the Paraphrase and Copy Claim baselines are not Wikipedia-like, making them ineffective for augmentation. All the baseline approaches augment the false claims with a supported evidence. However, the success of our method in producing supporting evidence while trying to maintain a Wikipedia-like structure, leads to more effective augmentations.


\paragraph{Masker Analysis}

\begin{table}[t]
\centering
\begin{tabular}{c|rrr|rrr}
\toprule
$\lambda$ & \textsc{Acc} & \textsc{size} & $\Delta$ & \textsc{Prec} & \textsc{Rec} & \textsc{F1}   \\ \midrule
.5 & 5.1      & 0.0         & 5   & 0.0    & 0.0      & 0.0    \\
.4 & 80.0       & 26.3      & \textbf{54}  & 27.2 & 75.1   & \textbf{39.9} \\
.3 & 77.0       & 27.5      & 50  & 25.9 & 71.6   & 38.0   \\
.2 & 81.6     & 31.1      & 51  & 23.1 & 74.8   & 35.3 \\
% .1 & 80.5     & 34.7      & 46  & 21.9 & 77.8   & 34.2 \\
% 0   & 80.0       & 37.1      & 43  & 22.6 & 81.7   & 35.5 \\
\bottomrule
\end{tabular}
\caption{Results of different values of $\lambda$ for the masker with syntactic regularization. The left three columns describe the accuracy and average mask size (\% of the sentence) over the FEVER development set with the masked evidence and a neutral target label. $\Delta$ is $\textsc{Acc}-\textsc{size}$. The right three columns contain the precision, recall and F1 of the masks that we have human annotations for. For results without syntactic regularization see the appendix.}
\label{tab:mask_res}
\end{table}


To evaluate the performance of the masker model, we test its capacity to modify $\A$ and $\D$ pairs from the FEVER development set to a neutral relation. We measure the accuracy of the pretrained classifier in predicting neutral versus the percentage of masked words from the sentence. For a finer evaluation, we manually annotated 75 $\A$ and 76 $\D$ pairs with the minimal required mask for neutrality and compute the per token \textsc{F1} score of the masker against them.

The results for different values of the regularization coefficient are reported in Table \ref{tab:mask_res}. Increasing the regularization coefficient helps to minimize the mask size and to improve the precision while maintaining the classifier accuracy and the mask recall. However, setting $\lambda$ too large, can collapse the solution to no masking at all. The generation experiments use the outputs of the $\lambda=0.4$ model.
\section{Conclusion}
\label{sec:discussion}
%make the first sentence like the second sentence
In this paper, we introduce the task of automatic fact-guided sentence modification. Given a claim and an old sentence, we learn to rewrite it to produce the updated sentence. Our method overcomes the challenges of this conditional generation task by breaking it into two steps. First, we identify the polarizing components in the original sentence and mask them. Then, using the residual sentence and the claim, we generate a new sentence which is consistent with the claim. Applied to a Wikipedia fact update evaluation set, our method successfully generates correct Wikipedia sentences using the guiding claims. Our method can also be used for data augmentation, to alleviate the bias in fact verification datasets without any external data, reducing the relative error by 13\%.
\section{Acknowledgments}
We thank the anonymous reviewers and the MIT NLP group for their helpful discussion and comments.
This work is supported by DSO grant DSOCL18002.

% 
Illum vel libero animi id ut error dolorem culpa, ex nihil placeat sequi dolore alias mollitia molestias eaque ipsum dolorum, pariatur cum velit asperiores voluptas quam corrupti sint molestiae facere, labore facilis atque facere repellendus explicabo, amet asperiores unde nemo repellat id nam dolorum incidunt magnam nulla.Deserunt quo consequuntur repudiandae voluptas tenetur dolorem voluptate doloremque explicabo debitis ex, minus nobis nemo eos, nisi possimus quasi corporis laborum?Reprehenderit earum dolores mollitia, explicabo iste odit ducimus quod perspiciatis eum ipsam voluptas optio, quaerat eius impedit ex cum sunt laboriosam voluptatem ducimus qui ullam, sequi vel libero corrupti?Architecto fugit impedit laboriosam, iure earum natus voluptate vitae fugit praesentium laboriosam tempora, veniam amet incidunt sint
\bibliography{aaai_gen}
\bibliographystyle{aaai}



\end{document}