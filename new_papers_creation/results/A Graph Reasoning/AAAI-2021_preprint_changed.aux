\relax 
\bibstyle{aaai}
\citation{mutual2020}
\citation{Tao2019,qu2019bert,su2019improving}
\citation{su2019improving}
\citation{Tao2020,Lu2020}
\citation{Lu2020,yeh2019flowdelta}
\citation{2017Variational,xu2019enhancing}
\citation{Fang2019,Qiu2020,Martin}
\@LN@col{1}
\@LN@col{2}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{dataem}{{1}{1}{An example from MuTual. All candidate responses are semantically relevant to utterances, but only the first one is the correct response. Clue words are purple and underlined. }{}{}}
\citation{Lan2020}
\citation{Tao2019,Tao2020,Lu2020}
\citation{Qiu2020}
\@LN@col{1}
\@LN@col{2}
\citation{Gururangan2020}
\citation{Gururangan2020}
\citation{grosz1995centering}
\citation{kiros2015skip,hill2016learning}
\citation{gan2017learning,Lan2020}
\newlabel{framework}{{2}{3}{Overview of our GRN. The UBERT on the left is pre-trained with the combining tasks using NUP and NOP based on the ALBERT. We fine-tune UBERT (right) on the downstream task.}{}{}}
\@LN@col{1}
\@LN@col{2}
\citation{devlin2018bert,Qiu2020}
\citation{Vaswani2017}
\citation{Martin,Fang2019,Qiu2020}
\citation{mihalcea_textrank}
\@LN@col{1}
\@LN@col{2}
\citation{kipf2016semi}
\citation{mutual2020}
\citation{Ricardo2016}
\citation{Vaswani2017}
\citation{2015Lowe}
\citation{Wu2017}
\citation{Zhou2018}
\citation{Vaswani2017}
\citation{yu2018qanet}
\citation{seo2017bidirectional}
\citation{wang2017gated}
\citation{devlin2018bert}
\citation{joshi2020spanbert}
\citation{2019RoBERTa}
\citation{Lan2020}
\citation{Radford2018}
\citation{devlin2018bert}
\citation{devlin2018bert}
\@LN@col{1}
\newlabel{dag}{{3}{5}{Different types of UDG. The black line represents the chronological dependency relationship between utterances. The green line represents the dependency between topics.}{}{}}
\@LN@col{2}
\citation{Lan2020}
\@LN@col{1}
\newlabel{compared_results}{{1}{6}{Experimental results of different methods on two testing sets}{}{}}
\newlabel{ablation_result}{{2}{6}{Ablation experimental results of GRN on MuTual validation set}{}{}}
\@LN@col{2}
\newlabel{pre-training_result}{{3}{6}{Performance comparison of UBERT using different pre-training methods on the validation dataset}{}{}}
\newlabel{dag_result}{{4}{6}{Performance comparison of different UDGs on the validation dataset}{}{}}
\citation{Lan2020}
\citation{klicpera2018predict}
\@LN@col{1}
\newlabel{acc_results}{{5}{7}{R@1 performance comparison of different number of turns on the test set. T denotes number of turns.}{}{}}
\newlabel{NGCN}{{4}{7}{R@1 performance comparison of different number of GCN layers on validation dataset.}{}{}}
\@LN@col{2}
\bibdata{reference}
\newlabel{case}{{5}{8}{Case Study. (a) is one of the keyword examples for utternces in Figure\nobreakspace  {}\ref {dataem} and red tokens is the keywords in utterance. (b) is the corresponding UDG, the tokens is the keywords for every utterance.}{}{}}
\@LN@col{1}
\newlabel{error}{{6}{8}{Error analysis}{}{}}
\@LN@col{2}
\@LN@col{1}
\@LN@col{2}
\gdef \@abspage@last{9}
