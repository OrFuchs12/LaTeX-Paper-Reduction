%File: formatting-instructions-latex-2024.tex
%release 2024.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
% \urlstyle{rm} % DO NOT CHANGE THIS
% \def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{xcolor}
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% % \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

% \usepackage[draft]{hyperref}
\usepackage{enumitem}
\usepackage{subfig}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{pifont}
\usepackage{mathalpha}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage[outdir=./images/]{epstopdf}
\input{figures/bind_ops.tex}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{todonotes}
\usepackage{multicol}
\usepackage{booktabs}

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai23.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Towards Generalization in Subitizing with Neuro-Symbolic Loss using Holographic Reduced Representations}


\author{Mohammad Mahmudul Alam \textsuperscript{\rm 1},
Edward Raff \textsuperscript{\rm 1, 2, 3},
Tim Oates \textsuperscript{\rm 1}
}
\affiliations{
\textsuperscript{\rm 1} University of Maryland, Baltimore County\\
\textsuperscript{\rm 2} Booz Allen Hamilton\\
\textsuperscript{\rm 3} Syracuse University
}


%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name,\textsuperscript{\rm 1}
    Second Author Name, \textsuperscript{\rm 2}
    Third Author Name \textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1} Affiliation 1\\
    \textsuperscript{\rm 2} Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
While deep learning has enjoyed significant success in computer vision tasks over the past decade, many shortcomings still exist from a Cognitive Science (CogSci) perspective. In particular, the ability to subitize, i.e., quickly and accurately identify the small $(\leq 6)$ count of items, is not well learned by current Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs) when using a standard cross-entropy (CE) loss. In this paper, we demonstrate that adapting tools used in CogSci research can improve the subitizing generalization of CNNs and ViTs by developing an alternative loss function using Holographic Reduced Representations (HRRs).  We investigate how this neuro-symbolic approach to learning affects the subitizing capability of CNNs and ViTs, and so we focus on specially crafted problems that isolate generalization to specific aspects of subitizing. Via saliency maps and out-of-distribution performance, we are able to empirically observe that the proposed HRR loss improves subitizing generalization though it does not completely solve the problem. In addition, we find that ViTs perform considerably worse compared to CNNs in most respects on subitizing, except on one axis where an HRR-based loss provides improvement. Code is available on \color{magenta}\url{https://github.com/MahmudulAlam/Subitizing}
\end{abstract}

%removedVspace

\section{Introduction}
Subitizing, also referred to as numerosity, is the ability to recognize small counts nearly instantaneously \cite{Kaufman1949}, allowing for fast, accurate, and confident identification of an object's count in limited space. The ability to recognize drops quickly after four items \cite{Saltzman1948}. Subitizing is a cognitive function distinct from explicit counting~\cite{Trick1994}, and recent work has shown that Convolutional Neural networks (CNNs) fail to subitize on simple MNIST-like tasks~\cite{b1}.
\par
%This inability is astonishing given the contemporary successes of computer vision models.
The failure is astonishing because a simple, hard-coded convolutional kernel is capable of perfectly solving the subitizing tasks \cite{b1}. This means a CNN captures the hypothesis space of a valid solution, so it is unclear what component is unable to reach this target goal. Seemingly there are two options: the need for better optimization strategies, or alternative loss functions. While a different loss function may sound implausible when using cross-entropy (CE) on a simple, clean dataset, we explore changing the loss function as the strategy in this work.
\par
The goal of this work is to investigate how a neuro-symbolic approach affects the generalization of subitizing in a CNN, but not to solve the problem. We devise a prediction and loss strategy built from the Holographic Reduced Representations (HRRs)~\cite{b2} which has a long successful history of its use in Cognitive Science (CogSci) research.
\par
The proposed loss function is applied to the same set of experiments as proposed by \cite{b1} where a CNN failed to subitize.  Our results indicate an improvement in generalization on most of the tasks under consideration but are not yet a complete answer to the subitizing task. Favorably, the errors in generalization with our approach are more congruent with the expectation that performance will decrease after 5 objects are present, though the accuracy is still lower than human performance. Moreover, the same set of experiments is performed on a Vision Transformer (ViT) \cite{vit} where the proposed loss function demonstrates improvement in generalization over CE loss and results are more in accordance with subitization expectation as well.
%However, we find that ViTs perform considerably worse compared to CNNs in most respects on subitizing, except on one axis where an HRR-based loss provides improvement. We find the result informative to potential differences in ViT and CNN capability that have not been clearly demonstrated on classic computer vision tasks today.
\par
In summary, our contributions are: 1) An adaption of the HRR into a loss function for classification. 2) A empirical evaluation of the impact of subitizing, and a qualitative evaluation of the cases where subitizing is improved or hindered based on the loss function. Note that improved predictive accuracy is not a goal, and difficult to deconflate from subitization performance due to background items. In addition, classic object detection methods (e.g., FasterRCNN \cite{ren2015faster}) are not a proxy of subitizing because such methods perform explicit object counting, where subitizing is a task of instantaneous recognition of numerosity --- not a sequential process of identification and counting.
\par
The remainder of the paper is organized as follows. First, different types of vector symbolic architectures, related works, and our motivation for using HRRs are covered. Next, a brief overview of HRRs is provided and the methodology of the proposed HRR loss function is described. Afterward, all the experiments and the corresponding results are described. Finally, concluding remarks, limitations, and future work are presented.
%The remainder of the paper is organized as follows. \autoref{sec:related_work} covers the different types of vector symbolic architectures, related works, and our motivation for using HRRs. In \autoref{sec:method}, a brief overview of HRRs is provided and the methodology of the proposed HRR loss function is described. \autoref{sec:experiments} describes all the experiments and the corresponding results. Finally, \autoref{sec:conclusion} will presents concluding remarks, limitations, and future work.


\section{Related Work} \label{sec:related_work}
Vector Symbolic Architectures (VSA) have been researched since seminal work by \cite{SMOLENSKY1990159}, who made an ever-green argument for their use. In short, VSAs provide a foundation for combining the benefits of connectionist architectures (robustness to deviations in input, and learning) with the benefits of symbolic AI (reasoning, logical inference). This is made possible by defining a system in which arbitrary concepts are assigned to specific vectors, and a set of \textit{binding} and \textit{unbinding} operations are defined, which associate or disassociate two vectors respectively~\cite{Schlegel2021}. Most VSAs use a fixed feature space for their representation, and thus necessarily introduce noise as more items are bound/unbound. Barring this noise they can symbolically manipulate the concepts associated with the original vectors.
\par
Many such VSAs exist today \cite{Gosmann:2019:VTB:3334291.3334293, Gayler1998MultiplicativeBR, 10.1162/NECO_a_00467, 10.5555/646256.684603}. %, Rachkovskij2001}.
For example, given vectors representing running, sleeping, cat, and dog, one can compose a vector $\boldsymbol{x} = $ bind(\textit{running}, \textit{cat}) + bind(\textit{sleeping}, \textit{dog}), and then generally determine which animal was sleeping by computing unbind($\boldsymbol{x}$, \textit{sleeping}) $\approx$ \textit{dog}. While the specifics vary between VSAs, we will use the Holographic Reduced Representation proposed by \cite{b2}, which is both commutative and associative in the binding and unbinding operations and has been used successfully in multiple differentiable applications ~\cite{pmlr-v162-alam22a,pmlr-v202-alam23a,pmlr-v187-saul23a,menet2023mimo}.
%Other works have looked at VSAs as a means of improving the reasoning abilities of neural networks. \cite{NEURIPS2018_a274315e} and \cite{huang-etal-2018-tensor} integrated Tensor Product Representations of \cite{SMOLENSKY1990159} by designing new recurrent neural networks with a structural prior for simple reasoning tasks, and showed improved reasoning and learning in natural language tasks. \cite{b3} used the HRR to design a loss for extreme multi-label classification by leveraging the symbolic properties of the HRR. In comparison, our work does not seek to use VSAs to manually construct a symbolic prior but instead seeks to experimentally understand if the HRR itself may encode a useful prior toward generalization in subitizing.
\par
The motivation for using HRR is that it may specifically engender better subitizing which is inspired by current literature in CogSci research that leverages the HRR. The seminal work by \cite{Eliasmith2012} developed ``Spaun,''\cite{Choo2018} a visual input-based brain model implemented using HRRs and able to perform several cognitive tasks like counting,
%(with timings matching human response times),
question answering, rapid variable creation, and others. The HRR has been implemented in a spiking infrastructure \cite{10.3389/fninf.2013.00048} for biological plausibility, but has also shown utility in analogy reasoning~\cite{Eliasmith2001IntegratingSA}, and solving Raven’s Progressive Matrices~\cite{10.1111/j.1756-8765.2010.01127.x}.
% bandit style action selection~\cite{10.3389/fnins.2012.00002}, and Tower of Hanoi ~\cite{Stewart2011} problems.
%These works require designing a fixed architecture that leverages HRRs to obtain their results, but they are motivating in their success given that many other VSAs have not been able to achieve the same plausibility in modeling results~\cite{10.1111/cogs.12261}. In contrast, our work will look at using HRRs as a loss function that is not designed to encode any special bias via symbolic manipulation or replication of known brain pathways, but instead, a simple mechanism that we hope imparts improved generalization.
\par
Little work has been done investigating subitizing via machine learning. Early work by \cite{Zhang2015c} treated the classification task from a purely ML perspective looking for enhanced performance. Later work showed that endowing an object segmentation network with the subitizing task improved the saliency of individual object recognition~\cite{He2017, Islam2018}. Our work is concerned with the generalization of subitizing in simple images, which a CNN is not able to do, as shown by \cite{b1}. We use their MNIST-like shape, color, and edge generalization tasks to measure if an HRR-based loss function can improve the generalization of subitizing in simple CNNs \cite{b1}. This allows us to isolate the problem to just subitization, and show that the HRR loss does improve results for most generalization tasks.
\par
Due to the severe deficiency of modern CNNs to subitize simple images, we consider many possible related tasks out of scope in our study. This includes prior work in other visual aspects like foveation \cite{10.1145/3355089.3356557} and visual reasoning \cite{10.5555/3495724.3497106}, which intersect machine learning and CogSci. Our goal is only to study how a tool in CogSci modeling, the HRR, impacts CNNs' robustness to the cognitive task of subitizing. Because CNNs cannot yet perform the task at human levels, we also consider matching human reaction times and performance matters for future work.


\section{Methodology} \label{sec:method}


\subsection{Background}
Before diving into the construction of our loss function, we will first review the details of the HRR. HRRs are a type of VSA that represent compositional structure using circular convolution in distributed representations~\cite{b2}. Given vectors $\boldsymbol{x}_i$ and $\boldsymbol{y}_i$ in a $d$-dimensional space $\mathbb{R}^d$, Plate (1995) used a circular convolution to define a \emph{binding} operation between these two vectors sampled from a Normal distribution. This can be specified more succinctly using the Fourier transform $\mathcal{F}(\cdot)$ and its inverse $\mathcal{F}^{-1}(\cdot)$. Specifically, the resulting vector $\mathcal{B} \in \mathbb{R}^d$ of binding $\boldsymbol{x}_i$ and $\boldsymbol{y}_i$ is given by
% \autoref{binding},
$\mathcal{B} = \boldsymbol{x}_i \bind \boldsymbol{y}_i = \mathcal{F}^{-1}(\mathcal{F}(\boldsymbol{x}_i) \odot \mathcal{F}(\boldsymbol{y}_i))$
where $\odot$ indicates element-wise multiplication.
% \begin{equation}
% \mathcal{B} = \boldsymbol{x}_i \bind \boldsymbol{y}_i = \mathcal{F}^{-1}(\mathcal{F}(\boldsymbol{x}_i) \odot \mathcal{F}(\boldsymbol{y}_i))
% \label{binding}
% \end{equation}
Here we use the symbol $\bind $ to denote the binding operation.

The retrieval of bound components is referred to as \emph{unbinding}. A vector can be retrieved by constructing an inverse function $\dagger: \mathbb{R}^d \to \mathbb{R}^d$ so that it complies with the identity function $\mathcal{F}(\boldsymbol{z}_{i}^{\dagger}) \cdot \mathcal{F}(\boldsymbol{z}_i) = \vec{1}$ where $\boldsymbol{z}^{\dagger}_i$ is the inverse of the vector $\boldsymbol{z}$ given
by $\boldsymbol{z}^{\dagger}_i = \mathcal{F}^{-1} \left( 1/\mathcal{F}(\boldsymbol{z}_i) \right)$.
% in \autoref{inverse}.
% \begin{equation}
% \boldsymbol{z}^{\dagger}_i = \mathcal{F}^{-1} \left( \frac{1}{\mathcal{F}(\boldsymbol{z}_i)} \right)
% \label{inverse}
% \end{equation}
To unbind $\boldsymbol{x}_i$ from $\mathcal{B}$, we circularly convolve its inverse: $\mathcal{B} \bind {\boldsymbol{x}_i}^\dagger \approx \boldsymbol{y}_i$. The necessary condition for these operations to behave as expected is an initialization procedure. As originally proposed by \cite{b2}, each vector is sampled from a Normal distribution as $\boldsymbol{z}_i \sim \mathcal{N}(0, 1/d)$. This sampling means that in expectation, the above binding and unbinding steps will work for random pairs of vectors. However, the inversion operation is numerically unstable, and originally a pseudo-inverse was proposed that traded a large numerical error for a smaller approximation error. However, more recently \cite{b3} proposed a projection operation $\pi(\cdot)$ to enforce that the inverse will be numerically stable, and exactly equal to the faster pseudo-inverse of \cite{b2}. This is done by a projection $\pi(\cdot)$ onto the ball of complex unit magnitude, $\pi(\boldsymbol{z}_i) = \mathcal{F}^{-1} \left(\: {\mathcal{F}(\boldsymbol{z}_i)}/{|\mathcal{F}(\boldsymbol{z}_i)|} \:\right)$.
% as shown in \autoref{eq:projection}.
% \begin{equation}
%     \pi(\boldsymbol{z}_i) = \mathcal{F}^{-1} \left(\: \frac{\mathcal{F}(\boldsymbol{z}_i)}{|\mathcal{F}(\boldsymbol{z}_i)|} \:\right)
%     \label{eq:projection}
% \end{equation}
We make use of this projection step to initialize the vectors in our work.


\subsection{HRR Loss Function}
In this paper, experiments are performed using both CNN and ViT models that take an image as input and predict the number of objects present in that image. To train such models, a standard softmax cross entropy (CE) loss can approximate the one-hot representation of the associated class/count. In our approach, we have taken a different strategy to devise the HRR loss function. We re-interpret the logits of CNN and ViT as an HRR vector instead of approximating a one-hot encoding. We then convert the logits to a class prediction by associating each class with its own unique HRR vector. To keep the comparison with CE loss fair, our HRR loss will maintain a classification style design in which each class corresponds to a distinct count of objects\footnote{One could select the HRR vectors to encode an ordinal style loss, but that amounts a prior for counting in the loss design. Our goal is to determine if the HRR alone has benefits separate from being able to implement inductive biases into the architecture. Thus a classification-oriented design maintains that goal.}.
\par
The idea here is to represent each class with a unique key-value $(\mathbf{K}-\mathbf{V})$ pair identifier. Each $\mathbf{K}$ and $\mathbf{V}$ is uniquely sampled from normal distribution with projection $\pi(\mathcal{N}(0, \mathbf{I}_H \cdot H^{-1}))$ where $H$ is the feature size. We use the concept of \emph{binding} and \emph{unbinding} operations of HRRs and the network will predict the linked key-value pair, i.e., the bound term. Therefore, if the unbinding operation is performed using the key $\mathrm{k}_\mathrm{n} \in \mathbf{K} = \{\mathrm{k}_1,~\mathrm{k}_2,~\cdots,~\mathrm{k}_\mathrm{C}\}$ where $\mathrm{C}$ is the number of classes, the associated value vector $\mathrm{v}_\mathrm{n} \in \mathbf{V} = \{\mathrm{v}_1,~\mathrm{v}_2,~\cdots,~\mathrm{v}_\mathrm{C}\}$ is expected to be the output,
% \begin{equation}
$\mathbf{K}, \mathbf{V} \in \mathbb{R}^{1 \times C \times H}$.
% \end{equation}

Let a network $\mathbf{F}$ predict bound vector $\hat{\mathbf{Y}} \in \mathbb{R}^{B \times 1 \times H}$ of feature size $H$ with $\tanh$ activation function in the final layer for input $\mathbf{X}$ of batch size $B$. The choice of $\tanh$ activation is intentional to keep the output in the range of $[-1, 1]$ as $\mathbf{K} \bind \mathbf{V}$ will remain in this range. This is due to sampling from a normal distribution with mean zero and standard deviation $1/\sqrt{H}$. $99.98\%$ of the data will be in the following range $-4/\sqrt{H} < \mathrm{k}_\mathrm{n}, \mathrm{v}_\mathrm{n} < 4/\sqrt{H}$ ($4\sigma$ rule where $\sigma$ is the standard deviation). Therefore, it is safe to assume that the extremum of $\mathrm{k}_\mathrm{n} \bind \mathrm{v}_\mathrm{n}$ would be $\leq \lvert 4\sqrt{2}/\sqrt{H} \rvert$. Choosing a sufficiently large value of $\{H:H\gg32\}$ would keep the value of $\mathbf{Y}=\mathbf{K} \bind \mathbf{V}$ in the $[-1, 1]$ range.
\par
To make sure that the network predicts the linked key-value pair associated with the input class of the image, the loss function is defined by Equation \ref{eq:loss}, where $\hat{\mathrm{y}}_i \in \hat{\mathbf{Y}} = \tanh(\mathbf{F}(\cdot))$ is the network's output.

Equation \ref{eq:loss} is sufficient for training the network, but we still need an explicit prediction for evaluation. To get the associated class label from the network output, we apply the $\mathbf{K}$ vectors of all the $\mathrm{C}$ classes to the $\hat{\mathbf{Y}}$ which will return the estimation of value vectors
$\hat{\mathbf{V}} = \mathbf{K} \bind \hat{\mathbf{Y}} \in \mathbb{R}^{B \times C \times H}$.
% $\hat{\mathbf{V}}$.
% \begin{equation}
% \hat{\mathbf{V}} = \mathbf{K} \bind \hat{\mathbf{Y}} \in \mathbb{R}^{B \times C \times H}
% \end{equation}
$\hat{\mathbf{V}}$ contains the values for all the $\mathrm{C}$ classes, however, the value for the associated input would be the most similar to the ground truth value after training. Accordingly, the cosine similarity score $\mathbf{S}$ is calculated given in Equation \ref{eq:sim}, and the $\argmax$ of $\mathbf{S}$ will be the predicted class/count output associated with the input image.
\begin{equation}
\mathcal{L} = \sum_{i=1}^{B} \; \norm{\; \mathrm{k}_i \bind \mathrm{v}_i - \hat{\mathrm{y}}_i \;}_2
\label{eq:loss}
\end{equation}
\begin{equation}
\mathbf{S} = \frac{\sum_{i=1}^{H} \mathbf{V}_{i} \cdot \hat{\mathbf{V}_{i}}}{\norm{\mathbf{V}}_2 \norm{\hat{\mathbf{V}}}_2} \in \mathbb{R}^{B \times C}
\label{eq:sim}
\end{equation}


% \begin{multicols}{2}
% \begin{equation}
% \mathcal{L} = \sum_{i=1}^{B} \; \norm{\; \mathrm{k}_i \bind \mathrm{v}_i - \hat{\mathrm{y}}_i \;}_2
% \label{eq:loss}
% \end{equation}
% \begin{equation}
% \mathbf{S} = \frac{\sum_{i=1}^{H} \mathbf{V}_{i} \cdot \hat{\mathbf{V}_{i}}}{\norm{\mathbf{V}}_2 \norm{\hat{\mathbf{V}}}_2} \in \mathbb{R}^{B \times C}
% \label{eq:sim}
% \end{equation}
% \end{multicols}


\section{Experiments and Results} \label{sec:experiments}
\citet{b1} examined the cognitive potential of a CNN in numerosity using four experiments. Numerosity is perhaps the simplest innate cognitive computing task that a child can do.  Disappointingly, the key finding of the work is the failure in the subitizing tasks of the CNN learned by CE loss. In this paper, we re-do the same experiments using the same CNN to show how our proposed HRR loss function, where each class is represented using a unique key-value pair, improves the CNN’s numerosity performance.
\par
Humans have a good sense of small numbers and can recognize the number of objects in a scene up to $4$ items without counting them explicitly~\cite{b4, b5, b6}. This ability is independent of the type, shape, and color of the object.  For example, if a child learns to subitize or count circles, that same skill is utilized to subitize or count squares even though circles and squares have different shapes. Nevertheless, current methods of training CNNs on subitizing perform poorly in comparison to humans.
\par
In the following experiments, we discuss how the basic skills of numerosity are lacking in CNNs and how the proposed loss helps to build a numerical sense. In all these experiments, the same CNN and dataset are used as in~\cite{b1}.  In addition, a ViT network is used in the same set of experiments. However, we modify the final layer of the networks with the HRR loss. Instead of predicting logits with softmax activation from the network, the network is used to predict features of size $H=64$ with a $\tanh$ activation function for both networks.
\par
The network is trained using the Numerosity database which has a total of $6000$ training images of dimension $100 \times 100$ with a varying number of circles from 1 to 6. The test dataset contains 7 variations (described below) of the training images. Each variation of the test split contains $6000$ images~\footnote{Training and test images are not publicly available. We got access to the dataset in correspondence with~\cite{b1}.}.

% training images
\begin{figure}[!htbp] % [13]{r}{0.5\columnwidth}
\centering
% %removedVspace
\subfloat[n=1]
{\includegraphics[keepaspectratio, width=0.125\columnwidth]{images/train_1.png}}
\hspace{0.5pt}
\subfloat[n=2]
{\includegraphics[keepaspectratio, width=0.125\columnwidth]{images/train_2.png}}
\hspace{0.5pt}
\subfloat[n=3]
{\includegraphics[keepaspectratio, width=0.125\columnwidth]{images/train_3.png}}
\hspace{0.5pt}
\subfloat[n=4]
{\includegraphics[keepaspectratio, width=0.125\columnwidth]{images/train_4.png}}
\hspace{0.5pt}
\subfloat[n=5]
{\includegraphics[keepaspectratio, width=0.125\columnwidth]{images/train_5.png}}
\hspace{0.5pt}
\subfloat[n=6]
{\includegraphics[keepaspectratio, width=0.125\columnwidth]{images/train_6.png}}
\caption{Sample training images of classes 1 to 6 are shown from (a) to (f) used to train the network for the first four experiments. The task is to predict the number of objects in an image. The generalization is tested using five different test sets in four groups that alter the size, shape, color, and infilling of the objects to make the task more difficult.}
\label{fig:train}
\end{figure}

The training set contains images of white circles on a black background. They are made such that the number of circles is independent of the total area of the circles to avoid any possible information leakage that may be used to ``cheat'' and obtain predictions without learning to actually subitize. The maximum number of circles, i.e., the total number of classes, is $C=6$. A sample image of each class is given in Figure \ref{fig:train}. For ViTs, images are divided into $10 \times 10$ patches. For each patch, a feature of size $256$ is used. In multi-head attention, 4 heads are used and the encoder block is repeated 6 times. Both networks are trained by optimizing the HRR loss function in Equation \ref{eq:loss} for a total of $300$ epochs on a single RTX 2070 Super 8GB GPU. The dropout rate is set to be $0.1$ and the initial learning rate is set to be $10^{-3}$ for the first 100 epochs which is lowered to $10^{-4}$ and $10^{-5}$ for every $100$ epochs.
\par
Framing the task in terms of classification presents challenges when interpreting the results. There are cases where the network consistently over-predicts the true number of items in an image (i.e., says ``4'' instead of ``3''). This causes cases of false success, in that the accuracy of predicting the target of ``6'' is near 100\% not because the network has successfully subitized, but because the network cannot over-predict beyond 6, and through this limit falsely appears to perform well. This situation is common, and \textit{we identify such cases with italics} to avoid incorrectly bringing the reader's attention to what is actually a failure, while simultaneously indicating the nature of the result. This also occurs with consistent under-counting and the ``1'' target class but is less prevalent in the results.
\par
With this caveat, we describe the set of experiments that were performed and their results. In the following subsections, the subitizing ability of a CNN and ViT is tested and compared using both CE and HRR loss. We also show saliency maps \cite{saliency} for each example test image. The saliency maps allow us to better understand why the HRR approach improves subitizing in the majority of cases over CE loss. The general result is that the standard cross-entropy loss has spurious attention placed on non-informative regions of the image. The HRR approach is not immune to this, especially since the network between approaches is the same, but it is noteworthy how significant the difference is.


\subsection{Experiment of Object Sizes}
The networks are originally trained using the images of circles shown in Figure \ref{fig:train} and it classifies all the training images with $100\%$ accuracy. In this experiment, we test the performance of the network with the test images of circles where the size of the circles is made $50\%$ larger than the original training images.  Apart from that, all other parameters such as color and shape are kept the same. The sample images of the circle with a bigger radius are illustrated in Figure \ref{fig:exp_1_saliency}. Results of this experiment are presented in the `$50\%$ Larger’ column of Table \ref{tbl:set1} and Table \ref{tbl:vit} for the CNN and ViT, respectively. Although varying object size does not cause the CE network’s accuracy to fall significantly for classes $1$ to $4$, for classes $5$ and $6$ of the CNN, and for class $5$ of the ViT, accuracy falls considerably. On the other hand, HRR loss can classify all the images with over $80\%$ accuracy using the CNN and over $50\%$ accuracy using the ViT for all the classes. It is interesting to note that the accuracy follows the subitizing pattern, i.e., as the number of circles in the image increases the probability of correctly recognizing them decreases. Figure \ref{fig:exp_1_saliency} shows the saliency maps of both HRR and CE loss for the CNN. HRR loss puts more restricted attention in the boundary regions whereas attention in the case of the CE loss spreads out broadly.


% experiment 1 images with saliency maps
\begin{figure}[!htbp]%[23]{r}{0.5\columnwidth}
\centering
% %removedVspace
\footnotesize
\begin{tabular}{cccccc}
\multicolumn{1}{c}{n=1} \hspace{-12pt} &
\multicolumn{1}{c}{n=2} \hspace{-12pt} &
\multicolumn{1}{c}{n=3} \hspace{-12pt} &
\multicolumn{1}{c}{n=4} \hspace{-12pt} &
\multicolumn{1}{c}{n=5} \hspace{-12pt} &
\multicolumn{1}{c}{n=6} \\

\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/circle_1.png}} \hspace{-12pt} &  \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/circle_2.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/circle_3.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/circle_4.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/circle_5.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/circle_6.png}} \\
\multicolumn{6}{c}{(a) Experiment 1 Images} \\

\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/circle_1.png}} \hspace{-12pt} &  \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/circle_2.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/circle_3.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/circle_4.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/circle_5.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/circle_6.png}} \\
\multicolumn{6}{c}{(b) HRR Loss} \\

\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/circle_1.png}} \hspace{-12pt} &  \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/circle_2.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/circle_3.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/circle_4.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/circle_5.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/circle_6.png}} \\
\multicolumn{6}{c}{(c) CE Loss}
\end{tabular}
\caption{Sample images of experiment 1 where the radius of the circles are $50\%$ greater than the circles of training images are shown in (a). Saliency maps of the experiment 1 images for both HRR and CE loss are shown in (b) and (c), respectively. HRR puts more attention toward the boundary regions whereas the network trained with CE loss function puts attention on both the inside and output of circles along with the boundary regions.}
\label{fig:exp_1_saliency}
\end{figure}


\begin{figure*}[!htbp]
\centering
%removedVspace
\footnotesize
\begin{tabular}{cccccccccccc}
\multicolumn{1}{c}{n=1} \hspace{-12pt} &
\multicolumn{1}{c}{n=2} \hspace{-12pt} &
\multicolumn{1}{c}{n=3} \hspace{-12pt} &
\multicolumn{1}{c}{n=4} \hspace{-12pt} &
\multicolumn{1}{c}{n=5} \hspace{-12pt} &
\multicolumn{1}{c}{n=6} \hspace{-12pt} &
\multicolumn{1}{c}{n=1} \hspace{-12pt} &
\multicolumn{1}{c}{n=2} \hspace{-12pt} &
\multicolumn{1}{c}{n=3} \hspace{-12pt} &
\multicolumn{1}{c}{n=4} \hspace{-12pt} &
\multicolumn{1}{c}{n=5} \hspace{-12pt} &
\multicolumn{1}{c}{n=6} \\

\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/triangle_1.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/triangle_2.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/triangle_3.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/triangle_4.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/triangle_5.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/triangle_6.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/square_1.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/square_2.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/square_3.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/square_4.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/square_5.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/square_6.png}} \\
\multicolumn{12}{c}{(b) Experiment 2 images (Triangles and Squares)} \\

\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/triangle_1.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/triangle_2.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/triangle_3.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/triangle_4.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/triangle_5.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/triangle_6.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/square_1.png}} \hspace{-12pt} &  \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/square_2.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/square_3.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/square_4.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/square_5.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/square_6.png}} \\
\multicolumn{12}{c}{(d) HRR Loss (Triangles and Squares)} \\

\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/triangle_1.png}} \hspace{-12pt} &  \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/triangle_2.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/triangle_3.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/triangle_4.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/triangle_5.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/triangle_6.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/square_1.png}} \hspace{-12pt} &  \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/square_2.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/square_3.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/square_4.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/square_5.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/square_6.png}} \\
\multicolumn{12}{c}{(f) CE Loss (Triangles and Squares)}
\end{tabular}
\caption{Sample images of experiment 2 where circles of classes $1$ to $6$ are replaced by triangles and squares shown in (a). Filters that rely on the curvature of a circle explicitly will perform poorly on this task, which is evident in the CE approach's lower accuracy. Saliency maps of the experiment 2 images are shown in (b) for HRR loss and (c) for CE loss. HRR’s attention is concentrated on the informative regions, i.e., boundary regions whereas attention is more distributive in the case of CE.}
\label{fig:exp_2_saliency}
\end{figure*}


\begin{table*}[!t]
\centering
\begin{tabular}{@{}ccccccccccc@{}}
\toprule
       & \multicolumn{2}{c}{50\% Larger} & \multicolumn{2}{c}{Triangles}   & \multicolumn{2}{c}{Squares}     & \multicolumn{2}{c}{Color Swap} & \multicolumn{2}{c}{White Rings} \\ \cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(lr){6-7} \cmidrule(r){8-9} \cmidrule(l){10-11}
Target & HRR            & CE             & HRR            & CE             & HRR            & CE             & HRR       & CE                 & HRR            & CE             \\ \midrule
1      & 1.000          & 1.000          & \textbf{0.997} & 0.327          & \textbf{1.000} & 0.876          & 0.093     & 0.160              & 0.033          & 0.004          \\
2      & 0.920          & \textbf{0.997} & \textbf{0.787} & 0.441          & \textbf{0.914} & 0.811          & 0.228     & \textbf{0.340}     & 0.007          & 0.002          \\
3      & 0.967          & \textbf{0.990} & \textbf{0.715} & 0.361          & \textbf{0.964} & 0.641          & 0.388     & \textbf{0.680}     & 0.000          & 0.010          \\
4      & 0.953          & \textbf{0.959} & \textbf{0.541} & 0.287          & \textbf{0.944} & 0.686          & 0.370     & \textbf{0.670}     & 0.003          & 0.096          \\
5      & \textbf{0.904} & 0.672          & \textbf{0.619} & 0.364          & \textbf{0.900} & 0.549          & 0.251     & \textbf{0.420}     & 0.019          & \textbf{0.194} \\
6      & \textbf{0.815} & 0.549          & 0.883          & \textit{0.930} & 0.888          & \textit{0.978} & 0.122     & \textbf{0.250}     & \textit{1.000} & \textit{0.989} \\ \bottomrule
\end{tabular}
\caption{Results of the CNN where \textbf{bold} are best unless the result is due to consistent \textit{over/under accounting at the boundary}. No result is marked ``best'' when performance is worse than random guessing ($\leq 16.7\%$) or similar. The HRR approach generalizes better for the first three tasks (or is closely behind) but degrades on the color swap task. Both methods fail on the last test.}
\label{tbl:set1}
\end{table*}

\begin{table*}[!t]
\centering
\begin{tabular}{@{}ccccccccccc@{}}
\toprule
& \multicolumn{2}{c}{50\% Larger} & \multicolumn{2}{c}{Triangles}   & \multicolumn{2}{c}{Squares}     & \multicolumn{2}{c}{Color Swap} & \multicolumn{2}{c}{White Rings} \\ \cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(lr){6-7} \cmidrule(r){8-9} \cmidrule(l){10-11}
Target & HRR & CE & HRR & CE & HRR & CE & HRR & CE & HRR & CE  \\ \midrule
1 & 1.000 & 1.000 & 0.637 & \textbf{0.681} & 0.942 & \textbf{0.977} & \textbf{0.020} & 0.000 & \textbf{0.632} & 0.053 \\
2 & 0.932 & \textbf{0.981} & 0.595 & \textbf{0.662} & 0.731 & \textbf{0.798} & \textbf{0.026} & 0.001 & \textbf{0.616} & 0.113 \\
3 & 0.920 & 0.923 & \textbf{0.488} & 0.470 & 0.553 & \textbf{0.567} & \textbf{0.062} & 0.001 & \textbf{0.467} & 0.187 \\
4 & 0.780 & 0.785 & \textbf{0.356} & 0.331 & \textbf{0.393} & 0.340 & \textbf{0.094} & 0.005 & \textbf{0.366} & 0.331 \\
5 & \textbf{0.555} & 0.372 & \textbf{0.431} & 0.312 & \textbf{0.401} & 0.276 & \textbf{0.283} & 0.024 & 0.267 & \textbf{0.382} \\
6 & 0.990 & 0.995 & 0.813 & \textbf{0.906} & 0.948 & \textbf{0.968} & 0.822 & \textit{0.995} & 0.269 & \textit{0.704} \\ \bottomrule
\end{tabular}
\caption{Results of the ViT where \textbf{bold} are best unless the result is due to consistent \textit{over/under accounting at the boundary}. No result is marked ``best'' when the performance of both methods is comparable. The HRR approach generalizes better or closely behind for all the tasks while using ViT. In the color swap task, we can see performance degrades for both but HRR yields better generalization.}
\label{tbl:vit}
\end{table*}

\subsection{Experiment of Object Shapes}
In this experiment, the networks are tested by replacing the circles with other shapes such as white equilateral triangles and squares on a black background, illustrated in Figure \ref{fig:exp_2_saliency}. Results of this experiment are presented in the `Triangles’ and `Squares’ columns of Table \ref{tbl:set1} and Table \ref{tbl:vit}. When only changing the shape of the object to triangles, the accuracy of the CE CNN drops below $50\%$ for all classes except for class $6$, with an average accuracy of $45.17\%$, revealing poor generalization. In the case of the images of squares, the network performs comparably well with an increase in average accuracy to $75.68\%$. By contrast, due to using the HRR loss and a key-value-based transformation layer, the accuracy of the same network is over $50\%$ for images of triangles and over $80\%$ for images of squares for all the classes. The average accuracy for triangles and squares is $75.7\%$ and $77.0\%$, respectively. In the case of ViT, the performance of both HRR and CE losses are similar. For images of triangles, the HRR loss average accuracy is $55.33\%$, slightly lagging behind the CE loss accuracy of $56.0\%$, whereas for images of squares, the HRR loss average accuracy is $65.66\%$, slightly lagging behind the CE loss accuracy of $66.0\%$. The saliency maps for both HRR and CE loss for the CNN are presented in Figure \ref{fig:exp_2_saliency}. Consistently, the HRR loss puts strict focus on the edges of the objects whereas the CE loss spreads attention throughout the image.

\subsection{Experiment of Object Colors}
The object's color in the test images is swapped in this experiment. The images contain newly generated synthetic circles of the same size as the training set circles, but the test circles are black on a white background. The results of this experiment are the `Color Swap’ column of the Table \ref{tbl:set1} and Table \ref{tbl:vit}. Figure \ref{fig:exp_3_saliency} shows the example images that are used in this experiment along with the saliency maps. From the figure, it is obvious that the changes in the test images are immense compared to the training images from a network’s perspective. From a human perspective, this is quite an easy task to generalize after learning from the training images. Both of the methods also fail the subitizing test. A human being can count a lower number of objects with less effort than a higher number of objects. Nevertheless, the CE classification approach has achieved $16\%$ accuracy for class $1$ and $25\%$ for class $6$. Likewise, the HRR-based method has achieved $9.3\%$ for class $1$ and $12.2\%$ for class $6$. However, in the case of the ViT, while the performance using both losses degrades and degenerates, the HRR loss shows better generalization compared to the CE approach.


% experiment 3 images with saliency maps
\begin{figure}[!htbp]%[19]{r}{0.5\columnwidth}
\centering
% %removedVspace
\footnotesize
\begin{tabular}{cccccc}
\multicolumn{1}{c}{n=1} \hspace{-12pt} &
\multicolumn{1}{c}{n=2} \hspace{-12pt} &
\multicolumn{1}{c}{n=3} \hspace{-12pt} &
\multicolumn{1}{c}{n=4} \hspace{-12pt} &
\multicolumn{1}{c}{n=5} \hspace{-12pt} &
\multicolumn{1}{c}{n=6} \\

\multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{images/swap_1.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{images/swap_2.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{images/swap_3.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{images/swap_4.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{images/swap_5.png}} \hspace{-12pt} &
\multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{images/swap_6.png}} \\
\multicolumn{6}{c}{(a) Experiment 3 Images} \\

\multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{saliency/hrr/circle/swap_1.png}} \hspace{-12pt} &  \multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{saliency/hrr/circle/swap_2.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{saliency/hrr/circle/swap_3.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{saliency/hrr/circle/swap_4.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{saliency/hrr/circle/swap_5.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{saliency/hrr/circle/swap_6.png}} \\
\multicolumn{6}{c}{(b) HRR Loss} \\

\multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{saliency/ce/circle/swap_1.png}} \hspace{-12pt} &  \multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{saliency/ce/circle/swap_2.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{saliency/ce/circle/swap_3.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{saliency/ce/circle/swap_4.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{saliency/ce/circle/swap_5.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\linewidth]{saliency/ce/circle/swap_6.png}} \\
\multicolumn{6}{c}{(c) CE Loss}
\end{tabular}
\caption{Sample images of experiment 3 where the circle and background colors are swapped in the test images shown in (a). Saliency maps of the HRR and CE loss are shown in (b) and (c), respectively. The attention of the network is more focused on the boundary region in the case of HRR.}
\label{fig:exp_3_saliency}
\end{figure}


\subsection{Experiment of Region-Boundary Duality}
Differentiating between objects from the boundary representation is vital to recognition~\cite{b7}. Humans can easily identify objects, separate and count objects given just their boundaries. To examine the network’s ability to generalize across the region-boundary duality, the network is tested using images of white circle rings on a black background. Examples of these test images along with saliency maps are presented in Figure \ref{fig:exp_4_saliency}, and the results are in the `White Rings’ columns of Table \ref{tbl:set1} and Table \ref{tbl:vit}.
\par
Recall that the network is originally trained on the images in Figure \ref{fig:train}.  From the network’s perspective, the rings of white circles are completely new images. As a result, both the CE classification approach with softmax activation and the HRR classification approach with the key-value transformation layer approach degrade in performance. In the case of CNN, we can see degeneracy for both CE and HRR losses except for class $6$ where both methods overcount and have achieved $98.9\%$ and $100\%$ accuracy, respectively. This is peculiar from the subitizing point of view because the accuracy for classes with a single ring of a circle in each approach is $0.4\%$ and $3.3\%$, respectively. However, in the case of the ViT, we can see the effectiveness of the HRR loss over CE loss for classes $1$ to $4$ with a big margin ranging from $4\%$ to $58\%$. For classes $5$ and $6$, HRR loss remains consistent with the subitizing pattern with lower accuracy than CE loss, but for class $6$ the CE loss overcounts. In conclusion, the CNN lacks the ability to generalize across the region-boundary duality and fails on this more complex subitizing task.  On the other hand, the ViT with HRR loss shows robust performance in generalization on this complex subitizing task.


% experiment 4 images with saliency maps
\begin{figure}[!htbp]%[19]{r}{0.5\columnwidth}
\centering
% %removedVspace
\footnotesize
\begin{tabular}{cccccc}
\multicolumn{1}{c}{n=1} \hspace{-12pt} &
\multicolumn{1}{c}{n=2} \hspace{-12pt} &
\multicolumn{1}{c}{n=3} \hspace{-12pt} &
\multicolumn{1}{c}{n=4} \hspace{-12pt} &
\multicolumn{1}{c}{n=5} \hspace{-12pt} &
\multicolumn{1}{c}{n=6} \\

\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/ring_1.png}} \hspace{-12pt} &  \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/ring_2.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/ring_3.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/ring_4.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/ring_5.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/ring_6.png}} \\
\multicolumn{6}{c}{(a) Experiment 4 Images} \\

\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/ring_1.png}} \hspace{-12pt} &  \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/ring_2.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/ring_3.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/ring_4.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/ring_5.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/circle/ring_6.png}} \\
\multicolumn{6}{c}{(b) HRR Loss} \\

\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/ring_1.png}} \hspace{-12pt} &  \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/ring_2.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/ring_3.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/ring_4.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/ring_5.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/circle/ring_6.png}} \\
\multicolumn{6}{c}{(c) CE Loss}
\end{tabular}
\caption{Sample images of experiment 4 where the circles are represented by the boundary edges shown in (a). This is the most challenging generalization task, as it changes the ratio of white and black pixels. Saliency maps for object region-boundary duality are shown in (b) and (c) for HRR and CE, respectively.}
\label{fig:exp_4_saliency}
\end{figure}


\subsection{Boundary Representation Tests}
Experiments 1 to 4 demonstrate CNN’s lack of generalization in learning. To improve the abstraction ability of CNNs, Wu et. al.~\cite{b1} suggested learning from the boundary representation of objects. Instead of learning from single-shaped images, each class is built with different-shaped polygons with n sides. This should eliminate the shape bias in test results. The size will be altered to allow isolation of generalization to fundamental subitizing ability rather than change the re-use of shape patterns. Moreover, each object is represented by its boundary which bridges the representation of the black object on a white background and the white object on a black background. Figure~\ref{fig:boundary_saliency} illustrates sample images of different shapes and sizes of objects with the boundary representation.
\par
The network is re-trained using $80\%$ of the images of Figure \ref{fig:boundary_saliency} and the remaining $20\%$ of the images is used for testing. The accuracy on a test set of in-distribution is shown in Table \ref{tbl:boundary_train}. While the CE loss appears to obtain better training accuracy, the goal of this study is the generalization of subitizing ability. As such the results in Table \ref{tbl:boundary_train} are more interesting because the in-distribution results are seen to imply that the HRR loss is worse, but we will see that it has a meaningful impact on generalization. This nuance would be difficult to identify in standard computer vision datasets.


% saliency maps of Boundary Representation Tests
\begin{figure}[!t]%[22]{l}{0.5\columnwidth}
\centering
% %removedVspace
\footnotesize
\begin{tabular}{cccccc}
\multicolumn{1}{c}{n=1} \hspace{-12pt} &
\multicolumn{1}{c}{n=2} \hspace{-12pt} &
\multicolumn{1}{c}{n=3} \hspace{-12pt} &
\multicolumn{1}{c}{n=4} \hspace{-12pt} &
\multicolumn{1}{c}{n=5} \hspace{-12pt} &
\multicolumn{1}{c}{n=6} \\

\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/edges_1.png}} \hspace{-12pt} &  \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/edges_2.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/edges_3.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/edges_4.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/edges_5.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{images/edges_6.png}} \\
\multicolumn{6}{c}{(a) Boundary representation images} \\

\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/edges/edges_1.png}} \hspace{-12pt} &  \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/edges/edges_2.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/edges/edges_3.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/edges/edges_4.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/edges/edges_5.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/hrr/edges/edges_6.png}} \\
\multicolumn{6}{c}{(b) HRR Loss} \\

\multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/edges/edges_1.png}} \hspace{-12pt} &  \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/edges/edges_2.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/edges/edges_3.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/edges/edges_4.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/edges/edges_5.png}} \hspace{-12pt} & \multicolumn{1}{c}{\includegraphics[width=0.125\columnwidth]{saliency/ce/edges/edges_6.png}} \\
\multicolumn{6}{c}{(c) CE Loss}
\end{tabular}
\caption{Sample images of boundary representation of the various shaped objects are shown in (a). In all cases with the CE loss shown in (c), we see spurious attention placed on empty regions of the input - generally increasing in magnitude with more items. By contrast, the HRR loss shown in (b) keeps activations focused on the actual object edges and appears to suffer only for large $n$ when objects are placed too close together.}
\label{fig:boundary_saliency}
\end{figure}


\begin{table}[!htbp]%[18]{l}{0.41\columnwidth}
%removedVspace
\centering
\begin{tabular}{@{}ccc@{}}
\toprule
      & \multicolumn{2}{c}{Boundary Edge Representation} \\ \cmidrule(l){2-3}
Target & HRR           & CE            \\ \midrule
1      & 1.000         & 1.000         \\
2      & 0.985         & 1.000         \\
3      & 0.950         & 0.970         \\
4      & 0.855         & 0.930         \\
5      & 0.635         & 0.790         \\
6      & 0.795         & 0.920         \\ \bottomrule
\end{tabular}
\caption{In distribution results, show baseline training performance of the HRR and CE-based loss functions on the edge-map distribution, rather than testing generalization. In practice, while the HRR has a lower training accuracy, it has better generalization.}
\label{tbl:boundary_train}
\end{table}

To inspect how much generalization is achieved by training the network with images of object boundaries, the test images are scaled up and down by $50\%$. Next, we will examine how boundary representation helps towards generalization. Intriguingly, the CE method does not follow the expected subitizing degradation pattern, though our HRR approach is closer to achieving it for the 50\% larger case.
\par
Table \ref{tab:boundaryResults} reveals how the results deteriorate by only changing the scale of the object. However, in the case of scaling up, both of the methods show solid evidence of human-like subitizing, i.e., the accuracy decreases as the number of objects in the image increases. The proposed HRR loss approach has achieved an average accuracy of $49\%$ whereas the CE approach has achieved an average accuracy of $45.6\%$, but the CE's performance is inflated in the sense that it has a higher training accuracy and drops precipitously.

\begin{table}[!htbp]%[17]{r}{0.45\columnwidth}
\centering
% %removedVspace
\begin{tabular}{@{}ccccc@{}}
\toprule
       & \multicolumn{2}{c}{50\% Larger} & \multicolumn{2}{c}{50\% Smaller } \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5}
Target & HRR                 & CE                  & HRR                  & CE                  \\ \midrule
1      & 0.935               & \textbf{0.991}      & \textit{1.000}       & 0.687               \\
2      & 0.715               & \textbf{0.984}      & 0.005                & \textbf{0.390}      \\
3      & \textbf{0.585}      & 0.496               & 0.005                & 0.021               \\
4      & \textbf{0.300}      & 0.207               & 0.000                & 0.014               \\
5      & \textbf{0.225}      & 0.032               & 0.000                & 0.043               \\
6      & \textbf{0.180}      & 0.026               & 0.000                & \textit{0.988}      \\ \bottomrule
\end{tabular}
\caption{Generalization results for the boundary edge maps. \textbf{Bold} results are the best unless the result is due to  \textit{over/under accounting at the boundary}. No result is marked ``best'' when worse than random guessing ($\leq 16.7\%$).}
\label{tab:boundaryResults}
\end{table}

In the case of scaling down, no apparent subitizing pattern is present for either method. The proposed method achieved $100\%$ accuracy for class $1$ due to under-counting and failed to generalize for the rest of the classes. Conversely, the CE approach has achieved $98.8\%$ accuracy due to over-counting for class $6$ and failed to generalize for the rest of the classes. Overall, the boundary representation has helped the network’s abstraction ability of subitizing but failed to generalize, especially in the case of scaling down.
\par
The saliency maps of the boundary representation test images are presented in Figure \ref{fig:boundary_saliency}. In the boundary representation tests, decisions are supposed to be made by the edge/boundary representation. The saliency maps reveal how HRR loss is concentrating networks' attention in the boundary regions whereas attention is much diffused in the case of CE loss. Moreover, based on the observation of saliency maps of correct and incorrect predictions following conclusions (see Appendix for details) are made:

\begin{itemize}[leftmargin=20pt, topsep=0pt]
\item Even when the CE-based model is \textbf{correct}, its saliency map indicates it uses the inside region of an object and the area around the object/background toward its prediction in almost all cases.
\item When the HRR loss-based model is \textbf{correct}, it rarely activates for anything besides the object boundary and does not tend to focus on the inside content of an object.
\item When the HRR-based model is \textbf{correct}, the edges of the objects in the saliency map are usually nearly-complete, and large noisy activations can be observed surrounding the boundary regions.
\item When the CE-based model is \textbf{incorrect}, it often has two objects that are nearby each other. When this happens, the CE saliency map tends to produce especially large activations between the objects, creating an artificial "bridge" between the two objects.
\item When the HRR-based loss is \textbf{incorrect}, it tends to have a saliency map that is either 1) activating on the inside content of the object, or 2) has large broken/incomplete edges detected for the object.
\end{itemize}

% The details of the observations with figures are provided in Appendix A. % \ref{appendix:saliency}.


\section{Conclusion and Future Work} \label{sec:conclusion}
In this paper, a neuro-symbolic loss function is proposed using HRR to investigate the subitizing ability of deep learning networks such as CNN and ViT. In the four experiments, the HRR-based loss appears to improve the results, especially toward higher subitizing generalization. ViT performed comparatively worse than CNN, however, in general, ViT with HRR loss shows better generalization. In one case of CNN, HRR’s performance has degraded, but still non-trivial performance, and in one case both the HRR loss and CE loss have degenerated worse-than-random guessing. In the case of ViT, HRR’s effectiveness in generalization remains consistent particularly in `white rings’ where it outperformed CE over a big margin ranging from $4\%$ to $58\%$.
\par
Our results are intriguing in that we did not design the HRR loss to be biased toward numerosity via symbolic manipulation, but instead defined a simple loss function as a counterpart to the CE loss that retains a classification focus. This may imply some unique benefit to the HRR operator in improving generalization and supports the years of prior work using it for CogSci research.
\par
While more work remains to improve innate subitizing generalization, we are not yet ready to move past these simplistic benchmarks. While \cite{b1} have thoroughly accounted for many potential information leakage sources, the under and over-counting bias remains a limitation to our work and others. This need for improved experimental design of simple tasks also highlights the general need to thoroughly test CNN and ViT broadly and the limitations and likelihood of encountering out-of-distribution data.

\clearpage
\begin{thebibliography}{33}
    \providecommand{\natexlab}[1]{#1}

    \bibitem[{Alam et~al.(2023)Alam, Raff, Biderman, Oates, and
      Holt}]{pmlr-v202-alam23a}
    Alam, M.~M.; Raff, E.; Biderman, S.; Oates, T.; and Holt, J. 2023.
    \newblock Recasting Self-Attention with Holographic Reduced Representations.
    \newblock In Krause, A.; Brunskill, E.; Cho, K.; Engelhardt, B.; Sabato, S.;
      and Scarlett, J., eds., \emph{Proceedings of the 40th International
      Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine
      Learning Research}, 490--507. PMLR.

    \bibitem[{Alam et~al.(2022)Alam, Raff, Oates, and Holt}]{pmlr-v162-alam22a}
    Alam, M.~M.; Raff, E.; Oates, T.; and Holt, J. 2022.
    \newblock Deploying Convolutional Networks on Untrusted Platforms Using 2{D}
      Holographic Reduced Representations.
    \newblock In Chaudhuri, K.; Jegelka, S.; Song, L.; Szepesvari, C.; Niu, G.; and
      Sabato, S., eds., \emph{Proceedings of the 39th International Conference on
      Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning
      Research}, 367--393. PMLR.

    \bibitem[{Bekolay et~al.(2014)Bekolay, Bergstra, Hunsberger, DeWolf, Stewart,
      Rasmussen, Choo, Voelker, and Eliasmith}]{10.3389/fninf.2013.00048}
    Bekolay, T.; Bergstra, J.; Hunsberger, E.; DeWolf, T.; Stewart, T.; Rasmussen,
      D.; Choo, X.; Voelker, A.; and Eliasmith, C. 2014.
    \newblock {Nengo: a Python tool for building large-scale functional brain
      models}.
    \newblock \emph{Frontiers in Neuroinformatics}, 7: 48.

    \bibitem[{Choo(2018)}]{Choo2018}
    Choo, F.-X. 2018.
    \newblock \emph{{Spaun 2.0: Extending the World's Largest Functional Brain
      Model}}.
    \newblock Ph.D. thesis, University of Waterloo.

    \bibitem[{Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
      Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly et~al.}]{vit}
    Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.;
      Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et~al.
      2020.
    \newblock An image is worth 16x16 words: Transformers for image recognition at
      scale.
    \newblock \emph{arXiv preprint arXiv:2010.11929}.

    \bibitem[{Eliasmith et~al.(2012)Eliasmith, Stewart, Choo, Bekolay, DeWolf,
      Tang, and Rasmussen}]{Eliasmith2012}
    Eliasmith, C.; Stewart, T.~C.; Choo, X.; Bekolay, T.; DeWolf, T.; Tang, Y.; and
      Rasmussen, D. 2012.
    \newblock {A Large-Scale Model of the Functioning Brain}.
    \newblock \emph{Science}, 338(6111): 1202--1205.

    \bibitem[{Eliasmith and Thagard(2001)}]{Eliasmith2001IntegratingSA}
    Eliasmith, C.; and Thagard, P. 2001.
    \newblock {Integrating structure and meaning: a distributed model of analogical
      mapping}.
    \newblock \emph{Cogn. Sci.}, 25: 245--286.

    \bibitem[{Gallant and Okaywe(2013)}]{10.1162/NECO_a_00467}
    Gallant, S.~I.; and Okaywe, T.~W. 2013.
    \newblock {Representing Objects, Relations, and Sequences}.
    \newblock \emph{Neural Comput.}, 25(8): 2038–2078.

    \bibitem[{Ganesan et~al.(2021)Ganesan, Gao, Gandhi, Raff, Oates, Holt, and
      McLean}]{b3}
    Ganesan, A.; Gao, H.; Gandhi, S.; Raff, E.; Oates, T.; Holt, J.; and McLean, M.
      2021.
    \newblock Learning with Holographic Reduced Representations.
    \newblock \emph{Advances in Neural Information Processing Systems}, 34.

    \bibitem[{Gayler(1998)}]{Gayler1998MultiplicativeBR}
    Gayler, R. 1998.
    \newblock {Multiplicative Binding, Representation Operators {\&} Analogy}.
    \newblock In \emph{Advances in analogy research: Integr. oftheory and data from
      the cogn., comp., and neural sciences}.

    \bibitem[{Gosmann and Eliasmith(2019)}]{Gosmann:2019:VTB:3334291.3334293}
    Gosmann, J.; and Eliasmith, C. 2019.
    \newblock {Vector-derived Transformation Binding: An Improved Binding Operation
      for Deep Symbol-like Processing in Neural Networks}.
    \newblock \emph{Neural Comput.}, 31(5): 849--869.

    \bibitem[{He et~al.(2017)He, Jiao, Zhang, Han, and Lau}]{He2017}
    He, S.; Jiao, J.; Zhang, X.; Han, G.; and Lau, R.~W. 2017.
    \newblock {Delving into Salient Object Subitizing and Detection}.
    \newblock In \emph{2017 IEEE International Conference on Computer Vision
      (ICCV)}, 1059--1067. IEEE.
    \newblock ISBN 978-1-5386-1032-9.

    \bibitem[{Islam, Kalash, and Bruce(2018)}]{Islam2018}
    Islam, M.~A.; Kalash, M.; and Bruce, N. D.~B. 2018.
    \newblock {Revisiting Salient Object Detection: Simultaneous Detection,
      Ranking, and Subitizing of Multiple Salient Objects}.
    \newblock In \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern
      Recognition}, 7142--7150. IEEE.
    \newblock ISBN 978-1-5386-6420-9.

    \bibitem[{Kanerva(1996)}]{10.5555/646256.684603}
    Kanerva, P. 1996.
    \newblock {Binary Spatter-Coding of Ordered K-Tuples}.
    \newblock In \emph{Proceedings of the 1996 International Conference on
      Artificial Neural Networks}, ICANN 96, 869–873. Berlin, Heidelberg:
      Springer-Verlag.
    \newblock ISBN 3540615105.

    \bibitem[{Kaplanyan et~al.(2019)Kaplanyan, Sochenov, Leimk{\"{u}}hler, Okunev,
      Goodall, and Rufo}]{10.1145/3355089.3356557}
    Kaplanyan, A.~S.; Sochenov, A.; Leimk{\"{u}}hler, T.; Okunev, M.; Goodall, T.;
      and Rufo, G. 2019.
    \newblock {DeepFovea: Neural Reconstruction for Foveated Rendering and Video
      Compression Using Learned Statistics of Natural Videos}.
    \newblock \emph{ACM Trans. Graph.}, 38(6).

    \bibitem[{Kaufman et~al.(1949)Kaufman, Lord, Reese, and Volkmann}]{Kaufman1949}
    Kaufman, E.~L.; Lord, M.~W.; Reese, T.~W.; and Volkmann, J. 1949.
    \newblock {The Discrimination of Visual Number}.
    \newblock \emph{The American Journal of Psychology}, 62(4): 498.

    \bibitem[{Marr(2010)}]{b7}
    Marr, D. 2010.
    \newblock \emph{Vision: A computational investigation into the human
      representation and processing of visual information}.
    \newblock MIT press.

    \bibitem[{Menet et~al.(2023)Menet, Hersche, Karunaratne, Benini, Sebastian, and
      Rahimi}]{menet2023mimo}
    Menet, N.; Hersche, M.; Karunaratne, G.; Benini, L.; Sebastian, A.; and Rahimi,
      A. 2023.
    \newblock MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting
      Computation in Superposition.
    \newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
      36.

    \bibitem[{Nie et~al.(2020)Nie, Yu, Mao, Patel, Zhu, and
      Anandkumar}]{10.5555/3495724.3497106}
    Nie, W.; Yu, Z.; Mao, L.; Patel, A.~B.; Zhu, Y.; and Anandkumar, A. 2020.
    \newblock {BONGARD-LOGO: A New Benchmark for Human-Level Concept Learning and
      Reasoning}.
    \newblock In \emph{Proceedings of the 34th International Conference on Neural
      Information Processing Systems}, NIPS'20. Red Hook, NY, USA: Curran
      Associates Inc.
    \newblock ISBN 9781713829546.

    \bibitem[{Nieder and Miller(2003)}]{b4}
    Nieder, A.; and Miller, E.~K. 2003.
    \newblock Coding of cognitive magnitude: Compressed scaling of numerical
      information in the primate prefrontal cortex.
    \newblock \emph{Neuron}, 37(1): 149--157.

    \bibitem[{Piazza et~al.(2004)Piazza, Izard, Pinel, Le~Bihan, and Dehaene}]{b5}
    Piazza, M.; Izard, V.; Pinel, P.; Le~Bihan, D.; and Dehaene, S. 2004.
    \newblock Tuning curves for approximate numerosity in the human intraparietal
      sulcus.
    \newblock \emph{Neuron}, 44(3): 547--555.

    \bibitem[{Plate(1995)}]{b2}
    Plate, T.~A. 1995.
    \newblock Holographic reduced representations.
    \newblock \emph{IEEE Transactions on Neural networks}, 6(3): 623--641.

    \bibitem[{Rasmussen and Eliasmith(2011)}]{10.1111/j.1756-8765.2010.01127.x}
    Rasmussen, D.; and Eliasmith, C. 2011.
    \newblock {A Neural Model of Rule Generation in Inductive Reasoning}.
    \newblock \emph{Topics in Cognitive Science}, 3(1): 140--153.

    \bibitem[{Ren et~al.(2015)Ren, He, Girshick, and Sun}]{ren2015faster}
    Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015.
    \newblock Faster r-cnn: Towards real-time object detection with region proposal
      networks.
    \newblock \emph{Advances in neural information processing systems}, 28.

    \bibitem[{Saltzman and Garner(1948)}]{Saltzman1948}
    Saltzman, I.~J.; and Garner, W.~R. 1948.
    \newblock {Reaction Time as a Measure of Span of Attention}.
    \newblock \emph{The Journal of Psychology}, 25(2): 227--241.

    \bibitem[{Saul et~al.(2023)Saul, Alam, Hurwitz, Raff, Oates, and
      Holt}]{pmlr-v187-saul23a}
    Saul, R.; Alam, M.~M.; Hurwitz, J.; Raff, E.; Oates, T.; and Holt, J. 2023.
    \newblock Lempel-Ziv Networks.
    \newblock In Antorán, J.; Blaas, A.; Feng, F.; Ghalebikesabi, S.; Mason, I.;
      Pradier, M.~F.; Rohde, D.; Ruiz, F. J.~R.; and Schein, A., eds.,
      \emph{Proceedings on "I Can't Believe It's Not Better! - Understanding Deep
      Learning Through Empirical Falsification" at NeurIPS 2022 Workshops}, volume
      187 of \emph{Proceedings of Machine Learning Research}, 1--11. PMLR.

    \bibitem[{Schlegel, Neubert, and Protzel(2021)}]{Schlegel2021}
    Schlegel, K.; Neubert, P.; and Protzel, P. 2021.
    \newblock {A comparison of vector symbolic architectures}.
    \newblock \emph{Artificial Intelligence Review}.

    \bibitem[{Simonyan, Vedaldi, and Zisserman(2013)}]{saliency}
    Simonyan, K.; Vedaldi, A.; and Zisserman, A. 2013.
    \newblock Deep inside convolutional networks: Visualising image classification
      models and saliency maps.
    \newblock \emph{arXiv preprint arXiv:1312.6034}.

    \bibitem[{Smolensky(1990)}]{SMOLENSKY1990159}
    Smolensky, P. 1990.
    \newblock {Tensor product variable binding and the representation of symbolic
      structures in connectionist systems}.
    \newblock \emph{Artificial Intelligence}, 46(1): 159--216.

    \bibitem[{Tokita and Ishiguchi(2010)}]{b6}
    Tokita, M.; and Ishiguchi, A. 2010.
    \newblock How might the discrepancy in the effects of perceptual variables on
      numerosity judgment be reconciled?
    \newblock \emph{Attention, Perception, \& Psychophysics}, 72(7): 1839--1853.

    \bibitem[{Trick and Pylyshyn(1994)}]{Trick1994}
    Trick, L.~M.; and Pylyshyn, Z.~W. 1994.
    \newblock {Why are small and large numbers enumerated differently? A
      limited-capacity preattentive stage in vision.}
    \newblock \emph{Psychological Review}, 101(1): 80--102.

    \bibitem[{Wu, Zhang, and Shu(2019)}]{b1}
    Wu, X.; Zhang, X.; and Shu, X. 2019.
    \newblock Cognitive deficit of deep learning in numerosity.
    \newblock In \emph{Proceedings of the AAAI Conference on Artificial
      Intelligence}, volume~33, 1303--1310.

    \bibitem[{Zhang et~al.(2015)Zhang, Ma, Sameki, Sclaroff, Betke, {Zhe Lin},
      {Xiaohui Shen}, Price, and Mech}]{Zhang2015c}
    Zhang, J.; Ma, S.; Sameki, M.; Sclaroff, S.; Betke, M.; {Zhe Lin}; {Xiaohui
      Shen}; Price, B.; and Mech, R. 2015.
    \newblock {Salient Object Subitizing}.
    \newblock In \emph{2015 IEEE Conference on Computer Vision and Pattern
      Recognition (CVPR)}, 4045--4054. IEEE.
    \newblock ISBN 978-1-4673-6964-0.

\end{thebibliography}


\appendix
% \newpage
\onecolumn


\section{Saliency Maps Reviews} \label{appendix:saliency}
The saliency maps of the correct and incorrect predictions by the network both in the case of CE and HRR loss are observed. Example images along with saliency maps for CE loss are given in Figure \ref{fig:ce_true} for correct prediction and in Figure \ref{fig:ce_false} for incorrect predictions.  When a network trained with CE loss makes a correct prediction, its saliency maps show it uses the inside region of an object and the area around the object/background toward its prediction in almost all cases.

\begin{figure*}[!htbp]
\centering
\footnotesize
\begin{tabular}{cccccccccc}
\multicolumn{1}{c}{n=1} \hspace{-11pt} &
\multicolumn{1}{c}{n=3} \hspace{-11pt} &
\multicolumn{1}{c}{n=6} \hspace{-11pt} &
\multicolumn{1}{c}{n=2} \hspace{-11pt} &
\multicolumn{1}{c}{n=3} \hspace{-11pt} &
\multicolumn{1}{c}{n=6} \hspace{-11pt} &
\multicolumn{1}{c}{n=6} \hspace{-11pt} &
\multicolumn{1}{c}{n=3} \hspace{-11pt} &
\multicolumn{1}{c}{n=6} \hspace{-11pt} &
\multicolumn{1}{c}{n=6} \\

\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/0_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/2_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/25_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/27_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/42_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/45_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/71_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/74_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/85_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/92_img.jpg}} \hspace{-11pt} \\

\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/0_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/2_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/25_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/27_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/42_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/45_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/71_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/74_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/85_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/true/92_slc.jpg}} \hspace{-11pt}
\end{tabular}
\caption{Sample images with saliency maps in a CE-based model for \textbf{correct} predictions.}
\label{fig:ce_true}
\end{figure*}

However, when a CE-based model makes an incorrect prediction, often its saliency map tends to produce large activations between the multiple objects, creating an artificial "bridge" among them.

\begin{figure*}[!htbp]
\centering
\footnotesize
\begin{tabular}{cccccccccc}
\multicolumn{1}{c}{n=6} \hspace{-11pt} &
\multicolumn{1}{c}{n=3} \hspace{-11pt} &
\multicolumn{1}{c}{n=3} \hspace{-11pt} &
\multicolumn{1}{c}{n=5} \hspace{-11pt} &
\multicolumn{1}{c}{n=4} \hspace{-11pt} &
\multicolumn{1}{c}{n=5} \hspace{-11pt} &
\multicolumn{1}{c}{n=6} \hspace{-11pt} &
\multicolumn{1}{c}{n=2} \hspace{-11pt} &
\multicolumn{1}{c}{n=1} \hspace{-11pt} &
\multicolumn{1}{c}{n=3} \\

\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/5_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/12_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/22_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/24_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/54_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/56_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/65_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/67_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/80_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/82_img.jpg}} \hspace{-11pt} \\

\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/5_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/12_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/22_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/24_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/54_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/56_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/65_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/67_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/80_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/ce/false/82_slc.jpg}} \hspace{-11pt}
\end{tabular}
\caption{Sample images with saliency maps in a CE-based model for \textbf{incorrect} predictions.}
\label{fig:ce_false}
\end{figure*}


Saliency maps along with sample images for HRR-based loss are given in Figure \ref{fig:hrr_true} for correct predictions and in Figure \ref{fig:hrr_false} for incorrect predictions. While making correct predictions, the edges of the objects in the saliency map of the HRR-based model are usually nearly-complete and we can observe large noisy activations surrounding the boundary regions.


\begin{figure*}[!htbp]
\centering
\footnotesize
\begin{tabular}{cccccccccc}
\multicolumn{1}{c}{n=3} \hspace{-11pt} &
\multicolumn{1}{c}{n=5} \hspace{-11pt} &
\multicolumn{1}{c}{n=6} \hspace{-11pt} &
\multicolumn{1}{c}{n=2} \hspace{-11pt} &
\multicolumn{1}{c}{n=1} \hspace{-11pt} &
\multicolumn{1}{c}{n=2} \hspace{-11pt} &
\multicolumn{1}{c}{n=2} \hspace{-11pt} &
\multicolumn{1}{c}{n=5} \hspace{-11pt} &
\multicolumn{1}{c}{n=6} \hspace{-11pt} &
\multicolumn{1}{c}{n=2} \\

\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/2_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/4_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/5_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/7_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/26_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/41_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/74_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/78_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/92_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/95_img.jpg}} \hspace{-11pt} \\

\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/2_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/4_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/5_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/7_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/26_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/41_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/74_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/78_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/92_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/true/95_slc.jpg}} \hspace{-11pt}
\end{tabular}
\caption{Sample images with saliency maps in a HRR-based model for \textbf{correct} predictions.}
\label{fig:hrr_true}
\end{figure*}

Nevertheless, when the HRR-based model makes an incorrect prediction, it tends to have a saliency map that is either 1) activating on the inside content of the object, or 2) has large broken/incomplete edges detected for the object.

\begin{figure*}[!htbp]
\centering
%removedVspace
\footnotesize
\begin{tabular}{cccccccccc}
\multicolumn{1}{c}{n=2} \hspace{-11pt} &
\multicolumn{1}{c}{n=4} \hspace{-11pt} &
\multicolumn{1}{c}{n=4} \hspace{-11pt} &
\multicolumn{1}{c}{n=2} \hspace{-11pt} &
\multicolumn{1}{c}{n=6} \hspace{-11pt} &
\multicolumn{1}{c}{n=2} \hspace{-11pt} &
\multicolumn{1}{c}{n=1} \hspace{-11pt} &
\multicolumn{1}{c}{n=3} \hspace{-11pt} &
\multicolumn{1}{c}{n=2} \hspace{-11pt} &
\multicolumn{1}{c}{n=4} \\

\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/15_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/17_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/38_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/47_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/53_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/54_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/66_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/68_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/87_img.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/89_img.jpg}} \hspace{-11pt} \\

\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/15_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/17_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/38_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/47_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/53_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/54_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/66_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/68_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/87_slc.jpg}} \hspace{-11pt} &
\multicolumn{1}{c}{\includegraphics[width=0.07\columnwidth]{figures/appendix/hrr/false/89_slc.jpg}} \hspace{-11pt}
\end{tabular}
\caption{Sample images with saliency maps in a HRR-based model for \textbf{incorrect} predictions.}
\label{fig:hrr_false}
\end{figure*}

\end{document}