\relax 
\bibstyle{aaai24}
\citation{Kaufman1949}
\citation{Saltzman1948}
\citation{Trick1994}
\citation{b1}
\citation{b1}
\citation{b2}
\citation{b1}
\citation{vit}
\citation{ren2015faster}
\providecommand \oddpage@label [2]{}
\citation{SMOLENSKY1990159}
\citation{Schlegel2021}
\citation{Gosmann:2019:VTB:3334291.3334293,Gayler1998MultiplicativeBR,10.1162/NECO_a_00467,10.5555/646256.684603}
\citation{b2}
\citation{pmlr-v162-alam22a,pmlr-v202-alam23a,pmlr-v187-saul23a,menet2023mimo}
\citation{Eliasmith2012}
\citation{Choo2018}
\citation{10.3389/fninf.2013.00048}
\citation{Eliasmith2001IntegratingSA}
\citation{10.1111/j.1756-8765.2010.01127.x}
\citation{Zhang2015c}
\citation{He2017,Islam2018}
\citation{b1}
\citation{b1}
\citation{10.1145/3355089.3356557}
\citation{10.5555/3495724.3497106}
\citation{b2}
\citation{b2}
\citation{b3}
\citation{b2}
\newlabel{sec:related_work}{{}{2}{}{}{}}
\newlabel{sec:method}{{}{2}{}{}{}}
\citation{b1}
\citation{b4,b5,b6}
\citation{b1}
\citation{b1}
\newlabel{eq:loss}{{1}{3}{}{}{}}
\newlabel{eq:sim}{{2}{3}{}{}{}}
\newlabel{sec:experiments}{{}{3}{}{}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:train}{{1}{3}{Sample training images of classes 1 to 6 are shown from (a) to (f) used to train the network for the first four experiments. The task is to predict the number of objects in an image. The generalization is tested using five different test sets in four groups that alter the size, shape, color, and infilling of the objects to make the task more difficult.}{}{}}
\citation{saliency}
\newlabel{fig:exp_1_saliency}{{2}{4}{Sample images of experiment 1 where the radius of the circles are $50\%$ greater than the circles of training images are shown in (a). Saliency maps of the experiment 1 images for both HRR and CE loss are shown in (b) and (c), respectively. HRR puts more attention toward the boundary regions whereas the network trained with CE loss function puts attention on both the inside and output of circles along with the boundary regions.}{}{}}
\citation{b7}
\newlabel{fig:exp_2_saliency}{{3}{5}{Sample images of experiment 2 where circles of classes $1$ to $6$ are replaced by triangles and squares shown in (a). Filters that rely on the curvature of a circle explicitly will perform poorly on this task, which is evident in the CE approach's lower accuracy. Saliency maps of the experiment 2 images are shown in (b) for HRR loss and (c) for CE loss. HRRâ€™s attention is concentrated on the informative regions, i.e., boundary regions whereas attention is more distributive in the case of CE.}{}{}}
\newlabel{tbl:set1}{{1}{5}{Results of the CNN where \textbf  {bold} are best unless the result is due to consistent \textit  {over/under accounting at the boundary}. No result is marked ``best'' when performance is worse than random guessing ($\leq 16.7\%$) or similar. The HRR approach generalizes better for the first three tasks (or is closely behind) but degrades on the color swap task. Both methods fail on the last test.}{}{}}
\newlabel{tbl:vit}{{2}{5}{Results of the ViT where \textbf  {bold} are best unless the result is due to consistent \textit  {over/under accounting at the boundary}. No result is marked ``best'' when the performance of both methods is comparable. The HRR approach generalizes better or closely behind for all the tasks while using ViT. In the color swap task, we can see performance degrades for both but HRR yields better generalization.}{}{}}
\citation{b1}
\newlabel{fig:exp_3_saliency}{{4}{6}{Sample images of experiment 3 where the circle and background colors are swapped in the test images shown in (a). Saliency maps of the HRR and CE loss are shown in (b) and (c), respectively. The attention of the network is more focused on the boundary region in the case of HRR.}{}{}}
\newlabel{fig:exp_4_saliency}{{5}{6}{Sample images of experiment 4 where the circles are represented by the boundary edges shown in (a). This is the most challenging generalization task, as it changes the ratio of white and black pixels. Saliency maps for object region-boundary duality are shown in (b) and (c) for HRR and CE, respectively.}{}{}}
\newlabel{fig:boundary_saliency}{{6}{6}{Sample images of boundary representation of the various shaped objects are shown in (a). In all cases with the CE loss shown in (c), we see spurious attention placed on empty regions of the input - generally increasing in magnitude with more items. By contrast, the HRR loss shown in (b) keeps activations focused on the actual object edges and appears to suffer only for large $n$ when objects are placed too close together.}{}{}}
\citation{b1}
\newlabel{tbl:boundary_train}{{3}{7}{In distribution results, show baseline training performance of the HRR and CE-based loss functions on the edge-map distribution, rather than testing generalization. In practice, while the HRR has a lower training accuracy, it has better generalization.}{}{}}
\newlabel{tab:boundaryResults}{{4}{7}{Generalization results for the boundary edge maps. \textbf  {Bold} results are the best unless the result is due to \textit  {over/under accounting at the boundary}. No result is marked ``best'' when worse than random guessing ($\leq 16.7\%$).}{}{}}
\newlabel{sec:conclusion}{{}{7}{}{}{}}
\bibcite{pmlr-v202-alam23a}{{1}{2023}{{Alam et~al.}}{{Alam, Raff, Biderman, Oates, and Holt}}}
\bibcite{pmlr-v162-alam22a}{{2}{2022}{{Alam et~al.}}{{Alam, Raff, Oates, and Holt}}}
\bibcite{10.3389/fninf.2013.00048}{{3}{2014}{{Bekolay et~al.}}{{Bekolay, Bergstra, Hunsberger, DeWolf, Stewart, Rasmussen, Choo, Voelker, and Eliasmith}}}
\bibcite{Choo2018}{{4}{2018}{{Choo}}{{}}}
\bibcite{vit}{{5}{2020}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly et~al.}}}
\bibcite{Eliasmith2012}{{6}{2012}{{Eliasmith et~al.}}{{Eliasmith, Stewart, Choo, Bekolay, DeWolf, Tang, and Rasmussen}}}
\bibcite{Eliasmith2001IntegratingSA}{{7}{2001}{{Eliasmith and Thagard}}{{}}}
\bibcite{10.1162/NECO_a_00467}{{8}{2013}{{Gallant and Okaywe}}{{}}}
\bibcite{b3}{{9}{2021}{{Ganesan et~al.}}{{Ganesan, Gao, Gandhi, Raff, Oates, Holt, and McLean}}}
\bibcite{Gayler1998MultiplicativeBR}{{10}{1998}{{Gayler}}{{}}}
\bibcite{Gosmann:2019:VTB:3334291.3334293}{{11}{2019}{{Gosmann and Eliasmith}}{{}}}
\bibcite{He2017}{{12}{2017}{{He et~al.}}{{He, Jiao, Zhang, Han, and Lau}}}
\bibcite{Islam2018}{{13}{2018}{{Islam, Kalash, and Bruce}}{{}}}
\bibcite{10.5555/646256.684603}{{14}{1996}{{Kanerva}}{{}}}
\bibcite{10.1145/3355089.3356557}{{15}{2019}{{Kaplanyan et~al.}}{{Kaplanyan, Sochenov, Leimk{\"{u}}hler, Okunev, Goodall, and Rufo}}}
\bibcite{Kaufman1949}{{16}{1949}{{Kaufman et~al.}}{{Kaufman, Lord, Reese, and Volkmann}}}
\bibcite{b7}{{17}{2010}{{Marr}}{{}}}
\bibcite{menet2023mimo}{{18}{2023}{{Menet et~al.}}{{Menet, Hersche, Karunaratne, Benini, Sebastian, and Rahimi}}}
\bibcite{10.5555/3495724.3497106}{{19}{2020}{{Nie et~al.}}{{Nie, Yu, Mao, Patel, Zhu, and Anandkumar}}}
\bibcite{b4}{{20}{2003}{{Nieder and Miller}}{{}}}
\bibcite{b5}{{21}{2004}{{Piazza et~al.}}{{Piazza, Izard, Pinel, Le~Bihan, and Dehaene}}}
\bibcite{b2}{{22}{1995}{{Plate}}{{}}}
\bibcite{10.1111/j.1756-8765.2010.01127.x}{{23}{2011}{{Rasmussen and Eliasmith}}{{}}}
\bibcite{ren2015faster}{{24}{2015}{{Ren et~al.}}{{Ren, He, Girshick, and Sun}}}
\bibcite{Saltzman1948}{{25}{1948}{{Saltzman and Garner}}{{}}}
\bibcite{pmlr-v187-saul23a}{{26}{2023}{{Saul et~al.}}{{Saul, Alam, Hurwitz, Raff, Oates, and Holt}}}
\bibcite{Schlegel2021}{{27}{2021}{{Schlegel, Neubert, and Protzel}}{{}}}
\bibcite{saliency}{{28}{2013}{{Simonyan, Vedaldi, and Zisserman}}{{}}}
\bibcite{SMOLENSKY1990159}{{29}{1990}{{Smolensky}}{{}}}
\bibcite{b6}{{30}{2010}{{Tokita and Ishiguchi}}{{}}}
\bibcite{Trick1994}{{31}{1994}{{Trick and Pylyshyn}}{{}}}
\bibcite{b1}{{32}{2019}{{Wu, Zhang, and Shu}}{{}}}
\bibcite{Zhang2015c}{{33}{2015}{{Zhang et~al.}}{{Zhang, Ma, Sameki, Sclaroff, Betke, {Zhe Lin}, {Xiaohui Shen}, Price, and Mech}}}
\newlabel{appendix:saliency}{{}{11}{}{}{}}
\newlabel{fig:ce_true}{{7}{11}{Sample images with saliency maps in a CE-based model for \textbf  {correct} predictions.}{}{}}
\newlabel{fig:ce_false}{{8}{11}{Sample images with saliency maps in a CE-based model for \textbf  {incorrect} predictions.}{}{}}
\newlabel{fig:hrr_true}{{9}{11}{Sample images with saliency maps in a HRR-based model for \textbf  {correct} predictions.}{}{}}
\newlabel{fig:hrr_false}{{10}{12}{Sample images with saliency maps in a HRR-based model for \textbf  {incorrect} predictions.}{}{}}
\gdef \@abspage@last{12}
