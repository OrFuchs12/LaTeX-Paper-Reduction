\section{Problem Definition}\label{prob-def}
In the following section, we provide a formal definition of the optimal allocation problem. Additionally, we define the necessary components of our reinforcement learning agent: the state space, action space, reward function, and state transition function.
\subsection{Optimal Allocation Problem}

In a physical retail environment $\mathcal{R}$ with a set of $n$ spatial regions, we represent the environment with a spatial graph $\mathcal{R} = (\mathcal{V}, \mathcal{E})$, where each region $r_i\in \mathcal{V}$ is a vertex in the graph, the spatial neighboring relation between two regions $r_i$ and $r_j$ are represented as $e_{ij}\in \mathcal{V}$. From $\mathcal{G}$, we can construct the adjacency matrix, $\textbf{A}$.

Additionally, we observe a set of $k$ products, $\mathcal{M} = \{m_j : 0 < j <=k\}$ that are sold. For each product, $m_j$, we know the retail price, $p_j$. 

The decision process faced by the retailer is to allocate each product in $\mathcal{M}$ across regions in $\mathcal{R}$. We define the allocation policy as a function $f$:

\begin{equation}
    f: \mathcal{R} \times \mathcal{M} \rightarrow \mathcal{Z}
\end{equation}
\begin{equation}
    \mathcal{Z} = \{\langle r_i, p_j \rangle , ... \langle r_w, p_q \rangle \}
\end{equation}

Where $\mathcal{Z}$ is the set of selected product region, such that $w <= n$, $q <= k$ and $\mathcal{Z} \subseteq \mathcal{R} \times \mathcal{M}$. This function is typically dynamic over time, which we denote as $f^{t}$. To simplify computation, we treat $\mathcal{Z}^{t}$ as an $(n \times k)$ grid and refer to it as the board configuration at time, $t$. An optimal retail strategy is to find the allocation policy that maximizes revenue:

\begin{equation}
    f^{\ast} = \sum_{t}^{T} \argmax_{f^{t}} \sum_{i, j \in f^{t}(\mathcal{R}, \mathcal{M})} p_j q_i
\end{equation}

where $p_j$ is the price for product $m_j$, and $q_i$ is the quantity sold in region $r_i$ and $T$ is the future time horizon of analysis. The main idea of the current work is to discover the long-term, optimal allocation policy, $f^{\ast}$ from data.

\subsection{Optimal Allocation as a Markov Decision Process}
We believe that the optimal allocation problem is well suited for reinforcement learning because the RL agent is designed for sequential decision making that maximizes expected discounted reward over time. We frame the inputs as a Markov Decision Process (MDP). An MDP is defined by the tuple $\langle \mathcal{S}, \mathcal{A}, P, r, \delta  \rangle$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the set of possible actions, $P$ is the (typically unkown) state transition function, $r$ is the reward function and $\delta \in [0,1]$ is the discount factor. 

\begin{itemize}
    \item \textbf{State} At each time, $t$, we observe the state of the retail environment, $\mathcal{E}$. We define the state, $s_t \in \mathcal{S}$, as the tuple of state features, $s_t = \langle \mathcal{Z}^{{t}}, d^{t}, \textbf{g}^{(t-1)}  \rangle$, where $\mathcal{Z}^{{t}}$ is the current board configuration, $d^t$ is the current day of the week (e.g., Sunday $\rightarrow$ 0), and $\textbf{g}^{(t-1)}$ is a vector denoting the revenue at the previous time, $(p_j q_i)^{(t-1)} \forall z \in \mathcal{Z}^t$

    \item \textbf{Action} We define the action space  $\mathcal{A} = \mathcal{R} \times \mathcal{M} \times \{-1, 1\} \cup \{0\}$, indicating ``to place'', ``take way'' or ``do nothing'' for each product, $m_j$ in each region, $r_i$.
    \item \textbf{Reward} The reward function in this case is the total product revenue at time $t$, constrained by the monetary cost, $c$, of placing a set of products in each region:
    \begin{equation}
        r(t) = \sum_{i=1}^n \sum_{j=1}^k p_j q_{ij}^{t} - c \sum_{i=1}^n \mathbbm{1}_{\mathcal{Z}}(r_i)
    \end{equation}
    
    \item \textbf{State transition function}: The state transition, $P$ is defined as $p(s^{t+1} | s^t, a^t): \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$, which gives the probability of moving to state, $s^{(t+1)}$ given the current state and action. In the optimal allocation problem the exact transition function, $P$ is unknown since the current state, $s^t$ depends on the results of the previous time, $\textbf{g}^{(t-1)}$. We model this transition as a stochastic process.
\end{itemize}