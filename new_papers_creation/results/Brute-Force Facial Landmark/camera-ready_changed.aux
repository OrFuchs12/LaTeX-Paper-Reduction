\relax 
\citation{Sagonas2016300FI}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaser}{{1}{1}{Face alignment is a regression problem, yet we solve it via large-scale classification. As shown in the bottom row, our model is able to handle severe occlusions and large pose variation and provide a global uncertainty estimate. Moreover, such uncertainty representation can used to produce conditional prediction in an interactive setup.}{}{}}
\citation{Dean2013FastAD}
\citation{joulin2016learning}
\citation{KemelmacherShlizerman2016TheMB}
\citation{Dean2013FastAD}
\citation{joulin2016learning}
\citation{shen2015first}
\citation{Chrysos2017ACP}
\citation{Zhu2016FaceAA}
\citation{Cootes1992}
\citation{Cootes1993}
\citation{Cootes1998}
\citation{Gross2005}
\citation{Matthews2004}
\citation{Saragih2011}
\citation{Sangineto13}
\citation{Baltrusaitis2012}
\citation{Yu2013}
\citation{Cao2013}
\citation{Yang2013}
\citation{Xiong2013}
\citation{Xiong2015}
\citation{Tzimiropoulos2015}
\citation{Zhu2015}
\citation{Yang2015FacialST}
\citation{Deng2016M3CM}
\citation{Kazemi2014}
\citation{Ren2014}
\citation{Fan2016ApproachingHL}
\citation{Zhang2016SP}
\citation{Zhu2016FaceAA}
\citation{Trigeorgis_2016_CVPR}
\citation{Peng2016ARE}
\citation{Zhang_2016_CVPR}
\citation{shen2015first}
\citation{Chrysos2017ACP}
\citation{Yang2015FacialST}
\citation{uricar2015real}
\citation{Xiao2015FacialLD}
\citation{Rajamanoharan2015MultiviewCL}
\citation{Wu2015ShapeAR}
\citation{SnchezLozano2016CascadedCR}
\citation{He2016DeepRL}
\newlabel{fig:computation}{{2}{3}{The computational constraints of our model at test time. Orange represents all layers prior to the last fully-connected layer (feature extraction), and blue represents the last fully-connected layer (classifier). With increasing number of classes, the running time barely increases while the memory consumption increases linearly.}{}{}}
\newlabel{sec:poseclass}{{3}{3}{}{}{}}
\newlabel{eq:joint}{{3}{3}{}{}{}}
\newlabel{fig:increasingK}{{3}{3}{Effect of different training losses with increasing number of classes. The landmark error is the standard normalized pt-pt error (RMSE). For multi-label error, we count the prediction incorrect only if it is not a member\ of the pose class. We see that the commonly used softmax log loss and the soft target does not scale up with the number of classes. We adopt multi-label loss for our classification network. In this diagnostic experiment, only real images are used during training and the number of exemplars is around 26,000.}{}{}}
\newlabel{eq:crossentropy}{{4}{3}{}{}{}}
\newlabel{eq:softmax}{{5}{3}{}{}{}}
\citation{zhu2014capturing}
\citation{rosch1978cognition}
\newlabel{fig:imgnbcnt}{{4}{4}{A distribution of images with different membership set sizes $|M_i|$. The positive set size represents the similarity with other examples and also the influence of the corresponding example at training time when using the multi-label loss.}{}{}}
\newlabel{sec:softtarget}{{3.2}{4}{}{}{}}
\newlabel{eq:members}{{6}{4}{}{}{}}
\newlabel{eq:softtarget}{{7}{4}{}{}{}}
\newlabel{fig:classify}{{5}{4}{Scaling up the number of classes in a classification network. This figure shows three different ways of training a multi-class classifier, with the mean validation error of the landmarks shown in the last column. The error is shown as a percentage of the error of a random classifier. We adopt the third method for our approach, where we train independent binary classifiers with {\em  example sharing}. The colors in the figure denote classes and the boxes in the last row circle the training examples used for a particular class (see the matching color). For example, the blue box denotes that the images of class A and B are used as the {\em  positive} examples for training class A.}{}{}}
\newlabel{eq:multilabel}{{8}{4}{}{}{}}
\citation{girshick14CVPR}
\citation{Xiong2013}
\citation{shen2015first}
\citation{Sagonas2013300FI}
\citation{Zhu2016FaceAA}
\newlabel{tab:nn}{{1}{5}{Comparison of our classification network with nearest neighbor classifier. Here nearest neighbor classifiers use cosine distance. The comparable performance shows that the features themselves are high quality summarization of the face images. In the exemplar setup, the classification filter weights can be viewed as an embedding of the training set.}{}{}}
\newlabel{fig:detref}{{6}{5}{Detection refinement. We train a linear regressor on top of the features of our exemplar model. Even though our model is trained in a classification setup, spatial information is learned useful for the detection refinement. Failure cases are shown on the rightmost column.}{}{}}
\citation{Chrysos2017ACP}
\citation{Yang2015FacialST}
\citation{Uricr2015FacialLT}
\citation{Xiao2015FacialLD}
\citation{Rajamanoharan2015MultiviewCL}
\citation{Wu2015ShapeAR}
\citation{Zhang2014FacialLD}
\citation{Zhu2016FaceAA}
\citation{Zhang2016SP}
\citation{liu2015deep}
\citation{yang2016wider}
\citation{Chrysos2017ACP}
\citation{Yang2015FacialST}
\citation{Yang2015FacialST}
\citation{Chrysos2017ACP}
\citation{SnchezLozano2016CascadedCR}
\citation{Zhu2016FaceAA}
\citation{Zhang2014FacialLD}
\citation{Zhu2016FaceAA}
\newlabel{fig:test4}{{7b}{6}{{\relax \fontsize  {9}{10}\selectfont  Category 3 - unconstrained}}{}{}}
\newlabel{sub@fig:test4}{{b}{6}{{\relax \fontsize  {9}{10}\selectfont  Category 3 - unconstrained}}{}{}}
\newlabel{fig:test6}{{7c}{6}{{\relax \fontsize  {9}{10}\selectfont  Subset of cat 3 - hard frames only}}{}{}}
\newlabel{sub@fig:test6}{{c}{6}{{\relax \fontsize  {9}{10}\selectfont  Subset of cat 3 - hard frames only}}{}{}}
\newlabel{fig:curves}{{7}{6}{\relax \fontsize  {9}{10}\selectfont  The cumulative error distribution curves on the 300VW benchmark. The statistics for (a) and (b) are summarized in Tab \ref {tab:300vw} . The Area Under Curve (AUC) in (c) is show next to the names.}{}{}}
\newlabel{tab:300vw}{{2}{6}{Comparing with existing methods on the 1st and 3rd category of 300VW benchmark. The \textcolor {cfirst}{1st}, \textcolor {csecond}{2nd} and the \textcolor {cthird}{3rd} place for each metric are color coded. Here AUC denotes the area under the CED curves in Fig \ref {fig:curves} and FR denotes the failure rate in percentage.}{}{}}
\citation{le2012interactive}
\newlabel{fig:300vw_vis}{{8}{7}{Qualitative results for Category 3 frames on 300VW. The competing method \def \def {##}## 1##2{\def {##1}##1 ##2}\unhbox \voidb@x \def -1000{-1000}\def (\nobreak  \hskip 0in{##})##2{(\nobreak  \hskip 0in{##1})}\let \reserved@d =[\def \par }{}{}}
\newlabel{sec:interactive}{{4.3}{7}{}{}{}}
\newlabel{fig:300vw_vis_hard}{{9}{7}{Qualitative results for Category 3-Hard frames on 300VW. We can see that by training on synthetic images (comparing Row 1 with Row 2), our method is robust to large pose variation. The last column shows a failure case.}{}{}}
\newlabel{fig:compocclusion}{{10}{7}{Uncertainty reasoning under occlusion. Our classification model ({a}) can report uncertainty in terms of global variables, \textit  {e.g.}yaw and roll. By integrating temporal information over time with an HMM ({b}), we can further increase accuracy and reduce uncertainty during such occlusions.}{}{}}
\bibdata{egbib}
\bibstyle{aaai}
\newlabel{fig:condpred}{{11}{8}{Conditional prediction. Our algorithm fails on images with severe motion blur (a) \& (b). However, if an annotator gives hint of where the nose tip is (the yellow cross in (b)), we can correct the mistake (c) by finding the most probable class {\em  conditioned} on this evidence.}{}{}}
\newlabel{tab:condpred}{{3}{8}{We simulate interactive annotation of all category 3 frames in 300VW. ``1-pt'' refers to a user labeling a fixed landmark (the nose) in each frame, while ``best'' refers to an upper-bound obtained labeling the optimal landmark minimizing the error. Our results suggest that with a single user-click per image (as opposed to 68 landmark clicks), one can correctly label 99.4\% of the frames.}{}{}}
\gdef \@abspage@last{9}
