\relax 
\bibstyle{aaai24}
\citation{JMMC,ProTrack,ADRNet,DMCNet,zhu2023visual}
\citation{GTOT,rgbt234,LasHeR,HMFT}
\citation{DAPNet,DAFNet}
\citation{mfDimp}
\citation{CAT,ADRNet,APFNet}
\citation{ProTrack,zhu2023visual}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dynamic}{{1}{1}{Different dominant modality in complex scenarios. The image with green box represents the dominant modality, and the red box represents the auxiliary modality. }{}{}}
\citation{rgbt234}
\citation{LasHeR}
\citation{ye2022joint,cui2022mixformer,lan2023procontext}
\citation{FANet}
\citation{HMFT}
\citation{APFNet}
\citation{khattak2023maple}
\citation{jia2022visual}
\citation{ProTrack}
\citation{zhu2023visual}
\newlabel{fig:framework}{{2}{3}{The overall architecture of our proposed BAT. We first transformed the template frame and search frame of each modality into tokens, then concatenated them together to pass the $N$-layer dual-stream transformer encoder, respectively. The bi-directional adapter is paralleled with the dual-stream encoder layer, which could learn feature prompts from one modality to another. To this end, the output features of the two branches are added and fed into the prediction head for final tracking result. }{}{}}
\citation{rgbt234}
\citation{LasHeR}
\newlabel{fig:adapter}{{3}{4}{The detailed architecture of bi-directional adapter. It consists of three linear projection layers, $t_{n}$ represents the token's num of each modality, the input token is first dimensional reduced to $d_{e}$ and passed through a linear projection layer, then up-project to the original dimension $d_{t}$ and fed into another modality as feature prompts. }{}{}}
\citation{loshchilov2019adamw}
\citation{ye2022joint}
\citation{DMCNet}
\citation{zhu2023visual}
\citation{atom}
\citation{dimp50}
\citation{mfDimp}
\citation{DAPNet}
\citation{siamfc++}
\citation{CAT}
\citation{CMPP}
\citation{stark}
\citation{transt}
\citation{JMMC}
\citation{MANet++}
\citation{FANet}
\citation{ADRNet}
\citation{ye2022joint}
\citation{APFNet}
\citation{DMCNet}
\citation{HMFT}
\citation{ProTrack}
\citation{zhu2023visual}
\newlabel{tabresults}{{\caption@xref {tabresults}{ on input line 397}}{5}{}{}{}}
\newlabel{tab:comparison}{{1}{5}{Overall performance on RGBT234 and LasHeR dataset. \textcolor {red}{Red}/\textcolor {green}{Green}/\textcolor {blue}{Blue} indicates the best/runner-up/third best results. Results are reported in percentage (\%).}{}{}}
\newlabel{fig:comparison}{{4}{6}{Visualization of tracking results. The green rectangles indicate target objects in the template frame. Our method shows the best performance in different frame sequences as the dominant modality dynamically changes.}{}{}}
\newlabel{fig:adaptervariant}{{5}{6}{Different variants of bi-directional adapter for dual-stream encoder framework. }{}{}}
\newlabel{fig:attribute}{{6}{6}{More comparisons of BAT and the competing methods under different attributes in the LasHeR dataset.}{}{}}
\newlabel{tab:adaptertype}{{2}{7}{Quantitative comparison between different variants of BAT on the LasHeR dataset}{}{}}
\newlabel{tabadapternum}{{3}{7}{Results of different bi-directional adapter layers in LasHeR}{}{}}
\newlabel{tab:attribute}{{4}{7}{More quantitative comparisons of PR and SR scores under five extreme attributes in LasHeR dataset}{}{}}
\bibdata{aaai24}
\gdef \@abspage@last{8}
