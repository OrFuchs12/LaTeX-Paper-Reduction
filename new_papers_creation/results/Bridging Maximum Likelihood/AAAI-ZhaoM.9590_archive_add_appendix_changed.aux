\relax 
\citation{bishop_2006_PRML}
\citation{goodfellow2014generative}
\citation{mclachlan2007algorithm}
\citation{jordan1999introduction}
\citation{blei2006variational}
\citation{kingma2014auto}
\citation{dai2018diagnosing}
\citation{minka2005divergence}
\citation{srivastava2017veegan}
\citation{goodfellow2014generative}
\citation{larsen2015autoencoding}
\citation{goodfellow2014generative}
\citation{nowozin2016f}
\citation{zhang2018self}
\citation{gulrajani2017improved}
\citation{brock2018large}
\citation{goodfellow2014generative}
\citation{nowozin2016f}
\citation{li2019adversarial}
\citation{GoogleCompareGAN}
\citation{nowozin2016f}
\citation{li2019adversarial}
\citation{nguyen2017dual}
\citation{larsen2015autoencoding}
\citation{mathieu2016disentangling}
\citation{zhang2019training}
\citation{kirkpatrick2017overcoming}
\citation{liang2018generative}
\newlabel{sec:Intro}{{1}{1}{}{}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:forward_reverse_KL_pro_con}{{1}{1}{Comparing maximum likelihood and adversarial learning. \let \reserved@d = *\def \@@par \parindent \caption@parindent \hangindent \caption@hangindent }{}{}}
\citation{zhu2017unpaired}
\citation{li2017alice}
\citation{jordan1999introduction}
\citation{kingma2014auto}
\citation{blei2017variational}
\citation{goodfellow2014generative}
\citation{gulrajani2017improved}
\citation{nowozin2016f}
\citation{brock2018large}
\citation{goodfellow2014generative}
\citation{goodfellow2014generative}
\citation{goodfellow2014generative}
\citation{nowozin2016f}
\citation{li2019adversarial}
\citation{nowozin2016f}
\citation{GoogleCompareGAN}
\newlabel{sec:Preliminary}{{2}{2}{}{}{}}
\newlabel{fig:explodes}{{1}{2}{ Demonstration of adversarial learning forgetting the information learned/initialized by ML learning on 25-Gaussians (the first two) and MNIST (the last two). From left to right are the snapshots of ML initialization and $20$ following iterations of adversarial learning, respectively. See Appendix \ref {sec:ML_AL_explode} for details. }{}{}}
\newlabel{eq:ML_loss}{{1}{2}{}{}{}}
\newlabel{eq:ML_grad}{{2}{2}{}{}{}}
\newlabel{eq:elbo_ML}{{3}{2}{}{}{}}
\newlabel{eq:GAN_lossd}{{4}{2}{}{}{}}
\newlabel{eq:p_theta_x_gan}{{5}{2}{}{}{}}
\newlabel{eq:f_log_ratio}{{6}{2}{}{}{}}
\citation{cichocki2010families}
\citation{cichocki2010families}
\citation{amari2009alpha}
\citation{cong2019go}
\newlabel{eq:RKL_loss_grad}{{7}{3}{}{}{}}
\newlabel{sec:Trans}{{3}{3}{}{}{}}
\newlabel{eq:alpha-div}{{8}{3}{}{}{}}
\newlabel{sec:Grad}{{3.1}{3}{}{}{}}
\newlabel{eq:alpha_grad}{{9}{3}{}{}{}}
\newlabel{eq:Forward_grad}{{10}{3}{}{}{}}
\newlabel{eq:Reverse_grad}{{11}{3}{}{}{}}
\newlabel{eq:combine_grad}{{12}{3}{}{}{}}
\citation{yin2018semi}
\newlabel{fig:GamAlphaLargeVarMain}{{2a}{4}{$\mu =3, \sigma =1$ \let \reserved@d = *\def \@@par \parindent \caption@parindent \hangindent \caption@hangindent }{}{}}
\newlabel{sub@fig:GamAlphaLargeVarMain}{{a}{4}{$\mu =3, \sigma =1$ \let \reserved@d = *\def \@@par \parindent \caption@parindent \hangindent \caption@hangindent }{}{}}
\newlabel{fig:GamELBOMain}{{2b}{4}{$\mu =1, \sigma =1$ \let \reserved@d = *\def \@@par \parindent \caption@parindent \hangindent \caption@hangindent }{}{}}
\newlabel{sub@fig:GamELBOMain}{{b}{4}{$\mu =1, \sigma =1$ \let \reserved@d = *\def \@@par \parindent \caption@parindent \hangindent \caption@hangindent }{}{}}
\newlabel{fig:alpha_grad_var}{{2}{4}{Illustration of different MC variance properties of different gradient estimators of $\nabla _{\{\mu ,\sigma \}}\ensuremath  {\mathcal  {D}} _{\alpha }[\ensuremath  {\mathcal  {N}} (x;\mu ,\sigma ^2)||\ensuremath  {\mathcal  {N}} (x;0,1)]$ for $\alpha \in (0,1)$. $1$ MC sample is used to estimate the gradient. The results are based on $100$ random trials. }{}{}}
\newlabel{sec:alpha_bridge}{{3.2}{4}{}{}{}}
\newlabel{alg:Alpha-divergence}{{1}{4}{$\alpha $-Bridge (from forward to reverse KL)}{}{}}
\newlabel{eq:tilde_p_theta_x_joint}{{13}{4}{}{}{}}
\newlabel{eq:approx_grad_loglike}{{14}{4}{}{}{}}
\newlabel{eq:combine_grad_practical}{{15}{4}{}{}{}}
\citation{zhu2017unpaired}
\citation{li2017alice}
\citation{li2017alice}
\citation{zhu2017unpaired}
\citation{kim2017learning}
\citation{li2017alice}
\citation{larsen2015autoencoding}
\citation{mathieu2016disentangling}
\citation{zhang2019training}
\citation{makhzani2015adversarial}
\citation{mescheder2017adversarial}
\citation{berthelot2017began}
\citation{ulyanov2018takes}
\citation{pu2017adversarial}
\citation{chen2018symmetric}
\citation{kingma2014adam}
\citation{kingma2014adam}
\citation{tao2018chi}
\citation{mescheder2018training}
\citation{miyato2018spectral}
\citation{GoogleCompareGAN}
\citation{gulrajani2017improved}
\citation{salimans2016improved}
\citation{parzen1962estimation}
\newlabel{sec:insight}{{3.3}{5}{}{}{}}
\newlabel{eq:cycle_reverseKL}{{16}{5}{}{}{}}
\newlabel{sec:Experim}{{5}{5}{}{}{}}
\newlabel{sec:exp_25Gaussian}{{5.1}{5}{}{}{}}
\citation{krizhevsky2009learning}
\citation{heusel2017gans}
\citation{lecun1998gradient}
\citation{liu2015deep}
\newlabel{fig:Sample_25G}{{3}{6}{Demonstration on the dynamic evolution of the generated samples (blue) from the compared methods during training. Columns correspond to 1K, 2K, 4K, 6K, and 10K iterations. Real data samples are shown in red. }{}{}}
\newlabel{fig:25Gs_4IS_0_1}{{4a}{6}{$\beta _1=0.1$ \let \reserved@d = *\def \@@par \parindent \caption@parindent \hangindent \caption@hangindent }{}{}}
\newlabel{sub@fig:25Gs_4IS_0_1}{{a}{6}{$\beta _1=0.1$ \let \reserved@d = *\def \@@par \parindent \caption@parindent \hangindent \caption@hangindent }{}{}}
\newlabel{fig:25Gs_4LL_0_1}{{4b}{6}{$\beta _1=0.1$ \let \reserved@d = *\def \@@par \parindent \caption@parindent \hangindent \caption@hangindent }{}{}}
\newlabel{sub@fig:25Gs_4LL_0_1}{{b}{6}{$\beta _1=0.1$ \let \reserved@d = *\def \@@par \parindent \caption@parindent \hangindent \caption@hangindent }{}{}}
\newlabel{fig:25Gs_4IS_0_5}{{4c}{6}{$\beta _1=0.5$ \let \reserved@d = *\def \@@par \parindent \caption@parindent \hangindent \caption@hangindent }{}{}}
\newlabel{sub@fig:25Gs_4IS_0_5}{{c}{6}{$\beta _1=0.5$ \let \reserved@d = *\def \@@par \parindent \caption@parindent \hangindent \caption@hangindent }{}{}}
\newlabel{fig:25Gs_4LL_0_5}{{4d}{6}{$\beta _1=0.5$ \let \reserved@d = *\def \@@par \parindent \caption@parindent \hangindent \caption@hangindent }{}{}}
\newlabel{sub@fig:25Gs_4LL_0_5}{{d}{6}{$\beta _1=0.5$ \let \reserved@d = *\def \@@par \parindent \caption@parindent \hangindent \caption@hangindent }{}{}}
\newlabel{fig:25Gs_IS_LL}{{4}{6}{Quantitative performance of the methods along the training process. Inception score (a)$/$(c) and estimated log-likelihood (b)$/$(d) when Adam \def \def {##}## 1##2{\def {##1}##1 ##2}\unhbox \voidb@x \def -1000{-1000}\def (\nobreak  \hskip 0in{##})##2{(\nobreak  \hskip 0in{##1})}\let \reserved@d =[\def \par }{}{}}
\citation{zhu2017unpaired}
\citation{li2017alice}
\newlabel{fig:Sample_MNIST_CelebA}{{5}{7}{Random samples generated along the training process of the $\alpha $-bridge on MNIST (top) and CelebA (bottom). Note most information is transferred from ML to adversarial learning, such as the class, rotation, and style of generated digits, and the basic tone, gender, expression, pose of the head, hair style of generated faces. }{}{}}
\newlabel{fig:attribute_inference_arm}{{6}{7}{Using the inference arm transplanted by $\alpha $-Bridge to reconstruct (top) and manipulate (bottom) GAN generated images. $\ensuremath  {\boldsymbol  {\phi }} $ and $\ensuremath  {\boldsymbol  {\theta }} $ denote the inference arm $q_{\ensuremath  {\boldsymbol  {\phi }} }(\ensuremath  {\boldsymbol  {z}} |\ensuremath  {\boldsymbol  {x}} )$ and the generator $G_{\ensuremath  {\boldsymbol  {\theta }} }(\ensuremath  {\boldsymbol  {z}} )$, respectively. }{}{}}
\bibdata{ReferencesCong}
\bibstyle{aaai}
\gdef \@abspage@last{8}
