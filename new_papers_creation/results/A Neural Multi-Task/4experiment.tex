\subsection{Datasets} 
\begin{table*}[tb]
	%\vspace{-0.1in}
	\small
	\centering
	\begin{tabular}{l|c|ccc}
		\hline
		\multirow{2}{*}{Corpus} & \multirow{2}{*}{\# of Articles} & \multicolumn{3}{c}{Entity Types and Counts}\\
		\cline{3-5}
		\multirow{2}{*}{}&  \multirow{2}{*}{}& \# of Disease Mentions & \# of Chemical Mention& \# of Concepts\\
		\hline
		BC5CDR&1,500&12,852&15,935& 5,818\\
		NCBI &793&6,881&0& 1,049\\
		\hline
	\end{tabular}
	\vspace{-0.1in}
	\caption{Overall statistics of BC5CDR and the NCBI.}\label{tab: datasets}	
	%\vspace{-0.15in}
	\vspace{-0.1in}
\end{table*}
We evaluate the performance of the MTL models on two corpora: BC5CDR task corpus \cite{Li2016BioCreative} and the NCBI Disease corpus \cite{Rezarta2014NCBI}. The BC5CDR corpus contains 1500 PubMed abstracts, which are equally partitioned into three sections for training, development and test, respectively. A disease mention in each abstract is manually annotated with the concept identifier to which it refers to a controlled vocabulary. The NCBI Disease corpus consists of 793
PubMed abstracts, which are also separated into training (593), development (100) and test (100) subsets. The NCBI Disease corpus is annotated with disease mentions, using concept identifiers from either MeSH or OMIM. Table~\ref{tab: datasets} gives the statistics of the two corpora. Due to the limit of the vocabulary of chemical, we only consider mapping disease mentions to a controlled vocabulary of diseases.   
To map disease mentions to MeSH/OMIM concepts (IDs), we use the Comparative Toxicogenomics Database (CTD) MEDIC disease vocabulary, which consists of 9700 unique diseases described by more than 67 000 terms (including synonyms).


\subsection{Pre-trained word embeddings}
\label{sec: embeddings}
We initialized the word embedding matrix with four types of publicly available pre-trained word embeddings respectively. 
The first is Word2Vec 50-dimensional embeddings trained on the PubMed abstracts together with all the full-text articles from PubMed Central (PMC) \cite{Pyysalo2013b}. The second is GloVe 100-dimensional embeddings trained on 6 billion words from Wikipedia and web text~\cite{pennington-socher-manning}. The third  is Senna 50-dimensional embeddings trained on Wikipedia and Reuters RCV-1 corpus~\cite{Collobert2011}. The fourth is the randomly initialized 100-dimensional embeddings which are uniformly sampled from range $[-\sqrt{\frac{3}{dim}},+\sqrt{\frac{3}{dim}}]$, where $dim$ is the dimension of embeddings~\cite{He:2015:DDR:2919332.2919814}.

\subsection{Evaluation Metrics and Settings} 
We perform experiments for both medical named entity recognition and medical named entity normalization.
We utilize the evaluation kit\footnote{http://www.biocreative.org/tasks/biocreative-v/track-3-cdr} for evaluating model performances. 
Metrics measured are %mention- and 
concept-level precision, recall and F1.
  
Our single- and multi-task networks are 3-layer, Bi-LSTM-CNNs with pre-trained word embeddings. For the neural multi-task learning model, we follow the training procedure outlined in Section~\ref{sec:method}. We use the word embeddings setup in Section~\ref{sec: embeddings}. Character embeddings are initialized with uniform samples from
$[-\sqrt{\frac{3}{dim}},+\sqrt{\frac{3}{dim}}]$, where we set $dim = 30$. 
 We follow \cite{S2014Deep} in using the same dimension for the hidden layers. We use a dropout rate of 0.5 and train these architectures with momentum SGD with the initial learning rate of 0.001 and momentum of 0.9 for 20 epochs.

%\textbf{Baseline} We built our multi-task learning framework by using deep bi-RNNs.  We compared the multi-task learning model with the following studies for medical named entity recognition and normalization separately. For these baselines, we tuned hyper-parameters for best results.
%\begin{itemize}
%	\setlength{\itemsep}{3pt}
%	\setlength{\parsep}{0pt}
%	\setlength{\parskip}{0pt}
%	\item \textbf{Bi-LSTM-CRF}: Bi-LSTM-CRF \cite{HuangXY15} is the state-of-the-art single model for NER. It combines an Bi-LSTM network and a CRF network to form an Bi-LSTM-CRF model. This network can efficiently use past input features via Bi-LSTM layers and sentence level tag information via a CRF layer. A CRF layer is presented by lines which connect consecutive output layers. We use this model on MER and MEN separately.
%	\item \textbf{LeadMine}: LeadMine \cite{Lowe2015LeadMineDI} is a pipeline framework, which uses CRFs for disease identification and Wikipedia for concept mapping.
%	\item \textbf{Dnorm}: Dnorm \cite{Leaman2013DNorm} is a joint framework for entity recognition and normalization, and is the baseline system from the organizer of the BioCreative V CDR (BC5CDR) task.
%	\item \textbf{TaggerOne}: TaggerOne \cite{Leaman2016TaggerOne} is a joint framework for entity recognition and normalization, which uses a semi-Markov structured linear classifier.
%	\item \textbf{Transition-based joint model}: Transition-based joint model \cite{Lou2017A} is a transition-based model to jointly perform disease named entity recognition and normalization, casting the output construction process into an incremental state transition process.
%	\item \textbf{MTL}: MTL is the baseline model we built multi-task learning with bi-RNNs on MER and MEN by just sharing representation (i.e., word embedding and hidden layers.)
%	\item \textbf{MTL-MER\_feedback}: MTL-MER\_feedback is the improved version of MTL. It considers the explicit feedback from MER to MEN on the top of the MTL.
%	\item \textbf{MTL-MEN\_feedback}: MTL-MER\_feedback is the improved version of MTL. It considers the explicit feedback from MEN to MER  on the top of the MTL.
%	\vspace{-0.07in}
%\end{itemize}

\subsection{Main Results}
\label{sec: main}
The first part of Table~\ref{tab: main} illustrates the results of 5 previous top-performance systems for (medical) named entity recognition and normalization. 
Among these previous studies, LeadMine and IDCNN are pipeline models, while Dnorm, TaggerOne, and Transition-based Model are joint models.
From the first part, it is clear that the joint models perform better than the pipeline models on both corpora. 
The second part of the table presents comparisons of Bi-LSTM and its variants for MER and MEN. Adding CRF layer on both Bi-LSTM and Bi-LSTM-CNNs can not bring significant improvements. It might because the most of entity mentions in our data sets are single-word entities, i.e., entity with one word. CNN layer for char-level representation causes significant improvements on Bi-LSTM and its variants for both MER and MEN. 
Bi-LSTM-CNNs and Bi-LSTM-CNNs-CRF significantly outperform Bi-LSTM and Bi-LSTM-CRF respectively, showing that character-level word representations are important for both recognition and normalization. The improvents rely on two clues, 1) different medical entities usually have the same prefix and suffix, such as \textbf{acet}ate, \textbf{acet}one, antr\textbf{itis} and pharyng\textbf{itis}. Modeling such character-level information can benefit recognition; 2) different names which refer to the same medical entity usually share the same character fragments, such as \textbf{Zo}lmitriptan, \textbf{Zomig} and \textbf{Zomig}on. Modeling such character-level information can benefit normalization.
\begin{table*}[tb]
	%\vspace{-0.1in}
	\small
	\centering
	\begin{tabular}{l|cc|cc}
		\hline
		\multirow{2}{*}{Method} & \multicolumn{2}{c|}{ \textbf{NCBI}} & \multicolumn{2}{c}{\textbf{BC5CDR}}\\
		%\cline{2-7}
		\multirow{2}{*}{}&  Recognition&Normalization&Recognition&Normalization\\
		\hline
		LeadMine \cite{Lowe2015LeadMineDI}&-&-&- &\textbf{0.8612}\\
		Dnorm \cite{Leaman2013DNorm} &0.7980& 0.7820 &- & 0.8064 \\
		TaggerOne \cite{Leaman2016TaggerOne} &\textbf{0.8290}& 0.8070&0.8260 & 0.8370\\
		Transition-based Model \cite{Lou2017A}            &0.8205& \textbf{0.8262}& \textbf{0.8382} & 0.8562 \\
		IDCNN \cite{strubell-EtAl:2017:EMNLP2017}  & 0.7983& 0.7425& 0.8011 & 0.8107 \\   
		\hline 
		Bi-LSTM         &0.8075&0.7934&0.8060&0.8136\\   
		Bi-LSTM-CRF     &0.8077&0.7933&0.8062&0.8136\\
		Bi-LSTM-CNNs    &0.8246&0.8059&0.8464&0.8447\\   
		Bi-LSTM-CNNs-CRF&0.8248&0.8061&0.8466&0.8449\\   
		\hline
		MTL                    && &  &   \\
		\hdashline
		+Bi-LSTM         &0.8532&0.8435&0.8321&0.8440\\
		+Bi-LSTM-CRF     &0.8532&0.8436&0.8321&0.8442\\   
		+Bi-LSTM-CNNs    &0.8647&0.8693&0.8632&0.8720\\   
		+Bi-LSTM-CNNs-CRF&0.8648&0.8693&0.8632&0.8722\\   
		\hline
		MTL-MEN\_feedback      && &  &  \\   
		\hdashline  
		+Bi-LSTM         &0.8574&0.8542&0.8419&0.8530  \\
		+Bi-LSTM-CRF     &0.8575&0.8542&0.8420&0.8532\\   
		+Bi-LSTM-CNNs    &0.8653&0.8706&0.8642&0.8813\\   
		+Bi-LSTM-CNNs-CRF&0.8654&0.8709&0.8645&0.8813\\   
		\hline
		MTL-MER\_feedback      && &   &   \\
		\hdashline
		+Bi-LSTM          &0.8637&0.8608&0.8474&0.8576 \\
		+Bi-LSTM-CRF      &0.8638&0.8609&0.8477&0.8576\\
		+Bi-LSTM-CNNs     &0.8723&0.8731&0.8736&0.8821\\
		+Bi-LSTM-CNNs-CRF &0.8725&0.8733&0.8739&0.8822\\
		\hline
		MTL-MEN\&MER\_feedback & &&    & \\
		\hdashline
		+Bi-LSTM          &0.8699&0.8657&0.8538&0.8645\\
		+Bi-LSTM-CRF      &0.8699&0.8658&0.8539&0.8647\\
		+Bi-LSTM-CNNs     &\textbf{0.8743}&\textbf{0.8823}&0.8762&\textbf{0.8917}\\
		+Bi-LSTM-CNNs-CRF &0.8743&0.8823 &\textbf{0.8763}&0.8917\\
		\hline
	\end{tabular}
	\vspace{-0.1in}	
	\caption{F1 score of medical named entity recognition and normalization on two corpora.}\label{tab: main}
	\vspace{-0.15in}
\end{table*}

From the third part of Table~\ref{tab: main}, we can see that MTL framework with Bi-LSTM and its variants significantly outperforms the pipeline use of Bi-LSTM and its variants, which indicates the contribution of general representations of MER and MEN provided by MTL.
The fourth part and fifth part of Table~\ref{tab: main} present the improvements by incorporating the feedback from MEN and the feedback from MER. Both feedback strategies can improve the performance of MER and MEN on both corpora, but the feedback from MER performs better. 
It makes sense because the original order of task hierarchy is from MER to MEN. Therefore, it is very natural that MEN needs more supports from MER than MER needs from MEN.
The last part of Table~\ref{tab: main} presents the results of Bi-LSTM and its variants in the MTL framework with both feedback of MEN and MER. This feedback-based MTL framework achieves the best result on each Bi-LSTM based models, indicating it is the best MTL framework on Bi-LSTM based models for MER and MEN.

\subsection{Effect of Dropout}
Table~\ref{tab: dropout} compares the results with and without dropout layers for training sets. All other
hyper-parameters and features remain the same as our best model in Table~\ref{tab: main}.
We observe slightly improvements for the two
tasks on both corpora. It confirms the function of dropout in reducing over-fitting reported by \citeauthor{Srivastava2014Dropout} (\citeyear{Srivastava2014Dropout}).
\begin{table}[h]
	%\vspace{-0.1in}
	\small
	\centering
	\begin{tabular}{l|c|c|c|c}
		\hline
		\multirow{2}{*}{}&\multicolumn{2}{c|}{\textbf{NCBI}}&\multicolumn{2}{c}{\textbf{BC5CDR}}\\
		\cline{2-5}
		\multirow{2}{*}{}& MER&MEN&MER&MEN\\
		\hline
		No&0.8669&0.8713&0.8722&0.8821\\
		Yes &\textbf{0.8743}&\textbf{0.8823}&\textbf{0.8763}&\textbf{0.8917}\\
		\hline
	\end{tabular}
	\vspace{-0.1in}
	\caption{Results with and without dropout on two
		tasks (F1 score for both MER and MEN).}\label{tab: dropout}	
	%\vspace{-0.15in}
	\vspace{-0.2in}
\end{table}

\subsection{Influence of Word Embeddings}
As mentioned in Section~\ref{sec: embeddings}, in order to test the importance of pre-trained word embeddings, we performed experiments with different sets of publicly published word embeddings, as well as a random sampling method, to initialize our model. Table~\ref{tab: embeddings} gives the performance of three different word embeddings, as well as the randomly sampled one.
According to the results in Table~\ref{tab: embeddings}, models using pre-trained word embeddings achieve a significant improvement as opposed to the ones using random embeddings. Both MER and MEN rely heavily on pre-trained embeddings. This is consistent with results of previous work~\cite{HuangXY15,TACL792}.

\begin{table*}[tb]
	%\vspace{-0.1in}
	\small
	\centering
	\begin{tabular}{l|c|c|c|c|c}
		\hline
		\multirow{2}{*}{\textbf{Embedding}}& \multirow{2}{*}{Dimension}&\multicolumn{2}{c|}{\textbf{NCBI}}&\multicolumn{2}{c}{\textbf{BC5CDR}}\\
		\cline{3-6}
		\multirow{2}{*}{}&  \multirow{2}{*}{}&Recognition&Normalization&Recognition&Normalization\\
		\hline
		Random&100&0.7532&0.7746&0.7665&0.7725\\
		Senna &50&0.7944&0.8016&0.7911&0.7966\\
		GloVe &100&0.7963&0.8042&0.8009&0.8062\\
		Word2Vec&50&\textbf{0.8743}&\textbf{0.8823}&\textbf{0.8763}&\textbf{0.8917}\\
		\hline
	\end{tabular}
	\vspace{-0.1in}
	\caption{Results with different choices of word embeddings on the two tasks (F1 score).}\label{tab: embeddings}	
	\vspace{-0.1in}
\end{table*}
For different pre-trained embeddings Word2Vec 50 dimensional embeddings achieve best results on both tasks.
 This is different from the results reported by \cite{MaH16}, where Glove achieved significantly better performance on NER than Word2Vec embedding.  Senna 50-dimensional embeddings obtain similar performance with Glove on MER and MEN, also significant behind Word2Vec. 
 One possible reason that Word2Vec is significantly better than the other two embeddings on MER and MEN is using domain related text for training embeddings. Word2Vec embeddings were trained on PubMed abstracts and the full-text articles. The other two embeddings which were trained on other domain text, leading to vocabulary mismatch of entities.% especially for medical named entities.

\subsection{Boundary Inconsistency Error Analysis}
Since we model MER and MEN as different sequence labeling tasks, the result of MER likely have different boundary with the result of MEN. Table~\ref{tab: boundary} compares the ratios of boundary inconsistency of MER and MEN on each test set of both corpora. It is clear that our proposed MTL framework with two feedback strategies on MER and MEN can significantly alleviate the boundary inconsistency of MER and MEN thus improve the performance.
\begin{table}[h]
	\small
	\centering
	\begin{tabularx}{0.47\textwidth}{X|c|c}
		\hline
		&\textbf{NCBI}&\textbf{BC5CDR}\\	
		\hline
		Bi-LSTM-CNNs-CRF&0.0635&0.0563\\
		\hdashline
		MTL &\multirow{2}{*}{0.0412}&\multirow{2}{*}{0.0383}\\
		+Bi-LSTM-CNNs-CRF&\multirow{2}{*}{}&\multirow{2}{*}{}\\
		\hdashline
		MTL-MEN\&MER\_feedback &\multirow{2}{*}{\textbf{0.0134}}&\multirow{2}{*}{\textbf{0.0114}}\\
		+Bi-LSTM-CNNs-CRF&\multirow{2}{*}{}&\multirow{2}{*}{}\\
		\hline
	\end{tabularx}
	\vspace{-0.1in}
	\caption{Ratios of the boundary inconsistency of MER and MEN on two test sets.}\label{tab: boundary}	
	\vspace{-0.2in}
\end{table}

\subsection{OOV Entities Error Analysis}
To better understand the behavior of our model, we perform error analysis on Out-of-Vocabulary words (OOV). Specifically, we partition each data set into four subsets â€” in-vocabulary words (IV), out-of-training-vocabulary words (OOTV), out-of-embedding-vocabulary words (OOEV) and
out-of-both-vocabulary words (OOBV). A word is considered IV if it appears in both the training
and embedding vocabulary, while OOBV if neither. OOTV words are the ones do not appear in
training set but in embedding vocabulary, while
OOEV are the ones do not appear in embedding vocabulary but in training set. 
An entity is considered as OOBV if there exists at lease
one word not in training set and at least one word
not in embedding vocabulary, and the other three
subsets can be done in similar manner. Table~\ref{tab: oov_stat} presents the statistics of the partition on each corpus.
The embedding we used is  pre-trained 50-dimensional embeddings in \cite{Pyysalo2013b}, the same as Section~\ref{sec: main}.
\begin{table}[h]
	%\vspace{-0.1in}
	\small
	\centering
	\begin{tabular}{l|c|c}
		\hline
		&\textbf{NCBI}&\textbf{BC5CDR}\\	
		\hline
		IV&987&5,421\\
		OOTV &33&127\\
		OOEV &16&33\\
		OOBV &10&142\\
		\hline
	\end{tabular}
	\vspace{-0.1in}
	\caption{Statistics of the partition on each test set.
		It lists the number of unique entities.}\label{tab: oov_stat}	
	\vspace{-0.2in}
\end{table}


Table~\ref{tab: oov_result} illustrates the performance of our best model
on different subsets of entities. 
The largest improvements appear on the IV and OOTV subsets of both the two corpora on both tasks. This demonstrates
that by feeding into multi-task learning framework with explicit feedback, our model
is more powerful on entities that appear in pre-trained embedding sets, which shows the superiority of our model to make better use of pre-trained word embeddings and deal with entities which do not appear in training set.
\begin{table}[h]
	%\vspace{-0.1in}
	\small
	\centering
	\begin{tabular}{l|c|c|c|c}
		\hline
		\multirow{2}{*}{}&\multicolumn{2}{c|}{\textbf{NCBI}}&\multicolumn{2}{c}{\textbf{BC5CDR}}\\
		\cline{2-5}
		\multirow{2}{*}{}& MER&MEN&MER&MEN\\
		\hline
		\multicolumn{5}{c}{Bi-LSTM-CNNs-CRF}\\
		\hdashline
		IV   &0.8451&0.8254&0.8738&0.8677\\
		OOTV &0.8046&0.8094&0.8279&0.8354\\
		OOEV &0.7776&0.8064&0.7835&0.7821\\
		OOBV &0.7221&0.7354&0.6937&0.7223\\
		\hline
		\multicolumn{5}{c}{MTL-MEN\&MER\_feedback+Bi-LSTM-CNNs-CRF}\\
		\hdashline
		IV   &0.8931&0.9017&0.9042&0.9136\\
		OOTV &0.8667&0.8753&0.8661&0.8832\\
		OOEV &0.8053&0.8132&0.8163&0.82217\\
		OOBV &0.7668&0.7713&0.7345&0.7804\\
		\hline	
	\end{tabular}
	\vspace{-0.1in}
	\caption{Comparison of performance of our model on different subsets of entities (F1 score).}\label{tab: oov_result}	
	%\vspace{-0.15in}
	\vspace{-0.2in}
\end{table}
