%Medical named entity recognition and normalization are fundamental tasks in medical literature mining because the main developments in this area are usually related to these two. 
%Existing studies typically use pipeline models to implement medical named entity recognition and normalization separately. However, these two tasks, which are hierarchical and have mutual dependencies, can promote each other in several ways. Inspired by potential mutual benefits between entity recognition and normalization, we jointly explore entity recognition and normalization and feed them into a deep neural multi-task learning framework with explicit feedback strategies. Our approach allows the information from each task to improve the performance of the other via sharing representations and involving strategies of the explicit mutual feedback between entity recognition and normalization. Our multi-task learning models perform significantly better than state-of-the-art approaches on two publicly available medical literature datasets.

%State-of-the-art studies have demonstrated the superiority of joint modeling of medical named entity recognition and normalization compared to pipeline implementation due to mutual benefits between medical named entity recognition and normalization. 
State-of-the-art studies have demonstrated the superiority of joint modeling over pipeline implementation for medical named entity recognition and normalization due to the mutual benefits between the two processes. 
%To exploit the mutual benefits in a more advanced and intelligent way, we propose a novel deep neural multi-task learning framework with explicit feedback strategies to jointly model recognition and normalization. 
To exploit these benefits in a more sophisticated way, we propose a novel deep neural multi-task learning framework with explicit feedback strategies to jointly model recognition and normalization.
%On the one hand, our method benefits from general representations for both tasks via multi-task learning. 
On one hand, our method benefits from the general representations of both tasks provided by multi-task learning.
%On the other hand, our method successfully converts hierarchical tasks into parallel multi-task mode but maintains the mutual supports between tasks. 
On the other hand, our method successfully converts hierarchical tasks into a parallel multi-task setting while maintaining the mutual supports between tasks. 
%Both these two aspects lead to better performance.
Both of these aspects improve the model performance. 
Experimental results demonstrate that our method performs significantly better than state-of-the-art approaches on two publicly available medical literature datasets.