% please sort the entries by their ids
% please use the id generated by Google Scholar bibtex 
% please use the peer-reviewed version of a paper if there is any, e.g. ICML version > arXiv version.
% good to include a URL to the work
% good to use the bibtex body from the conference site
@inproceedings{ahmad2021unified,
  title={Unified Pre-training for Program Understanding and Generation},
  author={Ahmad, Wasi and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2655--2668},
  year={2021}
}

@inproceedings{allamanis2013mining,
  title={Mining Source Code Repositories at Massive Scale using Language Modeling},
  author={Allamanis, Miltiadis and Sutton, Charles},
  booktitle={2013 10th Working Conference on Mining Software Repositories (MSR)},
  pages={207--216},
  year={2013},
  organization={IEEE}
}

@article{allamanis2018survey,
  title={A survey of machine learning for big code and naturalness},
  author={Allamanis, Miltiadis and Barr, Earl T and Devanbu, Premkumar and Sutton, Charles},
  journal={ACM Computing Surveys (CSUR)},
  volume={51},
  number={4},
  pages={1--37},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@inproceedings{alon2020structural,
  title={Structural Language Models of Code},
  author={Alon, Uri and Sadaka, Roy and Levy, Omer and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  pages={245--256},
  year={2020},
  organization={PMLR}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{baker2007finding,
  title={Finding clones with dup: Analysis of an experiment},
  author={Baker, Brenda S},
  journal={IEEE Transactions on Software Engineering},
  volume={33},
  number={9},
  pages={608--621},
  year={2007},
  publisher={IEEE}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{clement2021long,
  title={Long-Range Modeling of Source Code Files with {eWASH}: Extended Window Access by Syntax Hierarchy},
  author={Clement, Colin and Lu, Shuai and Liu, Xiaoyu and Tufano, Michele and Drain, Dawn and Duan, Nan and Sundaresan, Neel and Svyatkovskiy, Alexey},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={4713--4722},
  year={2021},
  url = "https://aclanthology.org/2021.emnlp-main.387",
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@inproceedings{feng2020codebert,
  title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={1536--1547},
  year={2020}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@inproceedings{guo2020graphcodebert,
  title={GraphCodeBERT: Pre-training Code Representations with Data Flow},
  author={Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Shujie, LIU and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{guo2022unixcoder,
  title={UniXcoder: Unified Cross-Modal Pre-training for Code Representation},
  author={Guo, Daya and Lu, Shuai and Duan, Nan and Wang, Yanlin and Zhou, Ming and Yin, Jian},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={7212--7225},
  year={2022}
}

@article{haque2022fixeval,
  title={FixEval: Execution-based Evaluation of Program Fixes for Competitive Programming Problems},
  author={Haque, Md Mahim Anjum and Ahmad, Wasi Uddin and Lourentzou, Ismini and Brown, Chris},
  journal={arXiv preprint arXiv:2206.07796},
  year={2022}
}

@inproceedings{hellendoorn2019global,
  title={Global Relational Models of Source Code},
  author={Hellendoorn, Vincent J and Sutton, Charles and Singh, Rishabh and Maniatis, Petros and Bieber, David},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{hendrycks2021measuring,
  title={Measuring Coding Challenge Competence With APPS},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year={2021}
}

@article{hindle2016naturalness,
  title={On the naturalness of software},
  author={Hindle, Abram and Barr, Earl T and Gabel, Mark and Su, Zhendong and Devanbu, Premkumar},
  journal={Communications of the ACM},
  volume={59},
  number={5},
  pages={122--131},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@article{husain2019codesearchnet,
  title={Codesearchnet challenge: Evaluating the state of semantic code search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}

@inproceedings{kanade2020learning,
  title={Learning and evaluating contextual embedding of source code},
  author={Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
  booktitle={International Conference on Machine Learning},
  pages={5110--5121},
  year={2020},
  organization={PMLR}
}

@inproceedings{karampatsis2020big,
  title={Big code!= big vocabulary: Open-vocabulary models for source code},
  author={Karampatsis, Rafael-Michael and Babii, Hlib and Robbes, Romain and Sutton, Charles and Janes, Andrea},
  booktitle={2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)},
  pages={1073--1085},
  year={2020},
  organization={IEEE}
}

@inproceedings{kim2021code,
  title={Code prediction by feeding trees to transformers},
  author={Kim, Seohyun and Zhao, Jinman and Tian, Yuchi and Chandra, Satish},
  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
  pages={150--162},
  year={2021},
  organization={IEEE}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and others},
  journal={arXiv preprint arXiv:2203.07814},
  year={2022}
}

@misc{liu2016neural,
  title={Neural code completion},
  author={Liu, Chang and Wang, Xin and Shin, Richard and Gonzalez, Joseph E and Song, Dawn},
  year={2016},
  howpublished={openreview.net/forum?id=rJbPBt9lg},
  note = {Accessed: 2022-08-15}
}

@inproceedings{li2018code,
  title={Code completion with neural attention and pointer networks},
  author={Li, Jian and Wang, Yue and Lyu, Michael R and King, Irwin},
  booktitle={Proceedings of the 27th International Joint Conference on Artificial Intelligence},
  pages={4159--25},
  year={2018}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{liu2020multi,
  title={Multi-task learning based pre-trained language model for code completion},
  author={Liu, Fang and Li, Ge and Zhao, Yunfei and Jin, Zhi},
  booktitle={Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
  pages={473--485},
  year={2020}
}

@inproceedings{liu2021kg,
  title={{KG-BART}: Knowledge Graph-Augmented {BART} for Generative Commonsense Reasoning},
  author={Liu, Ye and Wan, Yao and He, Lifang and Peng, Hao and Philip, S Yu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35(7)},
  pages={6418--6425},
  year={2021},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/16796},
}

@article{lopez2022ast,
  title={AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models},
  author={L{\'o}pez, Jos{\'e} Antonio Hern{\'a}ndez and Weyssow, Martin and Cuadrado, Jes{\'u}s S{\'a}nchez and Sahraoui, Houari},
  journal={arXiv preprint arXiv:2206.11719},
  year={2022}
}

@inproceedings{loshchilov2018decoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{lu2021codexglue,
  title={{CodeXGLUE}: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
  author={Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and others},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
  year={2021}
}

@inproceedings{lu2022reacc,
  title={{ReACC}: A Retrieval-Augmented Code Completion Framework},
  author={Lu, Shuai and Duan, Nan and Han, Hojae and Guo, Daya and Hwang, Seung-won and Svyatkovskiy, Alexey},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6227--6240},
  year={2022}
}

@article{lyu2021embedding,
  title={Embedding API dependency graph for neural code generation},
  author={Lyu, Chen and Wang, Ruyun and Zhang, Hongyu and Zhang, Hanwen and Hu, Songlin},
  journal={Empirical Software Engineering},
  volume={26},
  number={4},
  pages={1--51},
  year={2021},
  publisher={Springer}
}

@article{mukherjee2021neural,
  title={Neural program generation modulo static analysis},
  author={Mukherjee, Rohan and Wen, Yeming and Chaudhari, Dipak and Reps, Thomas and Chaudhuri, Swarat and Jermaine, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18984--18996},
  year={2021}
}

@article{nijkamp2022conversational,
  title={A conversational paradigm for program synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

@inproceedings{peng2021could,
  title={How could Neural Networks understand Programs?},
  author={Peng, Dinglan and Zheng, Shuxin and Li, Yatao and Ke, Guolin and He, Di and Liu, Tie-Yan},
  booktitle={International Conference on Machine Learning},
  pages={8476--8486},
  year={2021},
  organization={PMLR}
}

@article{poesia2022synchromesh,
  title={{Synchromesh}: Reliable code generation from pre-trained language models},
  author={Poesia, Gabriel and Polozov, Oleksandr and Le, Vu and Tiwari, Ashish and Soares, Gustavo and Meek, Christopher and Gulwani, Sumit},
  journal={arXiv preprint arXiv:2201.11227},
  year={2022}
}

@article{puri2021project,
  title={Project codenet: A large-scale ai for code dataset for learning a diversity of coding tasks},
  author={Puri, Ruchir and Kung, David S and Janssen, Geert and Zhang, Wei and Domeniconi, Giacomo and Zolotov, Vladmir and Dolby, Julian and Chen, Jie and Choudhury, Mihir and Decker, Lindsey and others},
  journal={arXiv preprint arXiv:2105.12655},
  volume={1035},
  year={2021}
}

@article{qiu2020pre,
  title={Pre-trained models for natural language processing: A survey},
  author={Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
  journal={Science China Technological Sciences},
  volume={63},
  number={10},
  pages={1872--1897},
  year={2020},
  publisher={Springer}
}

@misc{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI},
  url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{rahman2019natural,
  title={Natural software revisited},
  author={Rahman, Musfiqur and Palani, Dharani and Rigby, Peter C},
  booktitle={Proceedings of the 41st International Conference on Software Engineering},
  pages={37--48},
  year={2019}
}

@article{raychev2016probabilistic,
  title={Probabilistic Model for Code with Decision Trees},
  author={Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
  journal={ACM SIGPLAN Notices},
  pages={731--747},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@inproceedings{roy2008empirical,
  title={An empirical study of function clones in open source software},
  author={Roy, Chanchal K and Cordy, James R},
  booktitle={2008 15th Working Conference on Reverse Engineering},
  pages={81--90},
  year={2008},
  organization={IEEE}
}

@inproceedings{svyatkovskiy2020intellicode,
  title={Intellicode compose: Code generation using transformer},
  author={Svyatkovskiy, Alexey and Deng, Shao Kun and Fu, Shengyu and Sundaresan, Neel},
  booktitle={Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1433--1443},
  year={2020}
}

@inproceedings{svyatkovskiy2021fast,
  title={Fast and memory-efficient neural code completion},
  author={Svyatkovskiy, Alexey and Lee, Sebastian and Hadjitofi, Anna and Riechert, Maik and Franco, Juliana Vicente and Allamanis, Miltiadis},
  booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)},
  pages={329--340},
  year={2021},
  organization={IEEE}
}

@article{tufano2019empirical,
  title={An empirical study on learning bug-fixing patches in the wild via neural machine translation},
  author={Tufano, Michele and Watson, Cody and Bavota, Gabriele and Penta, Massimiliano Di and White, Martin and Poshyvanyk, Denys},
  journal={ACM Transactions on Software Engineering and Methodology (TOSEM)},
  volume={28},
  number={4},
  pages={1--29},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@inproceedings{wang2021code,
  title={Code completion by modeling flattened abstract syntax trees as graphs},
  author={Wang, Yanlin and Li, Hui},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={16},
  pages={14015--14023},
  year={2021}
}

@inproceedings{wang2021codet5,
  title={{CodeT5}: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={8696--8708},
  year={2021}
}

@misc{wang2021gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{github.com/kingoflolz/mesh-transformer-jax}},
  year = {2021},
  note = {Accessed: 2022-08-15}
}

@inproceedings{wang2021lightweight,
  title={Lightweight global and local contexts guided method name recommendation with prior knowledge},
  author={Wang, Shangwen and Wen, Ming and Lin, Bo and Mao, Xiaoguang},
  booktitle={Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={741--753},
  year={2021}
}

@article{weyssow2020combining,
  title={Combining code embedding with static analysis for function-call completion},
  author={Weyssow, Martin and Sahraoui, Houari and Fr{\'e}nay, Beno{\i}t and Vanderose, Beno{\i}t},
  journal={arXiv preprint arXiv:2008.03731},
  year={2020}
}

@inproceedings{yasunaga2021break,
  title={Break-it-fix-it: Unsupervised learning for program repair},
  author={Yasunaga, Michihiro and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={11941--11952},
  year={2021},
  organization={PMLR}
}

@inproceedings{zhang2021greaselm,
  title={{GreaseLM}: Graph REASoning Enhanced Language Models for Question Answering},
  author={Xikun Zhang and Antoine Bosselut and Michihiro Yasunaga and Hongyu Ren and Percy Liang and Christopher D Manning and Jure Leskovec},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=41e9o6cQPj}
}







