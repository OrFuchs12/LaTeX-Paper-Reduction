\section{Related Work}
\label{sec:related-work}

\paragraph{Datasets.}
The lack of cross-file and cross-project (e.g. dependencies) information is a general issue in current evaluation datasets for code.
In terms of code completion, common choices are Py150 \citep{raychev2016probabilistic} for Python and Github Java Corpus \citep{allamanis2013mining} for Java. Both datasets are constructed at file level, where source files are isolated from their project and dependencies and no consideration of project separation is taken in constructing training and test sets.
\citet{lu2022reacc} constructed a code completion dataset from CodeNet \citep{puri2021project}, which contains coding problems and solutions from online judge websites and also lacks project context. 
\citet{clement2021long} presented a real-world Python method generation task based on CodeSearchNet \citep{husain2019codesearchnet} but the auxiliary information they extract still comes from within a local file. 
\citet{svyatkovskiy2021fast} constructed a completion dataset based on top Python repositories on GitHub and released the URLs for these repositories. 
However, those repositories are not write-protected and can change over time. Besides, setting up the dependency environments at scale for further analysis is not easy. 
Both make their dataset difficult to reproduce.
In the contrast, we release the code and the dependencies for the projects to ensure reproducibility.
Apart from code completion, datasets for other code tasks such as Cloze test \citep[e.g.][]{feng2020codebert}, code refinement \citep[e.g.][]{tufano2019empirical, yasunaga2021break, haque2022fixeval}, and generating code from text descriptions \citep[e.g.][]{chen2021evaluating, hendrycks2021measuring, austin2021program}, are often small and mostly without project-level code context. 
Beyond-local information is beneficial for programmers to solve programming tasks in real-world settings. The lack of such information in the current dataset would restrict the progress into high-level semantic understanding and reasoning in the code domain.


\paragraph{Code language models.}
Encouraged by the success of pretrained language models in natural language processing \citep{devlin2019bert, liu2019roberta, lewis2019bart, raffel2020exploring} and the promise of naturalness in code \citep{hindle2016naturalness, allamanis2018survey}, we have seen rising adaptations of language models for code. For example, CuBERT \citep{kanade2020learning} and CodeBERT \citep{feng2020codebert} are pretrained based on masked language modeling. GPT-C \citep{svyatkovskiy2020intellicode} and CodeGPT \citep{lu2021codexglue} are both pretrained based on unidirectional language modeling. PLBART \citep{ahmad2021unified} and CodeT5 \citep{wang2021codet5} are pretrained encoder-decoder structures which adopts denoising objectives and can support code understanding and code generation. UnixCoder \citep{guo2022unixcoder} combines the above three pretraining objectives for a unified pretrained model. 




\paragraph{Code completion.}
Code completion is an essential feature for modern IDEs and an important topic for code intelligence. 
In recent years, deep neural networks \citep{liu2016neural, li2018code, alon2020structural, liu2020multi, kim2021code}, especially pretrained language models \citep{svyatkovskiy2020intellicode, lu2021codexglue} become the mainstream solution to this task. 
Still, incorporating additional information proved beneficial.
One popular choice is abstract syntax tree, e.g. \citet{kim2021code, peng2021could, guo2022unixcoder}. 
However, \citet{lopez2022ast} suggested that pretrained code language models may have already encoded the syntax.  
Other proposals seek to use data flow graph, control graph, and various graph relations, e.g. \citet{guo2020graphcodebert, hellendoorn2019global}.
However, information is still restricted from a single file.
We instead try to enhance the model with out-of-file information, similar to what is accessible in a development environment.

For project-level analyzer induced information, \citet{svyatkovskiy2021fast} described a way to use a static analyzer to refine completion candidates from neural methods.
\citet{weyssow2020combining} considered leveraging the project-wise contexts via embeddings for better function call completion performance.
Other than code completion, project-level information has been utilized for methods name prediction~\citep{wang2021lightweight} and generating code from text descriptions~\citep{lyu2021embedding}.
However, none of them tested their approaches with pretrained code language models. 
In terms of incorporating additional context through concatenation,
\citet{clement2021long} reported improvements from prioritize certain parts of in-file context.
Recently, \citet{lu2022reacc} proposed to enhance code language models by concatenating similar code fragments retrieved by a neural network. Despite the general similarity, we 1) use a simple lightweight way to retrieve auxiliary information instead of training a heavy retriever; 2) do not restrict ourselves on similar code fragments and show that dissimilar code fragments (function implementation) can be helpful; 3) explore task-specific fine-tuning with retrieved information for better completion.




