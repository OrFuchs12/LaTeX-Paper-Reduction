\section{Introduction}
\label{sec:introduction}

Following their counterparts in the natural language domain, we see rapid adoption of language models in the code domain, e.g. Code-GPT~\citep{lu2021codexglue}, GPT-J~\citep{wang2021gpt-j}, Codex~\citep{chen2021evaluating}, PLBART~\citep{ahmad2021unified}, CodeT5~\citep{wang2021codet5}, CodeGen~\citep{nijkamp2022conversational}, to name a few.
Such \emph{code language models} have demonstrated amazing abilities in achieving new state of the art in various code-related tasks such as clone detection \citep{feng2020codebert,guo2020graphcodebert, ahmad2021unified, wang2021codet5}, code completion \citep{svyatkovskiy2020intellicode, kim2021code, lu2021codexglue, guo2022unixcoder} and code translation \citep{ahmad2021unified, wang2021codet5, guo2022unixcoder}.
Notably, pretrained large code language models have been reported to solve programming problems at a high rate \cite{chen2021evaluating} or at a rate similar to average human contestants \citep{li2022competition}.

Despite their success, these code language models are usually restricted to file-level local code context, and thus miss richer context from project files and libraries. 
This is in contrast to the more common software development setting where the current piece of code only makes sense under the context and constraints posed by the codebase that it resides in.
As a result, models can generate outputs that appear locally plausible but contradict the constraints imposed by the codebase that they operate in. 

Machine-learning-for-code datasets and benchmarks also lack such project-level environments for training and evaluating code language models.
For example, a common choice for evaluating code completion models is Py150 \citep{raychev2016probabilistic} which contains Python source files from Github. 
However, its train-test split is at file level, making it infeasible to obtain dependency information or incorporate project-level analysis. 
Similar problems applies to other existing datasets in the code domain (more in Section \ref{sec:related-work}). 
This motivates us to create a new dataset that better captures what happens in the real-world where software is developed by incorporating external dependencies and spans multiple files. 
Another missed opportunity is the availability program analyzer smarts that are common in today's integrated development environments (IDEs).
Our dataset provides an extensible foundation for the integration of program analyzer information into machine learning for code models.%

We collect thousands of permissively licensed Python packages and create a development environment for each of them along with their dependencies. 
We utilize a language server which can provide language-specific smarts based on project-level analysis. 
We query the language server to extract environment information relevant to a certain code location. 
Our setup supports extracting all analyzer information that is available to a developer through an IDE, potentially beneficial to a wide range of code-related tasks. 
\ifaaai
\fi


As a case study, we focus on the task of \emph{function call argument completion}, a special case of code completion where we predict what arguments should be passed through a given function call.
This task is well suited to analyze the impact of missing cross-file and cross-project information as understanding function calls requires information that spans file boundaries.
A developer who needs to decide what to pass to a function first needs to understand the meaning and the expected usage.
They may also benefit from looking at other usage examples of the same function, especially those from within the current working project. 
In this regard, we build a task-specific dataset by querying the language server to extract function definition, implementation and usage information for each function call instance.
We show in \Secref{sec:code-completion-experiments} that function call argument completion is more challenging than the average code completion cases and it cannot be made trivial through copying similar occurrences (\Tabref{tab:fc_copy}).

We conduct extensive experiments on feeding function-call-related information to various pretrained code language models for function call argument completion under different settings, with or without further training the models with this additional type of information. 
We concatenate different types of analyzer-induced context together with the original local code context as model inputs.
We evaluated the performance of several state-of-the-art pretrained code language models, including decoder-only models CodeGPT, CodeGPT-adapt, CodeGen, and encoder-decoder models CodeT5, PLBART, UnixCoder. 

We find that providing code language models with analyzer-induced context universally improves their accuracy of call argument prediction.
Both the improvements and the best performances are greater with task-specific fine-tuning.
We also found that under similar settings, encoder-decoder models %
perform better than decoder-only models for this task, which is not the case for general code completion \citep{guo2022unixcoder}. 
See more in \Secref{sec:task-specific-experiments}.
The ablation study (\Secref{sec:ablation-study}) further reveals the unique effects of function implementation and function usage information on model completion performances.



Our main contributions are:
\begin{enumerate*}[label=\itshape\roman*\upshape)]
    \item We collect \PyEnvs, a large number of permissively licensed Python packages along with their isolated development environments which supports querying for project-related information. 
    \item We build \CallArgs, a new dataset for function call argument completion which provides function definition, implementation, and usage information for each function call instance.
    \item We conduct extensive experiments on feeding analyzer-induced code context as input into various pretrained code language models for function call argument completion, and report various findings.
\end{enumerate*}
Our code and data will be made available at \url{https://github.com/amazon-research/function-call-argument-completion}.


