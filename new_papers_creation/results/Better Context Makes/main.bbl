\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Ahmad et~al.(2021)Ahmad, Chakraborty, Ray, and
  Chang}]{ahmad2021unified}
Ahmad, W.; Chakraborty, S.; Ray, B.; and Chang, K.-W. 2021.
\newblock Unified Pre-training for Program Understanding and Generation.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, 2655--2668.

\bibitem[{Allamanis et~al.(2018)Allamanis, Barr, Devanbu, and
  Sutton}]{allamanis2018survey}
Allamanis, M.; Barr, E.~T.; Devanbu, P.; and Sutton, C. 2018.
\newblock A survey of machine learning for big code and naturalness.
\newblock \emph{ACM Computing Surveys (CSUR)}, 51(4): 1--37.

\bibitem[{Allamanis and Sutton(2013)}]{allamanis2013mining}
Allamanis, M.; and Sutton, C. 2013.
\newblock Mining Source Code Repositories at Massive Scale using Language
  Modeling.
\newblock In \emph{2013 10th Working Conference on Mining Software Repositories
  (MSR)}, 207--216. IEEE.

\bibitem[{Alon et~al.(2020)Alon, Sadaka, Levy, and Yahav}]{alon2020structural}
Alon, U.; Sadaka, R.; Levy, O.; and Yahav, E. 2020.
\newblock Structural Language Models of Code.
\newblock In \emph{International Conference on Machine Learning}, 245--256.
  PMLR.

\bibitem[{Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan,
  Jiang, Cai, Terry, Le et~al.}]{austin2021program}
Austin, J.; Odena, A.; Nye, M.; Bosma, M.; Michalewski, H.; Dohan, D.; Jiang,
  E.; Cai, C.; Terry, M.; Le, Q.; et~al. 2021.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}.

\bibitem[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,
  Burda, Joseph, Brockman et~al.}]{chen2021evaluating}
Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d.~O.; Kaplan, J.;
  Edwards, H.; Burda, Y.; Joseph, N.; Brockman, G.; et~al. 2021.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}.

\bibitem[{Clement et~al.(2021)Clement, Lu, Liu, Tufano, Drain, Duan,
  Sundaresan, and Svyatkovskiy}]{clement2021long}
Clement, C.; Lu, S.; Liu, X.; Tufano, M.; Drain, D.; Duan, N.; Sundaresan, N.;
  and Svyatkovskiy, A. 2021.
\newblock Long-Range Modeling of Source Code Files with {eWASH}: Extended
  Window Access by Syntax Hierarchy.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, 4713--4722.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{devlin2019bert}
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
\newblock BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, 4171--4186.

\bibitem[{Feng et~al.(2020)Feng, Guo, Tang, Duan, Feng, Gong, Shou, Qin, Liu,
  Jiang et~al.}]{feng2020codebert}
Feng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong, M.; Shou, L.; Qin, B.;
  Liu, T.; Jiang, D.; et~al. 2020.
\newblock CodeBERT: A Pre-Trained Model for Programming and Natural Languages.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, 1536--1547.

\bibitem[{Guo et~al.(2022)Guo, Lu, Duan, Wang, Zhou, and
  Yin}]{guo2022unixcoder}
Guo, D.; Lu, S.; Duan, N.; Wang, Y.; Zhou, M.; and Yin, J. 2022.
\newblock UniXcoder: Unified Cross-Modal Pre-training for Code Representation.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, 7212--7225.

\bibitem[{Guo et~al.(2020)Guo, Ren, Lu, Feng, Tang, Shujie, Zhou, Duan,
  Svyatkovskiy, Fu et~al.}]{guo2020graphcodebert}
Guo, D.; Ren, S.; Lu, S.; Feng, Z.; Tang, D.; Shujie, L.; Zhou, L.; Duan, N.;
  Svyatkovskiy, A.; Fu, S.; et~al. 2020.
\newblock GraphCodeBERT: Pre-training Code Representations with Data Flow.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Haque et~al.(2022)Haque, Ahmad, Lourentzou, and
  Brown}]{haque2022fixeval}
Haque, M. M.~A.; Ahmad, W.~U.; Lourentzou, I.; and Brown, C. 2022.
\newblock FixEval: Execution-based Evaluation of Program Fixes for Competitive
  Programming Problems.
\newblock \emph{arXiv preprint arXiv:2206.07796}.

\bibitem[{Hellendoorn et~al.(2019)Hellendoorn, Sutton, Singh, Maniatis, and
  Bieber}]{hellendoorn2019global}
Hellendoorn, V.~J.; Sutton, C.; Singh, R.; Maniatis, P.; and Bieber, D. 2019.
\newblock Global Relational Models of Source Code.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Hendrycks et~al.(2021)Hendrycks, Basart, Kadavath, Mazeika, Arora,
  Guo, Burns, Puranik, He, Song et~al.}]{hendrycks2021measuring}
Hendrycks, D.; Basart, S.; Kadavath, S.; Mazeika, M.; Arora, A.; Guo, E.;
  Burns, C.; Puranik, S.; He, H.; Song, D.; et~al. 2021.
\newblock Measuring Coding Challenge Competence With APPS.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 2)}.

\bibitem[{Hindle et~al.(2016)Hindle, Barr, Gabel, Su, and
  Devanbu}]{hindle2016naturalness}
Hindle, A.; Barr, E.~T.; Gabel, M.; Su, Z.; and Devanbu, P. 2016.
\newblock On the naturalness of software.
\newblock \emph{Communications of the ACM}, 59(5): 122--131.

\bibitem[{Husain et~al.(2019)Husain, Wu, Gazit, Allamanis, and
  Brockschmidt}]{husain2019codesearchnet}
Husain, H.; Wu, H.-H.; Gazit, T.; Allamanis, M.; and Brockschmidt, M. 2019.
\newblock Codesearchnet challenge: Evaluating the state of semantic code
  search.
\newblock \emph{arXiv preprint arXiv:1909.09436}.

\bibitem[{Kanade et~al.(2020)Kanade, Maniatis, Balakrishnan, and
  Shi}]{kanade2020learning}
Kanade, A.; Maniatis, P.; Balakrishnan, G.; and Shi, K. 2020.
\newblock Learning and evaluating contextual embedding of source code.
\newblock In \emph{International Conference on Machine Learning}, 5110--5121.
  PMLR.

\bibitem[{Karampatsis et~al.(2020)Karampatsis, Babii, Robbes, Sutton, and
  Janes}]{karampatsis2020big}
Karampatsis, R.-M.; Babii, H.; Robbes, R.; Sutton, C.; and Janes, A. 2020.
\newblock Big code!= big vocabulary: Open-vocabulary models for source code.
\newblock In \emph{2020 IEEE/ACM 42nd International Conference on Software
  Engineering (ICSE)}, 1073--1085. IEEE.

\bibitem[{Kim et~al.(2021)Kim, Zhao, Tian, and Chandra}]{kim2021code}
Kim, S.; Zhao, J.; Tian, Y.; and Chandra, S. 2021.
\newblock Code prediction by feeding trees to transformers.
\newblock In \emph{2021 IEEE/ACM 43rd International Conference on Software
  Engineering (ICSE)}, 150--162. IEEE.

\bibitem[{Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer}]{lewis2019bart}
Lewis, M.; Liu, Y.; Goyal, N.; Ghazvininejad, M.; Mohamed, A.; Levy, O.;
  Stoyanov, V.; and Zettlemoyer, L. 2019.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock \emph{arXiv preprint arXiv:1910.13461}.

\bibitem[{Li et~al.(2018)Li, Wang, Lyu, and King}]{li2018code}
Li, J.; Wang, Y.; Lyu, M.~R.; and King, I. 2018.
\newblock Code completion with neural attention and pointer networks.
\newblock In \emph{Proceedings of the 27th International Joint Conference on
  Artificial Intelligence}, 4159--25.

\bibitem[{Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond,
  Eccles, Keeling, Gimeno, Lago et~al.}]{li2022competition}
Li, Y.; Choi, D.; Chung, J.; Kushman, N.; Schrittwieser, J.; Leblond, R.;
  Eccles, T.; Keeling, J.; Gimeno, F.; Lago, A.~D.; et~al. 2022.
\newblock Competition-level code generation with alphacode.
\newblock \emph{arXiv preprint arXiv:2203.07814}.

\bibitem[{Liu et~al.(2016)Liu, Wang, Shin, Gonzalez, and Song}]{liu2016neural}
Liu, C.; Wang, X.; Shin, R.; Gonzalez, J.~E.; and Song, D. 2016.
\newblock Neural code completion.
\newblock openreview.net/forum?id=rJbPBt9lg.
\newblock Accessed: 2022-08-15.

\bibitem[{Liu et~al.(2020)Liu, Li, Zhao, and Jin}]{liu2020multi}
Liu, F.; Li, G.; Zhao, Y.; and Jin, Z. 2020.
\newblock Multi-task learning based pre-trained language model for code
  completion.
\newblock In \emph{Proceedings of the 35th IEEE/ACM International Conference on
  Automated Software Engineering}, 473--485.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{liu2019roberta}
Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.;
  Zettlemoyer, L.; and Stoyanov, V. 2019.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}.

\bibitem[{L{\'o}pez et~al.(2022)L{\'o}pez, Weyssow, Cuadrado, and
  Sahraoui}]{lopez2022ast}
L{\'o}pez, J. A.~H.; Weyssow, M.; Cuadrado, J.~S.; and Sahraoui, H. 2022.
\newblock AST-Probe: Recovering abstract syntax trees from hidden
  representations of pre-trained language models.
\newblock \emph{arXiv preprint arXiv:2206.11719}.

\bibitem[{Loshchilov and Hutter(2018)}]{loshchilov2018decoupled}
Loshchilov, I.; and Hutter, F. 2018.
\newblock Decoupled Weight Decay Regularization.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Lu et~al.(2022)Lu, Duan, Han, Guo, Hwang, and
  Svyatkovskiy}]{lu2022reacc}
Lu, S.; Duan, N.; Han, H.; Guo, D.; Hwang, S.-w.; and Svyatkovskiy, A. 2022.
\newblock {ReACC}: A Retrieval-Augmented Code Completion Framework.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, 6227--6240.

\bibitem[{Lu et~al.(2021)Lu, Guo, Ren, Huang, Svyatkovskiy, Blanco, Clement,
  Drain, Jiang, Tang et~al.}]{lu2021codexglue}
Lu, S.; Guo, D.; Ren, S.; Huang, J.; Svyatkovskiy, A.; Blanco, A.; Clement, C.;
  Drain, D.; Jiang, D.; Tang, D.; et~al. 2021.
\newblock {CodeXGLUE}: A Machine Learning Benchmark Dataset for Code
  Understanding and Generation.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 1)}.

\bibitem[{Lyu et~al.(2021)Lyu, Wang, Zhang, Zhang, and Hu}]{lyu2021embedding}
Lyu, C.; Wang, R.; Zhang, H.; Zhang, H.; and Hu, S. 2021.
\newblock Embedding API dependency graph for neural code generation.
\newblock \emph{Empirical Software Engineering}, 26(4): 1--51.

\bibitem[{Nijkamp et~al.(2022)Nijkamp, Pang, Hayashi, Tu, Wang, Zhou, Savarese,
  and Xiong}]{nijkamp2022conversational}
Nijkamp, E.; Pang, B.; Hayashi, H.; Tu, L.; Wang, H.; Zhou, Y.; Savarese, S.;
  and Xiong, C. 2022.
\newblock A conversational paradigm for program synthesis.
\newblock \emph{arXiv preprint arXiv:2203.13474}.

\bibitem[{Peng et~al.(2021)Peng, Zheng, Li, Ke, He, and Liu}]{peng2021could}
Peng, D.; Zheng, S.; Li, Y.; Ke, G.; He, D.; and Liu, T.-Y. 2021.
\newblock How could Neural Networks understand Programs?
\newblock In \emph{International Conference on Machine Learning}, 8476--8486.
  PMLR.

\bibitem[{Puri et~al.(2021)Puri, Kung, Janssen, Zhang, Domeniconi, Zolotov,
  Dolby, Chen, Choudhury, Decker et~al.}]{puri2021project}
Puri, R.; Kung, D.~S.; Janssen, G.; Zhang, W.; Domeniconi, G.; Zolotov, V.;
  Dolby, J.; Chen, J.; Choudhury, M.; Decker, L.; et~al. 2021.
\newblock Project codenet: A large-scale ai for code dataset for learning a
  diversity of coding tasks.
\newblock \emph{arXiv preprint arXiv:2105.12655}, 1035.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, Liu et~al.}]{raffel2020exploring}
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou,
  Y.; Li, W.; Liu, P.~J.; et~al. 2020.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21(140): 1--67.

\bibitem[{Rahman, Palani, and Rigby(2019)}]{rahman2019natural}
Rahman, M.; Palani, D.; and Rigby, P.~C. 2019.
\newblock Natural software revisited.
\newblock In \emph{Proceedings of the 41st International Conference on Software
  Engineering}, 37--48.

\bibitem[{Raychev, Bielik, and Vechev(2016)}]{raychev2016probabilistic}
Raychev, V.; Bielik, P.; and Vechev, M. 2016.
\newblock Probabilistic Model for Code with Decision Trees.
\newblock \emph{ACM SIGPLAN Notices}, 731--747.

\bibitem[{Svyatkovskiy et~al.(2020)Svyatkovskiy, Deng, Fu, and
  Sundaresan}]{svyatkovskiy2020intellicode}
Svyatkovskiy, A.; Deng, S.~K.; Fu, S.; and Sundaresan, N. 2020.
\newblock Intellicode compose: Code generation using transformer.
\newblock In \emph{Proceedings of the 28th ACM Joint Meeting on European
  Software Engineering Conference and Symposium on the Foundations of Software
  Engineering}, 1433--1443.

\bibitem[{Svyatkovskiy et~al.(2021)Svyatkovskiy, Lee, Hadjitofi, Riechert,
  Franco, and Allamanis}]{svyatkovskiy2021fast}
Svyatkovskiy, A.; Lee, S.; Hadjitofi, A.; Riechert, M.; Franco, J.~V.; and
  Allamanis, M. 2021.
\newblock Fast and memory-efficient neural code completion.
\newblock In \emph{2021 IEEE/ACM 18th International Conference on Mining
  Software Repositories (MSR)}, 329--340. IEEE.

\bibitem[{Tufano et~al.(2019)Tufano, Watson, Bavota, Penta, White, and
  Poshyvanyk}]{tufano2019empirical}
Tufano, M.; Watson, C.; Bavota, G.; Penta, M.~D.; White, M.; and Poshyvanyk, D.
  2019.
\newblock An empirical study on learning bug-fixing patches in the wild via
  neural machine translation.
\newblock \emph{ACM Transactions on Software Engineering and Methodology
  (TOSEM)}, 28(4): 1--29.

\bibitem[{Wang and Komatsuzaki(2021)}]{wang2021gpt-j}
Wang, B.; and Komatsuzaki, A. 2021.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{github.com/kingoflolz/mesh-transformer-jax}.
\newblock Accessed: 2022-08-15.

\bibitem[{Wang et~al.(2021{\natexlab{a}})Wang, Wen, Lin, and
  Mao}]{wang2021lightweight}
Wang, S.; Wen, M.; Lin, B.; and Mao, X. 2021{\natexlab{a}}.
\newblock Lightweight global and local contexts guided method name
  recommendation with prior knowledge.
\newblock In \emph{Proceedings of the 29th ACM Joint Meeting on European
  Software Engineering Conference and Symposium on the Foundations of Software
  Engineering}, 741--753.

\bibitem[{Wang et~al.(2021{\natexlab{b}})Wang, Wang, Joty, and
  Hoi}]{wang2021codet5}
Wang, Y.; Wang, W.; Joty, S.; and Hoi, S.~C. 2021{\natexlab{b}}.
\newblock {CodeT5}: Identifier-aware Unified Pre-trained Encoder-Decoder Models
  for Code Understanding and Generation.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, 8696--8708.

\bibitem[{Weyssow et~al.(2020)Weyssow, Sahraoui, Fr{\'e}nay, and
  Vanderose}]{weyssow2020combining}
Weyssow, M.; Sahraoui, H.; Fr{\'e}nay, B.; and Vanderose, B. 2020.
\newblock Combining code embedding with static analysis for function-call
  completion.
\newblock \emph{arXiv preprint arXiv:2008.03731}.

\bibitem[{Yasunaga and Liang(2021)}]{yasunaga2021break}
Yasunaga, M.; and Liang, P. 2021.
\newblock Break-it-fix-it: Unsupervised learning for program repair.
\newblock In \emph{International Conference on Machine Learning}, 11941--11952.
  PMLR.

\end{thebibliography}
