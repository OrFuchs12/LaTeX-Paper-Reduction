\relax 
\bibstyle{aaai23}
\citation{lu2021codexglue}
\citation{wang2021gpt-j}
\citation{chen2021evaluating}
\citation{ahmad2021unified}
\citation{wang2021codet5}
\citation{nijkamp2022conversational}
\citation{feng2020codebert,guo2020graphcodebert,ahmad2021unified,wang2021codet5}
\citation{svyatkovskiy2020intellicode,kim2021code,lu2021codexglue,guo2022unixcoder}
\citation{ahmad2021unified,wang2021codet5,guo2022unixcoder}
\citation{chen2021evaluating}
\citation{li2022competition}
\citation{raychev2016probabilistic}
\providecommand \oddpage@label [2]{}
\newlabel{sec:introduction}{{1}{1}{}{}{}}
\citation{guo2022unixcoder}
\newlabel{sec:dataset}{{2}{2}{}{}{}}
\newlabel{sec:dataset-pyenvs}{{2.1}{2}{}{}{}}
\newlabel{sec:dataset-callargs}{{2.2}{2}{}{}{}}
\citation{karampatsis2020big,lu2021codexglue,kim2021code,lu2022reacc}
\citation{feng2020codebert}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:generalstat}{{1}{3}{Statistics of the \textsc  {CallArgs}\xspace  dataset. }{}{}}
\newlabel{sec:method}{{3}{3}{}{}{}}
\newlabel{tab:examples}{{2}{3}{An argument completion example for \lstinline {self.arguments.map()}. For unidirectional prediction, the local context consists of the left context (Line 1-4) only; for in-filling prediction, the local context is given as the left local context (Line 1-4) and right local context (Line 6-7). }{}{}}
\citation{lu2021codexglue}
\citation{lu2021codexglue}
\citation{nijkamp2022conversational}
\citation{raychev2016probabilistic}
\citation{loshchilov2018decoupled}
\citation{rahman2019natural}
\citation{lu2022reacc}
\citation{lu2021codexglue}
\citation{guo2022unixcoder}
\citation{nijkamp2022conversational}
\citation{wang2021codet5}
\citation{ahmad2021unified}
\citation{guo2022unixcoder}
\newlabel{sec:method-incorporating}{{3.3}{4}{}{}{}}
\newlabel{sec:code-completion-experiments}{{4.1}{4}{}{}{}}
\newlabel{sec:task-specific-experiments}{{4.2}{4}{}{}{}}
\newlabel{sec:call-args-completion-results}{{4.2}{4}{}{}{}}
\newlabel{tab:form}{{3}{5}{The input formats for decoder-only and encoder-decoder models. \textless {}s\textgreater {}, \textless {}/s\textgreater {}, and \textless {}PREDICT\textgreater {} are special tokens. \textless {}PREDICT\textgreater {} suggests the location to fill in with the call arguments.}{}{}}
\newlabel{tab:autoregressive}{{4}{5}{The call-argument completion performance of several general code completion models on \textsc  {CallArgs}\xspace  . The token-level accuracy is for general code completion. }{}{}}
\newlabel{sec:ablation-study}{{4.3}{5}{}{}{}}
\citation{raychev2016probabilistic}
\citation{allamanis2013mining}
\citation{lu2022reacc}
\citation{puri2021project}
\citation{clement2021long}
\citation{husain2019codesearchnet}
\citation{svyatkovskiy2021fast}
\citation{feng2020codebert}
\citation{tufano2019empirical,yasunaga2021break,haque2022fixeval}
\citation{chen2021evaluating,hendrycks2021measuring,austin2021program}
\newlabel{tab:fc_all}{{5}{6}{Performance of different models with task-specific fine-tuning on \textsc  {CallArgs}\xspace  . Unidirectional results are grouped by model types: decoder-only (top) and encoder-decoder (middle). In-filling results are from encoder-decoder models (bottom). }{}{}}
\newlabel{tab:fc_result}{{6}{6}{Performance of CodeT5-base with different auxiliary contexts. }{}{}}
\newlabel{tab:fc_length}{{7}{6}{Performance of CodeT5-base when using different numbers and lengths of usage contexts. ``Usages'' column indicates the number of function usages used and the average length budget for each usage.}{}{}}
\newlabel{sec:related-work}{{5}{6}{}{}{}}
\citation{devlin2019bert,liu2019roberta,lewis2019bart,raffel2020exploring}
\citation{hindle2016naturalness,allamanis2018survey}
\citation{kanade2020learning}
\citation{feng2020codebert}
\citation{svyatkovskiy2020intellicode}
\citation{lu2021codexglue}
\citation{ahmad2021unified}
\citation{wang2021codet5}
\citation{guo2022unixcoder}
\citation{liu2016neural,li2018code,alon2020structural,liu2020multi,kim2021code}
\citation{svyatkovskiy2020intellicode,lu2021codexglue}
\citation{kim2021code,peng2021could,guo2022unixcoder}
\citation{lopez2022ast}
\citation{guo2020graphcodebert,hellendoorn2019global}
\citation{svyatkovskiy2021fast}
\citation{weyssow2020combining}
\citation{wang2021lightweight}
\citation{lyu2021embedding}
\citation{clement2021long}
\citation{lu2022reacc}
\newlabel{tab:fc_copy}{{8}{7}{Comparing using function usage information during training and during inference for CodeT5-base. }{}{}}
\bibdata{main}
\gdef \@abspage@last{8}
