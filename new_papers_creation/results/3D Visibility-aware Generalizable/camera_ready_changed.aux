\relax 
\bibstyle{aaai24}
\citation{mildenhall2021nerf,gao2022nerf,niemeyer2022regnerf,Johari_2022_CVPR}
\citation{jiang2022instantavatar}
\citation{poole2022dreamfusion}
\citation{turki2022mega}
\citation{muller2022instant,chen2022tensorf}
\citation{mihajlovic2022keypointnerf,kwon2021neural}
\citation{park2022handoccnet,deng2022recurrent}
\citation{mihajlovic2022keypointnerf}
\citation{saito2019pifu}
\citation{kwon2021neural}
\citation{moon2020interhand2}
\citation{meng20223d}
\citation{mildenhall2021nerf}
\citation{deng2022depth,mildenhall2022nerf,johari2022geonerf,niemeyer2022regnerf,Johari_2022_CVPR,martin2021nerf}
\citation{raj2021pixel,wang2021ibrnet,yu2021pixelnerf}
\citation{mihajlovic2022keypointnerf}
\citation{peng2021neural,kwon2021neural}
\citation{kwon2021neural}
\citation{loper2015smpl}
\citation{or2022stylesdf,hong2022eva3d,corona2022lisa}
\citation{corona2022lisa}
\citation{guo2023handnerf}
\citation{hu2023sherf}
\citation{romero2022embodied}
\citation{zhang2021interacting,chen2021camera,kulon2020weakly,zhou2020monocular}
\citation{chen2022hand}
\citation{chen2022alignsdf,karunratanakul2021skeleton}
\citation{gaodart2022}
\citation{goodfellow2020generative}
\citation{schwarz2020graf,chan2021pi,or2022stylesdf}
\citation{hong2022eva3d,deng2022gram,niemeyer2021giraffe,xu2021generative}
\citation{chan2022efficient}
\citation{hong2022eva3d}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:banner}{{1}{2}{Compared with previous generalizable NeRFs, our visibility-aware NeRF not only (a) generates images of better quality, but also tackles challenging tasks such as (b) inpainting obstructed areas and (c) removing hands in interacting scenes.}{}{}}
\citation{mildenhall2021nerf}
\citation{romero2017embodied}
\citation{mihajlovic2022keypointnerf}
\citation{newell2016stacked}
\citation{johnson2016perceptual}
\citation{hong2022eva3d}
\citation{wang2021ibrnet}
\newlabel{fig:framework}{{2}{3}{The framework of VA-NeRF. It consists of two key components and both of them are designed to leverage the visibility of 3D points. The first one is the visibility-aware feature fusion module that estimates appropriate features for query points, while the second one is the visibility-guided adversarial learning strategy that is used to enhance synthesized results.}{}{}}
\newlabel{sec:framework}{{}{3}{}{}{}}
\newlabel{eq:sdf}{{1}{3}{}{}{}}
\newlabel{sec:attn}{{}{3}{}{}{}}
\citation{guler2018densepose}
\citation{johnson2016perceptual}
\citation{mescheder2018training,hong2022eva3d}
\citation{mescheder2018training}
\newlabel{fig:dis}{{3}{4}{Comparison between the traditional binary-class discriminator (left) and the proposed visibility-guided discriminator (right). Note that the visibility map is conditioned on the input view and whether the target image is real or synthesized.}{}{}}
\newlabel{eq:feature}{{3}{4}{}{}{}}
\newlabel{sec:dis}{{}{4}{}{}{}}
\newlabel{eq:loss}{{4}{4}{}{}{}}
\newlabel{eq:vis_loss}{{5}{4}{}{}{}}
\citation{moon2020interhand2}
\citation{kingma2014adam}
\citation{mihajlovic2022keypointnerf}
\citation{kwon2021neural}
\citation{mihajlovic2022keypointnerf}
\citation{guo2023handnerf}
\citation{prokudin2021smplpix}
\citation{sara2019image}
\citation{wang2004image}
\citation{zhang2018unreasonable}
\newlabel{fig:quality}{{4}{5}{Visual comparison of the proposed method against state-of-the-art methods. Results of the proposed method better preserve hand structures and textures.}{}{}}
\newlabel{fig:large_view}{{5}{5}{Qualitative examples of novel-view rendering with large view variations (rotation angles $>$ 30 degrees).}{}{}}
\newlabel{fig:occlusion}{{6}{6}{Qualitative comparison of synthesized images in scenes involving severe occlusions.}{}{}}
\newlabel{tab:Comparison}{{1}{6}{Comparison with state-of-the-art methods on Interhand2.6M.}{}{}}
\newlabel{tab:quanti_view}{{2}{6}{Performance comparison among generalizable NeRFs under large view variations ($>$ 30 degrees).}{}{}}
\newlabel{tab:quanti_mask}{{3}{6}{Performance comparison among generalizable NeRFs under occlusions.}{}{}}
\bibdata{mybib}
\newlabel{fig:vis_dis}{{7}{7}{Visualization of visibility maps. $V_t /V$ are target-view/predicted visibility maps. Our NeRF successfully fools the discriminator as most areas in its synthesized images are recognized as visible.}{}{}}
\newlabel{tab:feats}{{4}{7}{Ablation study on selected features.}{}{}}
\newlabel{tab:attn}{{5}{7}{Ablation study on attention strategies.}{}{}}
\newlabel{tab:disc}{{6}{7}{Ablation study on discriminators.}{}{}}
\gdef \@abspage@last{8}
