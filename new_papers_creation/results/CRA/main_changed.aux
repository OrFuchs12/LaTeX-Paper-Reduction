\relax 
\bibstyle{aaai24}
\citation{cadena2016past,reddy2018carfusion,rusu2008towards}
\citation{yang2018foldingnet,choy20163d,girdhar2016learning,xie2020grnet}
\citation{qi2017pointnet}
\citation{qi2017pointnet++,wang2019dynamic,zhao2021point}
\citation{yuan2018pcn,wen2020point,wen2021pmp,tchapmi2019topnet,yu2021pointr,wang2020cascaded}
\citation{xiang2021snowflakenet,zhou2022seedformer,huang2020pf,yan2022fbnet,huang2020pf,tang2022lake,wang2022learning}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:intro}{{1}{1}{ {\bf  Illustration of our main idea.} Here, we analyze several point generation methods from the perspective of {\em  cross-resolution aggregation} (CRA). (a) Common pipeline of coarse-to-fine completion approaches. (b) The plain generation operation simply generates points without considering explicit CRA. (c) \& (d) Several methods exploit skip connections to aggregate features of other generated point clouds or partial inputs for the current point cloud, which can efficiently capture multi-scale features. (e) Our method not only extracts more fruitful multi-scale features with novel-designed enhanced inter-level CRA but also combines intra-and inter-level CRA for better capturing geometric characteristics.}{}{}}
\citation{yuan2018pcn,huang2020pf}
\citation{yifan2019patch}
\citation{yan2022fbnet,zhou2022seedformer}
\citation{wen2020point,yu2021pointr}
\citation{li2016vehicle,chen2017multi,lang2019pointpillars}
\citation{maturana2015voxnet,song2017semantic,riegler2017octnet,choy20194d}
\citation{qi2017pointnet}
\citation{qi2017pointnet++}
\citation{wang2019dynamic,zhao2019pointweb,simonovsky2017dynamic}
\citation{xu2018spidercnn,wu2019pointconv,thomas2019kpconv}
\citation{vaswani2017attention,zhao2021point,guo2021pct,park2022fast}
\citation{zhao2021point}
\citation{choy20163d,girdhar2016learning,han2017high}
\citation{yuan2018pcn}
\citation{yang2018foldingnet}
\citation{xiang2021snowflakenet,zhou2022seedformer,yan2022fbnet,tang2022lake,wang2022learning,li2023proxyformer,chen2023anchorformer}
\newlabel{sec:rw}{{}{2}{}{}{}}
\citation{zhao2021point}
\citation{qi2017pointnet++}
\citation{qian2021pu,he2023grad}
\newlabel{fig:crt}{{2}{3}{ {\bf  Illustration of Cross-Resolution Transformer (CRT).} CRT considers the cross-resolution aggregation on $m$ scales, and $m=3$ in this figure. (a) Inter-level CRT lets the query point cloud (blue) aggregate features from the support one (green); both of them are intermediate point clouds during the generation phase. (b) Intra-level CRT realizes cross-resolution aggregation inside the current point cloud and is a degenerate form of inter-level one.}{}{}}
\citation{yang2018foldingnet}
\citation{tchapmi2019topnet}
\citation{yuan2018pcn}
\citation{xie2020grnet}
\citation{yu2021pointr}
\citation{xiang2021snowflakenet}
\citation{yan2022fbnet}
\citation{li2023proxyformer}
\citation{zhou2022seedformer}
\citation{chen2023anchorformer}
\citation{qi2017pointnet++}
\citation{zhou2022seedformer}
\citation{xiang2021snowflakenet,wang2020cascaded}
\citation{qi2017pointnet}
\citation{xiang2021snowflakenet}
\citation{qi2017pointnet}
\newlabel{fig:arc}{{3}{4}{ (a) The overall architecture of CRA-PCN, which consists of encoder, seed generator, and stacked up-sampling blocks. (b) The details of the up-sampling block, which is composed of MLP, inter-level Cross-Resolution Transformer, intra-level Cross-Resolution Transformer, and deconvolution.}{}{}}
\citation{fan2017point}
\citation{qi2017pointnet++}
\citation{yuan2018pcn}
\citation{yu2021pointr}
\citation{pan2021multi}
\citation{paszke2019pytorch}
\citation{kingma2014adam}
\citation{yuan2018pcn}
\citation{yu2021pointr}
\citation{pan2021multi}
\citation{yuan2018pcn}
\citation{chang2015shapenet}
\citation{yuan2018pcn}
\newlabel{tab:pcn1}{{1}{5}{Results on PCN dataset in terms of L1 Chamfer Distance $\times $ $10^3$ (lower is better). }{}{}}
\newlabel{fig:pcn}{{4}{5}{Visual comparison on PCN dataset.}{}{}}
\newlabel{eqt:cd}{{}{5}{}{}{}}
\citation{xiang2021snowflakenet,zhou2022seedformer,yan2022fbnet}
\citation{yu2021pointr}
\citation{yu2021pointr}
\citation{yu2021pointr}
\citation{yang2018foldingnet,yuan2018pcn,tchapmi2019topnet,huang2020pf,xie2020grnet,yu2021pointr,zhou2022seedformer}
\citation{yuan2018pcn}
\citation{tchapmi2019topnet}
\citation{liu2020morphing}
\citation{wang2020cascaded}
\citation{pan2020ecg}
\citation{pan2021variational}
\newlabel{tab:shapenet55}{{2}{6}{Results on ShapeNet-55 in terms of L2 Chamfer Distance $\times $ $10^3$ (lower is better). }{}{}}
\newlabel{tab:shapenet34}{{3}{6}{Results on ShapeNet-34 in terms of L2 Chamfer Distance $\times $ $10^3$ (lower is better). }{}{}}
\newlabel{tab:mvp}{{4}{6}{Results on MVP dataset in terms of L2 Chamfer Distance $\times $ $10^4$ (lower is better) and F-Score (higher is better). }{}{}}
\citation{yan2022fbnet}
\citation{zhou2022seedformer}
\citation{chen2023anchorformer}
\citation{pan2021variational}
\citation{tatarchenko2019single}
\citation{zhao2021point}
\newlabel{tab:let}{{5}{7}{Run-time memory usage and latency, which were evaluated on a single GTX 1080Ti graphic card with a batch size of 32.}{}{}}
\newlabel{tab:ab1}{{6}{7}{Analysis of inter- and intra-level Cross-Resolution Transformer in up-sampling block.}{}{}}
\newlabel{tab:ab3}{{7}{7}{Analysis of the number of scales in Cross-Resolution Transformer.}{}{}}
\newlabel{sec:ablation}{{}{7}{}{}{}}
\bibdata{aaai24}
\gdef \@abspage@last{8}
