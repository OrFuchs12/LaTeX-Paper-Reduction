\relax 
\bibstyle{aaai21}
\citation{sun2018beyond,chen2019abd}
\citation{Wei2018PTGAN,qi2019DA,zhong2019invariance}
\citation{lin2019aBottom,li2018unsupervised,wu2019graph}
\citation{vanDerMaaten2008}
\citation{vanDerMaaten2008}
\citation{lin2019aBottom,zeng2020hierarchical}
\citation{li2018unsupervised,chen2018deepa}
\citation{ye2017dynamic,wu2019graph}
\citation{lin2019aBottom,zeng2020hierarchical,unsup_clustering,zhai2020ad}
\citation{krizhevsky2012imagenet}
\citation{7410490}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig_intro}{{1}{1}{(a) T-SNE\nobreakspace  {}\citep  {vanDerMaaten2008} visualization of the feature distribution on Market-1501. The features are extracted by an ImageNet-pretrained model for images of $20$ randomly selected IDs. The images from one camera are marked with the same colored bounding boxes. (b) and (c) display two sub-regions.}{}{}}
\citation{Chen2020SimCLR}
\citation{7410490}
\citation{zheng2017unlabeled}
\citation{wei2018person}
\citation{lin2019aBottom,zeng2020hierarchical}
\citation{li2018unsupervised,chen2018deepa}
\citation{ye2017dynamic,wu2019graph}
\citation{lin2019aBottom}
\citation{zeng2020hierarchical}
\citation{li2018unsupervised}
\citation{chen2018deepa}
\citation{wu2019graph}
\citation{Wei2018PTGAN,Deng2018SPGAN,Liu2019ATNet}
\citation{qi2019DA,Wu2019CA}
\citation{unsup_clustering,zhai2020ad,ge2020self}
\citation{zhong2019invariance}
\citation{unsup_clustering,zhong2019invariance,ge2020self}
\citation{zhai2020ad}
\citation{zhu2019intra,qi2019progressive}
\citation{qi2019progressive}
\citation{qi2019intra}
\citation{zhu2019intra}
\citation{zhu2020intra}
\citation{wang2021}
\citation{hermans2017defense}
\citation{Attias2017proxy}
\citation{Rippel2016multi-center}
\citation{Qian2018proxy}
\citation{qian2019softtriple}
\citation{sohoni2020no}
\citation{unsup_clustering,lin2019aBottom,zeng2020hierarchical}
\citation{unsup_clustering,lin2019aBottom,zeng2020hierarchical}
\citation{krizhevsky2012imagenet}
\citation{ester1996density}
\citation{wu2018memory}
\citation{wu2018memory,zhong2019invariance,Chen2020SimCLR,he2019momentum}
\citation{unsup_clustering,lin2019aBottom,zeng2020hierarchical,zhai2020ad}
\newlabel{fig_framework}{{2}{3}{An overview framework of the proposed method. It iteratively alternates between a clustering step and a model updating step. In the clustering step, a global clustering is first performed and then each cluster is split into multiple camera-aware proxies to generate pseudo labels. In the model updating step, intra- and inter-camera losses are designed based on a proxy-level memory bank to perform contrastive learning.}{}{}}
\newlabel{sec:baseline}{{3}{3}{}{}{}}
\newlabel{eq:mu}{{1}{3}{}{}{}}
\newlabel{eq_base_loss}{{2}{3}{}{}{}}
\citation{hermans2017defense}
\newlabel{eq_intra_loss}{{3}{4}{}{}{}}
\newlabel{fig_loss}{{3}{4}{Illustration of intra- and inter-camera losses.}{}{}}
\newlabel{eq_global_loss}{{4}{4}{}{}{}}
\newlabel{eq_overall_loss}{{5}{4}{}{}{}}
\citation{7410490}
\citation{zheng2017unlabeled}
\citation{wei2018person}
\citation{7410490}
\citation{zheng2017unlabeled}
\citation{ristani2016performance}
\citation{wei2018person}
\citation{Zhong2017reranking}
\citation{he2016deep}
\citation{Zhong2017reranking}
\citation{ester1996density}
\newlabel{our_algorithm}{{1}{5}{Camera-aware Proxy Assisted Learning}{}{}}
\citation{lin2019aBottom}
\citation{wu2019graph}
\citation{lin2020unsupervised}
\citation{wang2020unsupervised}
\citation{zeng2020hierarchical}
\citation{wang2020cycas}
\citation{ge2020self}
\citation{unsup_clustering}
\citation{deng2018similarity}
\citation{zhong2019invariance}
\citation{Wang_2020_CVPR}
\citation{wang2020unsupervised}
\citation{zhai2020ad}
\citation{ge2020mutual}
\citation{ge2020self}
\citation{sun2018beyond}
\citation{chen2019abd}
\citation{vanDerMaaten2008}
\citation{lin2019aBottom}
\citation{wu2019graph}
\citation{lin2020unsupervised}
\citation{zeng2020hierarchical}
\citation{wang2020cycas}
\citation{wang2020unsupervised}
\citation{ge2020self}
\newlabel{fig_compare_tsne}{{4}{6}{T-SNE visualization of features extracted by the models of Baseline, CAP2, and CAP6, respectively shown from left to right in the upper row. Typical examples of IDs \#4-7 are shown at bottom.}{}{}}
\newlabel{ablation_table}{{1}{6}{Comparison of the proposed method and its variants. $\mathcal  {L}_{Intra}$ refers to the intra-camera learning, $\mathcal  {L}_{Inter}$ is the inter-camera learning, and \textit  {PBsampling} is the proxy-balanced sampling strategy. When \textit  {PBsampling} is not selected, the model uses the class-balanced sampling strategy.}{}{}}
\citation{sun2018beyond}
\citation{chen2019abd}
\bibdata{reference_v3}
\newlabel{compare_SOTA_table}{{2}{7}{Comparison with state-of-the-art methods. Both purely unsupervised and UDA-based methods are included. We also provide several fully supervised methods for reference. The first and second best results among all unsupervised methods are, respectively, marked in {\color  {red}red} and {\color  {blue}blue}. $^\dagger $ indicates an UDA-based method working under the purely unsupervised setting.}{}{}}
\gdef \@abspage@last{8}
