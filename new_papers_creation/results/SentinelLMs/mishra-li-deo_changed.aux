\relax 
\bibstyle{aaai24}
\citation{devlin-etal-2019-bert}
\citation{liu2019roberta}
\citation{radford2018improving}
\citation{brown2020language}
\citation{chowdhery2022palm}
\citation{touvron2023llama}
\citation{otter2020survey,li2022pretrained}
\citation{callegati2009man}
\citation{o2003some,jones2005informed}
\citation{fernandes2015implementation}
\citation{blake2}
\citation{song2020information}
\citation{lee2022privacy}
\citation{lee2022privacy}
\citation{raeini2023privacy}
\citation{raeini2023privacy}
\citation{yu2021differentially}
\citation{yu2021differentially}
\citation{wang2019private}
\citation{wang2019private}
\citation{shejwalkar2021membership}
\citation{shejwalkar2021membership}
\citation{qu2021natural}
\citation{qu2021natural}
\citation{penard2008secure}
\citation{blake2}
\newlabel{sec:related}{{2}{2}{}{}{}}
\newlabel{ref:central_idea}{{3}{2}{}{}{}}
\citation{kudo2018sentencepiece}
\citation{shibata1999byte}
\citation{levy-etal-2015-improving,mikolov2013efficient}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:example_blake_table}{{1}{3}{Examples showing input, tokenized and encrypted text. We show word-pieces from BertTokenizer. Version1 and Version2 are different encrypted hashes obtained using \textit  {Blake-32} bit encryption with example passkeys \texttt  {llm123} and \texttt  {nlp2023} respectively. Hashes are truncated for brevity.}{}{}}
\newlabel{sec:lmadaptation}{{4}{3}{}{}{}}
\newlabel{sec:adaptdetails}{{4.1}{3}{}{}{}}
\newlabel{fig:flowdiag}{{1}{4}{Illustration of the Workflow: User-Initiated Password-Driven Language Adaptation and Fine-Tuning Process. (a) Initial phase where a user-generated passkey initiates a one-time language adaptation. (b) Subsequent stage involving a one-time fine-tuning process. (c) Run-time scenario showcasing server-side inference on encrypted user input.}{}{}}
\newlabel{eq:reflection}{{1}{4}{}{}{}}
\newlabel{eq:glide}{{2}{4}{}{}{}}
\citation{kruskal1978multidimensional}
\newlabel{fig:glide}{{2}{5}{2D plot of embeddings for 100 random tokens from the original bert-base-uncased model (in red dots) and same tokens from the transformed model after 10 iterations of glide reflection (in blue dots). The plot further illustrates that sample tokens ``vocalists'' and ``involved'' have altered positions with preserved spatial distance (dashed lines)}{}{}}
\newlabel{alg:encrypt}{{1}{5}{AdaptLM}{}{}}
\citation{wang2018glue}
\citation{sang2003introduction}
\citation{conneau2018xnli}
\newlabel{tab:glueresults}{{2}{6}{GLUE benchmark task metrics for the encryption adapted models starting with the letter $E$ with different passkeys and number of glides (nglides), specified in the subscripts. Metrics: CoLA - Matthews Correlation, SST-2 - Accuracy, MRPC - F1, STS-b - Pearson, QQP - Accuracy, MNLI - Matched Accuracy, QNLI - Accuracy, RTE - Accuracy, WNLI - Accuracy.}{}{}}
\newlabel{sec:expsetup}{{5}{6}{}{}{}}
\newlabel{sec:results}{{6}{6}{}{}{}}
\newlabel{tab:nerresults}{{3}{7}{Classification Metrics of Models on Sequence Labeling Task (NER) on CoNLL2003 Dataset}{}{}}
\newlabel{tab:xnliresults}{{4}{7}{Accuracy of Models Multilingual Natural Language Inference (XNLI) Task}{}{}}
\newlabel{label:conclusion}{{7}{7}{}{}{}}
\newlabel{sec:ethics}{{7}{7}{}{}{}}
\bibdata{aaai24}
\gdef \@abspage@last{8}
