@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@techreport{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  institution={OpenAI}
}
@article{otter2020survey,
  title={A survey of the usages of deep learning for natural language processing},
  author={Otter, Daniel W and Medina, Julian R and Kalita, Jugal K},
  journal={IEEE transactions on neural networks and learning systems},
  volume={32},
  number={2},
  pages={604--624},
  year={2020},
  publisher={IEEE}
}
@article{xia2020bert,
  title={Which* BERT? A survey organizing contextualized encoders},
  author={Xia, Patrick and Wu, Shijie and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:2010.00854},
  year={2020}
}
@article{li2022pretrained,
  title={Pretrained language models for text generation: A survey},
  author={Li, Junyi and Tang, Tianyi and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2201.05273},
  year={2022}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{callegati2009man,
  title={Man-in-the-Middle Attack to the HTTPS Protocol},
  author={Callegati, Franco and Cerroni, Walter and Ramilli, Marco},
  journal={IEEE Security \& Privacy},
  volume={7},
  number={1},
  pages={78--81},
  year={2009},
  publisher={IEEE}
}
@article{jones2005informed,
  title={Informed consent: it's not just signing a form},
  author={Jones, James W and McCullough, Lawrence B and Richman, Bruce W},
  journal={Thoracic surgery clinics},
  volume={15},
  number={4},
  pages={451--460},
  year={2005},
  publisher={Elsevier}
}
@article{o2003some,
  title={Some limits of informed consent},
  author={O'Neill, Onora},
  journal={Journal of medical ethics},
  volume={29},
  number={1},
  pages={4--7},
  year={2003},
  publisher={Institute of Medical Ethics}
}
@inproceedings{fernandes2015implementation,
  title={Implementation of Blake 256 hash function for password encryption and parallel CRC},
  author={Fernandes, Floyd and Gupta, Ritu and Sivanantham, S and Sivasankaran, K},
  booktitle={2015 Online International Conference on Green Engineering and Technologies (IC-GET)},
  pages={1--4},
  year={2015},
  organization={IEEE}
}
@inproceedings{blake2,
  title={BLAKE2: simpler, smaller, fast as MD5},
  author={Aumasson, Jean-Philippe and Neves, Samuel and Wilcox-Oâ€™Hearn, Zooko and Winnerlein, Christian},
  booktitle={Applied Cryptography and Network Security: 11th International Conference, ACNS 2013, Banff, AB, Canada, June 25-28, 2013. Proceedings 11},
  pages={119--135},
  year={2013},
  organization={Springer}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{lee2022privacy,
  title={Privacy-Preserving Text Classification on BERT Embeddings with Homomorphic Encryption},
  author={Lee, Garam and Kim, Minsoo and Park, Jai Hyun and Hwang, Seung-won and Cheon, Jung Hee},
  journal={arXiv preprint arXiv:2210.02574},
  year={2022}
}
@inproceedings{song2020information,
  title={Information leakage in embedding models},
  author={Song, Congzheng and Raghunathan, Ananth},
  booktitle={Proceedings of the 2020 ACM SIGSAC conference on computer and communications security},
  pages={377--390},
  year={2020}
}
@inproceedings{dhany2017encryption,
  title={Encryption and decryption using password based encryption, MD5, and DES},
  author={Dhany, Hanna Willa and Izhari, Fahmi and Fahmi, Hasanul and Tulus, Mr and Sutarman, Mr},
  booktitle={International Conference on Public Policy, Social Computing and Development 2017 (ICOPOSDev 2017)},
  pages={278--283},
  year={2017},
  organization={Atlantis Press}
}
@article{penard2008secure,
  title={On the secure hash algorithm family},
  author={Penard, Wouter and van Werkhoven, Tim},
  journal={Cryptography in context},
  pages={1--18},
  year={2008},
  publisher={Wiley New York}
}
@techreport{shibata1999byte,
  title={Byte Pair encoding: A text compression scheme that accelerates pattern matching},
  author={Shibata, Yusuke and Kida, Takuya and Fukamachi, Shuichi and Takeda, Masayuki and Shinohara, Ayumi and Shinohara, Takeshi and Arikawa, Setsuo},
  year={1999},
  institution={Department of Informatics, Kyushu University},
  number={DOI-TR-161}
}
@article{kudo2018sentencepiece,
  title={Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing},
  author={Kudo, Taku and Richardson, John},
  journal={arXiv preprint arXiv:1808.06226},
  year={2018}
}
@article{raeini2023privacy,
  title={Privacy-preserving large language models (PPLLMs)},
  author={Raeini, Mohammad},
  journal={Available at SSRN 4512071},
  year={2023}
}
@article{yu2021differentially,
  title={Differentially private fine-tuning of language models},
  author={Yu, Da and Naik, Saurabh and Backurs, Arturs and Gopi, Sivakanth and Inan, Huseyin A and Kamath, Gautam and Kulkarni, Janardhan and Lee, Yin Tat and Manoel, Andre and Wutschitz, Lukas and others},
  journal={arXiv preprint arXiv:2110.06500},
  year={2021}
}
@article{levy-etal-2015-improving,
    title = "Improving Distributional Similarity with Lessons Learned from Word Embeddings",
    author = "Levy, Omer  and
      Goldberg, Yoav  and
      Dagan, Ido",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "3",
    year = "2015",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q15-1016",
    doi = "10.1162/tacl_a_00134",
    pages = "211--225",
    abstract = "Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.",
}
@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}
@book{kruskal1978multidimensional,
  title={Multidimensional scaling},
  author={Kruskal, Joseph B and Wish, Myron},
  number={11},
  year={1978},
  publisher={Sage}
}
@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}
@article{sang2003introduction,
  title={Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition},
  author={Sang, Erik F and De Meulder, Fien},
  journal={arXiv preprint cs/0306050},
  year={2003}
}
@article{conneau2018xnli,
  title={XNLI: Evaluating cross-lingual sentence representations},
  author={Conneau, Alexis and Lample, Guillaume and Rinott, Ruty and Williams, Adina and Bowman, Samuel R and Schwenk, Holger and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1809.05053},
  year={2018}
}
@inproceedings{wang2019private,
  title={Private model compression via knowledge distillation},
  author={Wang, Ji and Bao, Weidong and Sun, Lichao and Zhu, Xiaomin and Cao, Bokai and Philip, S Yu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={1190--1197},
  year={2019}
}
@inproceedings{shejwalkar2021membership,
  title={Membership privacy for machine learning models through knowledge transfer},
  author={Shejwalkar, Virat and Houmansadr, Amir},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  pages={9549--9557},
  year={2021}
}
@article{pfeiffer2020adapterhub,
  title={Adapterhub: A framework for adapting transformers},
  author={Pfeiffer, Jonas and R{\"u}ckl{\'e}, Andreas and Poth, Clifton and Kamath, Aishwarya and Vuli{\'c}, Ivan and Ruder, Sebastian and Cho, Kyunghyun and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2007.07779},
  year={2020}
}
@inproceedings{qu2021natural,
  title={Natural language understanding with privacy-preserving bert},
  author={Qu, Chen and Kong, Weize and Yang, Liu and Zhang, Mingyang and Bendersky, Michael and Najork, Marc},
  booktitle={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  pages={1488--1497},
  year={2021}
}