\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Aumasson et~al.(2013)Aumasson, Neves, Wilcox-O’Hearn, and Winnerlein}]{blake2}
Aumasson, J.-P.; Neves, S.; Wilcox-O’Hearn, Z.; and Winnerlein, C. 2013.
\newblock BLAKE2: simpler, smaller, fast as MD5.
\newblock In \emph{Applied Cryptography and Network Security: 11th International Conference, ACNS 2013, Banff, AB, Canada, June 25-28, 2013. Proceedings 11}, 119--135. Springer.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.~D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33: 1877--1901.

\bibitem[{Callegati, Cerroni, and Ramilli(2009)}]{callegati2009man}
Callegati, F.; Cerroni, W.; and Ramilli, M. 2009.
\newblock Man-in-the-Middle Attack to the HTTPS Protocol.
\newblock \emph{IEEE Security \& Privacy}, 7(1): 78--81.

\bibitem[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{chowdhery2022palm}
Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H.~W.; Sutton, C.; Gehrmann, S.; et~al. 2022.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}.

\bibitem[{Conneau et~al.(2018)Conneau, Lample, Rinott, Williams, Bowman, Schwenk, and Stoyanov}]{conneau2018xnli}
Conneau, A.; Lample, G.; Rinott, R.; Williams, A.; Bowman, S.~R.; Schwenk, H.; and Stoyanov, V. 2018.
\newblock XNLI: Evaluating cross-lingual sentence representations.
\newblock \emph{arXiv preprint arXiv:1809.05053}.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova}]{devlin-etal-2019-bert}
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
\newblock {BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, 4171--4186. Minneapolis, Minnesota: Association for Computational Linguistics.

\bibitem[{Fernandes et~al.(2015)Fernandes, Gupta, Sivanantham, and Sivasankaran}]{fernandes2015implementation}
Fernandes, F.; Gupta, R.; Sivanantham, S.; and Sivasankaran, K. 2015.
\newblock Implementation of Blake 256 hash function for password encryption and parallel CRC.
\newblock In \emph{2015 Online International Conference on Green Engineering and Technologies (IC-GET)}, 1--4. IEEE.

\bibitem[{Jones, McCullough, and Richman(2005)}]{jones2005informed}
Jones, J.~W.; McCullough, L.~B.; and Richman, B.~W. 2005.
\newblock Informed consent: it's not just signing a form.
\newblock \emph{Thoracic surgery clinics}, 15(4): 451--460.

\bibitem[{Kruskal and Wish(1978)}]{kruskal1978multidimensional}
Kruskal, J.~B.; and Wish, M. 1978.
\newblock \emph{Multidimensional scaling}.
\newblock 11. Sage.

\bibitem[{Kudo and Richardson(2018)}]{kudo2018sentencepiece}
Kudo, T.; and Richardson, J. 2018.
\newblock Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.
\newblock \emph{arXiv preprint arXiv:1808.06226}.

\bibitem[{Lee et~al.(2022)Lee, Kim, Park, Hwang, and Cheon}]{lee2022privacy}
Lee, G.; Kim, M.; Park, J.~H.; Hwang, S.-w.; and Cheon, J.~H. 2022.
\newblock Privacy-Preserving Text Classification on BERT Embeddings with Homomorphic Encryption.
\newblock \emph{arXiv preprint arXiv:2210.02574}.

\bibitem[{Levy, Goldberg, and Dagan(2015)}]{levy-etal-2015-improving}
Levy, O.; Goldberg, Y.; and Dagan, I. 2015.
\newblock Improving Distributional Similarity with Lessons Learned from Word Embeddings.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 3: 211--225.

\bibitem[{Li et~al.(2022)Li, Tang, Zhao, Nie, and Wen}]{li2022pretrained}
Li, J.; Tang, T.; Zhao, W.~X.; Nie, J.-Y.; and Wen, J.-R. 2022.
\newblock Pretrained language models for text generation: A survey.
\newblock \emph{arXiv preprint arXiv:2201.05273}.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov}]{liu2019roberta}
Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}.

\bibitem[{Mikolov et~al.(2013)Mikolov, Chen, Corrado, and Dean}]{mikolov2013efficient}
Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013.
\newblock Efficient estimation of word representations in vector space.
\newblock \emph{arXiv preprint arXiv:1301.3781}.

\bibitem[{O'Neill(2003)}]{o2003some}
O'Neill, O. 2003.
\newblock Some limits of informed consent.
\newblock \emph{Journal of medical ethics}, 29(1): 4--7.

\bibitem[{Otter, Medina, and Kalita(2020)}]{otter2020survey}
Otter, D.~W.; Medina, J.~R.; and Kalita, J.~K. 2020.
\newblock A survey of the usages of deep learning for natural language processing.
\newblock \emph{IEEE transactions on neural networks and learning systems}, 32(2): 604--624.

\bibitem[{Penard and van Werkhoven(2008)}]{penard2008secure}
Penard, W.; and van Werkhoven, T. 2008.
\newblock On the secure hash algorithm family.
\newblock \emph{Cryptography in context}, 1--18.

\bibitem[{Qu et~al.(2021)Qu, Kong, Yang, Zhang, Bendersky, and Najork}]{qu2021natural}
Qu, C.; Kong, W.; Yang, L.; Zhang, M.; Bendersky, M.; and Najork, M. 2021.
\newblock Natural language understanding with privacy-preserving bert.
\newblock In \emph{Proceedings of the 30th ACM International Conference on Information \& Knowledge Management}, 1488--1497.

\bibitem[{Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever et~al.}]{radford2018improving}
Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.; et~al. 2018.
\newblock Improving language understanding by generative pre-training.
\newblock Technical report, OpenAI.

\bibitem[{Raeini(2023)}]{raeini2023privacy}
Raeini, M. 2023.
\newblock Privacy-preserving large language models (PPLLMs).
\newblock \emph{Available at SSRN 4512071}.

\bibitem[{Sang and De~Meulder(2003)}]{sang2003introduction}
Sang, E.~F.; and De~Meulder, F. 2003.
\newblock Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.
\newblock \emph{arXiv preprint cs/0306050}.

\bibitem[{Shejwalkar and Houmansadr(2021)}]{shejwalkar2021membership}
Shejwalkar, V.; and Houmansadr, A. 2021.
\newblock Membership privacy for machine learning models through knowledge transfer.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~35, 9549--9557.

\bibitem[{Shibata et~al.(1999)Shibata, Kida, Fukamachi, Takeda, Shinohara, Shinohara, and Arikawa}]{shibata1999byte}
Shibata, Y.; Kida, T.; Fukamachi, S.; Takeda, M.; Shinohara, A.; Shinohara, T.; and Arikawa, S. 1999.
\newblock Byte Pair encoding: A text compression scheme that accelerates pattern matching.
\newblock Technical Report DOI-TR-161, Department of Informatics, Kyushu University.

\bibitem[{Song and Raghunathan(2020)}]{song2020information}
Song, C.; and Raghunathan, A. 2020.
\newblock Information leakage in embedding models.
\newblock In \emph{Proceedings of the 2020 ACM SIGSAC conference on computer and communications security}, 377--390.

\bibitem[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama}
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozi{\`e}re, B.; Goyal, N.; Hambro, E.; Azhar, F.; et~al. 2023.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}.

\bibitem[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman}]{wang2018glue}
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S.~R. 2018.
\newblock GLUE: A multi-task benchmark and analysis platform for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1804.07461}.

\bibitem[{Wang et~al.(2019)Wang, Bao, Sun, Zhu, Cao, and Philip}]{wang2019private}
Wang, J.; Bao, W.; Sun, L.; Zhu, X.; Cao, B.; and Philip, S.~Y. 2019.
\newblock Private model compression via knowledge distillation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~33, 1190--1197.

\bibitem[{Yu et~al.(2021)Yu, Naik, Backurs, Gopi, Inan, Kamath, Kulkarni, Lee, Manoel, Wutschitz et~al.}]{yu2021differentially}
Yu, D.; Naik, S.; Backurs, A.; Gopi, S.; Inan, H.~A.; Kamath, G.; Kulkarni, J.; Lee, Y.~T.; Manoel, A.; Wutschitz, L.; et~al. 2021.
\newblock Differentially private fine-tuning of language models.
\newblock \emph{arXiv preprint arXiv:2110.06500}.

\end{thebibliography}
