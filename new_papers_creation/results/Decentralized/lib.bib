@book{ abelson-et-al:scheme,
  author = "Harold Abelson and Gerald~Jay Sussman and Julie Sussman",
  title = "Structure and Interpretation of Computer Programs",
  publisher = "MIT Press",
  address = "Cambridge, Massachusetts",
  year = "1985"
}

@inproceedings{ bgf:Lixto,
  author = "Robert Baumgartner and Georg Gottlob and Sergio Flesca",
  title = "Visual Information Extraction with {Lixto}",
  booktitle = "Proceedings of the 27th International Conference on Very Large Databases",
  pages = "119--128",
  publisher = "Morgan Kaufmann",
  address = "Rome, Italy",
  month = "September",
  year = "2001"
}

@article{ brachman-schmolze:kl-one,
  author = "Ronald~J. Brachman and James~G. Schmolze",
  title = "An overview of the {KL-ONE} knowledge representation system",
  journal = "Cognitive Science",
  volume = "9",
  number = "2",
  pages = "171--216",
  month = "April--June",
  year = "1985"
}

@article{ gottlob:nonmon,
  author = "Georg Gottlob",
  title = "Complexity results for nonmonotonic logics",
  journal = "Journal of Logic and Computation",
  volume = "2",
  number = "3",
  pages = "397--425",
  month = "June",
  year = "1992"
}

@article{ gls:hypertrees,
  author = "Georg Gottlob and Nicola Leone and Francesco Scarcello",
  title = "Hypertree Decompositions and Tractable Queries",
  journal = "Journal of Computer and System Sciences",
  volume = "64",
  number = "3",
  pages = "579--627",
  month = "May",
  year = "2002"
}

@article{ levesque:functional-foundations,
  author = "Hector~J. Levesque",
  title = "Foundations of a functional approach to knowledge representation",
  journal = "Artificial Intelligence",
  volume = "23",
  number = "2",
  pages = "155--212",
  month = "July",
  year = "1984"
}

@inproceedings{ levesque:belief,
  author = "Hector~J. Levesque",
  title = "A logic of implicit and explicit belief",
  booktitle = "Proceedings of the Fourth National Conference on Artificial Intelligence",
  publisher = "American Association for Artificial Intelligence",
  pages = "198--202",
  address = "Austin, Texas",
  month = "August",
  year = "1984"
}

@article{ nebel:jair-2000,
  author = "Bernhard Nebel",
  title = "On the compilability and expressive power of propositional planning formalisms",
  journal = "Journal of Artificial Intelligence Research",
  volume = "12",
  pages = "271--315",
  year = "2000"
}

 @misc{proceedings,
  author = {{IJCAI Proceedings}},
  title = {{IJCAI} Camera Ready Submission},
  howpublished = {\url{https://proceedings.ijcai.org/info}},
}



@article{yu2016optimal,
  title={Optimal multirobot path planning on graphs: Complete algorithms and effective heuristics},
  author={Yu, Jingjin and LaValle, Steven M},
  journal={IEEE Transactions on Robotics},
  volume={32},
  number={5},
  pages={1163--1177},
  year={2016},
  publisher={IEEE}
}

@inproceedings{surynek2010optimization,
  title={An optimization variant of multi-robot path planning is intractable},
  author={Surynek, Pavel},
  booktitle={Proceedings of the 24th AAAI Conference on Artificial Intelligence ({AAAI} 2010)},
  pages={1261--1263},
  year={2010}
}

@inproceedings{kornhauser1984coordinating,
  title={Coordinating Pebble Motion On Graphs, The Diameter Of Permutation Groups, And Applications},
  author={Kornhauser, D and Miller, G and Spirakis, P},
  booktitle={Proceedings of the 25th Annual Symposium on Foundations of Computer Science ({FOCS}) 1984},
  pages={241--250},
  year={1984}
}

@inproceedings{de2013push,
  title={Push and rotate: cooperative multi-agent path planning},
  author={de Wilde, Boris and ter Mors, Adriaan W and Witteveen, Cees},
  booktitle={Proceedings of the 12th International Conference on Autonomous Agents and Multiagent Systems ({AAMAS} 2013)},
  pages={87--94},
  year={2013}
}

@inproceedings{ma2017lifelong,
  title={Lifelong Multi-Agent Path Finding for Online Pickup and Delivery Tasks},
  author={Ma, Hang and Li, Jiaoyang and Kumar, TK and Koenig, Sven},
  booktitle={Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems ({AAMAS} 2017)},
  pages={837--845},
  year={2017}
}

@inproceedings{li2021lifelong,
  title={Lifelong multi-agent path finding in large-scale warehouses},
  author={Li, Jiaoyang and Tinka, Andrew and Kiesel, Scott and Durham, Joseph W and Kumar, TK Satish and Koenig, Sven},
  booktitle={Proceedings of the 35th AAAI Conference on Artificial Intelligence ({AAAI} 2021)},
  pages={11272--11281},
  year={2021}
}

@inproceedings{ma2019lifelong,
	author = {H. Ma and W. H\"{o}nig and T. K. S. Kumar and N. Ayanian and S. Koenig},
	title = {Lifelong Path Planning with Kinematic Constraints for Multi-Agent Pickup and Delivery},
	booktitle = {Proceedings of the 33rd AAAI Conference on Artificial Intelligence ({AAAI} 2019)},
  pages={7651--7658},
	year = 2019
}

@inproceedings{liu2019task,
  title={Task and path planning for multi-agent pickup and delivery},
  author={Liu, Minghua and Ma, Hang and Li, Jiaoyang and Koenig, Sven},
  booktitle={Proceedings of the 18th International Conference on Autonomous Agents and Multiagent Systems ({AAMAS} 2019)},
  pages = {1152--1160},
  year={2019}
}

@article{chen2021integrated,
  title={Integrated task assignment and path planning for capacitated multi-agent pickup and delivery},
  author={Chen, Zhe and Alonso-Mora, Javier and Bai, Xiaoshan and Harabor, Daniel D and Stuckey, Peter J},
  journal={IEEE Robotics and Automation Letters},
  volume={6},
  number={3},
  pages={5816--5823},
  year={2021}
}

@article{okumura2022priority,
  title={Priority inheritance with backtracking for iterative multi-agent path finding},
  author={Okumura, Keisuke and Machida, Manao and D{\'e}fago, Xavier and Tamura, Yasumasa},
  journal={Artificial Intelligence},
  volume={310},
  pages={103752},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{okumura2019priority,
  title={Priority inheritance with backtracking for iterative multi-agent path finding},
  author={Okumura, Keisuke and Machida, Manao and D{\'e}fago, Xavier and Tamura, Yasumasa},
  booktitle={Proceedings of the 28th International Joint Conference on Artificial Intelligence ({IJCAI 2019})},
  pages={535--542},
  year={2019}
}

@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
issn = {0028-0836},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}

@inproceedings{Rashid2018a,
abstract = {In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint actionvalues conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.},
archivePrefix = {arXiv},
arxivId = {1803.11485},
author = {Rashid, Tabish and Samvelyan, Mikayel and {De Witt}, Christian Schroeder and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1803.11485},
isbn = {9781510867963},
pages = {6846--6859},
title = {{QMIX: Monotonic value function factorisation for deep multi-agent reinforcement Learning}},
volume = {10},
year = {2018}
}

@inproceedings{Hafner2020a,
abstract = {Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, DreamerV2 reaches 200M frames and exceeds the final performance of the top single-GPU agents IQN and Rainbow.},
archivePrefix = {arXiv},
arxivId = {2010.02193},
author = {Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
booktitle = {ICLR},
eprint = {2010.02193},
issn = {23318422},
keywords = {DreamerV2},
title = {{Mastering atari with discrete world models}},
url = {https://openreview.net/forum?id=0oabwyZbOu},
year = {2021}
}

@article{Hafner2023,
abstract = {General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but are held back by the resources and knowledge required to tune them for new tasks. We present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, different data budgets, reward frequencies, and reward scales. We observe favorable scaling properties of DreamerV3, with larger models directly translating to higher data-efficiency and final performance. Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long-standing challenge in artificial intelligence. Our general algorithm makes reinforcement learning broadly applicable and allows scaling to hard decision making problems.},
archivePrefix = {arXiv},
arxivId = {2301.04104},
author = {Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
eprint = {2301.04104},
title = {{Mastering Diverse Domains through World Models}},
url = {http://arxiv.org/abs/2301.04104},
year = {2023}
}

@inproceedings{Milani2020,
abstract = {To facilitate research in the direction of sample-efficient reinforcement learning, we held the MineRL Competition on Sample-Efficient Reinforcement Learning Using Human Priors at the Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS 2019). The primary goal of this competition was to promote the development of algorithms that use human demonstrations alongside reinforcement learning to reduce the number of samples needed to solve complex, hierarchical, and sparse environments. We describe the competition and provide an overview of the top solutions, each of which uses deep reinforcement learning and/or imitation learning. We also discuss the impact of our organizational decisions on the competition as well as future directions for improvement.},
archivePrefix = {arXiv},
arxivId = {2003.05012},
author = {Milani, Stephanie and Topin, Nicholay and Houghton, Brandon and Guss, William H. and Mohanty, Sharada P. and Vinyals, Oriol and Kuno, Noboru Sean},
booktitle = {NeurIPS Competition track},
eprint = {2003.05012},
keywords = {imitation learning,reinforcement learning,reinforcement learning competition},
title = {{The MineRL Competition on Sample-Efficient Reinforcement Learning Using Human Priors: A Retrospective}},
url = {http://arxiv.org/abs/2003.05012},
year = {2020}
}


@inproceedings{Suarez2021,
abstract = {Neural MMO is a computationally accessible research platform that combines large agent populations, long time horizons, open-ended tasks, and modular game systems. Existing environments feature subsets of these properties, but Neural MMO is the first to combine them all. We present Neural MMO as free and open source software with active support, ongoing development, documentation, and additional training, logging, and visualization tools to help users adapt to this new setting. Initial baselines on the platform demonstrate that agents trained in large populations explore more and learn a progression of skills. We raise other more difficult problems such as many-team cooperation as open research questions which Neural MMO is well-suited to answer. Finally, we discuss current limitations of the platform, potential mitigations, and plans for continued development.},
archivePrefix = {arXiv},
arxivId = {2110.07594},
author = {Suarez, Joseph and Du, Yilun and Zhu, Clare and Mordatch, Igor and Isola, Phillip},
booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
eprint = {2110.07594},
title = {{The Neural MMO Platform for Massively Multiagent Research}},
url = {http://arxiv.org/abs/2110.07594 https://openreview.net/forum?id=J0d-I8yFtP},
year = {2021}
}

@article{Pack1998,
abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPs) and partially observable MDPs (POMDPs). We then outline a novel algorithm for solving POMDPs off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPs, and of some possibilities for finding approximate solutions.},
archivePrefix = {arXiv},
arxivId = {https://doi.org},
author = {Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
doi = {10.1016/S0004-3702(98)00023-X},
eprint = {/doi.org},
isbn = {00043702 (ISSN)},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {a large office building,a robot navigating in,completely reliable,consider the problem of,from hallway intersection to,however,intersection and can make,it,its actions are not,local observations of its,move,partially observable markov decision,planning,processes,sometimes,the robot can move,uncertainty,when it intends to,world},
month = {may},
number = {1-2},
pages = {99--134},
pmid = {20552381},
primaryClass = {https:},
title = {{Planning and acting in partially observable stochastic domains}},
url = {papers3://publication/uuid/5ECA204D-77B0-4C68-A0A6-7323A9D8893B https://linkinghub.elsevier.com/retrieve/pii/S000437029800023X},
volume = {101},
year = {1998}
}

@book{Sutton2018,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core, online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new for the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
author = {{Richard S. Sutton and Andrew G. Barto}},
edition = {Second},
isbn = {978-0-262-19398-6},
pages = {550},
publisher = {MIT Press},
title = {{Reinforcement Learning. An Introduction}},
year = {2018}
}

@book{Puterman2005,
abstract = {Similarity search (nearest neighbor search) is a problem of pursuing the data items whose distances to a query item are the smallest from a large database. Various methods have been developed to address this problem, and recently a lot of efforts have been devoted to approximate search. In this paper, we present a survey on one of the main solutions, hashing, which has been widely studied since the pioneering work locality sensitive hashing. We divide the hashing algorithms two main categories: locality sensitive hashing, which designs hash functions without exploring the data distribution and learning to hash, which learns hash functions according the data distribution, and review them from various aspects, including hash function design and distance measure and search scheme in the hash coding space.},
author = {Puterman, Martin L.},
isbn = {0-471-72782-2},
pages = {392},
publisher = {Wiley-Interscience},
title = {{Markov Decision Processes: Discrete Stochastic Dynamic Programming}},
year = {2005}
}

@inproceedings{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task involving finding rewards in random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
booktitle = {Proceedings of The 33rd International Conference on Machine Learning, PMLR},
eprint = {1602.01783},
isbn = {9781510829008},
keywords = {A2C},
pages = {1928--1937},
pmid = {1000272564},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}

@INPROCEEDINGS{Degris2012,
  author={Degris, Thomas and Pilarski, Patrick M. and Sutton, Richard S.},
  booktitle={2012 American Control Conference (ACC)}, 
  title={Model-Free reinforcement learning with continuous action in practice}, 
  year={2012},
  volume={},
  number={},
  pages={2177-2182},
  doi={10.1109/ACC.2012.6315022}
}

@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
pages = {1--12},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}

@inproceedings{Samvelyan2019,
abstract = {In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap.1 SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-theart algorithms.2 We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ-obZ0.},
archivePrefix = {arXiv},
arxivId = {1902.04043},
author = {Samvelyan, Mikayel and Rashid, Tabish and {De Witt}, Christian Schroeder and Farquhar, Gregory and Nardelli, Nantas and Rudner, Tim G.J. and Hung, Chia Man and Torr, Philip H.S. and Foerster, Jakob and Whiteson, Shimon},
booktitle = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
eprint = {1902.04043},
isbn = {9781510892002},
issn = {15582914},
keywords = {Multi-agent learning,Reinforcement learning,StarCraft},
pages = {2186--2188},
title = {{The StarCraft multi-agent challenge}},
volume = {4},
year = {2019}
}

@article{yu2022surprising,
  title={The surprising effectiveness of ppo in cooperative multi-agent games},
  author={Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24611--24624},
  year={2022}
}

@article{Ellis2022,
archivePrefix = {arXiv},
arxivId = {arXiv:2212.07489v1},
author = {Ellis, Benjamin and Moalla, Skander and Samvelyan, Mikayel and Sun, Mingfei and Mahajan, Anuj and Foerster, Jakob N and Whiteson, Shimon},
eprint = {arXiv:2212.07489v1},
number = {NeurIPS},
pages = {1--15},
title = {{SMACv2 : A New Benchmark for Cooperative Multi-Agent Reinforcement Learning}},
year = {2022}
}

@article{Skrynnik2021,
abstract = {In this work we study the behavior of groups of autonomous vehicles, which are the part of the Internet ofVehicles systems. One of the challenging modes of operation of such systems is the case when the observability of each vehicle is limited and the global/local communication is unstable, e.g. in the crowded parking lots. In such scenarios the vehicles have to rely on the local observations and exhibit cooperative behavior to ensure safe and efficient trips. This type of problems can be abstracted to the so-called multi- agent pathfinding when a group of agents, confined to a graph, have to find collision-free paths to their goals (ideally, minimizing an objective function e.g. travel time). Widely used algorithms for solving this problem rely on the assumption that a central controller exists for which the full state of the environment (i.e. the agents current positions, their targets, configuration of the static obstacles etc.) is known and they cannot be straightforwardly be adapted to the partially-observable setups. To this end, we suggest a novel approach which is based on the decomposition of the problem into the two sub-tasks: reaching the goal and avoiding the collisions. To accomplish each of this task we utilize reinforcement learning methods such as Deep Monte Carlo Tree Search, Q-mixing networks, and policy gradients methods to design the policies that map the agents' observations to actions. Next, we introduce the policy-mixing mechanism to end up with a single hybrid policy that allows each agent to exhibit both types of behavior – the individual one (reaching the goal) and the cooperative one (avoiding the collisions with other agents). We conduct an extensive empirical evaluation that shows that the suggested hybrid-policy outperforms standalone stat-of-the-art reinforcement learning methods for this kind of problems by a notable margin.},
author = {Skrynnik, Alexey and Yakovleva, Alexandra and Davydov, Vasilii and Yakovlev, Konstantin and Panov, Aleksandr I.},
doi = {10.1109/ACCESS.2021.3111321},
issn = {2169-3536},
journal = {IEEE Access},
pages = {126034--126047},
title = {{Hybrid Policy Learning for Multi-Agent Pathfinding}},
volume = {9},
year = {2021}
}

@article{Wang2020,
abstract = {Path planning for mobile robots in large dynamic environments is a challenging problem, as the robots are required to efficiently reach their given goals while simultaneously avoiding potential conflicts with other robots or dynamic objects. In the presence of dynamic obstacles, traditional solutions usually employ re-planning strategies, which re-call a planning algorithm to search for an alternative path whenever the robot encounters a conflict. However, such re-planning strategies often cause unnecessary detours. To address this issue, we propose a learning-based technique that exploits environmental spatio-temporal information. Different from existing learning-based methods, we introduce a globally guided reinforcement learning approach (G2RL), which incorporates a novel reward structure that generalizes to arbitrary environments. We apply G2RL to solve the multi-robot path planning problem in a fully distributed reactive manner. We evaluate our method across different map types, obstacle densities, and the number of robots. Experimental results show that G2RL generalizes well, outperforming existing distributed methods, and performing very similarly to fully centralized state-of-the-art benchmarks.},
archivePrefix = {arXiv},
arxivId = {2005.05420},
author = {Wang, Binyu and Liu, Zhe and Li, Qingbiao and Prorok, Amanda},
doi = {10.1109/LRA.2020.3026638},
eprint = {2005.05420},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Hierarchical path planning,mobile robots,reinforcement learning,scalability},
number = {4},
pages = {6932--6939},
title = {{Mobile robot path planning in dynamic environments through globally guided reinforcement learning}},
volume = {5},
year = {2020}
}

@article{Wong2022,
abstract = {This paper surveys the field of deep multiagent reinforcement learning (RL). The combination of deep neural networks with RL has gained increased traction in recent years and is slowly shifting the focus from single-agent to multiagent environments. Dealing with multiple agents is inherently more complex as (a) the future rewards depend on multiple players' joint actions and (b) the computational complexity increases. We present the most common multiagent problem representations and their main challenges, and identify five research areas that address one or more of these challenges: centralised training and decentralised execution, opponent modelling, communication, efficient coordination, and reward shaping. We find that many computational studies rely on unrealistic assumptions or are not generalisable to other settings; they struggle to overcome the curse of dimensionality or nonstationarity. Approaches from psychology and sociology capture promising relevant behaviours, such as communication and coordination, to help agents achieve better performance in multiagent settings. We suggest that, for multiagent RL to be successful, future research should address these challenges with an interdisciplinary approach to open up new possibilities in multiagent RL.},
archivePrefix = {arXiv},
arxivId = {2106.15691},
author = {Wong, Annie and B{\"{a}}ck, Thomas and Kononova, Anna V. and Plaat, Aske},
doi = {10.1007/s10462-022-10299-x},
eprint = {2106.15691},
isbn = {0123456789},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {Deep learning,Evolutionary algorithms,Multiagent systems,Psychology,Reinforcement learning,Survey},
month = {oct},
number = {0123456789},
publisher = {Springer Netherlands},
title = {{Deep multiagent reinforcement learning: challenges and directions}},
url = {https://doi.org/10.1007/s10462-022-10299-x https://link.springer.com/10.1007/s10462-022-10299-x},
year = {2022}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@inproceedings{wang_scrimp_2023,
	title = {{SCRIMP}: {Scalable} {Communication} for {Reinforcement}- and {Imitation}-{Learning}-{Based} {Multi}-{Agent} {Pathfinding}},
	shorttitle = {{SCRIMP}},
	url = {http://arxiv.org/abs/2303.00605},
	abstract = {Trading off performance guarantees in favor of scalability, the Multi-Agent Path Finding (MAPF) community has recently started to embrace Multi-Agent Reinforcement Learning (MARL), where agents learn to collaboratively generate individual, collision-free (but often suboptimal) paths. Scalability is usually achieved by assuming a local ﬁeld of view (FOV) around the agents, helping scale to arbitrary world sizes. However, this assumption signiﬁcantly limits the amount of information available to the agents, making it difﬁcult for them to enact the type of joint maneuvers needed in denser MAPF tasks. In this paper, we propose SCRIMP, where agents learn individual policies from even very small (down to 3x3) FOVs, by relying on a highly-scalable global/local communication mechanism based on a modiﬁed transformer. We further equip agents with a state-value-based tie-breaking strategy to further improve performance in symmetric situations, and introduce intrinsic rewards to encourage exploration while mitigating the long-term credit assignment problem. Empirical evaluations on a set of experiments indicate that SCRIMP can achieve higher performance with improved scalability compared to other stateof-the-art learning-based MAPF planners with larger FOVs, and even yields similar performance as a classical centralized planner in many cases. Ablation studies further validate the effectiveness of our proposed techniques. Finally, we show that our trained model can be directly implemented on real robots for online MAPF through high-ﬁdelity simulations in gazebo.},
	language = {en},
	urldate = {2023-07-06},
	booktitle = {Proceedings of the 2023 {International} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}},
	author = {Wang, Yutong and Xiang, Bairan and Huang, Shinan and Sartoretti, Guillaume},
	year = {2023},
	keywords = {Computer Science - Robotics},
	pages = {2598--2600},
	annote = {Comment: Submitted to the IEEE/RSJ International Conference on Intelligent Robots (IROS 2023)}
}


@article{Li2022MultiAgentPF,
  title={Multi-Agent Path Finding with Prioritized Communication Learning},
  author={Wenhao Li and Hongjun Chen and Bo Jin and Wenzhe Tan and Hong Zha and Xiangfeng Wang},
  journal={2022 International Conference on Robotics and Automation (ICRA)},
  year={2022},
  pages={10695-10701}
}

@article{damani2021primal,
  title={PRIMAL $ \_2 $: Pathfinding via reinforcement and imitation multi-agent learning-lifelong},
  author={Damani, Mehul and Luo, Zhiyao and Wenzel, Emerson and Sartoretti, Guillaume},
  journal={IEEE Robotics and Automation Letters},
  volume={6},
  number={2},
  pages={2666--2673},
  year={2021},
  publisher={IEEE}
}

@article{sartoretti2019primal,
  title={Primal: Pathfinding via reinforcement and imitation multi-agent learning},
  author={Sartoretti, Guillaume and Kerr, Justin and Shi, Yunfei and Wagner, Glenn and Kumar, TK Satish and Koenig, Sven and Choset, Howie},
  journal={IEEE Robotics and Automation Letters},
  volume={4},
  number={3},
  pages={2378--2385},
  year={2019},
  publisher={IEEE}
}

@inproceedings{ma2021distributed,
  title={Distributed heuristic multi-agent path finding with communication},
  author={Ma, Ziyuan and Luo, Yudong and Ma, Hang},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={8699--8705},
  year={2021},
  organization={IEEE}
}

@article{wang2011mapp,
  title={MAPP: a scalable multi-agent path planning algorithm with tractability and completeness guarantees},
  author={Wang, Ko-Hsin Cindy and Botea, Adi},
  journal={Journal of Artificial Intelligence Research},
  volume={42},
  pages={55--90},
  year={2011}
}

@incollection{berg2011reciprocal,
  title={Reciprocal n-body collision avoidance},
  author={Berg, Jur van den and Guy, Stephen J and Lin, Ming and Manocha, Dinesh},
  booktitle={Robotics research},
  pages={3--19},
  year={2011},
  publisher={Springer}
}

@article{Greff2017,
abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ({\$}\backslashapprox 15{\$} years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
archivePrefix = {arXiv},
arxivId = {1503.04069},
author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
doi = {10.1109/TNNLS.2016.2582924},
eprint = {1503.04069},
isbn = {9788578110796},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Functional ANalysis Of VAriance (fANOVA),long short-term memory (LSTM),random search,recurrent neural networks,sequence learning},
number = {10},
pages = {2222--2232},
pmid = {25246403},
title = {{LSTM: A Search Space Odyssey}},
volume = {28},
year = {2017}
}

@inproceedings{ma2019searching,
  title={Searching with consistent prioritization for multi-agent path finding},
  author={Ma, Hang and Harabor, Daniel and Stuckey, Peter J and Li, Jiaoyang and Koenig, Sven},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={7643--7650},
  year={2019}
}

@article{sharon2015conflict,
  title={Conflict-based search for optimal multi-agent pathfinding},
  author={Sharon, Guni and Stern, Roni and Felner, Ariel and Sturtevant, Nathan R},
  journal={Artificial Intelligence},
  volume={219},
  pages={40--66},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{van2008reciprocal,
  title={Reciprocal velocity obstacles for real-time multi-agent navigation},
  author={Van den Berg, Jur and Lin, Ming and Manocha, Dinesh},
  booktitle={Proceedings of The 2008 IEEE International Conference on Robotics and Automation ({ICRA} 2008)},
  pages={1928--1935},
  year={2008},
  organization={IEEE}
}

@article{lumelsky1997decentralized,
  title={Decentralized motion planning for multiple mobile robots: The cocktail party model},
  author={Lumelsky, Vladimir J. and Harinarayan, KR},
  journal={Autonomous Robots},
  volume={4},
  number={1},
  pages={121--135},
  year={1997},
  publisher={Springer}
}

@article{zhu2022decentralized,
  title={Decentralized probabilistic multi-robot collision avoidance using buffered uncertainty-aware Voronoi cells},
  author={Zhu, Hai and Brito, Bruno and Alonso-Mora, Javier},
  journal={Autonomous Robots},
  volume={46},
  number={2},
  pages={401--420},
  year={2022},
  publisher={Springer}
}

@article{riviere2020glas,
  title={Glas: Global-to-local safe autonomy synthesis for multi-robot motion planning with end-to-end learning},
  author={Riviere, Benjamin and H{\"o}nig, Wolfgang and Yue, Yisong and Chung, Soon-Jo},
  journal={IEEE Robotics and Automation Letters},
  volume={5},
  number={3},
  pages={4249--4256},
  year={2020},
  publisher={IEEE}
}


@article{Li_2020,
doi = {10.1088/1742-6596/1624/4/042008},
url = {https://dx.doi.org/10.1088/1742-6596/1624/4/042008},
year = {2020},
month = {oct},
publisher = {IOP Publishing},
volume = {1624},
number = {4},
pages = {042008},
author = {Bo Li and Hongbin Liang},
title = {Multi-Robot Path Planning Method Based on Prior Knowledge and Q-learning Algorithms},
journal = {Journal of Physics: Conference Series},
abstract = {The path planning method based on prior knowledge and Q-learning algorithm proposed in this paper solves the problems of low operating efficiency and slow learning speed in existing multi-robot coordination and collision avoidance. This method can be divided into two stages. Firstly, the improved Q-learning algorithm is used to plan the static collision free path of a single robot in the built model; After the Q-table is initialized with the prior knowledge obtained in the previous step, the Q-learning algorithm is used to achieve conflict-free motion among multiple robots. Finally, the simulation experiment shows that the proposed path planning method based on prior knowledge and Q-learning algorithm performs well in handling collision avoidance path planning of multiple robots.}
}

@Article{app9153057,
AUTHOR = {Bae, Hyansu and Kim, Gidong and Kim, Jonguk and Qian, Dianwei and Lee, Sukgyu},
TITLE = {Multi-Robot Path Planning Method Using Reinforcement Learning},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {3057},
URL = {https://www.mdpi.com/2076-3417/9/15/3057},
ISSN = {2076-3417},
ABSTRACT = {This paper proposes a noble multi-robot path planning algorithm using Deep q learning combined with CNN (Convolution Neural Network) algorithm. In conventional path planning algorithms, robots need to search a comparatively wide area for navigation and move in a predesigned formation under a given environment. Each robot in the multi-robot system is inherently required to navigate independently with collaborating with other robots for efficient performance. In addition, the robot collaboration scheme is highly depends on the conditions of each robot, such as its position and velocity. However, the conventional method does not actively cope with variable situations since each robot has difficulty to recognize the moving robot around it as an obstacle or a cooperative robot. To compensate for these shortcomings, we apply Deep q learning to strengthen the learning algorithm combined with CNN algorithm, which is needed to analyze the situation efficiently. CNN analyzes the exact situation using image information on its environment and the robot navigates based on the situation analyzed through Deep q learning. The simulation results using the proposed algorithm shows the flexible and efficient movement of the robots comparing with conventional methods under various environments.},
DOI = {10.3390/app9153057}
}

@Article{robotics11050085,
AUTHOR = {Wesselhöft, Mike and Hinckeldeyn, Johannes and Kreutzfeldt, Jochen},
TITLE = {Controlling Fleets of Autonomous Mobile Robots with Reinforcement Learning: A Brief Survey},
JOURNAL = {Robotics},
VOLUME = {11},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {85},
URL = {https://www.mdpi.com/2218-6581/11/5/85},
ISSN = {2218-6581},
ABSTRACT = {Controlling a fleet of autonomous mobile robots (AMR) is a complex problem of optimization. Many approached have been conducted for solving this problem. They range from heuristics, which usually do not find an optimum, to mathematical models, which are limited due to their high computational effort. Machine Learning (ML) methods offer another potential trajectory for solving such complex problems. The focus of this brief survey is on Reinforcement Learning (RL) as a particular type of ML. Due to the reward-based optimization, RL offers a good basis for the control of fleets of AMR. In the context of this survey, different control approaches are investigated and the aspects of fleet control of AMR with respect to RL are evaluated. As a result, six fundamental key problems should be put on the current research agenda to enable a broader application in industry: (1) overcoming the &ldquo;sim-to-real gap&rdquo;, (2) increasing the robustness of algorithms, (3) improving data efficiency, (4) integrating different fields of application, (5) enabling heterogeneous fleets with different types of AMR and (6) handling of deadlocks.},
DOI = {10.3390/robotics11050085}
}

@INPROCEEDINGS{8560457,
  author={Vitolo, Emanuele and Miguel, Alberto San and Civera, Javier and Mahulea, Cristian},
  booktitle={2018 IEEE 14th International Conference on Automation Science and Engineering (CASE)}, 
  title={Performance Evaluation of the Dyna-Q algorithm for Robot Navigation}, 
  year={2018},
  volume={},
  number={},
  pages={322-327},
  doi={10.1109/COASE.2018.8560457}
}

@INPROCEEDINGS{9340876,
  author={Liu, Zuxin and Chen, Baiming and Zhou, Hongyi and Koushik, Guru and Hebert, Martial and Zhao, Ding},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={MAPPER: Multi-Agent Path Planning with Evolutionary Reinforcement Learning in Mixed Dynamic Environments}, 
  year={2020},
  volume={},
  number={},
  pages={11748-11754},
  doi={10.1109/IROS45743.2020.9340876}
}

@ARTICLE{9387150,
  author={Zhi, Jixuan and Lien, Jyh-Ming},
  journal={IEEE Robotics and Automation Letters}, 
  title={Learning to Herd Agents Amongst Obstacles: Training Robust Shepherding Behaviors Using Deep Reinforcement Learning}, 
  year={2021},
  volume={6},
  number={2},
  pages={4163-4168},
  doi={10.1109/LRA.2021.3068955}
}

@INPROCEEDINGS{8934450,
  author={Meerza, Syed Irfan Ali and Islam, Moinul and Uzzal, Md. Mohiuddin},
  booktitle={2019 1st International Conference on Advances in Science, Engineering and Robotics Technology (ICASERT)}, 
  title={Q-Learning Based Particle Swarm Optimization Algorithm for Optimal Path Planning of Swarm of Mobile Robots}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICASERT.2019.8934450}
}

@article{bernstein2002complexity,
  title={The complexity of decentralized control of Markov decision processes},
  author={Bernstein, Daniel S and Givan, Robert and Immerman, Neil and Zilberstein, Shlomo},
  journal={Mathematics of operations research},
  volume={27},
  number={4},
  pages={819--840},
  year={2002},
  publisher={INFORMS}
}



@article{chung2014empirical,
  title={Empirical evaluation of gated recurrent neural networks on sequence modeling},
  author={Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.3555},
  year={2014}
}


@article{browne2012survey,
  title={A survey of monte carlo tree search methods},
  author={Browne, Cameron B and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M and Cowling, Peter I and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  journal={IEEE Transactions on Computational Intelligence and AI in games},
  volume={4},
  number={1},
  pages={1--43},
  year={2012},
  publisher={IEEE}
}

@article{chaslot2008monte,
  title={Monte-Carlo Tree Search: A New Framework for Game AI.},
  author={Chaslot, Guillaume and Bakkes, Sander and Szita, Istvan and Spronck, Pieter},
  journal={AIIDE},
  volume={8},
  pages={216--217},
  year={2008}
}

@article{vodopivec2017monte,
  title={On monte carlo tree search and reinforcement learning},
  author={Vodopivec, Tom and Samothrakis, Spyridon and Ster, Branko},
  journal={Journal of Artificial Intelligence Research},
  volume={60},
  pages={881--936},
  year={2017}
}

@inproceedings{grill2020monte,
  title={Monte-carlo tree search as regularized policy optimization},
  author={Grill, Jean-Bastien and Altch{\'e}, Florent and Tang, Yunhao and Hubert, Thomas and Valko, Michal and Antonoglou, Ioannis and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={3769--3778},
  year={2020},
  organization={PMLR}
}

@inproceedings{santos2017monte,
  title={Monte carlo tree search experiments in hearthstone},
  author={Santos, Andr{\'e} and Santos, Pedro A and Melo, Francisco S},
  booktitle={2017 IEEE Conference on Computational Intelligence and Games (CIG)},
  pages={272--279},
  year={2017},
  organization={IEEE}
}

@inproceedings{zerbel2019multiagent,
  title={Multiagent monte carlo tree search},
  author={Zerbel, Nicholas and Yliniemi, Logan},
  booktitle={Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  pages={2309--2311},
  year={2019}
}

@article{skrynnik2021hybrid,
  title={Hybrid Policy Learning for Multi-Agent Pathfinding},
  author={Skrynnik, Alexey and Yakovleva, Alexandra and Davydov, Vasilii and Yakovlev, Konstantin and Panov, Aleksandr I},
  journal={IEEE Access},
  volume={9},
  pages={126034--126047},
  year={2021},
  publisher={IEEE}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}


@article{ye2021mastering,
  title={Mastering atari games with limited data},
  author={Ye, Weirui and Liu, Shaohuai and Kurutach, Thanard and Abbeel, Pieter and Gao, Yang},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}







@inproceedings{rashid2018qmix,
  title={Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning},
  author={Rashid, Tabish and Samvelyan, Mikayel and Schroeder, Christian and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={4295--4304},
  year={2018},
  organization={PMLR}
}





















@inproceedings{gedda2018monte,
  title={Monte Carlo methods for the game Kingdomino},
  author={Gedda, Magnus and Lagerkvist, Mikael Z and Butler, Martin},
  booktitle={2018 IEEE Conference on Computational Intelligence and Games (CIG)},
  pages={1--8},
  year={2018},
  organization={IEEE}
}

@article{baier2018emulating,
  title={Emulating human play in a leading mobile card game},
  author={Baier, Hendrik and Sattaur, Adam and Powley, Edward J and Devlin, Sam and Rollason, Jeff and Cowling, Peter I},
  journal={IEEE Transactions on Games},
  volume={11},
  number={4},
  pages={386--395},
  year={2018},
  publisher={IEEE}
}






@article{dam2022monte,
  title={Monte-Carlo robot path planning},
  author={Dam, Tuan and Chalvatzaki, Georgia and Peters, Jan and Pajarinen, Joni},
  journal={IEEE Robotics and Automation Letters},
  volume={7},
  number={4},
  pages={11213--11220},
  year={2022},
  publisher={IEEE}
}

@inproceedings{yakovlev2023planning,
  title={Planning and Learning in Multi-Agent Path Finding},
  author={Yakovlev, KS and Andreychuk, AA and Skrynnik, AA and Panov, AI},
  booktitle={Doklady Mathematics},
  pages={1--6},
  year={2023},
  organization={Springer}
}

@article{yu2023asynchronous,
  title={Asynchronous Multi-Agent Reinforcement Learning for Efficient Real-Time Multi-Robot Cooperative Exploration},
  author={Yu, Chao and Yang, Xinyi and Gao, Jiaxuan and Chen, Jiayu and Li, Yunfei and Liu, Jijia and Xiang, Yunfei and Huang, Ruixin and Yang, Huazhong and Wu, Yi and others},
  journal={arXiv preprint arXiv:2301.03398},
  year={2023}
}

@inproceedings{noh2022adaptive,
  title={Adaptive coverage path planning policy for a cleaning robot with deep reinforcement learning},
  author={Noh, DongKi and Lee, WooJu and Kim, Hyoung-Rock and Cho, Il-Soo and Shim, In-Bo and Baek, SeungMin},
  booktitle={2022 IEEE International Conference on Consumer Electronics (ICCE)},
  pages={1--6},
  year={2022},
  organization={IEEE}
}



@article{skrynnik2022pogema,
  title={POGEMA: partially observable grid environment for multiple agents},
  author={Skrynnik, Alexey and Andreychuk, Anton and Yakovlev, Konstantin and Panov, Aleksandr I},
  journal={arXiv preprint arXiv:2206.10944},
  year={2022}
}

@article{nawaz2022multi,
  title={Multi-agent multi-target path planning in Markov decision processes},
  author={Nawaz, Farhad and Ornik, Melkior},
  journal={arXiv preprint arXiv:2205.15841},
  year={2022}
}

@article{best2019dec,
  title={Dec-MCTS: Decentralized planning for multi-robot active perception},
  author={Best, Graeme and Cliff, Oliver M and Patten, Timothy and Mettu, Ramgopal R and Fitch, Robert},
  journal={The International Journal of Robotics Research},
  volume={38},
  number={2-3},
  pages={316--337},
  year={2019},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{ju2020path,
  title={Path planning using an improved a-star algorithm},
  author={Ju, Chunyu and Luo, Qinghua and Yan, Xiaozhen},
  booktitle={2020 11th International Conference on Prognostics and System Health Management (PHM-2020 Jinan)},
  pages={23--26},
  year={2020},
  organization={IEEE}
}


@article{Silver2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	issn = {0028-0836},
	url = {http://dx.doi.org/10.1038/nature16961},
	doi = {10.1038/nature16961},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	number = {7587},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	year = {2016},
	pmid = {26819042},
	pages = {484--489}
}


@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{rashid2020monotonic,
  title={Monotonic value function factorisation for deep multi-agent reinforcement learning},
  author={Rashid, Tabish and Samvelyan, Mikayel and De Witt, Christian Schroeder and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={7234--7284},
  year={2020},
  publisher={JMLRORG}
}

@article{auer2002finite,
  title={Finite-time analysis of the multiarmed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  journal={Machine learning},
  volume={47},
  pages={235--256},
  year={2002},
  publisher={Springer}
}

@inproceedings{kocsis2006bandit,
  title={Bandit based monte-carlo planning},
  author={Kocsis, Levente and Szepesv{\'a}ri, Csaba},
  booktitle={Machine Learning: ECML 2006: 17th European Conference on Machine Learning Berlin, Germany, September 18-22, 2006 Proceedings 17},
  pages={282--293},
  year={2006},
  organization={Springer}
}

@inproceedings{stern2019multi,
  title={Multi-agent pathfinding: Definitions, variants, and benchmarks},
  author={Stern, Roni and Sturtevant, Nathan and Felner, Ariel and Koenig, Sven and Ma, Hang and Walker, Thayne and Li, Jiaoyang and Atzmon, Dor and Cohen, Liron and Kumar, TK and others},
  booktitle={Proceedings of the International Symposium on Combinatorial Search},
  volume={10},
  pages={151--158},
  year={2019}
}

@article{fawzi2022discovering,
  title={Discovering faster matrix multiplication algorithms with reinforcement learning},
  author={Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R Ruiz, Francisco J and Schrittwieser, Julian and Swirszcz, Grzegorz and others},
  journal={Nature},
  volume={610},
  number={7930},
  pages={47--53},
  year={2022},
  publisher={Nature Publishing Group}
}

@article{lample2022hypertree,
  title={Hypertree proof search for neural theorem proving},
  author={Lample, Guillaume and Lacroix, Timothee and Lachaux, Marie-Anne and Rodriguez, Aurelien and Hayat, Amaury and Lavril, Thibaut and Ebner, Gabriel and Martinet, Xavier},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={26337--26349},
  year={2022}
}

@article{schrittwieser2020mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@InProceedings{Wagner2011,
  author    = {Glenn Wagner and Howie Choset},
  title     = {M*: {A} complete multirobot path planning algorithm with performance bounds},
  booktitle = {Proceedings of The 2011 {IEEE/RSJ} International Conference on Intelligent Robots and Systems ({IROS} 2011)},
  pages     = {3260--3267},
  year      = {2011},
}

@inproceedings{ma2016optimal,
  title={Optimal Target Assignment and Path Finding for Teams of Agents},
  author={Ma, Hang and Koenig, Sven},
  booktitle={Proceedings of the 15th International Conference on Autonomous Agents and Multiagent Systems ({AAMAS} 2016)},
  pages={1144--1152},
  year={2016}
}

@inproceedings{honig2018conflict,
  title={Conflict-based search with optimal task assignment},
  author={H{\"o}nig, Wolfgang and Kiesel, Scott and Tinka, Andrew and Durham, Joseph and Ayanian, Nora},
  booktitle={Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems ({AAMAS} 2018)},
  pages={757--765},
  year={2018}
}




@inproceedings{czech2021improving,
  title={Improving AlphaZero Using Monte-Carlo Graph Search},
  author={Czech, Johannes and Korus, Patrick and Kersting, Kristian},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={31},
  pages={103--111},
  year={2021}
}

@inproceedings{danihelka2022policy,
  title={Policy improvement by planning with Gumbel},
  author={Danihelka, Ivo and Guez, Arthur and Schrittwieser, Julian and Silver, David},
  booktitle={International Conference on Learning Representations},
  year={2022}
}


@article{rosin_multi-armed_2011,
	title = {Multi-armed bandits with episode context},
	volume = {61},
	issn = {1573-7470},
	url = {https://doi.org/10.1007/s10472-011-9258-6},
	doi = {10.1007/s10472-011-9258-6},
	abstract = {A multi-armed bandit episode consists of n trials, each allowing selection of one of K arms, resulting in payoff from a distribution over [0,1] associated with that arm. We assume contextual side information is available at the start of the episode. This context enables an arm predictor to identify possible favorable arms, but predictions may be imperfect so that they need to be combined with further exploration during the episode. Our setting is an alternative to classical multi-armed bandits which provide no contextual side information, and is also an alternative to contextual bandits which provide new context each individual trial. Multi-armed bandits with episode context can arise naturally, for example in computer Go where context is used to bias move decisions made by a multi-armed bandit algorithm. The UCB1 algorithm for multi-armed bandits achieves worst-case regret bounded by \$O{\textbackslash}left({\textbackslash}sqrt\{Kn{\textbackslash}log(n)\}{\textbackslash}right)\$. We seek to improve this using episode context, particularly in the case where K is large. Using a predictor that places weight Mi {\textgreater} 0 on arm i with weights summing to 1, we present the PUCB algorithm which achieves regret \$O{\textbackslash}left({\textbackslash}frac\{1\}\{M\_\{{\textbackslash}ast\}\}{\textbackslash}sqrt\{n{\textbackslash}log(n)\}{\textbackslash}right)\$where M ∗  is the weight on the optimal arm. We illustrate the behavior of PUCB with small simulation experiments, present extensions that provide additional capabilities for PUCB, and describe methods for obtaining suitable predictors for use with PUCB.},
	number = {3},
	journal = {Annals of Mathematics and Artificial Intelligence},
	author = {Rosin, Christopher D.},
	month = mar,
	year = {2011},
	pages = {203--230},
}


@article{peng2021facmac,
  title={Facmac: Factored multi-agent centralised policy gradients},
  author={Peng, Bei and Rashid, Tabish and Schroeder de Witt, Christian and Kamienny, Pierre-Alexandre and Torr, Philip and B{\"o}hmer, Wendelin and Whiteson, Shimon},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12208--12221},
  year={2021}
}

@inproceedings{li2020graph,
  title={Graph neural networks for decentralized multi-robot path planning},
  author={Li, Qingbiao and Gama, Fernando and Ribeiro, Alejandro and Prorok, Amanda},
  booktitle={Proceedings of the 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems ({IROS} 2020)},
  pages={11785--11792},
  year={2020},
  organization={IEEE}
}

