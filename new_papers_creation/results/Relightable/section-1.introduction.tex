\section{Introduction}
Human digitizing has been rapidly developed in recent years, in which the reconstruction and animation of 3D clothed human avatars have many applications in telepresence, AR/VR, and virtual try-on.
One important goal here is to render the human avatar in desired lighting environment with desired poses.
Therefore, the human avatars need to be both relightable and animatable and achieve photorealistic rendering quality.
Usually, the generation of these high-quality human avatars relies on high-quality data like the ones recorded by Light Stages~\cite{lightstage} which are complicated and expensive. 

Recently, the emergence of Neural Radiance Fileds (NeRF) \cite{mildenhall2020nerf} opens a new window to generate animatable and relightable 3D human avatars just from the daily recorded videos.
NeRF-based methods have achieved remarkable success in 3D object representation and photorealistic rendering of both static and dynamic objects including human bodies \cite{neuralbody, animatablenerf, hnerf, humannerf, selfrecon, neuman, ARAH, peng2022animatable, monohuman, su2023npc}.
Also, NeRF can be used for intrinsic decomposition to achieve impressive relighting results for static objects \cite{NeRFactor, NeILF, NeRD, NeRV, Neural-PIL, InvRender, tensoir}.
However, NeRF-based dynamic object relighting is rarely studied. 
One key challenge is that the dynamics cause dramatic changes in object shading, which is hard to model with the current NeRF techniques. 

In this work, we propose to reconstruct both relightable and animatable 3D human avatars from sparse videos recorded under uncalibrated illuminations. 
To achieve this goal, we need to reconstruct the body geometry, material, and environmental light.
The dynamic body geometry is modeled by a static geometry in a canonical space and the motion to deform it to the shape in the observation space of each frame.
We propose an invertible neural deformation field that builds a bidirectional mapping between points of the canonical space and all observation spaces. 
With this bidirectional mapping, we can easily leverage the body mesh extracted in the canonical pose to better solve the inverse linear blend skinning problem, thus achieving high-quality geometry reconstruction.
After the geometry reconstruction of all frames, we propose a light visibility estimation module to better model the dynamic self-occlusion effect for material and light reconstruction.
We transfer the global pose-related visibility estimation task into multiple, part-wise, local ones, which dramatically simplifies the complexity of light visibility estimation.
This model has good generalization capability with limited training data benefiting from the part-wise architecture, and thus successfully estimates the light visibility under various body poses and lighting conditions.
Finally, we optimize the body material and lighting parameters, and then our method can render photorealistic images under any desired body pose, lighting, and viewpoint.
In summary, the contributions include:
\begin{itemize}
    % \itemsep0em
    \item 
    the first method that is able to reconstruct both relightable and animatable human avatars with plausible shadow effects from sparse multiview videos,
    \item 
    an invertible deformation field that better solves the inverse skinning problem, leading to accurate dense correspondence between different body poses,
    \item 
    part-wise light visibility networks that better estimate pose and light-related shading cues with high generalization capability.
\end{itemize}
