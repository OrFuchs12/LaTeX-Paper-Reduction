\relax 
\bibstyle{aaai24}
\citation{lightstage}
\citation{mildenhall2020nerf}
\citation{neuralbody,animatablenerf,hnerf,humannerf,selfrecon,neuman,ARAH,peng2022animatable,monohuman,su2023npc}
\citation{NeRFactor,NeILF,NeRD,NeRV,Neural-PIL,InvRender,tensoir}
\citation{mildenhall2020nerf}
\citation{neuralbody,humannerf}
\citation{ARAH,neuman,slrf,ani-nerf,animatablenerf,peng2022animatable,selfrecon,monohuman}
\citation{pose_deformation}
\citation{ARAH,tava}
\citation{animatablenerf,ani-nerf,peng2022animatable}
\citation{monohuman}
\citation{sun2019single,wang2020single,zhou2019deep,kanamori2019relighting,pandey2021total,ji2022geometry}
\citation{guo2019relightables}
\citation{Relighting4D}
\citation{rana}
\citation{alldieck2018video}
\citation{nice,real-nvp,i-resnet,neural-ode,glow}
\citation{normalizingflow}
\citation{occflow,shapeflow,neural-part,cadex}
\citation{yang2021geometry}
\citation{ndr}
\citation{ins}
\citation{loper2015smpl}
\citation{pose_deformation}
\citation{loper2015smpl}
\citation{animatablenerf,peng2022animatable,humannerf}
\newlabel{sec:geo_rec}{{3.1}{2}{}{}{}}
\citation{real-nvp}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pipeline}{{1}{3}{The pipeline of our method. The invertible deformation field in \textit  {Geometry and Motion Reconstruction} contributes to reconstruct more accurate dynamic body geometry (Sec.\ref {sec:geo_rec}). Then the networks in \textit  {Part-wise Light Visibility Estimation} are trained to estimate pose-aware light visibility in an effective manner (Sec.\ref {sec:vis_est}). With these two parts fixed, the networks and lighting coefficients in \textit  {Material and Light Estimation} are trained and optimized by the photometric losses (Sec.\ref {sec:mat_est}).}{}{}}
\citation{volsdf}
\citation{eik}
\citation{NeRFactor,InvRender,Relighting4D}
\citation{Relighting4D}
\citation{disneyBRDF}
\citation{zhuo17,li22,psnerf}
\citation{SG}
\newlabel{sec:vis_est}{{3.2}{4}{}{}{}}
\newlabel{sec:mat_est}{{3.3}{4}{}{}{}}
\citation{InvRender}
\citation{Relighting4D}
\citation{Relighting4D}
\citation{ARAH}
\citation{peng2022animatable}
\citation{neuralbody}
\citation{h36m}
\citation{deepcap}
\citation{alldieck2018video}
\citation{Relighting4D}
\citation{peng2022animatable}
\citation{ARAH}
\newlabel{eq:lsmooth}{{14}{5}{}{}{}}
\newlabel{eq:lmat}{{15}{5}{}{}{}}
\newlabel{fig:albedo_rec}{{2}{5}{Qualitative comparison of the reconstructed albedo and lighting on synthetic data. Environment lighting is shown on top of the albedo in each result.}{}{}}
\citation{Relighting4D}
\citation{ssim}
\citation{lpips}
\citation{peng2022animatable}
\citation{peng2022animatable}
\newlabel{tab:syn}{{1}{6}{Quantitative comparison of the reconstructed albedo and the relighting results on synthetic data.}{}{}}
\newlabel{fig:relit_real}{{3}{6}{Qualitative comparison of relighting results on real data. The environment lighting of the rendered results is shown at the bottom.}{}{}}
\newlabel{tab:geo}{{2}{6}{Quantitative comparison of the reconstructed geometry on synthetic data.}{}{}}
\newlabel{sec:ablation}{{4.3}{6}{}{}{}}
\citation{instant-ngp}
\citation{zheng2023avatarrex,shen2023xavatar}
\newlabel{fig:geo_artifact}{{4}{7}{Qualitative results of novel poses synthesis on real data. This novel pose results reflect the accuracy of the reconstructed geometry to a certain extent. }{}{}}
\newlabel{fig:ablation_lvis}{{5}{7}{Ablation study on part-wise light visibility. See our method synthesizing plausible self-occlusions.}{}{}}
\bibdata{main}
\citation{Relighting4D}
\citation{idr}
\citation{loper2015smpl}
\citation{marchingcubes}
\citation{aist}
\citation{adam}
\citation{aist}
\citation{peng2022animatable}
\citation{neuralbody}
\citation{h36m}
\citation{deepcap}
\citation{alldieck2018video}
\citation{peng2022animatable}
\citation{h36m}
\citation{animatablenerf}
\citation{su2023npc}
\citation{neuralbody}
\citation{animatablenerf}
\citation{a-nerf}
\citation{neuralbody}
\citation{animatablenerf}
\newlabel{fig:geo_net}{{6}{9}{Architecture of the geometry and color network.}{}{}}
\newlabel{fig:def_net}{{7}{9}{Architecture of the basic block in the invertible deformation network.}{}{}}
\citation{neuralbody}
\citation{animatablenerf}
\citation{a-nerf}
\citation{ARAH}
\citation{danbo}
\citation{tava}
\citation{su2023npc}
\citation{neuralbody}
\citation{animatablenerf}
\citation{a-nerf}
\citation{ARAH}
\citation{danbo}
\citation{tava}
\citation{su2023npc}
\citation{deepcap}
\citation{alldieck2018video}
\citation{neuralbody}
\newlabel{fig:corres}{{8}{10}{Visualization of estimated correspondences.}{}{}}
\newlabel{fig:h36m}{{9}{10}{Qualitative comparisons of novel pose synthesis on the Human3.6M dataset.}{}{}}
\newlabel{tab:non-relit-sp}{{3}{11}{Novel-view synthesis comparisons on the Human3.6M dataset.}{}{}}
\newlabel{tab:non-relit-np}{{4}{11}{Novel-pose synthesis comparisons on the Human3.6M dataset.}{}{}}
\newlabel{fig:deepcap}{{10}{11}{Results of our technique on the DeepCap dataset. From left to right of each result: the albedo of an animated pose, the corresponding normal in this pose, and two relighting results. }{}{}}
\newlabel{fig:ps}{{11}{12}{Results of our technique on the PeopleSnapshot dataset. From left to right of each result: the albedo of an animated pose, the corresponding normal in this pose, and two relighting results. }{}{}}
\newlabel{fig:single_view}{{12}{13}{Results on the ZJU-MoCap dataset with single-view input. From top to bottom: the reconstructed albedo, normal of the reconstructed geometry, and two relighting results.}{}{}}
\gdef \@abspage@last{13}
