\relax 
\bibstyle{aaai23}
\citation{robertson1995okapi,robertson2009probabilistic}
\citation{karpukhin-etal-2020-dense,xiong2021approximate}
\citation{lee-etal-2019-latent}
\citation{karpukhin-etal-2020-dense}
\citation{thakur2021beir}
\citation{sciavolino-etal-2021-simple}
\citation{sciavolino-etal-2021-simple}
\citation{lewis-etal-2021-paq}
\citation{karpukhin-etal-2020-dense}
\citation{Chen2021SalientPA}
\citation{chen-etal-2020-recall}
\citation{chen-etal-2017-reading}
\citation{robertson1995okapi,robertson2009probabilistic}
\citation{thakur2021beir}
\citation{sachan2022improving}
\citation{thakur2021beir}
\citation{sciavolino-etal-2021-simple}
\citation{lewis-etal-2021-paq}
\citation{sciavolino-etal-2021-simple}
\citation{replication_dpr}
\citation{karpukhin-etal-2020-dense}
\citation{Chen2021SalientPA}
\citation{zhao-etal-2021-sparta}
\citation{gao-etal-2021-coil}
\citation{khattab-etal-2021-relevance}
\citation{yamada-etal-2020-luke}
\newlabel{sec:related}{{}{2}{}{}{}}
\newlabel{sec:method}{{}{2}{}{}{}}
\newlabel{sec:pipeline}{{}{2}{}{}{}}
\citation{yamada-etal-2020-luke}
\citation{tjong-kim-sang-de-meulder-2003-introduction}
\citation{sciavolino-etal-2021-simple}
\citation{roberta}
\citation{karpukhin-etal-2020-dense}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:overview}{{1}{3}{ An overview of our proposed zero-shot dense retrieval system. Named entities in a question and passages are extracted via an off-the-shelf named-entity recognition model. Then, the extracted named entities (hollow rectangles) are encoded into dense representations (solid rectangles) with a frozen pretrained language model (denoted as \textit  {Encoder}). Along with the entity span, a full sequence of the text is used to condition the semantics of the entire sentence on the embedding (outlined arrows). For instance, the dense representation corresponding to \textit  {Ted Howard} is conditioned by P2, shown as ``Ted Howard $\mid $ P2'' in this Figure. We also use the embedding of the entire span of a passage title as a retrieval key. The passages can have multiple keys depending on how many entity names the passage has. The similarity between a query and a key is measured using cosine similarity (denoted as \textit  {Sim}). For multiple keys in each passage, single relevance score for each passage is calculated via maximum pooling. Note that the proposed system requires no additional training if the encoder language model is pretrained. }{}{}}
\newlabel{sec:method_ner}{{}{3}{}{}{}}
\newlabel{sec:contextualized_entity}{{}{3}{}{}{}}
\newlabel{sec:experiments}{{}{3}{}{}{}}
\citation{sciavolino-etal-2021-simple}
\citation{vrandevcic2014wikidata}
\citation{elsahar-etal-2018-rex}
\citation{robertson2009probabilistic}
\citation{anserini_Yang0L17}
\citation{karpukhin-etal-2020-dense}
\citation{kwiatkowski-etal-2019-natural}
\citation{kwiatkowski-etal-2019-natural}
\citation{joshi-etal-2017-triviaqa}
\citation{berant-etal-2013-semantic}
\citation{curatedtrec}
\citation{lewis-etal-2021-paq}
\citation{Chen2021SalientPA}
\citation{lewis-etal-2021-paq}
\citation{Chen2021SalientPA}
\citation{contriever}
\citation{wenzek-etal-2020-ccnet}
\citation{lee-etal-2019-latent}
\newlabel{tab:acc}{{1}{4}{ The macro average of the top-20 retrieval accuracies (denoted as Recall@20) on the 24 relations of EQ test set. \textit  {\# LM} refers to the total number of language models used in each dense retriever with different weight parameters. \textit  {Retrieval training} denotes fine-tuning methods for retrieval of each dense retriever. See text for details of the baseline models. }{}{}}
\newlabel{sec:datasets}{{}{4}{}{}{}}
\newlabel{sec:q_k_gen}{{}{4}{}{}{}}
\citation{zhou-srikumar-2022-closer}
\citation{match_your_words}
\newlabel{fig:idf}{{2}{5}{ The top-20 retrieval accuracy on \texttt  {\relax \fontsize  {9}{10}\selectfont  author} (P50, \textit  {Who is the author of [E]?}), where the test set questions were grouped by the $\mathrm  {IDF}_{ent}$ value of the entity name in the questions. The questions are grouped into five buckets according to the maximum IDF value in the entity tokens. The number of questions in each bucket is 200. }{}{}}
\newlabel{sec:results_eq}{{}{5}{}{}{}}
\newlabel{sec:analysis}{{}{5}{}{}{}}
\newlabel{sec:exp_idf}{{}{5}{}{}{}}
\citation{luan-etal-2021-sparse}
\citation{umap}
\newlabel{fig:embedding_umap}{{3}{6}{ A UMAP projection of the queries and the corresponding top-20 keys retrieved by (a) DPR-NQ and (b) our method. For the queries, we used five randomly selected questions from the \texttt  {\relax \fontsize  {9}{10}\selectfont  educated at} questions (P69, \textit  {Where was [E] educated?}). The retrieved keys with respect to a query (outlined circle) are plotted in the same color as the query. The outlined rectangles represent the embeddings of a synthesized context that contains multiple relational information points for two people, (\textit  {Max Bernhauer} and \textit  {Marie Rutkoski}). Note that the number of the plotted embeddings of the synthesized context is one for DPR-NQ but two for ours. }{}{}}
\newlabel{sec:comparison_with_dpr}{{}{6}{}{}{}}
\newlabel{sec:ablation_multiple_keys}{{}{6}{}{}{}}
\newlabel{fig:one_key}{{4}{7}{ The top-20 retrieval accuracy for \texttt  {\relax \fontsize  {9}{10}\selectfont  author} (P50, \textit  {Who is the author of [E]?}). The format is the same as in Figure\nobreakspace  {}\ref {fig:idf}, and the Full Set is the same result as the one in that figure. For each passage from Wikipedia, only one key was selected using random selection (denoted as Random), along with the key corresponding to the maximum $\mathrm  {IDF}_{ent}$ (denoted as Max $\mathrm  {IDF}_{ent}$). The mean accuracies of Random and Max $\mathrm  {IDF}_{ent}$ are 35.8\% and 21.0\%, respectively, while the mean accuracy of Full Set is 82.0\%. }{}{}}
\newlabel{sec:ablation_contextualized_embedding}{{}{7}{}{}{}}
\newlabel{tab:conditioning}{{2}{7}{ The top-20 and top-100 retrieval accuracies (\%) on EQ test sets. The highest score is indicated in bold. For example, given a question ``\textit  {Where was Ted Howard born?}'', the entity span is ``\textit  {Ted Howard}'', and the full span of questions is ``\textit  {Where was Ted Howard born?}''. }{}{}}
\bibdata{retriever_zeroner}
\gdef \@abspage@last{8}
