\relax 
\bibstyle{aaai24}
\citation{Yun_2022_surveillance}
\citation{REN_2017_UAVDetection,REN_2021_UAVDetection}
\citation{Alami_2023_FleetNavigation}
\citation{Tokekar_2016_agriculture}
\citation{Xi_2021_SeanHE,Yue_2020_SeanHe,Bouguettaya_2022_review}
\citation{Ge_2021_YOLOX}
\citation{Deng_2021_GLSAN,Xu_2022_AdaZoom}
\citation{Deng_2021_GLSAN,Xu_2022_AdaZoom}
\citation{Huang_2022_UFPMP}
\@LN@col{1}
\newlabel{sec: intro}{{}{1}{}{}{}}
\@LN@col{2}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: intro}{{1}{1}{The number of objects (\textit  {y-axis}) that are optimally detected using the scaling factor (\textit  {x-axis}) for ultra-small, small, medium and large objects, respectively on the VisDrone dataset. \textit  {The optimal scales are significantly different for different objects.}}{}{}}
\citation{he2023hierarchical,Zhang_2023_Spatial}
\citation{Bouguettaya_2022_review}
\citation{Ge_2021_YOLOX}
\citation{Huang_2022_UFPMP}
\citation{Zhu_2022_VisDrone}
\citation{Cheng_2019_Learning}
\citation{Li_2017_Perceptual}
\citation{Bai_2018_SODMTGAN}
\citation{Bouguettaya_2022_review}
\citation{Sommer_2017_Fast}
\citation{Wang_2019_Spatial}
\citation{Zhang_2019_Scale}
\citation{Zhou_2019_SAICFPN}
\citation{Bouguettaya_2022_review}
\citation{Unel_2019_TilingSOD}
\citation{Yang_2019_Clustered}
\citation{Xu_2022_AdaZoom}
\citation{Huang_2022_UFPMP}
\citation{Xu_2022_AdaZoom}
\citation{Huang_2022_UFPMP}
\citation{Cai_2018_Cascade}
\citation{Ren_2015_LBPVisualRecognition}
\citation{Lowe_2004_SIFT}
\citation{Dalal_2005_HOG}
\citation{Bouguettaya_2022_review}
\citation{Ren_2017_Faster,Ge_2021_YOLOX}
\citation{Girshick_2014_Rich}
\citation{Ren_2017_Faster}
\citation{He_2017_Mask}
\citation{Redmon_2016_You,Redmon_2017_YOLO9000,Ge_2021_YOLOX}
\citation{Lin_2020_Focal}
\citation{Tan_2020_EfficientDet}
\@LN@col{1}
\@LN@col{2}
\newlabel{sec: related-work}{{}{2}{}{}{}}
\citation{Wang_2021_ScaledYOLOv4}
\citation{Ge_2021_YOLOX}
\citation{Zhang_2022_Spatial,Zhang_2023_Spatial}
\citation{Huang_2022_UFPMP}
\citation{Huang_2022_UFPMP}
\newlabel{fig: structure}{{2}{3}{Overview of the proposed model. A YOLOX variant is first utilized to generate regions of interests. The regions are expanded to include the background context and merged to form cluster regions. An evolutionary reinforcement learning (EVORL) agent with three rewards is designed to determine the optimal scale for each patch. The spatial-semantic attention is designed to boost the patch features. After determining the optimal scales through the proposed EVORL, the regions are scaled and consolidated into a mosaic image, and passed back to the detector for fine detection. }{}{}}
\@LN@col{1}
\newlabel{sec: methodology}{{}{3}{}{}{}}
\newlabel{sec: overallFramework}{{}{3}{}{}{}}
\@LN@col{2}
\newlabel{sssec:problemFormulation}{{}{3}{}{}{}}
\citation{jiang2018acquisition}
\citation{song2023siamese}
\citation{yi2023automated}
\citation{Hui_2023_EvoRLSurvey}
\citation{tu2023deep,chen2022cooperative}
\citation{hu2018squeeze}
\@LN@col{1}
\newlabel{sssec:att}{{}{4}{}{}{}}
\newlabel{eqn:FA}{{1}{4}{}{}{}}
\newlabel{sssec:reward}{{}{4}{}{}{}}
\@LN@col{2}
\newlabel{eq:rs}{{2}{4}{}{}{}}
\newlabel{equ:rewardFunction}{{3}{4}{}{}{}}
\newlabel{ssec:EvoRLStrategy}{{}{4}{}{}{}}
\citation{araslanov2019actor}
\citation{araslanov2019actor}
\citation{Du_2018_UAVDT}
\citation{Zhu_2022_VisDrone}
\citation{Liu_2021_HRDNet,Ge_2022_ZoomAndReasoning}
\citation{Ren_2017_Faster}
\citation{Liu_2021_HRDNet}
\citation{Li_2020_Density_Workshops}
\citation{Zhou_2019_SAICFPN}
\citation{Deng_2021_GLSAN}
\citation{Xu_2022_AdaZoom}
\citation{Yang_2019_Clustered}
\citation{Huang_2022_UFPMP}
\citation{Ge_2022_ZoomAndReasoning}
\citation{Li_2020_GFL}
\@LN@col{1}
\newlabel{eqn:actor_gradient}{{4}{5}{}{}{}}
\newlabel{eqn:critic_gradient}{{5}{5}{}{}{}}
\newlabel{sec: experiments}{{}{5}{}{}{}}
\newlabel{ssec: experimental settings}{{}{5}{}{}{}}
\newlabel{sssec:dataset}{{}{5}{}{}{}}
\@LN@col{2}
\newlabel{alg:ppo}{{1}{5}{Training procedures for the proposed EVORL}{}{}}
\citation{Huang_2022_UFPMP}
\citation{Huang_2022_UFPMP,Xu_2022_AdaZoom}
\citation{Huang_2022_UFPMP}
\citation{Huang_2022_UFPMP}
\citation{Ge_2022_ZoomAndReasoning}
\citation{Huang_2022_UFPMP}
\citation{Huang_2022_UFPMP}
\citation{Zhu_2022_VisDrone}
\citation{Xu_2022_AdaZoom}
\citation{wang2023improved}
\citation{Zhu_2022_VisDrone}
\@LN@col{1}
\newlabel{tab:results_uavdt}{{1}{6}{Comparison with the state-of-the-art methods on the UAVDT dataset. The proposed method consistently and significantly outperforms all the compared methods.}{}{}}
\newlabel{sssec:implementationDetails}{{}{6}{}{}{}}
\newlabel{sssec:uavdt}{{}{6}{}{}{}}
\@LN@col{2}
\newlabel{tab:results_sml}{{2}{6}{Comparison with state-of-the-art methods on the UAVDT dataset in terms of $AP^S$, $AP^M$ and $AP^L$.}{}{}}
\newlabel{tab:results_visdrone}{{3}{6}{Comparisons with state-of-the-art methods on the VisDrone dataset. The proposed method significantly outperforms the compared methods in terms of $AP$ and $AP_{75}$. }{}{}}
\newlabel{sssec:visdrone}{{}{6}{}{}{}}
\citation{Huang_2022_UFPMP}
\newlabel{fig: visualization}{{3}{7}{Visual comparison with UFPMP-Det\nobreakspace  {}\citep  {Huang_2022_UFPMP} on the VisDrone dataset. The proposed method correctly detects more objects than UFPMP-Det, as annotated in green. }{}{}}
\@LN@col{1}
\newlabel{tab:ablation}{{4}{7}{Ablation study of major components of the proposed method on the VisDrone dataset. }{}{}}
\newlabel{ssec: ablation-study}{{}{7}{}{}{}}
\newlabel{ssec:visualization}{{}{7}{}{}{}}
\@LN@col{2}
\newlabel{sec: conclusion}{{}{7}{}{}{}}
\@LN@col{1}
\@LN@col{2}
\bibdata{aaai24}
\gdef \@abspage@last{8}
