%File: formatting-instructions-latex-2024.tex
%release 2024.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% % \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

%FRED
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{colortbl}
\usepackage[noabbrev, capitalize]{cleveref}

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{FRED: Towards a Full Rotation-Equivariance in Aerial Image Object Detection}
\author{
    Chanho Lee\textsuperscript{\rm 1},
    Jinsu Son\textsuperscript{\rm 1},
    Hyounguk Shon\textsuperscript{\rm 1},
    Yunho Jeon\textsuperscript{\rm 2},
    Junmo Kim\textsuperscript{\rm 1}
}
\affiliations{
    \textsuperscript{\rm 1}Korea Advanced Institute of Science and Technology, South Korea\\
    \textsuperscript{\rm 2}Hanbat National University, South Korea\\
    \{yiwan99, sonjs, hyounguk.shon\}@kaist.ac.kr, yhjeon@hanbat.ac.kr, junmo.kim@kaist.ac.kr

}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle


\begin{abstract}
Rotation-equivariance is an essential yet challenging property in oriented object detection. While general object detectors naturally leverage robustness to spatial shifts due to the translation-equivariance of the conventional CNNs, achieving rotation-equivariance remains an elusive goal. Current detectors deploy various alignment techniques to derive rotation-invariant features, but still rely on high capacity models and heavy data augmentation with all possible rotations. In this paper, we introduce a Fully Rotation-Equivariant Oriented Object Detector (FRED), whose entire process from the image to the bounding box prediction is strictly equivariant. Specifically, we decouple the invariant task (object classification) and the equivariant task (object localization) to achieve end-to-end equivariance. We represent the bounding box as a set of rotation-equivariant vectors to implement rotation-equivariant localization. Moreover, we utilized these rotation-equivariant vectors as offsets in the deformable convolution, thereby enhancing the existing advantages of spatial adaptation. Leveraging full rotation-equivariance, our FRED demonstrates higher robustness to image-level rotation compared to existing methods. Furthermore, we show that FRED is one step closer to non-axis aligned learning through our experiments. Compared to state-of-the-art methods, our proposed method delivers comparable performance on DOTA-v1.0 and outperforms by 1.5 mAP on DOTA-v1.5, all while significantly reducing the model parameters to 16\%.
\end{abstract}



\section{Introduction}
Aerial object detection is an emerging field in the domain of computer vision. Since aerial images capture objects with arbitrary orientations and are often densely packed, oriented bounding box (OBB) can provide a tighter representation in such cases. One distinguishing feature of aerial images is the non-axis aligned nature, absence of any top-bottom or left-right bias. The ideal aerial object detector should consistently deliver predictions irrespective of object orientation. If the image undergoes rotation, the predicted OBB should also rotate concurrently. Hence, for an oriented object detector to be reliable, it must exhibit rotation-equivariance.
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/figure1-v3.pdf}
\caption{\textbf{Overview of the fully rotation-equivariant object detector (FRED).} FRED consists of a rotation-equivariant backbone which predicts a point set followed by two prediction branches -- (1) a rotation-equivariant box regression head and (2) a rotation-invariant classification head. We achieve end-to-end equivariance for object detection.}
\label{fig:figure1}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=0.8\textwidth]{figs/Figure3-v3.pdf}
\caption{\textbf{Overall model architecture of the proposed Fully Rotation-Equivariant Detector (FRED).} \( C_N \)-equivariant features are fed into the rotation-equivariant head up to two deformable convolution blocks. The Rotation-Equivariant Deformable Convolution (RE-DCN) tilizes an initial point set as an offset and refines it through spatial adaptation without breaking rotation-equivariance. The Rotation-Invariant Deformable Convolution (RI-DCN) performs an orientation alignment to produce rotation-invariant features using an align reference vector sourced from the localization branch. As both the deformable offsets and the reference vector maintain rotation-equivariance, the classification branch achieves instance-level rotation-invariance.}
\label{fig:figure2}
\end{figure*}
However, achieving rotation-equivariance on oriented object detection is challenging, since most researches are extended from horizontal object detection models. Most methods rely on the assumption of accurate orientation estimation and focus on making features invariant to rotation. A representative approach to achieve this is the RoI Transformer \citep{roitransformer}, which leverages rotation-sensitive region-of-interest (RoI) pooling on a rotated RoI (RRoI) to acquire instance-level rotation-invariance. Such orientation-specific feature refinement has demonstrated its efficiency across one-stage object detectors \citep{han2021s2anet} and anchor-free detectors \citep{pan2020drn}. Another strategy is using point set representation which implicitly represents the oriented bounding box as a set of adaptively learned points \citep{guo2021beyond, li2022oriented}. These approaches have managed to separate out the prediction of orientation itself, naturally leads to non-axis aligned feature learning. Yet, these aforementioned methods rely heavily on data augmentation using random rotations and remain distant from achieving true rotation-equivariance.
% \begin{figure}[t]
%     \centering
%     \begin{subfigure}{.45\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs/Fig1example.png}
%         \caption{Previous point set based oriented object detector}
%         \label{fig:previous}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{.45\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs/Fig1exampleb.png}
%         \caption{Our Rotation-Equivariant FRED}
%         \label{fig:proposed}
%     \end{subfigure}
%     \caption{ }
%     \label{fig:methods}
% \end{figure}

Recently, \citet{han2021ReDet} proposed ReDet, a rotation-equivariant detector firstly employing rotation-equivariant CNNs \citep{weiler2019general}. Their Rotation-invariant ROI Align (RiRoI Align) leverages the characteristics of rotation-equivariant features, enabling the extraction of rotation-invariant features dependent on the rotated RoI. However, it is worth noting that even though their RiRoI Align operates based on a rotation-equivariant theory, the orientation of the predicted RRoI itself is not equivariant. Due to the ambiguity and angular discontinuity of OBB representations, rotation-invariance of RiRoI Align is vulnerable to significant degrees of rotations. To design a model that remains consistent across any rotation, we utilize a point set representation in place of the bounding box, and endow the entire localization process with strict rotation-equivariance.

In this work, we propose a fully rotation-equivariant oriented object detector named \emph{FRED} which leverages point set representation to achieve full rotation-equivariance on both classification and localization. We conceptualize the bounding box as a set of rotation-equivariant vectors. By employing this idea, we ensure that with any rotation, the vectors not only shift in accordance with the image-level rotation but also change their orientation simultaneously. This trait perfectly satisfies the attributes needed for oriented bounding box prediction as depicted in \cref{fig:figure1}. Furthermore, we apply these rotation-equivariant vectors as offsets of deformable convolution. This allows us to propose Rotation-Equivariant Deformable Convolution (RE-DCN) and Rotation-Invariant DCN (RI-DCN), which can simultaneously achieve spatial and orientation alignment through an rotation-equivariant receptive field. Compared to previous methods, our FRED is highly robust to image rotations powered by end-to-end rotation-equivariance. Moreover, FRED maximizes the benefits of the high-level weight sharing of rotation-equivariant CNNs, showcasing superior performance with fewer learnable parameters than any other detectors.

%% Modified version
Moreover, we discovered a promising phenomenon while training our rotation-equivariant model. Typically, rotation-equivariance is associated with robustness to different rotations for a single instance. If there exists an instance group with similar context and scale, we can anticipate rotation-equivariance among them. We observed that FRED, just before full convergence, learned the relative pose of objects without any direction-specific supervision. While this tendency diminishes during the convergence process, it can be seen as a reflection of FRED being trained in a genuinely non-axis aligned manner. Through our experiments, we demonstrate that previous non-axis aligned methods are still being trained in an axis-overfitted manner, while our FRED showcases a more genuine non-axis aligned learning.

In summary, our main contributions in this paper are as follows:
\begin{itemize}
    \item To the best of our knowledge, we are the first to propose a fully rotation-equivariant oriented object detector. Compared to previous state-of-the-art methods, FRED guarantees more robust predictions against image rotations.
    \item We propose novel methods that combine deformable convolution and rotation-equivariant vectors to simultaneously perform spatial and orientation alignment without disrupting equivariance.
    \item Our experiments demonstrate that FRED achieves promising results with significantly fewer parameters, and offers a new insight into axis-free learning.
\end{itemize}




\section{Related Works}

\subsection{Oriented Object Detection}
The main approach for oriented object detection extends from horizontal object detection with additional regression for orientation. Challenges in oriented object detection in aerial images arise from arbitrary oriented and densely packed objects. RoI Transformer \citep{roitransformer} proposed a rotation-sensitive RoI pooling for obtaining rotation-invariance. SCRDet \citep{scrdet}, DRN \citep{pan2020drn}, S2A-Net \citep{han2021s2anet}, and R$^3$Det \citep{yang2021r3det} adressessed the challenges through methods that refine features. Most research focuses on developing methods to apply the axis-aligned property to non-axis aligned oriented object detection, yet rely on well predefined anchors and angular discontinuity of OBB.\citep{qian2021rsdet, yang2022kfiou}


\subsection{Alternative Bounding Box Representations}
The introduction of point set representation offers a promising alternative to these issues, escaping from traditional anchors and bounding boxes. By adaptively capturing object context, point set has demonstrated their capability to learn richer representations as shown in RepPoints \citep{yang2019reppoints}. Through strategies like convex-hull feature adaptation \citep{guo2021beyond} and orientation-sensitive sampling \citep{li2022oriented}, point set based methods outperform the previous anchor-based detectors. Our FRED introduces rotation-equivariant point set prediction and its benefit, showcasing its closer alignment with true non-axis aligned learning.



\subsection{Rotation-Equivariant Neural Networks}
Beginning with the Group-Equivariant CNN proposed by \citet{cohen2016group}, several methods have introduced rotation-equivariant CNNs using steerable filters \citep{weiler2019general, cesa2021program} and they have been proven effective in various imagery fields \citep{veeling2018rotation, gupta2021rotation, lee2023learning}. Recently, \citet{han2021ReDet} introduced ReDet, a rotation-equivariant detector for oriented object detection that firstly utilizes the rotation-equivariant CNNs. While RiRoI Align offers a rotation-invariant transform, its dependency on the non-equivariantly predicted RRoI still introduces a residual challenge to achieving full rotation-equivariance.

\section{Preliminaries}
This section offers a brief overview on the concept of rotation-equivariance. Given the prevalent use of steerable filters to yield rotation-equivariant features, the finite rotation group and its representation through group-wise permutation are introduced.

Let \( G \) be a group which can be any transformations on image space \( X \). Then a function \( \Phi: X \to Y \) is said to be \textit{equivariant} if
\begin{equation}
     \Phi\left(T^X_g(x)\right) = T^Y_g\left(\Phi(x)\right)  \quad \forall g \in G ,\, \forall x \in X
\end{equation}
where \(T^X_g\) and \(T^Y_g\) is a group action defined on each space. If  \(T^Y_g\) is identity mapping, then invariance holds. For example, the conventional CNN \(\Phi\) shares convolution weight at every location, so satisfies the equation above on translation action.

\subsection{Rotation-Equivariance}
In this paper, we are addressing rotation-equivariance, so group can be formulated as the semi-direct product of the translation group \( (R^2, +) \) and rotation group \( H \). For a group \(G\), the rotation-equivariance of function \(f: X \to Y\) can be expressed as
\begin{equation}
     f\left(T^X_g(x)\right) = T^Y_g\left(f(x)\right)  \quad \forall g \in G ,\, \forall x \in X
\end{equation}
 given \(T^X_g\) and \(T^Y_g\) as rotation action on \(X\) and \(Y\) respectively. If \(T^X_g\) and \(T^Y_g\) are isomorphic image-level rotations in each space, then function \(f\) can be viewed as transforming the image space into a rotation-equivariant scalar field.

On the other hand, let a function \(v: X \to V\) be a mapping from the image space \(X\) to 2-dimensional vector field space \(V\). To provide clearness in notation,  let us denote \(T_g\)  as the group action of \(G\) that operates on both \(X\) and \(V\). Then rotation-equivariance of vector field can be expressed as:
\begin{equation}
     v\left(T_g(x)\right) = R_g T_g\left(v(x)\right)  \quad \forall g \in G ,\, \forall x \in X
\end{equation}
% \(R_g\) is a 2-dimensional rotation matrix that rotates vectors in parallel with \(T_g\).
In this context, \(R_g\) is a group action on \(G\) that rotates every vector in parallel with \(T_g\). For clarity, a rotation-equivariant vector field necessitates not just image-level rotations \(T_g\) but also ensures that the vector predicted at each image pixel rotates by \(R_g\).

% Let \( G \) be a group which can be any transformations on image space \( X \). Then a function \( \Phi: X \to Y \) is said to be \textit{equivariant} if
% \begin{equation}
%      T^Y_g(\Phi(x))= \Phi(T^X_g(x))  \quad \forall g \in G ,\, \forall x \in X
% \end{equation}

% when \(T^X_g\) and \(T^Y_g\) is a group action defined on each space. In this paper, we are addressing rotation-equivariance, so group \(G\) can be formulated as the semi-direct product of the translation group \( (R^2, +) \) and rotation group \( H \) in this context. If \(T_g\) is a group action defined on rotation group G, intuitively image-level rotation and let \( R_g \in \mathbb{R}^{2 \times 2} \) is a rotation matrix corresponds to \(T_g\).  Then a function maps to \(K\)-dimensional feature \( f: \mathbb{Z}^2 \to \mathbb{R}^K \) is rotation-equivariant if

% \begin{equation}
%    T_g f(x) = f(R^{-1}_g(x))   \quad \forall g \in G ,\, \forall x \in X
% \end{equation}

% On the other hand, we can define a function maps image to vector field as \( v: \mathbb{R}^2 \to \mathbb{R}^2 \). Then the rotation-equivariance of the vector field is defined as

% \begin{equation}
%     T_gv(x) = R_g v(R^{-1}_g x)  \quad \forall g \in G ,\, \forall x \in X
% \end{equation}

% where \( R(g) \in \mathbb{R}^{2 \times 2} \) is a rotation matrix corresponds to rotation action \(T_g\).

% TgX~=TgY, f(x) is rotation-equivariant scalar feature field.
% If \(f(x)\in \mathbb{R}^{2}\), R? R(g)? rotation matrix corresponds to ..

% \subsection{rotation-equivariance}
% Let \( G \) be a group which can be any transformations on image space \( X \). Then a function \( f: X \to Y \) is said to be \textit{equivariant} if
% \begin{equation}
%     f(T^X_g(x)) = T^Y_g(f(x))  \quad \forall g \in G ,\, \forall x \in X
% \end{equation}

% when \(T^X_g\) and \(T^Y_g\) is a group action defined on each space.
% TgX~=TgY, f(x) is rotation-equivariant scalar feature field.
% If \(f(x)\in \mathbb{R}^{2}\), R? R(g)? rotation matrix corresponds to ..
% %In addressing rotation-equivariance, group G can be formulated as the semi-direct product of the translation group \( (R^2, +) \) and rotation group \( H \) in this context. First, we dissect an ideal oriented object detector into separate branches for classification and localization, denoted as \( f^{cls} \) and \( f^{loc} \) respectively. We then aim to compare their rotation-equivariances. The ideal classification branch, \( f^{cls}: X \to Y^{cls} \), maps image space \( X \) to a dense classification map \( Y^{cls} \). Then, rotation-equivariance of \( f^{cls} \) is expressed as a special case of equation 1 when \( T^X_g = T^Y_g \), called as rotation-equivariant scalar feature field.

% %\begin{equation}
% %    C(T_g(x)) = T_g(C(x)) \quad \forall g \in G \quad \forall x \in X
% %\end{equation}

% Prior literature often referred to this as instance-level rotation-invariance\cite{roitransformer, han2021ReDet}. From the perspective of any given instance, regardless of the image-level rotation, its predicted class probability must remain rotation-invariant. In the rest of this paper, to distinguish from localization, we will describe the scalar-field rotation-equivariance as \textit{rotation-invariance} for clarity.

% Likewise, the ideal object localization branch \(  f^{loc} \) is defined as \( f^{loc}: X \to Y^{loc} \), mapping an image to a dense localization space \(Y^{loc} \). Let \( \rho^B(g) \) be a group action that rotates the orientation of any bounding box \( B \) concurrently with \( T_g \). Under this scenario, the rotation-equivariance of the ideal localization branch can be expressed as follows:

% \begin{equation}
%     f^{loc}(T_g(x)) = \rho^B(g) T_g(f^{loc}(x)) \quad \forall g \in G ,\, \forall x \in X
% \end{equation}

% However, explicitly defining the \( \rho^B(g) \) is intricate. This arises due to the representation ambiguity of OBB, the height and width of a bounding box can be freely interchanged, as highlighted in \citet{yang2021rethinking}. To address this issue, using a point set \( S = \{(x^k; y^k)\}^{k=1...K} \) as an alternative representation for \( B \) simplifies matters. We can define the point set rotation action \( \rho^S(g)  \) as 2-D rotation matrix \( R(g) \in \mathbb{R}^{2 \times 2} \), which satisfies \( \rho^S_g(S) = \{ R(g)(x^k; y^k) \}^{k=1...K} \). Then the localization branch with point set representation is rotation-equivariant if

% \begin{equation}
%      f^{loc}(T_g(x)) = R(g) T_g(f^{loc}(x)) \quad \forall g \in G ,\, \forall x \in X
% \end{equation}

% Therefore, point set prediction can be viewed as an ordered set of rotation-equivariant vector fields\citep{marcos2017rotation}. Intuitively, each point is a vector pointing to arbitrary representative parts of the object from each feature grid. Given that the transformation of point set \(S\) to its corresponding convex-hull or bounding box \(B\) is rotation-agnostic process, \(f^{loc}\) maps to rotation-equivariant oriented bounding boxes.

\begin{figure*}[t]
% \centering
    \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/re-dcn.pdf}
    \caption{\textbf{Rotation-equivariant Deformable Convolution}}
    \label{fig:figure3a}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
    \centering
    % \includegraphics[width=\linewidth]{figs/figure4.pdf}
    % \includegraphics[width=\linewidth]{figs/FRED-figure4.pdf}
    \includegraphics[width=0.9\linewidth]{figs/ri-dcn.pdf}
    \caption{\textbf{Rotation-invariant Deformable Convolution}}
    \label{fig:figure3b}
    \end{subfigure}
\caption{This example illustrates a 4-equivariant rotation group ($C_4$) and 2x2 deformable kernel for simplicity. The deformable convolution (DCN) layer parameters are shared between rotation groups.}
\label{fig4}
\end{figure*}

\subsection{Steerable Filters and Cyclic-Equivariance} %이름 고민
A commonly used method to implement a practical rotation-equivariant CNN is to utilize steerable filters. If a steerable filter rotates at intervals of \(2\pi/N\) degree and forms \(N\) weight-shared filters, we can create a convolution layer that is discretely rotation-equivariant to the cyclic group \(C_N\). For a group action \(T_g\) on the cyclic group \(C_N\), cyclic-equivariance can be expressed as:
\begin{equation}
     f\left(T_g(x)\right) = P_g T_g\left(f(x)\right)  \quad \forall g \in C_N ,\, \forall x \in X
\end{equation}
where \(P_g\) is rotation group-wise permutation operator. For example, when the image is rotated by \(2\pi n/N\), the \(C_N\)-equivariant feature undergoes both an image-level rotation and a group-wise permutation of degree \(n\). It's worth noting that every intermediate feature of a rotation-equivariant CNN is cyclic-equivariant. One major property of cyclic-equivariant feature is that it can be transformed into either rotation-invariant or rotation-sensitive features through a pooling operation. As in \citet{weiler2019general}, rotation-invariant features are obtained from the maximum response across all rotations, achieved through rotation group-wise max pooling such as \(\max_g f(x)\).

On the other hand, rotation-equivariant vectors can be obtained not just from the max response, but also using the argmax operator. By combining max response \(\max_g f(x)\) and its orientation \(\theta=\frac{2\pi}{N} \arg\max_g f(x)\), we can transform a cyclic-equivariant feature to a vector field \(v(x)\) as
\begin{align}
     v(x) = \left(\max_g f(x)\right) \cdot \left[\cos(\theta), \sin(\theta)\right]^T
     \label{eq:vec}
\end{align}

For distinguishing the rotation-equivariant vector field, we will call the rotation-equivariant scalar field as instance-level rotation-invariance.
%For a comprehensive overview of group-equivariant CNNs, we refer readers to \citet{weiler2019general}.



\section{Methodology}
Initially, rotation-equivariant features are extracted from the rotation-equivariant backbone and neck. To ensure rotation-equivariant localization with the point set \( S = \{[x^k, y^k]^\top\}_{k=1}^K \), we transform cyclic-equivariant features into \(K\) vector fields as \cref{eq:vec}, through a vector-field transformation layer. Our core idea is to utilize this point set as an offset for deformable convolution, enabling a rotation-equivariant adaptive receptive field. In the localization branch, Rotation-Equivariant Deformable Convolution (RE-DCN) maintains cyclic-equivariance to perform localization refinement through spatial adaptation. On the other hand, the Rotation-Invariant Deformable Convolution (RI-DCN) transforms features into invariant ones to ensure robust classification. We have advanced the orientation alignment proposed in ReDet \citep{han2021ReDet}, obtaining better rotation-equivariance and strengthening the connection between localization and classification through our alignment reference vector driven by point set prediction. Finally, we introduce the edge constraint loss to assure orientation sensitivity and stable training of the alignment reference vector. The overall architecture of our proposed method is depicted in \cref{fig:figure2}.

\subsection{Rotation-Equivariant Deformable Convolution} % 적기 힘든 부분
The key role of rotation-equivariant offset is to guarantee that each kernel of the deformable convolution consistently focuses on a semantically identical area. However, directly applying regular deformable convolution breaks rotation-equivariance since it blends the features of all rotation groups, being agnostic to their distinctions. To solve this, we introduce Rotation-Equivariant Deformable Convolution (RE-DCN) which allows all \(N\) rotation groups to perform independently. As shown in \cref{fig:figure3a}, each rotation group independently executes deformable convolution using shared offsets and weights. Through the use of RE-DCN, the output feature retains cyclic-equivariance, allowing it to be transformed into a rotation-equivariant vector field. This approach not only ensures spatial feature refinement without compromising rotation-sensitivity but also achieves an N-factor parameter efficiency through weight sharing. A detailed structure and the corresponding pseudo code are provided in the appendix.
%We implement this by extending the \(N\) weight filters with shape \( \mathbb{R}^{C/N\times C/N \times K} \) to circulant matrix, where \(K\) is number of kernels typically set to nine. Therefore, our RE-DCN successfully integrates deformable convolution with rotation-equivariant features while retaining parameter efficiency.

%The fundamental concept behind point set refinement is to optimize localization using the object-wise receptive field obtained from the initial point set. In our rotation-equivariant model, it is crucial to maintain the spatial adaptation of the receptive field, as highlighted in Lemma1, while preventing convolution operations from losing orientation-sensitivity. Drawing inspiration from weight filters with cyclic equivariance, our VE-DCN is constructed using a circulant weight matrix, allowing us to extend the previous rotation-equivariant CNN into a deformable convolution.
% 수식 추가해서 equivariance reasoning
%As depicted in Figure 4-a, when a rotation-equivariant offset is applied, the receptive field semantically remains invariant. However, it appears with cyclic permuted form proportional to the transformation applied to the image. In example, in VE-DCN with 9 kernels, the input sampled according to the offset experiences only channel permutation and does not undergo image-level rotation. In contrast, a general 3×3 sized rotation-equivariant convolution experiences both rotation of the input kernel and channel permutation simultaneously. Thus, VE-DCN can be simply viewed as a weighted sum of 1×1 deformable convolutions. Consistent with the conventional approach, VE-DCN engages in weight sharing across the N factors in the \(C_N\) feature, ensuring the operation preserves the cyclic equivariance of the input feature.

% Input feature $f=\texttt{stack}\left([f_1, ..., f_N]\right)$, output feature $F=\texttt{stack}\left([F_1, ..., F_N]\right)$, deformable offset $o\in\mathbb{R}^{9\times k \times k \times 2}$, DCN layer parameter $\theta$. RE-DCN is defined as,
% \begin{align}
%     F = \texttt{RE-DCN}(f,o,\theta) = \texttt{stack}\left([F_1, ..., F_N]\right)
% \end{align}
% where $F_j$ is,
% \begin{align}
%     F_j = \texttt{DCN}\left(\texttt{stack}\left(\texttt{roll}_j\left([f_1, ..., f_N]\right)\right)o, \theta\right)
% \end{align}
% Then, for a rotated feature $T_g(f)$ and offset $R(g)o$, the output is,
% \begin{align}
%     \texttt{RE-DCN}(T_g(f),R(g)o,\theta) &= \texttt{stack}(\texttt{DCN}(\text{blah..})) \\
%     & = T_g(F)
% \end{align}
% Therefore, RE-DCN is rotation-eqivariant.

\subsection{Rotation-Invariant Deformable Convolution}
The classification branch needs consistent prediction that comes from rotation-invariance, rather than retaining the feature's orientation. At a basic level, if we make the feature that is input to the deformable convolution rotation-invariant, the output can also achieve invariance. A potential approach to this is the rotation group-wise max pooling mentioned earlier \citep{cohen2016group}. However, this strategy leads to a significant reduction in the model's capacity due to a dramatic channel reduction, and it also sweeps away all sensitive orientation information.

To tackle this issue, ReDet \citep{han2021ReDet} introduced the orientation alignment, which can be formulated as:
\begin{equation}
    OA(f, \theta) = \textit{Int}\left(SC(f, r), \theta\right), \quad r=\lfloor \theta N/2\pi \rfloor
\end{equation}

where $\textit{Int}(\cdot)$ denotes bilinear interpolation between adjacent rotation groups and \(SC(\cdot)\) is rotation group-wise permutation. In simpler terms, orientation alignment can be seen as an inverse direction of cyclic permutation \(P_g\), aiming to achieve rotation-invariance. While the orientation alignment is itself a rotation-equivariant operator, it is the \(\theta\) that truly determines rotation-invariance.  It is important to highlight that the orientation alignment introduced in ReDet has certain shortcomings in ensuring rotation-invariance. Specifically, (a) the \(\theta\) is derived from the RoI head, which comprises regular CNNs, so eventually loses its rotation-equivariance, and (b) even if
\(\theta\) is predicted with utmost accuracy, its efficacy still hinges on the specific representation style of the oriented bounding box and predefined anchors.


In contrast, each individual point of our predicted localization is rotation-equivariant, making it suitable for orientation alignment. Our approach ensures that (a) through our proposed localization branch, \(\theta\) naturally attains rotation-equivariance, (b) the network can adaptively learn the appropriate direction without constraints imposed by the OBB representation style, and (c) it strengthens the correlation between localization and classification. To ensure robustness to the feature grid location, we define the angular reference vector \(\vec{p}^{ref}\) for orientation alignment as \(\vec{p}^{ref}=[x^{ref}-x^c,y^{ref}-y^c]^\top\), where  \([x^{ref}, y^{ref}]^\top\) is the alignment reference point and \([x^c, y^c]^\top\) is the center point of the convex hull formed through the point set \( S = \{[x^k, y^k]^\top\}_{k=1}^K \), respectively. Then, our oriented alignment is defined as
$OA(f, \measuredangle \vec{p}^{ref} )$ where $\measuredangle(\cdot)$ is the angle from the x-axis.

We emphasize the fact that our point set prediction is made up of rotation-equivariant vectors. This means any point within the set can serve as an alignment reference, ensuring rotation-invariance of alignment.
Through our proposed orientation alignment, the transformed rotation-invariant feature is fed into the deformable convolution and utilized for classification. We refer to this entire process as Rotation-Invariant Deformable Convolution (RI-DCN), which is depicted in \cref{fig:figure3a}. % cref


\subsection{Orientation Alignment without Degeneration}
Although every rotation-equivariant point can be utilized as an alignment reference, we found that randomly selecting a point can sometimes lead to significant training instability. In point set based methods \citep{yang2019reppoints, guo2021beyond, li2022oriented}, we observed that a few points might be always excluded from the convex-hull, and wander around the center of the object. Although such points still remain rotation-equivariant, their direction can change dramatically during training, potentially leading to noisy alignment and unstable learning. Therefore, without majorly impacting the existing localization performance, there is a need to ensure that the reference point is influential in both localization and the formation of the convex hull. We introduce the edge constraint loss, pushing the reference point towards the nearest center points of the four edges of the ground truth bounding box. Given the four center points from the ground truth box edges, denoted as \( \{[x^g_i, y^g_i]^\top\}_{i=1}^4 \), the edge constraint loss \( \mathcal{L}_{ec}\) can be formulated as
\begin{equation}
    \mathcal{L}_{ec} = \min_i \left\lVert [x^{ref}, y^{ref}]^\top - [x^g_i, y^g_i]^\top \right\rVert_2
\end{equation}
We set a default weight of edge constraint loss as 0.025 to minimize its influence on localization loss. Without losing generality, we can set the first point of the predicted point set as the alignment reference.


\begin{table*}[t]
   \input{tables/dota_sota.tex}
    \caption{\textbf{Comparisons with state-of-the-art methods on DOTA-v1.0 OBB Task}. Res50, Res101, H104, and ReRes50 mean ResNet50, ResNet101, Hourglass104, Rotation-equivariant ResNet50, respectively. The best and second-best results are boldfaced and underlined, respectively.}
   \label{tab:dota_sota}
\end{table*}

\begin{table*}[t]
   \input{tables/dota_v15_sota.tex}
    \caption{\textbf{Comparisons with state-of-the-art methods on DOTA-v1.5 OBB Task.} RetinaNet-O, Mask R-CNN, HTC, and ReDet reported from \citep{han2021ReDet}.
    % RetinaNet-O, Mask R-CNN, HTC, and ReDet reported from \citep{han2021ReDet}. Oriented RepPoints is our re-implemented version for DOTA-v1.5.
    }
   \label{tab:dotav15_sota}
\end{table*}



\section{Experiments}

\subsection{Benchmark and Implementation Details}
\textbf{DOTA dataset} \citep{DOTA,9560031} is a large-scale benchmark designed for assessing oriented object detection in aerial images. It includes 2,806 aerial images with sizes varying from $800\times800$ to $4000\times4000$. In \textbf{DOTA-v1.0}, there are 188,282 instances distributed among 15 specific categories: Plane (PL), Baseball diamond (BD), Bridge (BR), Ground track field (GTF), Small vehicle (SV), Large vehicle (LV), Ship (SH), Tennis court (TC), Basketball court (BC), Storage tank (ST), Soccer-ball field (SBF), Roundabout (RA), Harbor (HA), Swimming pool (SP), and Helicopter (HC). \textbf{DOTA-v1.5} shares the image set with DOTA-v1.0 but introduces an additional class: Container Crane (CC), bringing the total to 402,089 instances. With the inclusion of extremely small objects, DOTA-v1.5 is more challenging, yet it seems to provide more reliable labels. For experimental settings, both the training and validation sets from DOTA are combined for training, with the test set reserved for evaluations. Images are typically split into 1024×1024 patches using an 824 stride. For evaluation metric, the mean average precision (mAP) metric \citep{everingham2010pascal} is used.

\textbf{Model Architecture.} Our implementation is based on the MMRotate \citep{zhou2022mmrotate} and $E(2)$-CNN \citep{weiler2019general} framework. Our proposed model is based on \(C_8\) ReResNet-50 backbone pretrained on ImageNet, with ReFPN neck proposed in \citet{han2021ReDet}. We stacked three C8 rotation-equivariant convolution layers for each branch, and used modulated deformable convolution \citep{zhu2019deformable} for the classification branch. Focal loss \citep{lin2017focal}, convex IOU loss \citep{rezatofighi2019convexiou} and spatial constraint loss with APAA strategy as described in \citet{li2022oriented} is employed for training. We set the weight of our edge constraint loss as 0.0025 as default.

\textbf{Training Scheme.} The training was conducted with the stochastic gradient descent optimizer with the momentum and the weight decay set to $0.9$ and $0.0001$, respectively. The initial learning rate is 0.008, and the model is trained for 40 epochs with batch size 8, using a step decay schedule.To ensure a fair comparison, we refrained from fine-tuning any hyper-parameters related to losses, sampling strategies, and training schedules, in line with the settings from  \citet{guo2021beyond} and \citet{li2022oriented}.


\textbf{Strided Convolution and Equivariance.} Due to the discreteness of images, rotation-equivariant CNNs are strictly equivariant only for rotations that are multiples of 90 degree.  However, employing even-sized images and strided convolution layers can disrupt this strict equivariance, even for 90-degree rotations. This issue arises because strided convolution always samples the top-left corner, as reported by \citet{romero2020attentive}. While the impact on performance might be minimal, given our goal to achieve rotation robustness through not approximate but perfect rotation-equivariance, we decided to pursue a more stringent rotation-equivariance. To address this issue, by simply adding zero-padding to ensure strided convolution always experience odd-numbered images, strict equivariance can be achieved without any significant modifications to the network structure. A more detailed analysis of how strided convolution can disrupt rotation-equivariance is covered in the appendix.
\begin{figure}[t]
    \centering
    % \begin{subfigure}{.45\textwidth}
% \includegraphics[width=\linewidth]{figs/figure4.pdf}
\includegraphics[width=\linewidth]{figs/figure5-v5.pdf}
\caption{\textbf{Robustness against rotation} estimated with DOTA-v1.0. We compared the performance degradation of various models as the image rotates. Excluding discrete rotations at 90-degree intervals, the loss of rotated image information always results a decreased mAP.}
\label{fig5}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{figs/figure6-v3.pdf}
\caption{\textbf{Examples of detection results using FRED on DOTA-v1.5}}
\label{fig:figure6}
\end{figure*}

\begin{table}[t]
    \input{tables/ablation.tex}
\caption{\textbf{Ablations incrementally added equivariances.} ``Vector-field'' refers to vector-field transformation layer. }
\label{table:equivariance-ablation}
\end{table}



\begin{figure}[t]
    \centering
    \begin{subfigure}{.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/figure7_rep-v2.pdf}
        % \caption{Previous point set base method \citep{li2022oriented}}
        \caption{Oriented RepPoints}
        \label{fig:previous}
    \end{subfigure}
    % \hfill
    \begin{subfigure}{.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/figure7_ours-v2.pdf}
        \caption{FRED (Ours)}
        \label{fig:proposed}
    \end{subfigure}
    \caption{\textbf{Directionality emerges with the equivariant point set representation.} Compared to Oriented RepPoints \citep{li2022oriented}, FRED maintains object orientation without explicit supervision. For visualization purposes, we color-coded each point in the predicted point set with respect to the ordering. Arrow indicates the first point in the set. Both models are trained for 30 epochs and early-stopped before convergence.}
    \label{fig:figure5}
\end{figure}





\subsection{Comparison with the State-of-the-art Methods}
\textbf{DOTA-v1.0.} Based on the experimental results on the single-scale DOTA-v1.0 dataset, we aim for a fair comparison with previous methods. Our model, using the ReResNet50 and ReFPN backbone, shows comparable result ranked second among anchor-free methods. we note that our model consists of only 16\% of model size.

\noindent\textbf{DOTA-v1.5.} Similar to DOTA-v1.0, we also report single-scale experiments for DOTA-v1.5. For a fair comparison, we reimplemented OrientedRepPoints using the official code. Our model achieved 78.3 mAP and surpasses the state-of-the-art anchor-free method by 1.4 mAP. Given that our model demonstrates superior results in more difficult and reliable dataset settings, we assert that it possesses a greater generalization capability.

\noindent\textbf{Robustness against rotations.} The goal of our fully rotation-equivariant model is to provide consistently reliable predictions during the inference phase regardless of rotations. We observed how the performance changes by varying the rotation of the test images. As seen in \cref{fig5}, while our method consistently performs regardless of image rotation, non-equivariant models exhibit significant variations in performance. Specifically, while the previous methods show performance degradation for large rotations, our model comparably maintains its performance. Even though our model is designed to be discretely rotation-equivariant at 45-degree intervals, it exhibits approximate rotation-equivariance across every continuous rotations.

\section{Discussion}
% \subsection{Ablations}
\noindent\textbf{Ablation on rotation-equivariances.}
 To understand the effectiveness of rotation-equivariance for both object classification and localization, we conducted experiments on the DOTA-v1.0 dataset. To gauge the efficacy of rotation-equivariance on each branch, we measured performance by incrementally adding various rotation-equivariances based on the ReResNet50 and ReFPN feature extractor.

In \cref{table:equivariance-ablation}, the term 'Vector-field' refers to the implementation of a rotation-equivariant vector-field in the localization branch. GroupPool and Orientation Alignment (OA) are both rotation-invariance for classification branch, but with different way. The efficiency of rotation-equivariance for both classification and localization is evident. Without any equivariance transformation, a model simply becomes in a capacity reduced model with rotation-agnostic behavior. Since group pooling removes every orientation sensitivity and thus exhibits relatively lower performance.

\noindent\textbf{Parameter Efficiency.} Our method leverages rotation-equivariant convolution across all layers, from the backbone to the head, to ensure full rotation-equivariance. This maximizes the benefits of weight sharing, allowing us to outperform existing models with only 16\% of the parameters. As seen in \cref{tab:param}, even when compared to ReDet, which utilizes the same rotation-equivariant backbone, FRED has only 18\% of the size.





\begin{table}[t]
   \input{tables/parameter.tex}
    \caption{\textbf{Comparison of model size on DOTA-v1.5}}
   \label{tab:param}
\end{table}

\noindent\textbf{Observations on point set directionality.} Originally, rotation-equivariance is defined for various rotations of a single instance and is not applied to different instances. If rotation-equivariance also operates between multiple objects of the same class, we expect FRED to understand their relative orientations without any supervision. We observed that FRED captures the relative pose between objects during training (see \cref{fig:figure5}), even though its performance is lower than fully trained model. This tendency tends to diminish during the training process and only weakly appears in the converged model. We hypothesize that rotation-equivariance might coarsely cluster objects with similar shapes, sizes, and colors in the early stages of training, naturally making the model aware of their poses. However, since objects within the same class can have varied distributions, those pose sensitivity derived from coarse rotation-equivariance fades away. Nevertheless, this distinctly indicates that FRED engages in non-axis aligned feature learning. In contrast, the previous point set based methods produce predictions solely based on the bounding box distribution, agnostic to the object's direction. Even if they are not aligned to the horizontal or vertical axis, the predictions appear to be aligned to a fixed axis inherent to each point. We may leave this intriguing behavior of FRED as our future work.

% todo.
\section{Conclusion}
 detector for aerial images. Our novel rotation-equivariant deformable convolution blocks offer improved alignment for spatial region and orientation with rotation-equivariant receptive fields. Moreover, through rotation-invariant orientation alignment, we strengthened the correlation between classification and localization. Not only does our model display rotation robustness, parameter efficiency, and promising quantitative results, but it also hints at the potential for unsupervised pose estimation.

\section{Acknowledgments}
This research was supported by a grant of the Korea Health Technology R\&D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health and Welfare, Republic of Korea (grant number : HI20C1234).



\appendix


 dolor, libero ab id doloribus fuga nulla incidunt ullam sapiente illum.Quas excepturi cumque perferendis tenetur aperiam eum voluptatem, sunt maiores iusto, laboriosam minus enim praesentium possimus dicta, obcaecati ducimus sit vero temporibus maiores ipsa nesciunt ut facilis.Adipisci commodi et aliquam explicabo quam ab id itaque impedit necessitatibus, enim architecto vel veniam id, vel quia ipsam nihil fugit, ex laboriosam explicabo culpa quisquam asperiores molestias cupiditate odit consectetur nobis voluptatibus, deserunt mollitia sunt ipsa expedita animi vero repellat quisquam modi obcaecati.\clearpage
\bibliography{aaai24}

\end{document}