\relax 
\bibstyle{aaai24}
\citation{roitransformer}
\citation{han2021s2anet}
\citation{pan2020drn}
\citation{guo2021beyond,li2022oriented}
\citation{han2021ReDet}
\citation{weiler2019general}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:figure1}{{1}{1}{\textbf  {Overview of the fully rotation-equivariant object detector (FRED).} FRED consists of a rotation-equivariant backbone which predicts a point set followed by two prediction branches -- (1) a rotation-equivariant box regression head and (2) a rotation-invariant classification head. We achieve end-to-end equivariance for object detection.}{}{}}
\newlabel{fig:figure1@cref}{{[figure][1][]1}{[1][1][]1}}
\citation{roitransformer}
\citation{scrdet}
\citation{pan2020drn}
\citation{han2021s2anet}
\citation{yang2021r3det}
\citation{qian2021rsdet,yang2022kfiou}
\newlabel{fig:figure2}{{2}{2}{\textbf  {Overall model architecture of the proposed Fully Rotation-Equivariant Detector (FRED).} \( C_N \)-equivariant features are fed into the rotation-equivariant head up to two deformable convolution blocks. The Rotation-Equivariant Deformable Convolution (RE-DCN) tilizes an initial point set as an offset and refines it through spatial adaptation without breaking rotation-equivariance. The Rotation-Invariant Deformable Convolution (RI-DCN) performs an orientation alignment to produce rotation-invariant features using an align reference vector sourced from the localization branch. As both the deformable offsets and the reference vector maintain rotation-equivariance, the classification branch achieves instance-level rotation-invariance.}{}{}}
\newlabel{fig:figure2@cref}{{[figure][2][]2}{[1][1][]2}}
\citation{yang2019reppoints}
\citation{guo2021beyond}
\citation{li2022oriented}
\citation{cohen2016group}
\citation{weiler2019general,cesa2021program}
\citation{veeling2018rotation,gupta2021rotation,lee2023learning}
\citation{han2021ReDet}
\citation{weiler2019general}
\citation{han2021ReDet}
\newlabel{eq:vec}{{5}{3}{}{}{}}
\newlabel{eq:vec@cref}{{[equation][5][]5}{[1][3][]3}}
\citation{cohen2016group}
\citation{han2021ReDet}
\newlabel{fig:figure3a}{{3a}{4}{\textbf  {Rotation-equivariant Deformable Convolution}}{}{}}
\newlabel{fig:figure3a@cref}{{[subfigure][1][3]3a}{[1][3][]4}}
\newlabel{sub@fig:figure3a}{{a}{4}{\textbf  {Rotation-equivariant Deformable Convolution}}{}{}}
\newlabel{sub@fig:figure3a@cref}{{[subfigure][1][3]3a}{[1][3][]4}}
\newlabel{fig:figure3b}{{3b}{4}{\textbf  {Rotation-invariant Deformable Convolution}}{}{}}
\newlabel{fig:figure3b@cref}{{[subfigure][2][3]3b}{[1][3][]4}}
\newlabel{sub@fig:figure3b}{{b}{4}{\textbf  {Rotation-invariant Deformable Convolution}}{}{}}
\newlabel{sub@fig:figure3b@cref}{{[subfigure][2][3]3b}{[1][3][]4}}
\newlabel{fig4}{{3}{4}{This example illustrates a 4-equivariant rotation group ($C_4$) and 2x2 deformable kernel for simplicity. The deformable convolution (DCN) layer parameters are shared between rotation groups.}{}{}}
\newlabel{fig4@cref}{{[figure][3][]3}{[1][3][]4}}
\citation{yang2019reppoints,guo2021beyond,li2022oriented}
\citation{roitransformer}
\citation{li2019FAOD}
\citation{scrdet}
\citation{xu2020gliding}
\citation{yang2021r3det}
\citation{han2021s2anet}
\citation{ming2021dynamic}
\citation{yang2022kfiou}
\citation{han2021ReDet}
\citation{chen2020piou}
\citation{wei2020o2det}
\citation{pan2020drn}
\citation{guo2021CFA}
\citation{qian2022rsdet++}
\citation{li2022oriented}
\citation{lin2017focal}
\citation{he2017mask}
\citation{chen2019hybrid}
\citation{roitransformer}
\citation{han2021ReDet}
\citation{li2022oriented}
\citation{han2021ReDet}
\citation{han2021ReDet}
\citation{DOTA,9560031}
\citation{everingham2010pascal}
\citation{zhou2022mmrotate}
\citation{weiler2019general}
\citation{han2021ReDet}
\citation{zhu2019deformable}
\citation{lin2017focal}
\citation{rezatofighi2019convexiou}
\citation{li2022oriented}
\citation{guo2021beyond}
\citation{li2022oriented}
\citation{romero2020attentive}
\citation{li2022oriented}
\citation{li2022oriented}
\newlabel{fig5}{{4}{5}{\textbf  {Robustness against rotation} estimated with DOTA-v1.0. We compared the performance degradation of various models as the image rotates. Excluding discrete rotations at 90-degree intervals, the loss of rotated image information always results a decreased mAP.}{}{}}
\newlabel{fig5@cref}{{[figure][4][]4}{[1][5][]5}}
\newlabel{tab:dota_sota}{{1}{6}{\textbf  {Comparisons with state-of-the-art methods on DOTA-v1.0 OBB Task}. Res50, Res101, H104, and ReRes50 mean ResNet50, ResNet101, Hourglass104, Rotation-equivariant ResNet50, respectively. The best and second-best results are boldfaced and underlined, respectively.}{}{}}
\newlabel{tab:dota_sota@cref}{{[table][1][]1}{[1][5][]6}}
\newlabel{tab:dotav15_sota}{{2}{6}{\textbf  {Comparisons with state-of-the-art methods on DOTA-v1.5 OBB Task.} RetinaNet-O, Mask R-CNN, HTC, and ReDet reported from \citep  {han2021ReDet}. }{}{}}
\newlabel{tab:dotav15_sota@cref}{{[table][2][]2}{[1][5][]6}}
\newlabel{table:equivariance-ablation}{{3}{6}{\textbf  {Ablations incrementally added equivariances.} ``Vector-field'' refers to vector-field transformation layer. }{}{}}
\newlabel{table:equivariance-ablation@cref}{{[table][3][]3}{[1][5][]6}}
\newlabel{fig:figure6}{{5}{7}{\textbf  {Examples of detection results using FRED on DOTA-v1.5}}{}{}}
\newlabel{fig:figure6@cref}{{[figure][5][]5}{[1][5][]7}}
\newlabel{fig:previous}{{6a}{7}{Oriented RepPoints}{}{}}
\newlabel{fig:previous@cref}{{[subfigure][1][6]6a}{[1][5][]7}}
\newlabel{sub@fig:previous}{{a}{7}{Oriented RepPoints}{}{}}
\newlabel{sub@fig:previous@cref}{{[subfigure][1][6]6a}{[1][5][]7}}
\newlabel{fig:proposed}{{6b}{7}{FRED (Ours)}{}{}}
\newlabel{fig:proposed@cref}{{[subfigure][2][6]6b}{[1][5][]7}}
\newlabel{sub@fig:proposed}{{b}{7}{FRED (Ours)}{}{}}
\newlabel{sub@fig:proposed@cref}{{[subfigure][2][6]6b}{[1][5][]7}}
\newlabel{fig:figure5}{{6}{7}{\textbf  {Directionality emerges with the equivariant point set representation.} Compared to Oriented RepPoints \citep  {li2022oriented}, FRED maintains object orientation without explicit supervision. For visualization purposes, we color-coded each point in the predicted point set with respect to the ordering. Arrow indicates the first point in the set. Both models are trained for 30 epochs and early-stopped before convergence.}{}{}}
\newlabel{fig:figure5@cref}{{[figure][6][]6}{[1][5][]7}}
\newlabel{tab:param}{{4}{7}{\textbf  {Comparison of model size on DOTA-v1.5}}{}{}}
\newlabel{tab:param@cref}{{[table][4][]4}{[1][7][]7}}
\bibdata{aaai24}
\gdef \@abspage@last{8}
