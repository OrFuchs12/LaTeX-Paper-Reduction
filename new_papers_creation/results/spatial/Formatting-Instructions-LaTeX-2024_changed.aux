\relax 
\bibstyle{aaai24}
\citation{chen2020cross,sengupta2023humaniflow,cao2017realtime}
\citation{schepers2018xsens}
\citation{huang2018deep,yi2021transpose,jiang2022transformer,von2017sparse,yi2022physical}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig1}{{1}{1}{Considering specific postures such as standing and sitting, the rotational data and acceleration output by the sensors are largely invariant. Incorporating additional information such as text can help to address this challenge.}{}{}}
\citation{moon2022imu2clip}
\citation{radford2021learning}
\citation{BABEL:CVPR:2021}
\citation{BABEL:CVPR:2021}
\citation{schepers2018xsens}
\citation{slyper2008action,tautges2011motion}
\citation{von2017sparse}
\citation{loper2015smpl}
\citation{huang2018deep}
\citation{yi2021transpose}
\citation{dittadi2021full}
\citation{jiang2022avatarposer}
\citation{yi2022physical}
\citation{jiang2022transformer}
\citation{von2018recovering,malleson2017real,von2016human}
\citation{kim2022fusion}
\citation{guo2022generating,zhang2022motiondiffuse,tevet2022motionclip}
\citation{zhou2019continuity}
\citation{radford2021learning}
\citation{vaswani2017attention}
\citation{kingma2022autoencoding}
\newlabel{fig2}{{2}{3}{Overview of our method. Our model encapsulates three distinct encoders: a Text Encoder, a Sensor Encoder, and a Text-Sensor Fusion Module. The details of the Sensor Encoder and the Hierarchical Temporal Transformer module are illustrated on the right. The schematic of the model output is adapted from \citep  {BABEL:CVPR:2021}.}{}{}}
\citation{vaswani2017attention}
\citation{liu2021swin}
\newlabel{fig3}{{3}{4}{An illustration of window self-attention (left) and shifted window self-attention (right).}{}{}}
\newlabel{fig4}{{4}{4}{An efficient methodology for batch computation of self-attention within the context of shifted window partitioning.}{}{}}
\citation{kendall2017uncertainties}
\citation{BABEL:CVPR:2021}
\citation{mahmood2019amass}
\citation{huang2018deep}
\citation{jiang2022transformer}
\citation{trumble2017total}
\citation{yi2021transpose}
\citation{kingma2014adam}
\newlabel{e4}{{4}{5}{}{}{}}
\newlabel{e5}{{5}{5}{}{}{}}
\newlabel{e6}{{6}{5}{}{}{}}
\newlabel{e7}{{7}{5}{}{}{}}
\citation{von2017sparse}
\citation{huang2018deep}
\citation{yi2021transpose}
\citation{yi2022physical}
\citation{jiang2022transformer}
\newlabel{table1}{{1}{6}{ In offline settings, our method is evaluated against SIP, DIP, and Transpose on the Totalcapture and DIP-IMU datasets, focusing on the assessment of body poses. The mean values, along with the standard deviations (enclosed in parentheses), for the sip error, angular error, positional error, mesh error, and jitter error, are presented in the report. Bold numbers indicate the best performing entries. }{}{}}
\newlabel{table2}{{2}{6}{ In online settings, our method is evaluated against DIP, PIP, TIP, and Transpose on the Totalcapture and DIP-IMU datasets, focusing on the assessment of body poses. Bold numbers indicate the best performing entries. }{}{}}
\newlabel{fig5}{{5}{6}{Mesh error distribution and qualitative comparisons between our method (with/without text) and Transpose. The text description of the motion is provided below, with the sequence label illustrated in green and the frame label presented in blue.}{}{}}
\newlabel{table3}{{3}{7}{Evaluation of Ablation Models on the Totalcapture Dataset. Bold numbers indicate the best performing entries.}{}{}}
\newlabel{fig6}{{6}{7}{We demonstrated a comparison between our method (with/without text) and Transpose in a sitting situation, focusing on the analysis of upper leg rotation error.}{}{}}
\newlabel{fig7}{{7}{7}{Temporal Evolution of Uncertainty Across Six Sensors: Each row represents a different sensor, with color variations indicating changes in uncertainty.}{}{}}
\bibdata{aaai24}
\gdef \@abspage@last{8}
