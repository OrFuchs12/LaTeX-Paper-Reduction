%##################################################################################################
\begin{algorithm}[t!]
\caption{ECLIPSE: PyTorch Pseudocode}
\label{alg:code}
\begin{lstlisting}[language=python,escapeinside=;;]
# fm, fs: image encoders (teacher, student)
# ft: text encoder
# hi, ht: image, text projection & normalization
# tps, tpt: student and teacher temperatures
# C: momentum centering coefficient
# q: proportion of KL_loss (;$\lambda$; in the paper)

# load minibatch with N (image-text) pairs
for (img, txt) in loader:
    # image, text encodings
    wm, ws, wt = fm(img), fs(img), ft(txt)

    # normalized projection embeddings
    zm, zs, zt = hi(wm), hi(ws), ht(wt)-C

    # ECLIPSE loss
    loss = clip(zm, zt) + distill(zm, zs, zt) 
    loss.backward() # back-propagate

    # SGD update: student, text encoders
    update([fs, ft, hi, ht])

    # momentum update: teacher encoder
    fm.params = m * fm.params + (1-m) * fs.params
    C = m * C + (1-m) * wt

def clip(zm, zt):
    label = range(N)
    logit = exp(s) * zm @ zt.T # s: learnable scale

    li = CrossEntropy(logit, label)
    lt = CrossEntropy(logit.T, label)

    return (li + lt) / 2 # CLIP loss
    
# q: balancing coefficient
def distill(zm, zs, zt):
    CE_loss = clip(zs, zt) # student-text hard loss
    logit_t = zm @ zt.T
    logit_s = zs @ zt.T.detach()
    
    t1 = log_softmax(logit_t / tpt, dim=1)
    s1 = log_softmax(logit_s / tps, dim=1)
    
    t2 = log_softmax(logit_t / tpt, dim=0).T
    s2 = log_softmax(logit_s / tps, dim=0).T
    
    # KL divergence
    KL_loss = 0.5 * (KL(t1, s1)+KL(t2, s2)).mean()
    
    return q * CE_loss + (1-q) * KL_loss # balance
\end{lstlisting}
\end{algorithm}
%##################################################################################################