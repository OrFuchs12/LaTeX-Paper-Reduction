\begin{table}[t!]
  \centering
  \small
  \begin{tabular}{l | c c c c}
    \toprule
    Method & VLC & $\text{SSL}$ & MLM & Top1(\%) \\ \hline \\[-9pt]
    (a) CLIP & \checkmark & & & 17.10 \\
    (b) CLIP w/ EViT & \checkmark & & & 16.55 \\
    (c) ECLIPSE (CLIP) & \checkmark & & & \textbf{19.67} \\ \hline
    (d) SLIP & \checkmark & \checkmark & & 22.94 \\
    (e) SLIP w/ EViT & \checkmark & \checkmark & & 21.32 \\
    (f) ECLIPSE (SLIP) & \checkmark & \checkmark & & \textbf{24.42} \\ \hline
    (h) DeCLIP & \checkmark & \checkmark & \checkmark & 25.40 \\
    (i) DeCLIP w/ EViT & \checkmark & \checkmark & \checkmark & 23.26 \\
    (g) \textbf{ECLIPSE} & \checkmark & \checkmark & & \textbf{26.41} \\
    \bottomrule
  \end{tabular}
  \vspace{0.5em}
  \caption{
  ImageNet-1k Top 1 zero shot accuracy with models pretrained on CC3M dataset under three training configurations. Details of each configuration is denoted in Sec.~\ref{sec:cc3m_comparison}.
  ECLIPSE outperforms previous CLIP variants~\cite{radford2021learning,mu2021slip,li2022supervision} across all training configurations.
  %We use $\kappa$=0.7 for both EViT~\cite{liang2022evit} and ECLIPSE, resulting in 45\% speed up in inference time (see Table~\ref{tab:keep}).
  Note that na\"ively adopting EViT to existing methods suffers from performance drop after acceleration.
  %(especially in (e) where additional forward paths are introduced for SSL).
  %On the other hand, accelerated ViTs in our meta-architecture ECLIPSE outperforms previous CLIP variants~\cite{radford2021learning,mu2021slip,li2022supervision} across all training configurations.
  %1) \{a,b,c\}: batch 2048, contrastive learning with (image, text) pair.\\
  %2) \{d,e,f\}: batch 1024, VLC with SimCLR~\cite{chen2020simple}. \\
  %3) \{g,h,i\}: batch 1024, cross-modal contrastive loss between two augmented image views and text augmented with EDA~\cite{wei2019eda}.
  % Comparison of the effect of SSL on Vision-Language Pretraining scenarios.
  }
  \label{tab:cc3m}
\end{table}