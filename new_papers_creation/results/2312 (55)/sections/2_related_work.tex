\section{Related Work}
Vision-Language Pretraining (VLP) learns a joint representation between two modalities on large-scale image-text pairs.
VLP covers both \textit{single}-stream models~\cite{li2019visualbert,chen2019uniter,huang2019unicoder,li2020oscar,li2020unimo,lu2019vilbert,tan2019lxmert} and \textit{dual}-stream models~\cite{jia2021scaling,radford2021learning,li2022supervision}.
Single-stream models jointly encode both image and text input with a single multi-modal encoder.
Though they have shown impressive performance in several image-text downstream tasks, single-stream models suffer from their large inference cost for the cross-modal retrieval.
Also, how to transfer the pretrained joint encoder to the unimodal downstream tasks, e.g., image recognition, is not trivial.
On the contrary, dual-stream models encode the images and texts separately with independent encoders, and thus have several advantages: simplicity, versatility, and relatively cheaper computational cost.
In this work, we focuses on a dual-stream encoder trained with a contrastive objective.

\label{sec:related_work}
\subsection{Contrastive Language-Image Pretraining}
In Contrastive Language-Image Pretraining~\cite{jia2021scaling,li2020unimo,radford2021learning}, the model is trained via a contrastive loss with large-scale image-text pairs, where the matching image-text pairs comprise a positive pair while other arbitrary pairs are treated as negative pairs.
Several works~\cite{mu2021slip,li2022supervision} introduce additional form of supervision such as self-supervision between augmented views of the image (e.g., SimCLR~\cite{chen2020simple}, SimSiam~\cite{chen2021exploring}), language self-supervision (e.g., supervision with text augmentation~\cite{wei2019eda}, masked language modeling~\cite{devlin2018bert}) and momentum contrast with nearest neighbor~\cite{he2020moco} to further improve downstream performance.
Recently, FLIP~\cite{li2022scaling} borrowed random masking strategy~\cite{he2022masked} for the input token which substantially improves the training efficiency. 
However, FLIP employs unmasked images during inference, which means that there is no speed improvement at inference time. Furthermore, due to the discrepancy between the training and test distributions, an additional unmasked tuning process is necessary.
On the other hand, ECLIPSE improves both training and inference speed without extra tuning strategy.

\subsection{Distillation and Momentum Contrast for VLP}
Knowledge distillation~\cite{hinton2015distilling} has been initially proposed to transfer the knowledge of a large model~(the teacher) to a smaller model~(the student).
Consecutive works~\cite{Romero2015FitNet,Park2019RelKD} have been explored different distilling targets other than direct output.
Extending the concept, distillation from an identically structured model~\cite{Furlanello2018BornAgain,Bagherinezhad2018LabelRefinery} or a momentum network (teacher)~\cite{Tarvainen2017meanteacher,he2020moco,grill2020bootstrap,caron2021emerging}, whose parameters are updated with the exponential moving average (EMA) of a online network (student), have been proposed.
Momentum contrast (MoCo~\cite{he2020moco}) is the pioneering contrastive learning method for images without labels that uses momentum encoder and memory queue to increase the number of negative samples. 
Inspired from MoCo, HIT~\cite{Liu2021HiT} adopted momentum encoders and memory bank for video-text contrastive matching without distillation.
ALBEF~\cite{li2021align} and COTS~\cite{Lu2022COTS} distill soft-alignment matrix obtained from both image and text momentum encoders to the online encoders.
Andonian et al.~\cite{Andonian2022robust} proposed self-distillation via swapping image-text alignment matrix without momentum encoder.
MCD~\cite{kim2023misalign} proposes a distillation where the misalignments caused by image augmentation serves as a training signal.
We introduce a novel effective distillation method called ECLIPSE whose online and momentum image encoder share text encoder.
Through a systematic analysis, we validate diverse distillation designs~(Table~\ref{tab:distill}) and demonstrate effectiveness of ECLIPSE.