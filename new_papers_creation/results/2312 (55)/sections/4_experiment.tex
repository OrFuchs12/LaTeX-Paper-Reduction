\section{Experiment}
\subsection{Implementation Details and Datasets}
For implementation details, our work is built on top of the open-source SLIP codebase~\cite{mu2021slip}\footnote{https://github.com/facebookresearch/SLIP \label{slip_github}}.
For DeCLIP~\cite{li2022supervision}, we follow the implementation details of the official code release\footnote{https://github.com/Sense-GVT/DeCLIP}.
The performance on GPU-machine runs for CLIP and SLIP follows the exact implementation details upon this codebase unless mentioned otherwise.
All of our models are pretrained in 16$\times$ A100 GPUs.
Further details can be found in the Appendix.

\paragraph{Pretraining datasets.}
To validate the effectiveness of ECLIPSE, we pretrain ECLIPSE on large-scale open-source datasets, CC (Conceptual Captions) 3M~\cite{sharma2018cc3m} and YFCC (Yahoo Flickr Creative Commons) 15M~\cite{Thomee2016YFCC100M}.
Furthermore, to show the scalability of ECLIPSE, we curate 88M image-text pairs\footnote{Details of our curated dataset will be in our supplement}.
Since the large-scale datasets (e.g., YFCC15M, 88M) feature extremely noisy text captions, intensive analysis is done with models pretrained on the relatively clean CC3M dataset.

\paragraph{Downstream datasets.}
Following CLIP~\cite{radford2021learning}, we evaluate the transferability of pretrained ECLIPSE on 11 widely used downstream datasets.
We also transfer to zero-shot Image-Text retrieval tasks on Flickr30K and MS-COCO datasets.
The evaluation settings for each dataset are consistent with CLIP as in the open-source implementation\footref{slip_github}.
See more details of downstream datasets in our supplement.

\subsection{Comparing ECLIPSE with CLIP variants}
\label{sec:cc3m_comparison}
\input{tables/1_cc3m}
\input{tables/2_cc3m_distill}
We first compare ECLIPSE against other state-of-the-art Contrastive Language-Image Pretraining approaches~\cite{radford2021learning,mu2021slip,li2022supervision}.
Table~\ref{tab:cc3m} shows the ImageNet zero-shot results of ECLIPSE and other CLIP variants, each grouped under identical experimental settings.
All models are pretrained on the CC3M dataset with a learning rate 5e-4 for 40 epochs\footnote{More detailed training configuration will be provided in supplement.}.
We use $\kappa$=0.7 for EViT with a ViT-B/16 backbone.
For the first group (a,b,c), we compare models that only leverage Vision-Language Contrastive learning~(VLC) between image-text pairs without any augmentation.
In the second group (d,e,f), SimCLR loss~(SSL) with two augmented image views is added to the aforementioned VLC.
In the last group (h,i,g), we compare ECLIPSE with models trained with additional text augmentation~\cite{wei2019eda}~(EDA).

In Table~\ref{tab:cc3m}, we can observe that ECLIPSE on top of existing contrastive language-image pretraining pipelines, i.e., CLIP, SLIP, and DeCLIP, outperforms its baseline by a noticeable margin even with the online EViT encoder.
On the other hand, na\"ively applying EViT to existing pretraining pipelines (denoted as w/ EViT) results in lower performance.
Note that ECLIPSE requires less training costs (see Sec.~\ref{sec:discussion}) and achieves 54\% speed up in inference time (see Table~\ref{tab:keep}).
Furthermore, (g) is a simple extension of (f) where we add additional distillation loss for the augmented views (see details in our supplement).
Even without leveraging language self-supervision (Masked Language Modeling), our streamlined ViT of ECLIPSE outperforms DeCLIP.

\input{tables/3_keep_rate}
\input{tables/4_yfcc15m_cls}
\subsection{Ablation Study}
\label{subsec:ablation}
Here, we conduct ablation studies to validate how each component of ECLIPSE contributes to the final performance.
All models in this section are pretrained in CC3M dataset with $\kappa=0.7$ unless mentioned otherwise.

\paragraph{Variables for our Distillation.}
ECLIPSE is powered by a unique knowledge distillation structure where the image-text alignment matrix obtained by the online encoder and the text encoder predicts the alignment matrix jointly estimated by the momentum encoder and the text encoder.
In Table~\ref{tab:distill}, we ablate $\lambda$ and distillation target in Eq~\ref{eq:distill}.
First, ECLIPSE can be trained with only hard labels without distillation ($\lambda=1$).
We observe that ECLIPSE outperforms (a) learning from only hard labels, validating that our distillation loss contributes to the final performance.
We also found that (b) progressively changing $\lambda$ from 0.5 to 1 is worse than our default setting.
We also checked the other extreme case, learning from only distillation ($\lambda=0$), results in training failure as expected.
Second, we compare ECLIPSE with previously proposed (c) feature-level distillation~\cite{caron2021emerging} where the student network directly predicts the output of the momentum teacher and (d-e) soft alignment matrix distillation~\cite{li2021align,Lu2022COTS} where image-text alignment matrix obtained from online encoders predicts the alignment matrix from momentum encoders for both image and text.
We also test (f-g) replacing stop-gradient of text encoder with text momentum encoder which causes increase in training time.
The result shows the supremacy of our proposed distillation over the existing distillation methods and ECLIPSE variants with additional text momentum encoder.

\paragraph{Token Keep Rate.}
Table~\ref{tab:keep} shows time\footnote{We measure throughputs (128 batch, Avg of 100 runs) with https://github.com/youweiliang/evit/blob/master/helpers.py} vs performance analysis of different keep rates ($\kappa$) for EViT~\cite{liang2022evit}.
We compare our proposed ECLIPSE with CLIP where EViT is directly applied.
In the case of CLIP, the performance degrades as $\kappa$ is lowered.
On the other hand, ECLIPSE with ($\kappa=0.7$) shows the highest performance among keep rates excluding full vision~($\kappa=1.0$).
We conjecture that the token dropping of EViT can affect contrastive learning since the partial view of the attentive tokens can be interpreted as an additional augmentation on the student network.

\subsection{Pretraining ECLIPSE on Larger Datasets}
In this section, we pretrain ECLIPSE on larger scale dataset (e.g., YFCC15M)\footnote{More details of training configuration will be provided in supplement} and evaluate its transferability in single-modal and multi-modal downstream tasks.
For simplicity, we measure the effectiveness of our ECLIPSE model with two versions: (i) ECLIPSE using only the original image-text pair ((c) in Table~\ref{tab:cc3m}) and (ii) ECLIPSE with SimCLR~\cite{chen2020simple} loss between two augmented views ((f) in Table~\ref{tab:cc3m}).
% Since our work focuses on streamlining ViTs under image-text contrastive learning, we do not leverage additional self-supervision between multiple forward paths, e.g., Text Augmented View, Masked Language Modeling, and Nearest Neighbor.

\paragraph{Zero-shot Classification.}
For single-modal experiments, we test the zero-shot classification performance on 11 downstream datasets.
Table~\ref{result:cls} shows the zero-shot classification accuracy of ECLIPSE pretrained on YFCC15M dataset and transferred to downstream classification datasets.
For the test phase, the learned text encoder $f_T$ synthesizes a zero-shot linear classifier by embedding the arbitrary categories of the test dataset.
As classes are in the form of a single word, we use prompts including the label (e.g., ``\texttt{a photo of a \{label\}}") as in CLIP~\cite{radford2021learning}.
ECLIPSE with CLIP-level supervision outperforms CLIP across all 11 datasets, while ECLIPSE with additional supervision outperforms its corresponding baseline across 9 out of 11 datasets.
% Note that even without the heavy additional supervision leveraged in DeCLIP~\cite{li2022supervision}, our streamlined ECLIPSE achieves comparable performance.

\input{tables/6_data_scale}

\paragraph{Image--Text Retrieval}
For multi-modal evaluations, we test the zero-shot image--text retrieval on Flickr30k and COCO Captions benchmarks.
The image-text retrieval task can be split into two sub-tasks according to the target modality: image retrieval and text retrieval.
Image-text pairs are ranked according to their similarity scores.
Table~\ref{result:cls} shows the zero-shot performance for image--text retrieval tasks of ECLIPSE pretrained on YFCC15M dataset.
Our ECLIPSE outperforms its counterpart CLIP across all measures with a considerable gap.
% After joined with additional self-supervision, ECLIPSE outperforms SLIP~\cite{mu2021slip} across 10 out of 12 measures while achieving comparable performance with DeCLIP~\cite{li2022supervision}.
% This validates that our proposed meta-architecture ECLIPSE aligns vision and language features successfully with much less supervision.

\paragraph{Scalability}
In this section, we examine how ECLIPSE performs under various scales of pretraining dataset.
In order to emphasize the effect of our meta-architecture ECLIPSE under image-text contrastive learning, we take the most simple form of Contrastive Language-Image Pretraining without any augmentation or self-supervision.
Table~\ref{tab:scale} shows the zero-shot ImageNet Top1 accuracy of our streamlined ViT ($\kappa=0.7$) after pretraining on each CC3M, YFCC15M, and our curated 88M dataset.
Across various scales of pretraining datasets, ECLIPSE shows consistent performance improvement, thus validating the data scalability of our proposed method.