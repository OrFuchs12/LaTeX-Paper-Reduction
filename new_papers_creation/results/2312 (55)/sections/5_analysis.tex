\section{Discussion}
\label{sec:discussion}

\paragraph{Training Cost.}
In Table~\ref{tab:cost}, we compare training speed of CLIP and ECLIPSE. 
The result shows that ECLIPSE can reach similar training speed of CLIP even with distillation.
Consistent with our hypothesis, removing text momentum encoder ($\bar{T}$) and introducing expedition to the online encoder ($\kappa$=0.7) substantially boost the training speed compared to the na\"ive distillation with text momentum encoder.
We also measure the average GPU memory usage during training\footnote{Tested with 128 batches per GPU}.
With our GPU machine with 16$\times$A100 GPUs, ECLIPSE ($\kappa=0.7$) shows 18912 MiB/GPU average usage, showing a negligable increase compared to CLIP w/ EViT 18604 MiB/GPU, while being sufficiently efficient than CLIP trained with full ViT, 21758 MiB/GPU.
\input{tables/7_cost}

\paragraph{Efficient Image Self-Supervision for ECLIPSE.}
Previous CLIP variants~\cite{mu2021slip,li2022supervision} have shown that incorporating self-supervised learning with augmented image views (e.g., SimCLR~\cite{chen2020simple}, SimSiam~\cite{chen2021exploring}) to the contrastive language-image pretraining can be advantageous for learning better visual representations.
These works add additional forward and backward paths and MLP layers to treat the augmented views of an image.
For example, SLIP-style image self-supervision can easily be applied to ECLIPSE as in Table~\ref{tab:cc3m}(f).
However, this requires two additional forward and backward computations, resulting in longer training time.
Towards a more efficient self-supervised training, we here incorporate image SSL with the online and momentum branches of ECLIPSE.
We introduce a shared projection head on top of the momentum and online encoder for SSL.
This strategy is analogous to MoCo~\cite{he2020moco,Chen2020mocov2} without a memory queue.
We compare this efficient version (ECLIPSE-ES) with SLIP-style SSL and investigate the effect of augmentation in Table~\ref{tab:ssl}.

First, ECLIPSE-ES (b) surpasses the original SLIP and ECLIPSE (SLIP) even with fewer augmented views.
From this result, we assume that the momentum encoder plays an essential role in the improved performance just as MoCo outperforms SimCLR in image SSL.
Moreover, as the forward path of ECLIPSE is computed with the momentum encoder which does not require backward computation, ECLIPSE-ES features much shorter training time (see TPS column in Table~\ref{tab:ssl}).
Second, we found that feeding augmented image views for both paths is better than using the original image for the teacher encoder: ECLIPSE-ES(b) vs ECLIPSE-ES(a).
%Lastly, ECLIPSE-ES(c) using an additional text stream with EDA~\cite{wei2019eda} can boost the performance and outperform DeCLIP with less training cost (i.e., during training, the throughput is 121 img/s vs 221 img/s).
Consequently, ECLIPSE-ES demonstrates its training efficiency and opens up the possibility of advancement in integrating image SSL into VLP.

\input{tables/8_ssl}