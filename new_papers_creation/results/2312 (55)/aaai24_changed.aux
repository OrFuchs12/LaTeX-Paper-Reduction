\relax 
\bibstyle{aaai24}
\citation{vaswani2017attention}
\citation{dosovitskiy2021an,touvron2021training,jiang2021all,graham2021levit}
\citation{carion2020end}
\citation{xie2021segformer,liu2021swin,wang2021pyramid}
\citation{kim2021hotr,kim2022mstr}
\citation{li2019visualbert,chen2019uniter,huang2019unicoder,li2020oscar,li2020unimo,lu2019vilbert,tan2019lxmert,jia2021scaling,radford2021learning}
\citation{li2021align,Lu2022COTS}
\citation{hinton2015distilling}
\citation{jia2021scaling,radford2021learning}
\citation{chen2021exploring}
\citation{liang2022evit}
\providecommand \oddpage@label [2]{}
\newlabel{sec:introduction}{{}{1}{}{}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:fig_speed}{{1}{1}{Time vs. ImageNet zero-shot performance analysis for Contrastive Language-Image Pretraining with existing ViT accleration framework (EViT). We compare the results between EViT directly applied on CLIP and EViT trained with our proposed meta-architecture, ECLIPSE. Our proposed framework enables even streamlined ViTs with 101\% faster throughputs to outperform the full ViT of CLIP. Model performance and inference time are measured with ViT-B/16 backbone. }{}{}}
\citation{oord2018representation}
\citation{radford2021learning}
\citation{li2019visualbert,chen2019uniter,huang2019unicoder,li2020oscar,li2020unimo,lu2019vilbert,tan2019lxmert}
\citation{jia2021scaling,radford2021learning,li2022supervision}
\citation{jia2021scaling,li2020unimo,radford2021learning}
\citation{mu2021slip,li2022supervision}
\citation{chen2020simple}
\citation{chen2021exploring}
\citation{wei2019eda}
\citation{devlin2018bert}
\citation{he2020moco}
\citation{li2022scaling}
\citation{he2022masked}
\citation{hinton2015distilling}
\citation{Romero2015FitNet,Park2019RelKD}
\citation{Furlanello2018BornAgain,Bagherinezhad2018LabelRefinery}
\citation{Tarvainen2017meanteacher,he2020moco,grill2020bootstrap,caron2021emerging}
\citation{he2020moco}
\citation{Liu2021HiT}
\citation{li2021align}
\citation{Lu2022COTS}
\citation{Andonian2022robust}
\citation{kim2023misalign}
\citation{liang2022evit,rao2021dynamicvit,liang2022expediting}
\citation{liang2022evit}
\citation{liang2022evit,rao2021dynamicvit,liang2022expediting}
\citation{liang2022evit}
\newlabel{fig:fig_overview}{{2}{2}{Overview of ECLIPSE. Student encoder is trained to estimate the soft alignment matrix $\bar  {A}$ predicted by Text Encoder and the Teacher network. sg stands for stop-gradient, $I$ and $\bar  {I}$ are encoded image with student and teacher network, respectively. }{}{}}
\newlabel{sec:related_work}{{}{2}{}{}{}}
\citation{liang2022evit}
\citation{radford2021learning}
\citation{oord2018representation}
\citation{rao2021dynamicvit,liang2022evit}
\citation{liang2022evit}
\citation{hinton2015distilling}
\citation{hinton2015distilling,touvron2021training,caron2021emerging}
\newlabel{sec:method}{{}{3}{}{}{}}
\newlabel{eq:align}{{1}{3}{}{}{}}
\newlabel{eq:infoNCE}{{2}{3}{}{}{}}
\newlabel{eq:CLIP}{{3}{3}{}{}{}}
\citation{touvron2021training}
\citation{caron2021emerging}
\citation{mu2021slip}
\citation{li2022supervision}
\citation{sharma2018cc3m}
\citation{Thomee2016YFCC100M}
\citation{radford2021learning}
\citation{radford2021learning,mu2021slip,li2022supervision}
\citation{radford2021learning,mu2021slip,li2022supervision}
\newlabel{fig:fig_architecture}{{3}{4}{Overview of our proposed ECLIPSE. ECLIPSE is a meta-architecture for contrastive language-image pretraining that features a text encoder $f_T$, a momentum teacher encoder (Full ViT, $\bar  {f}_I$), and a streamlined online encoder (ViT with token sparsification, $f_I$). Though the online network of ECLIPSE is compatible with any ViT acceleration method in literature\nobreakspace  {}\citep  {liang2022evit,rao2021dynamicvit,liang2022expediting}, we choose EViT\nobreakspace  {}\citep  {liang2022evit} due to its simple architecture without introducing additional parameters. Full ViTs without any sparsification can be also adopted for the online network, in which ECLIPSE then provides a full-capacity model with enhanced performance. }{}{}}
\newlabel{eq:distill}{{6}{4}{}{}{}}
\newlabel{slip_github}{{1}{4}{}{}{}}
\citation{radford2021learning,mu2021slip,li2022supervision}
\citation{wei2019eda}
\citation{liang2022evit}
\citation{liang2022evit}
\citation{caron2021emerging}
\citation{li2021align,Lu2022COTS}
\newlabel{tab:cc3m}{{1}{5}{ ImageNet-1k Top 1 zero shot accuracy with models pretrained on CC3M dataset under three training configurations. Details of each configuration is denoted in Sec.\nobreakspace  {}\ref {sec:cc3m_comparison}. ECLIPSE outperforms previous CLIP variants\nobreakspace  {}\citep  {radford2021learning,mu2021slip,li2022supervision} across all training configurations. Note that na\"ively adopting EViT to existing methods suffers from performance drop after acceleration. }{}{}}
\newlabel{tab:distill}{{2}{5}{ImageNet-1k Top-1 zero-shot accuracy for different $\mathcal  {L_{\text  {online}}}$ in Eq.\nobreakspace  {}\ref {eq:distill}. Different $\lambda $ and ($\bar  {A}$, $A$): (a-b) $\lambda $ schedules (c) distillation of output, (d-e) distillation of momentum alignment matrix and (f-g) additional use of text momentum encoder for ECLIPSE, are tested. $T\times I$: alignment matrix between text and image embeddings; overbar indicates embeddings from momentum encoder. sg: stop-gradient.}{}{}}
\newlabel{sec:cc3m_comparison}{{}{5}{}{}{}}
\newlabel{tab:keep}{{3}{5}{ ImageNet-1k Top-1 zero-shot accuracy for CLIP and ECLIPSE after expediting vision encoders with different keep ratios\nobreakspace  {}\citep  {liang2022evit}. All models were pretrained on CC3M dataset with a ViT-B/16 backbone. The relative performance difference compared to CLIP-ViT model is presented in the paranthesis. }{}{}}
\newlabel{subsec:ablation}{{}{5}{}{}{}}
\citation{liang2022evit}
\citation{chen2020simple}
\citation{radford2021learning}
\newlabel{result:cls}{{4}{6}{Zero-shot image classification performance (single-modal) on 11 downstream datasets and image--text retrieval (multi-modal) on the test splits of Flickr30k and COCO Captions with models pre-trained on YFCC15M. Our ECLIPSE achieves competitive performance with other state-of-the-art works while resulting in 54\% acceleration. Additional Supervisions other than Contrastive loss for image-text pairs are abbreviated as S: SSL between Augmentations.}{}{}}
\newlabel{tab:scale}{{5}{6}{ ImageNet-1k Top 1 zero shot accuracy for vision-language pretraining on different dataset scales. Both models were pretrained with a ViT-B/16 backbone for 3M and ViT-B/32 backbone for others. ECLIPSE shows a consistent tendency across various scales of pretrain datasets. }{}{}}
\citation{mu2021slip,li2022supervision}
\citation{chen2020simple}
\citation{chen2021exploring}
\citation{he2020moco,Chen2020mocov2}
\newlabel{tab:cost}{{6}{7}{ Training time comparison between CLIP and ECLIPSE variants. ECLIPSE achieves comparable training speed with CLIP even with disillation by removing text momentum encoder ($\bar  {T}$) and replacing full ViT online encoder to streamline ViT. }{}{}}
\newlabel{sec:discussion}{{}{7}{}{}{}}
\newlabel{tab:ssl}{{7}{7}{ ImageNet-1k Top 1 zero shot accuracy with models pretrained on CC3M dataset under different number of image \textit  {views} with either simple cropping (Img) or data augmentation (Aug). M:momentum, TPS: throughput per second. }{}{}}
\newlabel{sec:conclusion}{{}{7}{}{}{}}
\bibdata{aaai24}
\gdef \@abspage@last{8}
