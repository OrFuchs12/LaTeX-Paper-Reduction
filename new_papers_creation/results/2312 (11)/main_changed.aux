\relax 
\bibstyle{aaai24}
\citation{ma2019searching,sharon2015conflict,Wagner2011}
\citation{stern2019multi}
\citation{ma2016optimal}
\citation{honig2018conflict}
\citation{li2021lifelong}
\citation{sartoretti2019primal,damani2021primal,ma2021distributed,Li2022MultiAgentPF}
\citation{Wang2020,9340876}
\citation{browne2012survey}
\citation{Silver2016,silver2017mastering}
\newlabel{sec:intro}{{}{1}{}{}{}}
\citation{damani2021primal}
\citation{wang_scrimp_2023}
\citation{sartoretti2019primal}
\citation{damani2021primal}
\citation{riviere2020glas,Wang2020}
\citation{li2020graph,ma2021distributed,Li2022MultiAgentPF}
\citation{wang_scrimp_2023}
\citation{silver2017mastering}
\citation{schrittwieser2020mastering,ye2021mastering}
\citation{fawzi2022discovering}
\citation{lample2022hypertree}
\citation{best2019dec,dam2022monte}
\citation{zerbel2019multiagent}
\citation{skrynnik2021hybrid}
\citation{stern2019multi}
\citation{bernstein2002complexity,Pack1998}
\citation{silver2017mastering,ye2021mastering}
\citation{rosin_multi-armed_2011}
\citation{schulman2017proximal}
\citation{schulman2017proximal}
\citation{yu2022surprising}
\citation{peng2021facmac}
\citation{silver2017mastering}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:scheme-cost-tracer}{{1}{4}{The figure depicts the scheme of the \textsc  {CostTracer} algorithm. The approach takes two matrices as input: one encodes obstacles, normalized reversed cost-to-go; the other has local agent positions. The entire pipeline is trained with the PPO algorithm, using a reward function that only provides positive feedback when the agent gets closer to its global goal.}{}{}}
\citation{schrittwieser2020mastering}
\newlabel{fig:mmcts}{{2}{5}{ The scheme of the \textsc  {MATS-LP} approach. First, an intrinsic MDP (IMDP) is constructed using a global map of static obstacles, agents within the field of view, and their current targets. This MDP serves as the basis for planning using MCTS. In this approach, each tree node represents a joint action of all agents present in the IMDP, along with the associated node statistics. Action probabilities and node values are computed using the \textsc  {CostTracer} algorithm. Notably, each agent's value is computed individually, and the node's estimate is derived as the sum of values across all agents. The planning procedure exclusively concentrates on agents within close proximity. For example, in the given scenario, this includes the red agent itself and the brown agent. This deliberate decision to limit consideration to nearby agents significantly simplifies the decision-making process. Agent actions are visually depicted using circular representations, and bold arrows highlight actions featuring the highest probabilities. For distant agents, only a single action with maximum probability is considered. }{}{}}
\citation{damani2021primal}
\citation{wang_scrimp_2023}
\citation{Li2022MultiAgentPF}
\citation{ma2021distributed}
\citation{li2021lifelong}
\citation{li2021lifelong}
\newlabel{fig:results_random}{{3}{7}{Average throughput of MATS-LP, SCRIMP, and PRIMAL2 on random maps $20\times 20$ with various obstacle densities. The $\star $ symbol marks the approaches that were trained on the corresponding type of maps. The shaded areas indicate the 95\% confidence intervals.}{}{}}
\newlabel{fig:results_mazes}{{4}{7}{Average throughput of MATS-LP, SCRIMP and PRIMAL2 on maze-like maps with various sizes. The $\star $ symbol marks the approaches that were trained on the corresponding type of maps.}{}{}}
\citation{schrittwieser2020mastering}
\newlabel{fig:warehouse}{{5}{8}{Average throughput and average decision time of MATS-LP, SCRIMP and PRIMAL2 and ablation study of MATS-LP on warehouse-like map. The shaded areas indicate the 95\% confidence intervals.}{}{}}
\newlabel{table:parameters}{{1}{8}{Parameters of \textsc  {CostTracer} and \textsc  {MATS-LP}. }{}{}}
\bibdata{lib}
\gdef \@abspage@last{9}
