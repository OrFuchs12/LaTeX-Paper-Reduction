\section{Effect of Model Size}

In this section, we present results for training larger transformer models to evaluate the impact of increase in model size with increase in training set size.


\input{figures/model_size_ending_square}

Table~\ref{tab:results-ending-size} presents results with transformer models of sizes varying from GPT2-small to GPT2-medium.
We also introduce a new configuration, referred to as GPT2-intermediate, which serves as an intermediate between GPT2-small and GPT2-medium.
For Train-S, GPT2-small outperforms both GPT2-intermediate and GPT2-medium on almost all evaluations.
However, with increasing in training data,  GPT2-intermediate and GPT2-medium are are able to
outperform GPT2-small on most evaluations. 

These results are along the expected lines of larger training sets alleviating the overfitting problem with larger models~\citep{kaplan2020scaling}.  
Note that we stick with the default GPT2 configuration for all our experiments. Tuning the regularization hyperparameters such as dropout, can further improve results for bigger models trained with small training sets. 






