\section{Related Work}
\paragraph{Simulated Worlds.} %
There have been several prior efforts in relating simulated worlds to natural language. 
The bAbI framework simulates a world modeled via templates to generate question answering tasks \citep{weston2015aicomplete}. 
The recent TextWorld framework facilitates generating, training, and evaluating interactive text-based games \citep{cote18textworld}. 
\citet{hermann17grounded} and \citet{hill17understanding} develop and use 3D world simulations for learning grounded language.
These efforts are similar to our work in the sense that the true world state is, by construction, available, but our setup differs in that it provides a natural way of probing the state tracking of a model trained with an LM objective.




\paragraph{Cloze Tasks for Natural Language Models.}
There has been a great deal of work on cloze tasks for evaluating natural language models~\citep{hermann2015cnn, hill2016cbt}. 
These tasks range from testing general text understanding~\citep{paperno-etal-2016-lambada} to targeting particular aspects of natural language, such as commonsense/pragmatics \citep{mostafazadeh-etal-2016-corpus, ettinger2020bert}, narrative understanding \citep{mostafazadeh-etal-2017-lsdsem}, and factual knowledge \citep{petroni-etal-2019-language}.
Creating these tasks often requires human curation, and the evaluation is typically limited to exact match.\footnote{Automated cloze tasks without human filtering can yield instances which even humans can't answer \citep{hill2016cbt}.}  
Our proposed tasks are a form of cloze tasks, but can be precisely 
automated so that they require no human curation, and can be evaluated at a fine-grained level. 


\paragraph{Probing.}
One of the goals of this work is to probe the language model's board state tracking capability.
A typical solution used by prior work is to train a probing model on top of a pretrained model  
\citep{ettinger-etal-2016-probing,Alain2017UnderstandingIL, adi17probing, tenney2019probing,hewitt-liang-2019-designing}. %
This setup is time-consuming as it requires training probing models for all tasks. 
Moreover, the complexity of the probing model can also affect the conclusions \citep{pimentel-etal-2020-information}. 
In our case, by using an appropriate choice of notation, probing for board state can be accomplished via simple prompts (Section ~\ref{sec:probing}). 

\paragraph{Deep Learning for Chess.}
Deep networks have been used in prior work to predict the next move given the true game state~\cite{david16deepchess, Oshri2015PredictingMI}.
For example, using only self-play and the rules of chess, AlphaZero achieves superhuman performance starting from random play~\citep{silver18general}.
The focus of this prior work is the quality of game play given the true board state, while we use chess as a testbed for evaluating a language model's board state tracking capability.
Recently there has also been work focusing on transformer language models for chess \citep{presser2020chess,cheng2020chess,noever2020chess}. 
This work is similar to ours in the sense that the input is limited to the move sequence without the true board state, but the focus is again the quality of game play rather than the model's awareness of the underlying state. 

