\relax 
\citation{4}
\citation{9}
\citation{31}
\citation{37}
\citation{41}
\citation{42}
\citation{57}
\citation{58}
\citation{59}
\citation{29}
\citation{61}
\citation{63}
\citation{74}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pull_figure}{{1}{1}{More robust models tolerate larger noise levels. Thus, To assess the robustness of VQA models, we apply noise at a controllable level to the main question. We sort a dataset of basic questions based on their similarity to the main question and append three basic questions at a time as noise. The robustness is then measured by $R_{\text  {score}}$ as the deterioration in accuracy over a given testing dataset.}{}{}}
\citation{49}
\citation{68}
\citation{67}
\citation{69}
\citation{26}
\citation{33}
\citation{54}
\citation{65}
\citation{66}
\citation{49}
\citation{70}
\citation{71}
\citation{69}
\citation{68}
\citation{67}
\citation{11}
\citation{43}
\citation{47}
\citation{43}
\citation{10}
\citation{13}
\citation{15}
\citation{35}
\citation{42}
\citation{42}
\citation{41}
\citation{26}
\citation{57}
\citation{58}
\citation{59}
\citation{26}
\citation{58}
\citation{59}
\citation{57}
\citation{61}
\citation{72}
\citation{4}
\citation{4}
\citation{43}
\citation{10}
\citation{4}
\citation{4}
\citation{51}
\newlabel{eq:lasso}{{1}{3}{}{}{}}
\citation{4}
\citation{41}
\citation{57}
\citation{4}
\citation{4}
\citation{1}
\citation{19}
\citation{27}
\citation{28}
\citation{48}
\newlabel{fig:lasso_gbqd}{{2a}{4}{LASSO\_GBQD}{}{}}
\newlabel{sub@fig:lasso_gbqd}{{a}{4}{LASSO\_GBQD}{}{}}
\newlabel{fig:lasso_ynbqd}{{2b}{4}{LASSO\_YNBQD}{}{}}
\newlabel{sub@fig:lasso_ynbqd}{{b}{4}{LASSO\_YNBQD}{}{}}
\newlabel{fig:lasso}{{2}{4}{Compares the accuracy of six VQA models with increasing noise levels from both GBQD and YNBQD. The x-axis is the partition index with 0 meaning MQ without noise. We can see a monotonous trend as the noise increases.}{}{}}
\newlabel{fig:others_gbqd}{{3}{5}{Compares the accuracy of six VQA models with increasing noise levels from GBQD generated by BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE, CIDEr and METEOR. We could not observe a trend as the noise level increase like in Figure \ref {fig:lasso}.}{}{}}
\newlabel{tbl:lasso_examples}{{1}{5}{Demonstrates the limitations of \textit  {LASSO} ranking with two examples and their corresponding similarity scores despite performing well in our experiments. From the table, we readily see how the misspellings and sentence re-phrasings can affect the quality of the generated BQD. This can be attributed to the limitation of the encoding and with better semantic encoders and appropriate distance metrics this framework should improve out-of-the-box without any significant changes. In addition, one can utilize the feature vectors to filter out the unique questions to form the BQD instead of relying on raw string comparisons.}{}{}}
\newlabel{tbl:lasso_gbqd}{{2}{6}{Compares the accuracy and $R_{\text  {score}}$ of six VQA models with increasing noise levels from GBQD generated by \textit  {LASSO} evaluated on dev and std. The results are split by the question type; Numerical (Num), Yes/No (Y/N), or Other. There is a multitude of things to consider here in order to give a well-informed interpretation of these scores. One of which is the way $\text  {Acc}_\text  {diff}$ is defined in Eq \ref {eq:lasso}. The absolute value can be replaced with a ReLU ({\em  i.e.}, $max(x, 0)$) but this is a design decision which is widely accepted in the literature of adversarial attacks. As we can see, the accuracy of all tested models decrease as they are evaluated on noisier partitions ranked by \textit  {LASSO} (i.e., partition 7 has a higher noise level than partition 1).}{}{}}
\newlabel{tbl:lasso_ynbqd}{{3}{7}{Compares the accuracy and $R_{\text  {score}}$ of six VQA models with increasing noise levels from YNBQD generated by \textit  {LASSO} evaluated on dev and std. The results are split by the question type; Numerical (Num), Yes/No (Y/N), or Other. We notice that the $R_{\text  {score}}$ of some models under YNBQD is better than GBQD ({\em  e.g.}, HAR) and vice versa ({\em  e.g.}, MUA). So, the $R_{\text  {score}}$ can only be compared in different models if they have the same BQD because it can show certain biases in the models under investigation that are related to the type of the BQD. For example, we can clearly see how HAR is more robust towards Yes/No questions than general questions. Whereas, HAV is apparently agnostic to this property ({\em  i.e.}, the answer type being Yes/No versus general).}{}{}}
\bibdata{references}
\bibstyle{aaai}
\gdef \@abspage@last{8}
