\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Balcan, Khodak, and Talwalkar(2019)}]{balcan2019provable}
Balcan, M.-F.; Khodak, M.; and Talwalkar, A. 2019.
\newblock Provable guarantees for gradient-based meta-learning.
\newblock In \emph{International Conference on Machine Learning}, 424--433.
  PMLR.

\bibitem[{Behl, Baydin, and Torr(2019)}]{behl2019alpha}
Behl, H.~S.; Baydin, A.~G.; and Torr, P.~H. 2019.
\newblock Alpha MAML: Adaptive Model-Agnostic Meta-Learning.
\newblock \emph{arXiv preprint arXiv:1905.07435}.

\bibitem[{Cai et~al.(2020)Cai, Sheth, Mackey, and Fusi}]{cai2020weightedmeta}
Cai, D.; Sheth, R.; Mackey, L.; and Fusi, N. 2020.
\newblock Weighted Meta-learning.
\newblock \emph{arXiv preprint arXiv:2003.09465}.

\bibitem[{Chen et~al.(2019)Chen, Liu, Kira, Wang, and Huang}]{Chen2019ICLR}
Chen, W.-Y.; Liu, Y.-C.; Kira, Z.; Wang, Y.-C.~F.; and Huang, J.-B. 2019.
\newblock A Closer Look at Few-shot Classification.
\newblock \emph{International Conference on Learning Representations (ICLR)}.

\bibitem[{Dhillon et~al.(2019)Dhillon, Chaudhari, Ravichandran, and
  Soatto}]{dhillon2019baseline}
Dhillon, G.~S.; Chaudhari, P.; Ravichandran, A.; and Soatto, S. 2019.
\newblock A baseline for few-shot image classification.
\newblock \emph{arXiv preprint arXiv:1909.02729}.

\bibitem[{Fallah, Mokhtari, and Ozdaglar(2020)}]{fallah2020convergence}
Fallah, A.; Mokhtari, A.; and Ozdaglar, A. 2020.
\newblock On the convergence theory of gradient-based model-agnostic
  meta-learning algorithms.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 1082--1092. PMLR.

\bibitem[{Finn, Abbeel, and Levine(2017)}]{finn2017model}
Finn, C.; Abbeel, P.; and Levine, S. 2017.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, 1126--1135. JMLR. org.

\bibitem[{Finn et~al.(2019)Finn, Rajeswaran, Kakade, and
  Levine}]{finn2019online}
Finn, C.; Rajeswaran, A.; Kakade, S.; and Levine, S. 2019.
\newblock Online Meta-Learning.
\newblock In \emph{International Conference on Machine Learning}, 1920--1930.

\bibitem[{Finn, Xu, and Levine(2018)}]{finn2018probabilistic}
Finn, C.; Xu, K.; and Levine, S. 2018.
\newblock Probabilistic model-agnostic meta-learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  9516--9527.

\bibitem[{Goldblum, Fowl, and Goldstein(2020)}]{goldblum2020adversarially}
Goldblum, M.; Fowl, L.; and Goldstein, T. 2020.
\newblock Adversarially Robust Few-Shot Learning: A Meta-Learning Approach.
\newblock \emph{Advances in Neural Information Processing Systems}, 33.

\bibitem[{Jalal et~al.(2017)Jalal, Ilyas, Daskalakis, and
  Dimakis}]{jalal2017robust}
Jalal, A.; Ilyas, A.; Daskalakis, C.; and Dimakis, A.~G. 2017.
\newblock The robust manifold defense: Adversarial training using generative
  models.
\newblock \emph{arXiv preprint arXiv:1712.09196}.

\bibitem[{Jeong and Kim(2020)}]{jeong2020ood}
Jeong, T.; and Kim, H. 2020.
\newblock OOD-MAML: Meta-learning for few-shot out-of-distribution detection
  and classification.
\newblock \emph{Advances in Neural Information Processing Systems}, 33.

\bibitem[{Killamsetty et~al.(2020)Killamsetty, Sivasubramanian, Ramakrishnan,
  and Iyer}]{killamsetty2020glister}
Killamsetty, K.; Sivasubramanian, D.; Ramakrishnan, G.; and Iyer, R. 2020.
\newblock GLISTER: Generalization based Data Subset Selection for Efficient and
  Robust Learning.
\newblock \emph{arXiv preprint arXiv:2012.10630}.

\bibitem[{Lee et~al.(2020)Lee, Lee, Na, Kim, Park, Yang, and
  Hwang}]{lee2020l2b}
Lee, H.~B.; Lee, H.; Na, D.; Kim, S.; Park, M.; Yang, E.; and Hwang, S.~J.
  2020.
\newblock Learning to Balance: Bayesian Meta-Learning for Imbalanced and
  Out-of-distribution Tasks.
\newblock In \emph{ICLR}.

\bibitem[{Lu et~al.(2020)Lu, Jin, Liang, and Zhang}]{lu2020robust}
Lu, J.; Jin, S.; Liang, J.; and Zhang, C. 2020.
\newblock Robust Few-Shot Learning for User-Provided Data.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}.

\bibitem[{Luo et~al.(2015)Luo, Boix, Roig, Poggio, and Zhao}]{luo2015foveation}
Luo, Y.; Boix, X.; Roig, G.; Poggio, T.; and Zhao, Q. 2015.
\newblock Foveation-based mechanisms alleviate adversarial examples.
\newblock \emph{arXiv preprint arXiv:1511.06292}.

\bibitem[{Naik and Mammone(1992)}]{Naik1992MetaneuralNT}
Naik, D.~K.; and Mammone, R. 1992.
\newblock Meta-neural networks that learn by learning.
\newblock \emph{[Proceedings 1992] IJCNN International Joint Conference on
  Neural Networks}, 1: 437--442 vol.1.

\bibitem[{Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng}]{netzer2011reading}
Netzer, Y.; Wang, T.; Coates, A.; Bissacco, A.; Wu, B.; and Ng, A.~Y. 2011.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock \emph{NIPS Workshop on Deep Learning and Unsupervised Feature
  Learning}.

\bibitem[{Nichol, Achiam, and Schulman(2018)}]{nichol2018first}
Nichol, A.; Achiam, J.; and Schulman, J. 2018.
\newblock On first-order meta-learning algorithms.
\newblock \emph{arXiv preprint arXiv:1803.02999}.

\bibitem[{Raghu et~al.(2019)Raghu, Raghu, Bengio, and Vinyals}]{raghu2019rapid}
Raghu, A.; Raghu, M.; Bengio, S.; and Vinyals, O. 2019.
\newblock Rapid learning or feature reuse? towards understanding the
  effectiveness of maml.
\newblock \emph{arXiv preprint arXiv:1909.09157}.

\bibitem[{Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade, and
  Levine}]{rajeswaran2019meta}
Rajeswaran, A.; Finn, C.; Kakade, S.~M.; and Levine, S. 2019.
\newblock Meta-learning with implicit gradients.
\newblock In \emph{Advances in Neural Information Processing Systems},
  113--124.

\bibitem[{Ravi and Larochelle(2016)}]{ravi2016optimization}
Ravi, S.; and Larochelle, H. 2016.
\newblock Optimization as a model for few-shot learning.
\newblock \emph{ICLR}.

\bibitem[{Ren et~al.(2018)Ren, Zeng, Yang, and Urtasun}]{ren2018learning}
Ren, M.; Zeng, W.; Yang, B.; and Urtasun, R. 2018.
\newblock Learning to reweight examples for robust deep learning.
\newblock In \emph{International Conference on Machine Learning}, 4334--4343.
  PMLR.

\bibitem[{Rusu et~al.(2018)Rusu, Rao, Sygnowski, Vinyals, Pascanu, Osindero,
  and Hadsell}]{rusu2018meta}
Rusu, A.~A.; Rao, D.; Sygnowski, J.; Vinyals, O.; Pascanu, R.; Osindero, S.;
  and Hadsell, R. 2018.
\newblock Meta-learning with latent embedding optimization.
\newblock \emph{arXiv preprint arXiv:1807.05960}.

\bibitem[{Santoro et~al.(2016)Santoro, Bartunov, Botvinick, Wierstra, and
  Lillicrap}]{santoro2016meta}
Santoro, A.; Bartunov, S.; Botvinick, M.; Wierstra, D.; and Lillicrap, T. 2016.
\newblock Meta-learning with memory-augmented neural networks.
\newblock In \emph{International conference on machine learning}, 1842--1850.

\bibitem[{Schmidhuber(1987)}]{schmidhuber1987}
Schmidhuber, J. 1987.
\newblock \emph{Evolutionary Principles in Self-Referential Learning. On
  Learning now to Learn: The Meta-Meta-Meta...-Hook}.
\newblock Diploma thesis, Technische Universitat Munchen, Germany.

\bibitem[{Schneider et~al.(2020)Schneider, Rusak, Eck, Bringmann, Brendel, and
  Bethge}]{schneider2020improving}
Schneider, S.; Rusak, E.; Eck, L.; Bringmann, O.; Brendel, W.; and Bethge, M.
  2020.
\newblock Improving robustness against common corruptions by covariate shift
  adaptation.
\newblock \emph{Advances in Neural Information Processing Systems}, 33.

\bibitem[{Shu et~al.(2019)Shu, Xie, Yi, Zhao, Zhou, Xu, and Meng}]{shu2019meta}
Shu, J.; Xie, Q.; Yi, L.; Zhao, Q.; Zhou, S.; Xu, Z.; and Meng, D. 2019.
\newblock Meta-weight-net: Learning an explicit mapping for sample weighting.
\newblock In \emph{Advances in Neural Information Processing Systems},
  1919--1930.

\bibitem[{Triantafillou et~al.(2020)Triantafillou, Zhu, Dumoulin, Lamblin,
  Evci, Xu, Goroshin, Gelada, Swersky, Manzagol et~al.}]{triantafillou2019meta}
Triantafillou, E.; Zhu, T.; Dumoulin, V.; Lamblin, P.; Evci, U.; Xu, K.;
  Goroshin, R.; Gelada, C.; Swersky, K.; Manzagol, P.-A.; et~al. 2020.
\newblock Meta-dataset: A dataset of datasets for learning to learn from few
  examples.
\newblock \emph{ICLR}.

\bibitem[{Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Wierstra
  et~al.}]{vinyals2016matching}
Vinyals, O.; Blundell, C.; Lillicrap, T.; Wierstra, D.; et~al. 2016.
\newblock Matching networks for one shot learning.
\newblock In \emph{Advances in neural information processing systems},
  3630--3638.

\bibitem[{Vuorio et~al.(2019)Vuorio, Sun, Hu, and Lim}]{vuorio2019multimodal}
Vuorio, R.; Sun, S.-H.; Hu, H.; and Lim, J.~J. 2019.
\newblock Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 1--12.

\bibitem[{Wang and Yu(2019)}]{wang2019direct}
Wang, H.; and Yu, C.-N. 2019.
\newblock A direct approach to robust deep learning using adversarial networks.
\newblock \emph{arXiv preprint arXiv:1905.09591}.

\bibitem[{Xiao, Rasul, and Vollgraf(2017)}]{xiao2017fashion}
Xiao, H.; Rasul, K.; and Vollgraf, R. 2017.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}.

\bibitem[{Yao et~al.(2020)Yao, Wu, Tao, Li, Ding, Li, and
  Li}]{yao2020automated}
Yao, H.; Wu, X.; Tao, Z.; Li, Y.; Ding, B.; Li, R.; and Li, Z. 2020.
\newblock Automated relational meta-learning.
\newblock \emph{ICLR}.

\bibitem[{Yin et~al.(2018)Yin, Tang, Xu, and Wang}]{yin2018adversarial}
Yin, C.; Tang, J.; Xu, Z.; and Wang, Y. 2018.
\newblock Adversarial meta-learning.
\newblock \emph{arXiv preprint arXiv:1806.03316}.

\bibitem[{Zhao et~al.(2020)Zhao, Li, Li, and Chen}]{zhao2020fair}
Zhao, C.; Li, C.; Li, J.; and Chen, F. 2020.
\newblock Fair meta-learning for few-shot classification.
\newblock In \emph{2020 IEEE International Conference on Knowledge Graph
  (ICKG)}, 275--282. IEEE.

\bibitem[{Zhou, Knowles, and Finn(2021)}]{zhou2020meta}
Zhou, A.; Knowles, T.; and Finn, C. 2021.
\newblock Meta-learning Symmetries by Reparameterization.
\newblock In \emph{International Conference on Learning Representations}.

\end{thebibliography}
