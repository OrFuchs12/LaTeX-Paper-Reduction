\relax 
\bibstyle{aaai24}
\citation{RN2,RN3,RN4,RN5,zheng2023generating,wang2023scene}
\citation{RN7,RN28,RN29}
\citation{RN31}
\citation{RN28}
\newlabel{sec:intro}{{}{1}{}{}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Introduction}{{1}{1}{The spurious correlation between query words and temporal location of target moments. The horizontal and vertical axes represent the normalized starting time and duration of the target moment, respectively. The color represents the text-video pair density, which is obtained by using kernel density estimation with the Gaussian kernel.}{}{}}
\citation{RN8,RN9,RN10,RN11,RN5}
\citation{RN2,RN13,RN14,RN15,RN16,RN17}
\citation{RN18,RN19,RN20,RN21,RN3,zheng2023generating,wang2023scene}
\citation{RN22,RN23,RN25,RN26,RN4,zheng2023phrase}
\citation{RN34,RN36,RN28}
\citation{RN30,RN35,RN29,lan2023curriculum,qi2023self,lan2022closer,yoon2023counterfactual}
\citation{G3AN}
\newlabel{Overview}{{2}{3}{An overview of our BSSARD. The orange background is only used during the training period. Best viewed in color.}{}{}}
\newlabel{eq1}{{1}{3}{}{}{}}
\citation{RN2}
\citation{RN21}
\citation{RN20}
\citation{RN4}
\citation{RN29}
\citation{RN30}
\citation{lan2022closer}
\citation{hao2022query}
\citation{RN3}
\citation{RN4}
\citation{lan2022closer}
\newlabel{eq2}{{2}{4}{}{}{}}
\newlabel{eq3}{{3}{4}{}{}{}}
\newlabel{eq4}{{4}{4}{}{}{}}
\newlabel{g_loss_cls}{{5}{4}{}{}{}}
\newlabel{g_loss_loc}{{6}{4}{}{}{}}
\newlabel{g_loss}{{7}{4}{}{}{}}
\newlabel{Training Process}{{1}{4}{Training process in one epoch.}{}{}}
\newlabel{d_loss_cls}{{8}{4}{}{}{}}
\newlabel{d_loss_loc}{{9}{4}{}{}{}}
\newlabel{loss_kl_d}{{10}{4}{}{}{}}
\newlabel{eq12}{{11}{4}{}{}{}}
\citation{Glove}
\citation{I3D}
\citation{C3D}
\citation{AdamW}
\newlabel{sota-redivided}{{1}{5}{Comparison results with state-of-the-arts. VSLNet* replaces the encoder in VSLNet with a transformer block.}{}{}}
\newlabel{loss term study}{{2}{5}{Ablation study about loss functions.}{}{}}
\newlabel{backbones_charades_cd}{{3}{6}{Ablation study about bias generator.}{}{}}
\newlabel{bias_injection_positions}{{4}{6}{Ablation study about bias injection positions. ``before'' and ``after'' represent the injection position before and after the feature encoder, respectively.}{}{}}
\newlabel{fusion method}{{5}{6}{Ablation study about fusion method of $z_p$.}{}{}}
\newlabel{Training strategy}{{6}{6}{Ablation study about training strategy.}{}{}}
\bibdata{aaai24}
\newlabel{visualization_sample1}{{3(a)}{7}{}{}{}}
\newlabel{sub@visualization_sample1}{{(a)}{7}}
\newlabel{visualization_sample2}{{3(b)}{7}{}{}{}}
\newlabel{sub@visualization_sample2}{{(b)}{7}}
\newlabel{visualization_sample3}{{3(c)}{7}{}{}{}}
\newlabel{sub@visualization_sample3}{{(c)}{7}}
\newlabel{visualization_sample4}{{3(d)}{7}{}{}{}}
\newlabel{sub@visualization_sample4}{{(d)}{7}}
\newlabel{visualization_sample}{{3}{7}{The visualization comparison results between VLSNet* and BSSAR-VLSNet*.}{}{}}
\newlabel{ablation_removing_language_bias}{{7}{7}{Comparison results under random text input.}{}{}}
\newlabel{sample_bias_distribution}{{4}{7}{The temporal distribution of target moments for video-text samples with certain common verbs. The horizontal and vertical axes denote the normalized starting time and duration of the target moment, respectively. The color represents the sample density.}{}{}}
\gdef \@abspage@last{8}
