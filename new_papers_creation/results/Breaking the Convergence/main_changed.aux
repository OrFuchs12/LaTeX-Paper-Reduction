\relax 
\bibstyle{aaai22}
\citation{sra2012optimization}
\citation{shalev2014understanding}
\citation{kingma2015adam}
\citation{huo2018accelerated,li2020accelerated}
\citation{brown1989}
\citation{su2016}
\citation{andre2016}
\citation{bhat2000}
\citation{polyakov2011nonlinear}
\citation{cortes2006}
\citation{chen2018}
\citation{kunal2021}
\newlabel{sec: Intro}{{}{1}{}{}{}}
\citation{polyakov2019consistent}
\citation{benosman2020optimizing}
\citation{garg2021MVIP}
\citation{karimi2016linear}
\citation{karimi2016linear}
\citation{polyakov2011nonlinear}
\citation{polyakov2011nonlinear}
\citation{kunal2021}
\citation{kunal2021}
\citation{kunal2021}
\newlabel{sec: prelims}{{}{2}{}{}{}}
\newlabel{opt_prob}{{1}{2}{}{}{}}
\newlabel{basicasmp}{{1}{2}{}{}{}}
\newlabel{plineq}{{2}{2}{}{}{}}
\newlabel{grad_dom}{{2}{2}{}{}{}}
\newlabel{quadgrowth}{{3}{2}{}{}{}}
\newlabel{dynasys}{{4}{2}{}{}{}}
\newlabel{fxts_lemma}{{1}{2}{}{}{}}
\newlabel{eq:dotV cond}{{5}{2}{}{}{}}
\newlabel{fxtsgf}{{7}{2}{}{}{}}
\citation{sun2020continuous}
\newlabel{lemma:fxts_V}{{1}{3}{}{}{}}
\newlabel{noisyfxts}{{8}{3}{}{}{}}
\newlabel{as:noiseterm}{{3}{3}{}{}{}}
\newlabel{eq: noisebound2}{{9}{3}{}{}{}}
\newlabel{thm:Robustness}{{2}{3}{}{}{}}
\newlabel{eq:robust_bound}{{10}{3}{}{}{}}
\newlabel{eq: dot V robust proof}{{11}{3}{}{}{}}
\citation{garg2021MVIP,benosman2020optimizing}
\citation{garg2021MVIP}
\newlabel{sec: regret analysis}{{}{4}{}{}{}}
\newlabel{thm:Regret}{{3}{4}{}{}{}}
\newlabel{vbound1}{{13}{4}{}{}{}}
\newlabel{vbound2}{{14}{4}{}{}{}}
\newlabel{sec: discretization}{{}{4}{}{}{}}
\newlabel{lemma: weak conv disc FxTS}{{2}{4}{}{}{}}
\newlabel{eq:discrete cont dyn}{{15}{4}{}{}{}}
\newlabel{eq:x disc bound}{{16}{4}{}{}{}}
\newlabel{eq:discrete dyn FxTS}{{17}{4}{}{}{}}
\citation{polyak1964some}
\newlabel{lemma:V p1 p2}{{3}{5}{}{}{}}
\newlabel{eq: p1 p2 cond}{{18}{5}{}{}{}}
\newlabel{thm: discretized}{{4}{5}{}{}{}}
\newlabel{eq: dot x disc}{{19}{5}{}{}{}}
\newlabel{eq: disc bound}{{20}{5}{}{}{}}
\newlabel{sec:FxTS-M-GF}{{}{5}{}{}{}}
\newlabel{eq:SGD-momentum}{{21}{5}{}{}{}}
\newlabel{eq:SGD-momentum-cont}{{22}{5}{}{}{}}
\newlabel{eq:FxTS-M-GF}{{23}{5}{}{}{}}
\newlabel{ass:LipGradient}{{4}{5}{}{}{}}
\newlabel{thm:FxTS-M-GF}{{5}{5}{}{}{}}
\newlabel{eq:FxTS-M-GF-V}{{24}{5}{}{}{}}
\newlabel{eq:FxTS-M-GF-V1V2}{{25}{5}{}{}{}}
\citation{kingma2015adam}
\citation{polyak1964some,sutskever2013importance}
\citation{lecun1998gradient}
\citation{krizhevsky2009learning}
\citation{rosenbrock1960automatic}
\newlabel{eq:V1dot}{{26}{6}{}{}{}}
\newlabel{eq:V2dot}{{27}{6}{}{}{}}
\newlabel{eq:V2dot2}{{28}{6}{}{}{}}
\newlabel{eq:finalVdot}{{29}{6}{}{}{}}
\newlabel{sec: experiments}{{}{6}{}{}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sub-first}{{\caption@xref {fig:sub-first}{ on input line 594}}{6}{}{}{}}
\newlabel{fig:sub-second}{{\caption@xref {fig:sub-second}{ on input line 600}}{6}{}{}{}}
\newlabel{fig:Rosenbrock}{{1}{6}{Minimization of Rosenbrock function. (a) Comparison of various optimization algorithms for the initial condition $(0.3,0.8)$. (b) Performance of the FxTS(M)-GF algorithm at varying initial conditions.}{}{}}
\newlabel{subsec:Rosenbrock}{{}{6}{}{}{}}
\newlabel{subsec:TrainDNN}{{}{6}{}{}{}}
\newlabel{fig:NN}{{2}{7}{Comparison of several optimization algorithms for training deep neural networks on MNIST and CIFAR datasets across five random seeds. FxTS(M) outperforms Adam and NAG optimizers on various performance measures.}{}{}}
\newlabel{sec: conclusion}{{}{7}{}{}{}}
\bibdata{aaai22}
\gdef \@abspage@last{8}
