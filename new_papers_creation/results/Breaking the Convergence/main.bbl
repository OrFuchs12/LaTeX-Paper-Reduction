\begin{thebibliography}{23}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Benosman, Romero, and Cherian(2020)}]{benosman2020optimizing}
Benosman, M.; Romero, O.; and Cherian, A. 2020.
\newblock Optimizing deep neural networks via discretization of finite-time
  convergent flows.
\newblock \emph{arXiv preprint arXiv:2010.02990}.

\bibitem[{Bhat and Bernstein(2000)}]{bhat2000}
Bhat, S.~P.; and Bernstein, D.~S. 2000.
\newblock Finite-Time Stability of Continuous Autonomous Systems.
\newblock \emph{SIAM Journal on Control and Optimization}, 38(3): 751--766.

\bibitem[{Brown and Bartholomew-Biggs(1989)}]{brown1989}
Brown, A.; and Bartholomew-Biggs, M. 1989.
\newblock Some effective methods for unconstrained optimization based on the
  solution of systems of ordinary differential equations.
\newblock \emph{Journal of Optimization Theory and Applications}, 62: 211--224.

\bibitem[{Chen and Ren(2018)}]{chen2018}
Chen, F.; and Ren, W. 2018.
\newblock Convex Optimization via Finite-Time Projected Gradient Flows.
\newblock In \emph{2018 IEEE Conference on Decision and Control (CDC)},
  4072--4077.

\bibitem[{Cortés(2006)}]{cortes2006}
Cortés, J. 2006.
\newblock Finite-time convergent gradient flows with applications to network
  consensus.
\newblock \emph{Automatica}, 42(11): 1993--2000.

\bibitem[{Garg et~al.(2021)Garg, Baranwal, Gupta, and Benosman}]{garg2021MVIP}
Garg, K.; Baranwal, M.; Gupta, R.; and Benosman, M. 2021.
\newblock Fixed-Time Stable Proximal Dynamical System for Solving MVIPs.
\newblock ArXiv e-Print.

\bibitem[{Garg and Panagou(2021)}]{kunal2021}
Garg, K.; and Panagou, D. 2021.
\newblock Fixed-Time Stable Gradient Flows: Applications to Continuous-Time
  Optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 66(5): 2002--2015.

\bibitem[{Huo et~al.(2018)Huo, Gu, Liu, and Huang}]{huo2018accelerated}
Huo, Z.; Gu, B.; Liu, J.; and Huang, H. 2018.
\newblock Accelerated method for stochastic composition optimization with
  nonsmooth regularization.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence}.

\bibitem[{Karimi, Nutini, and Schmidt(2016)}]{karimi2016linear}
Karimi, H.; Nutini, J.; and Schmidt, M. 2016.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, 795--811. Springer.

\bibitem[{Kingma and Ba(2015)}]{kingma2015adam}
Kingma, D.~P.; and Ba, J. 2015.
\newblock Adam: {A} Method for Stochastic Optimization.
\newblock In \emph{3rd International Conference on Learning Representations San
  Diego}.

\bibitem[{Krizhevsky and Hinton(2009)}]{krizhevsky2009learning}
Krizhevsky, A.; and Hinton, G. 2009.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, Department of Computer Science, University of
  Toronto}.

\bibitem[{LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner}]{lecun1998gradient}
LeCun, Y.; Bottou, L.; Bengio, Y.; and Haffner, P. 1998.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86(11): 2278--2324.

\bibitem[{Li, Fang, and Lin(2020)}]{li2020accelerated}
Li, H.; Fang, C.; and Lin, Z. 2020.
\newblock Accelerated first-order optimization algorithms for machine learning.
\newblock \emph{Proceedings of the IEEE}, 108(11): 2067--2082.

\bibitem[{Polyak(1964)}]{polyak1964some}
Polyak, B.~T. 1964.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{Ussr computational mathematics and mathematical physics}, 4(5):
  1--17.

\bibitem[{Polyakov(2011)}]{polyakov2011nonlinear}
Polyakov, A. 2011.
\newblock Nonlinear feedback design for fixed-time stabilization of linear
  control systems.
\newblock \emph{IEEE Transactions on Automatic Control}, 57(8): 2106--2110.

\bibitem[{Polyakov, Efimov, and Brogliato(2019)}]{polyakov2019consistent}
Polyakov, A.; Efimov, D.; and Brogliato, B. 2019.
\newblock {C}onsistent {d}iscretization of {f}inite-time and {f}ixed-time
  {s}table {s}ystems.
\newblock \emph{SIAM Journal on {C}ontrol and {O}ptimization}, 57(1): 78--103.

\bibitem[{Rosenbrock(1960)}]{rosenbrock1960automatic}
Rosenbrock, H. 1960.
\newblock An automatic method for finding the greatest or least value of a
  function.
\newblock \emph{The Computer Journal}, 3(3): 175--184.

\bibitem[{Shalev-Shwartz and Ben-David(2014)}]{shalev2014understanding}
Shalev-Shwartz, S.; and Ben-David, S. 2014.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge University Press.

\bibitem[{Sra, Nowozin, and Wright(2012)}]{sra2012optimization}
Sra, S.; Nowozin, S.; and Wright, S.~J. 2012.
\newblock \emph{Optimization for machine learning}.
\newblock MIT Press.

\bibitem[{Su, Boyd, and Cand{{\`e}}s(2016)}]{su2016}
Su, W.; Boyd, S.; and Cand{{\`e}}s, E.~J. 2016.
\newblock A Differential Equation for Modeling Nesterov's Accelerated Gradient
  Method: Theory and Insights.
\newblock \emph{Journal of Machine Learning Research}, 17(153): 1--43.

\bibitem[{Sun and Hu(2020)}]{sun2020continuous}
Sun, C.; and Hu, G. 2020.
\newblock A Continuous-Time Nesterov Accelerated Gradient Method for
  Centralized and Distributed Online Convex Optimization.
\newblock \emph{arXiv preprint arXiv:2009.12545}.

\bibitem[{Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton}]{sutskever2013importance}
Sutskever, I.; Martens, J.; Dahl, G.; and Hinton, G. 2013.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning}, 1139--1147.
  PMLR.

\bibitem[{Wibisono, Wilson, and Jordan(2016)}]{andre2016}
Wibisono, A.; Wilson, A.~C.; and Jordan, M.~I. 2016.
\newblock A variational perspective on accelerated methods in optimization.
\newblock \emph{Proceedings of the National Academy of Sciences of the United
  States of America}, 113(47): E7351—E7358.

\end{thebibliography}
