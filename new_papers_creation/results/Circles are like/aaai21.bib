%%%%%%%%%%%%%
@inproceedings{cattle2018recognizing,
  title={Recognizing humour using word associations and humour anchor extraction},
  author={Cattle, Andrew and Ma, Xiaojuan},
  booktitle={Proceedings of the 27th International Conference on Computational Linguistics},
  pages={1849--1858},
  year={2018}
}

@article{miller1995wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM New York, NY, USA}
}

@inproceedings{gawron-2014-improving,
    title = "Improving sparse word similarity models with asymmetric measures",
    author = "Gawron, Jean Mark",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P14-2049",
    doi = "10.3115/v1/P14-2049",
    pages = "296--301",
}

@inproceedings{cattle-ma-2017-predicting,
    title = "Predicting Word Association Strengths",
    author = "Cattle, Andrew  and
      Ma, Xiaojuan",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1132",
    doi = "10.18653/v1/D17-1132",
    pages = "1283--1288",
    abstract = "This paper looks at the task of predicting word association strengths across three datasets; WordNet Evocation (Boyd-Graber et al., 2006), University of Southern Florida Free Association norms (Nelson et al., 2004), and Edinburgh Associative Thesaurus (Kiss et al., 1973). We achieve results of r=0.357 and p=0.379, r=0.344 and p=0.300, and r=0.292 and p=0.363, respectively. We find Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) cosine similarities, as well as vector offsets, to be the highest performing features. Furthermore, we examine the usefulness of Gaussian embeddings (Vilnis and McCallum, 2014) for predicting word association strength, the first work to do so.",
}

%%%%%%%%%%%%% count based models
@inproceedings{bulat-etal-2017-speaking,
    title = "Speaking, Seeing, Understanding: Correlating semantic models with conceptual representation in the brain",
    author = "Bulat, Luana  and
      Clark, Stephen  and
      Shutova, Ekaterina",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1113",
    doi = "10.18653/v1/D17-1113",
    pages = "1081--1091",
    abstract = "Research in computational semantics is increasingly guided by our understanding of human semantic processing. However, semantic models are typically studied in the context of natural language processing system performance. In this paper, we present a systematic evaluation and comparison of a range of widely-used, state-of-the-art semantic models in their ability to predict patterns of conceptual representation in the human brain. Our results provide new insights both for the design of computational semantic models and for further research in cognitive neuroscience.",
}

@article{vankrunkelsven2018predicting,
  title={Predicting Lexical Norms: A Comparison between a Word Association Model and Text-Based Word Co-occurrence Models},
  author={Vankrunkelsven, Hendrik and Verheyen, Steven and Storms, Gert and De Deyne, Simon},
  journal={Journal of Cognition},
  volume={1},
  number={1},
  year={2018},
  publisher={Ubiquity Press}
}

@inproceedings{matusevych2018analyzing,
  title={Analyzing and modeling free word associations.},
  author={Matusevych, Yevgen and Stevenson, Suzanne},
  booktitle={CogSci},
  year={2018}
}

@inproceedings{baroni-etal-2014-dont,
    title = "Don{'}t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    author = "Baroni, Marco  and
      Dinu, Georgiana  and
      Kruszewski, Germ{\'a}n",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P14-1023",
    doi = "10.3115/v1/P14-1023",
    pages = "238--247",
}


@inproceedings{de-deyne-etal-2016-predicting,
    title = "Predicting human similarity judgments with distributional models: The value of word associations.",
    author = "De Deyne, Simon  and
      Perfors, Amy  and
      Navarro, Daniel J",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/C16-1175",
    pages = "1861--1870",
    abstract = "Most distributional lexico-semantic models derive their representations based on external language resources such as text corpora. In this study, we propose that internal language models, that are more closely aligned to the mental representations of words could provide important insights into cognitive science, including linguistics. Doing so allows us to reflect upon theoretical questions regarding the structure of the mental lexicon, and also puts into perspective a number of assumptions underlying recently proposed distributional text-based models. In particular, we focus on word-embedding models which have been proposed to learn aspects of word meaning in a manner similar to humans. These are contrasted with internal language models derived from a new extensive data set of word associations. Using relatedness and similarity judgments we evaluate these models and find that the word-association-based internal language models consistently outperform current state-of-the art text-based external language models, often with a large margin. These results are not just a performance improvement; they also have implications for our understanding of how distributional knowledge is used by people.",
}


%%%%%%%%%%%% Contextual representations

@inproceedings{Peters:2018,
  author={Peters, Matthew E. and  Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  title={Deep contextualized word representations},
  booktitle={Proc. of NAACL},
  year={2018}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}


%%%%%%%%%%% Probing BERT 

@article{tenney2019bert,
  title={Bert rediscovers the classical nlp pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  journal={arXiv preprint arXiv:1905.05950},
  year={2019}
}

@incollection{reif2019visualizing,
title = {Visualizing and Measuring the Geometry of BERT},
author = {Reif, Emily and Yuan, Ann and Wattenberg, Martin and Viegas, Fernanda B and Coenen, Andy and Pearce, Adam and Kim, Been},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8592--8600},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9065-visualizing-and-measuring-the-geometry-of-bert.pdf}
}

@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@article{clark2019what,
  author    = {Kevin Clark and
               Urvashi Khandelwal and
               Omer Levy and
               Christopher D. Manning},
  title     = {What Does {BERT} Look At? An Analysis of BERT's Attention},
  journal   = {BlackboxNLP},
  year      = {2019},
}

@article{bouraoui2019inducing,
  title={Inducing Relational Knowledge from BERT},
  author={Bouraoui, Zied and Camacho-Collados, Jose and Schockaert, Steven},
  journal={AAAI},
  year={2020}
}

@article{liu2019k,
  title={K-bert: Enabling language representation with knowledge graph},
  author={Liu, Weijie and Zhou, Peng and Zhao, Zhe and Wang, Zhiruo and Ju, Qi and Deng, Haotang and Wang, Ping},
  journal={AAAI},
  year={2020}
}

@article{brown2020language,
    title={Language Models are Few-Shot Learners},
    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year={2020},
    eprint={2005.14165},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{vig2019visualizing,
  author    = {Jesse Vig},
  title     = {Visualizing Attention in Transformer-Based Language Representation
               Models},
  journal   = {CoRR},
  volume    = {abs/1904.02679},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.02679},
  archivePrefix = {arXiv},
  eprint    = {1904.02679},
  timestamp = {Wed, 24 Apr 2019 12:21:25 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1904-02679},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019}
}

@article{tenney2019you,
  title={What do you learn from context? probing for sentence structure in contextualized word representations},
  author={Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel R and Das, Dipanjan and others},
  journal={arXiv preprint arXiv:1905.06316},
  year={2019}
}

%%%%%%%%%%% Semantic measurements

@inproceedings{Resnik:1995:UIC:1625855.1625914,
 author = {Resnik, Philip},
 title = {Using Information Content to Evaluate Semantic Similarity in a Taxonomy},
 booktitle = {Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 1},
 series = {IJCAI'95},
 year = {1995},
 isbn = {1-55860-363-8, 978-1-558-60363-9},
 location = {Montreal, Quebec, Canada},
 pages = {448--453},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=1625855.1625914},
 acmid = {1625914},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 
[download]

@article{Budanitsky:2006:EWM:1168106.1168108,
 author = {Budanitsky, Alexander and Hirst, Graeme},
 title = {Evaluating WordNet-based Measures of Lexical Semantic Relatedness},
 journal = {Comput. Linguist.},
 issue_date = {March 2006},
 volume = {32},
 number = {1},
 month = mar,
 year = {2006},
 issn = {0891-2017},
 pages = {13--47},
 numpages = {35},
 url = {http://dx.doi.org/10.1162/coli.2006.32.1.13},
 doi = {10.1162/coli.2006.32.1.13},
 acmid = {1168108},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@inproceedings{zesch2008using,
  title={Using Wiktionary for Computing Semantic Relatedness.},
  author={Zesch, Torsten and M{\"u}ller, Christof and Gurevych, Iryna},
  booktitle={AAAI},
  volume={8},
  pages={861--866},
  year={2008}
}


%%%%%%%%%%% Similarity dataset

@article{bruni2014multimodal,
  title={Multimodal distributional semantics},
  author={Bruni, Elia and Tran, Nam-Khanh and Baroni, Marco},
  journal={Journal of Artificial Intelligence Research},
  volume={49},
  pages={1--47},
  year={2014}
}


@article{rubenstein1965contextual,
  title={Contextual correlates of synonymy},
  author={Rubenstein, Herbert and Goodenough, John B},
  journal={Communications of the ACM},
  volume={8},
  number={10},
  pages={627--633},
  year={1965},
  publisher={Citeseer}
}

@article{finkelstein2002placing,
  title={Placing search in context: The concept revisited},
  author={Finkelstein, Lev and Gabrilovich, Evgeniy and Matias, Yossi and Rivlin, Ehud and Solan, Zach and Wolfman, Gadi and Ruppin, Eytan},
  journal={ACM Transactions on information systems},
  volume={20},
  number={1},
  pages={116--131},
  year={2002}
}

@article{yang2006verb, title={Verb similarity on the taxonomy of WordNet}, author={Yang, Dongqiang and Powers, David MW}, journal={Proceedings of GWC-06}, pages={121--128}, year={2006} }



%%%%%%%%%%%% BERT embedding

@article{coenen2019visualizing,
  title={Visualizing and Measuring the Geometry of BERT},
  author={Coenen, Andy and Reif, Emily and Yuan, Ann and Kim, Been and Pearce, Adam and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  journal={arXiv preprint arXiv:1906.02715},
  year={2019}
}

@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}

@misc{mickus2019mean,
    title={What do you mean, BERT? Assessing BERT as a Distributional Semantics Model},
    author={Timothee Mickus and Denis Paperno and Mathieu Constant and Kees van Deemeter},
    year={2019},
    eprint={1911.05758},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

%%%%%%%%% static embedding

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@article{joulin2016fasttext,
  title={Fasttext. zip: Compressing text classification models},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{\'e}gou, H{\'e}rve and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1612.03651},
  year={2016}
}

@inproceedings{Levy2014NeuralWE,
  title={Neural Word Embedding as Implicit Matrix Factorization},
  author={Omer Levy and Yoav Goldberg},
  booktitle={NIPS},
  year={2014}
}

@article{arora-etal-2016-latent,
    title = "A Latent Variable Model Approach to {PMI}-based Word Embeddings",
    author = "Arora, Sanjeev  and
      Li, Yuanzhi  and
      Liang, Yingyu  and
      Ma, Tengyu  and
      Risteski, Andrej",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    url = "https://www.aclweb.org/anthology/Q16-1028",
    doi = "10.1162/tacl_a_00106",
    pages = "385--399",
    abstract = "Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods. This paper proposes a new generative model, a dynamic version of the log-linear topic model of Mnih and Hinton (2007). The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by Mikolov et al. (2013a) and many subsequent papers. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space.",
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{petroni2019language,
  title={Language Models as Knowledge Bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Riedel, Sebastian and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2463--2473},
  year={2019}
}

@article{Ethayarajh2019HowCA,
  title={How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings},
  author={Kawin Ethayarajh},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.00512}
}

@inproceedings{miller2016key,
  title={Key-Value Memory Networks for Directly Reading Documents},
  author={Miller, Alexander and Fisch, Adam and Dodge, Jesse and Karimi, Amir-Hossein and Bordes, Antoine and Weston, Jason},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={1400--1409},
  year={2016}
}

%%%%%%%%%%%%%%%%%%%% Linguistics


@article{tversky1977features,
  title={Features of similarity.},
  author={Tversky, Amos},
  journal={Psychological review},
  volume={84},
  number={4},
  pages={327},
  year={1977},
  publisher={American Psychological Association}
}

@article{ogden1930basic,
  title={Basic English: A general introduction with rules and grammar},
  author={Ogden, Charles Kay},
  year={1930},
  publisher={Paul Treber}
}

%%%%%%%%%%%%%%%%%%%% FA data
@article{nelson2004university,
  title={The University of South Florida free association, rhyme, and word fragment norms},
  author={Nelson, Douglas L and McEvoy, Cathy L and Schreiber, Thomas A},
  journal={Behavior Research Methods, Instruments, \& Computers},
  volume={36},
  number={3},
  pages={402--407},
  year={2004},
  publisher={Springer}
}

@article{de2019small,
  title={The “Small World of Words” English word association norms for over 12,000 cue words},
  author={De Deyne, Simon and Navarro, Danielle J and Perfors, Amy and Brysbaert, Marc and Storms, Gert},
  journal={Behavior research methods},
  volume={51},
  number={3},
  pages={987--1006},
  year={2019},
  publisher={Springer}
}

@article{kiss1973associative,
  title={An associative thesaurus of English and its computer analysis},
  author={Kiss, George R and Armstrong, Christine and Milroy, Robert and Piper, James},
  journal={The computer and literary studies},
  pages={153--165},
  year={1973}
}

@inproceedings{boyd2006adding,
  title={Adding dense, weighted connections to WordNet},
  author={Boyd-Graber, Jordan and Fellbaum, Christiane and Osherson, Daniel and Schapire, Robert},
  booktitle={Proceedings of the third international WordNet conference},
  pages={29--36},
  year={2006},
  organization={Citeseer}
}

@article{jouravlev2016thematic,
  title={Thematic relatedness production norms for 100 object concepts},
  author={Jouravlev, Olessia and McRae, Ken},
  journal={Behavior research methods},
  volume={48},
  number={4},
  pages={1349--1357},
  year={2016},
  publisher={Springer}
}

%%%%%%%%%%%%%%%%%%

@article{wang2018glue,
  title={Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={BlackBoxNLP},
  year={2018}
}

@article{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={NIPS},
  year={2019}
}
%%%%%%%%%%%%%%%%%%% Inability to solve commonsense reasoning from large corpora

@inproceedings{Klein2019AttentionI,
  title={Attention Is (not) All You Need for Commonsense Reasoning},
  author={Tassilo Klein and Moin Nabi},
  booktitle={ACL},
  year={2019}
}

%%%%%%%%%%%%%%%%%%% KG data
@inproceedings{speer2017conceptnet,
  title={Conceptnet 5.5: An open multilingual graph of general knowledge},
  author={Speer, Robert and Chin, Joshua and Havasi, Catherine},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}


%%%%%%%%%%%%%%%%%%% types of relations in psycho 
@article{lin2001thematic,
  title={Thematic relations in adults' concepts.},
  author={Lin, Emilie L and Murphy, Gregory L},
  journal={Journal of experimental psychology: General},
  volume={130},
  number={1},
  pages={3},
  year={2001},
  publisher={American Psychological Association}
}

%%%%%%%%% Train embedding from FA data directly  

@article{bel2019wan2vec,
  title={Wan2vec: Embeddings learned on word association norms},
  author={Bel-Enguix, Gemma and G{\'o}mez-Adorno, Helena and Reyes-Maga{\~n}a, Jorge and Sierra, Gerardo},
  journal={Semantic Web},
  number={Preprint},
  pages={1--16},
  publisher={IOS Press},
  year={2019}
}

@inproceedings{gomez2019spanish,
  title={Spanish word embeddings learned on word association norms},
  author={G{\'o}mez-Adorno, Helena and Reyes-Maga{\~n}a, Jorge and Bel-Enguix, Gemma and Martinez, Gerardo Eugenio Sierra},
  booktitle={CEUR Workshop Proceedings},
  volume={2369},
  year={2019}
}

@inproceedings{kiela-etal-2015-specializing,
    title = "Specializing Word Embeddings for Similarity or Relatedness",
    author = "Kiela, Douwe  and
      Hill, Felix  and
      Clark, Stephen",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1242",
    doi = "10.18653/v1/D15-1242",
    pages = "2044--2048",
}
@inproceedings{de-deyne-etal-2016-predicting,
    title = "Predicting human similarity judgments with distributional models: The value of word associations.",
    author = "De Deyne, Simon  and
      Perfors, Amy  and
      Navarro, Daniel J",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/C16-1175",
    pages = "1861--1870",
    abstract = "Most distributional lexico-semantic models derive their representations based on external language resources such as text corpora. In this study, we propose that internal language models, that are more closely aligned to the mental representations of words could provide important insights into cognitive science, including linguistics. Doing so allows us to reflect upon theoretical questions regarding the structure of the mental lexicon, and also puts into perspective a number of assumptions underlying recently proposed distributional text-based models. In particular, we focus on word-embedding models which have been proposed to learn aspects of word meaning in a manner similar to humans. These are contrasted with internal language models derived from a new extensive data set of word associations. Using relatedness and similarity judgments we evaluate these models and find that the word-association-based internal language models consistently outperform current state-of-the art text-based external language models, often with a large margin. These results are not just a performance improvement; they also have implications for our understanding of how distributional knowledge is used by people.",
}

@inproceedings{Korhonen2017EvaluationBA,
  title={Evaluation by Association: A Systematic Study of Quantitative Word Association Evaluation},
  author={Anna Korhonen and Ivan Vulic and Douwe Kiela},
  booktitle={EACL},
  year={2017}
}

%%%%%%%%%%%%%%%%%%% Dictionary based embedding learning %%%%%%%%%%%%%%%%

@inproceedings{tissier2017dict2vec,
  title={Dict2vec: Learning word embeddings using lexical dictionaries},
  author={Tissier, Julien and Gravier, Christophe and Habrard, Amaury},
  year={2017}
}

%%%%%%%%%%%%%%%%% FA as test data %%%%%%%%%%%%

@inproceedings{nematzadeh2017evaluating,
  title={Evaluating Vector-Space Models of Word Representation, or, The Unreasonable Effectiveness of Counting Words Near Other Words.},
  author={Nematzadeh, Aida and Meylan, Stephan C and Griffiths, Thomas L},
  year={2017},
  journal={Cognitive Science}
}

@article{griffiths2007topics,
  title={Topics in semantic representation.},
  author={Griffiths, Thomas L and Steyvers, Mark and Tenenbaum, Joshua B},
  journal={Psychological review},
  volume={114},
  number={2},
  pages={211},
  year={2007},
  publisher={American Psychological Association}
}

%%%%%%%%%%%%%%%%%% LM/embedding %%%%%%%%%%%%%%%%%%%%%

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  year={2019}
}

@inproceedings{mikolov2018advances,
  title={Advances in Pre-Training Distributed Word Representations},
  author={Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin, Armand},
  booktitle={Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)},
  year={2018}
}

 %%%%%%%%%%%%%%%%%%%% bias in word embeddings
@incollection{NIPS2016_man_to_computer,
title = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {4349--4357},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf}
}

@article{caliskan2017semantics,
  title={Semantics derived automatically from language corpora contain human-like biases},
  author={Caliskan, Aylin and Bryson, Joanna J and Narayanan, Arvind},
  journal={Science},
  volume={356},
  number={6334},
  pages={183--186},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{lauscher2019we,
  title={Are We Consistently Biased? Multidimensional Analysis of Biases in Distributional Word Vectors},
  author={Lauscher, Anne and Glava{\v{s}}, Goran},
  booktitle={Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (* SEM 2019)},
  pages={85--91},
  year={2019}
}

@inproceedings{cer2018universal,
  title={Universal sentence encoder for English},
  author={Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and others},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={169--174},
  year={2018}
}

@inproceedings{may2019measuring,
  title={On Measuring Social Biases in Sentence Encoders},
  author={May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel and Rudinger, Rachel},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={622--628},
  year={2019}
}

@article{zhao2019gender,
  title={Gender bias in contextualized word embeddings},
  author={Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Cotterell, Ryan and Ordonez, Vicente and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:1904.03310},
  year={2019}
}


@inproceedings{kiritchenko-mohammad-2018-examining,
    title = "Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems",
    author = "Kiritchenko, Svetlana  and
      Mohammad, Saif",
    booktitle = "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S18-2005",
    doi = "10.18653/v1/S18-2005",
    pages = "43--53",
    abstract = "Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 {`}Affect in Tweets{'}. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.",
}

%%%%%%%%%%%%% direction and structure

@inproceedings{bordes2011learning,
  title={Learning structured embeddings of knowledge bases},
  author={Bordes, Antoine and Weston, Jason and Collobert, Ronan and Bengio, Yoshua},
  booktitle={Twenty-Fifth AAAI Conference on Artificial Intelligence},
  year={2011}
}

@InProceedings{pmlr-v22-bordes12,
  title = 	 {Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing},
  author = 	 {Antoine Bordes and Xavier Glorot and Jason Weston and Yoshua Bengio},
  booktitle = 	 {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {127--135},
  year = 	 {2012},
  editor = 	 {Neil D. Lawrence and Mark Girolami},
  volume = 	 {22},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {La Palma, Canary Islands},
  month = 	 {21--23 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v22/bordes12/bordes12.pdf},
  url = 	 {http://proceedings.mlr.press/v22/bordes12.html},
  abstract = 	 {Open-text semantic parsers are designed to interpret any statement in natural language by inferring a corresponding meaning representation (MR - a formal representation of its sense). Unfortunately, large scale systems cannot be easily machine-learned due to lack of directly supervised data. We propose a method that learns to assign MRs to a wide range of text (using a dictionary of more than 70,000 words mapped to more than 40,000 entities) thanks to a training scheme that combines learning from knowledge bases (e.g. WordNet) with learning from raw text. The model jointly learns representations of words, entities and MRs via a multi-task training process operating on these diverse sources of data. Hence, the system ends up providing methods for knowledge acquisition and word-sense disambiguation within the context of semantic parsing in a single elegant framework. Experiments on these various tasks indicate the promise of the approach.}
}

%%%%%%%%%%%%% multi embeddings

@article{schutze1998automatic,
  title={Automatic word sense discrimination},
  author={Sch{\"u}tze, Hinrich},
  journal={Computational linguistics},
  volume={24},
  number={1},
  pages={97--123},
  year={1998},
  publisher={MIT Press}
}

@inproceedings{reisinger2010multi,
  title={Multi-prototype vector-space models of word meaning},
  author={Reisinger, Joseph and Mooney, Raymond J},
  booktitle={Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={109--117},
  year={2010},
  organization={Association for Computational Linguistics}
}

@inproceedings{HuangEtAl2012,
author = {Eric H. Huang and Richard Socher and Christopher D. Manning and Andrew Y. Ng},
title = {{Improving Word Representations via Global Context and Multiple Word Prototypes}},
booktitle = {Annual Meeting of the Association for Computational Linguistics (ACL)},
year = 2012
}


%%%%%%%%%%%%% improve symmetric embedding 
@inproceedings{song-etal-2018-directional,
    title = "Directional Skip-Gram: Explicitly Distinguishing Left and Right Context for Word Embeddings",
    author = "Song, Yan  and
      Shi, Shuming  and
      Li, Jing  and
      Zhang, Haisong",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/N18-2028",
    pages = "175--180",
    abstract = "In this paper, we present directional skip-gram (DSG), a simple but effective enhancement of the skip-gram model by explicitly distinguishing left and right context in word prediction. In doing so, a direction vector is introduced for each word, whose embedding is thus learned by not only word co-occurrence patterns in its context, but also the directions of its contextual words. Theoretical and empirical studies on complexity illustrate that our model can be trained as efficient as the original skip-gram model, when compared to other extensions of the skip-gram model. Experimental results show that our model outperforms others on different datasets in semantic (word similarity measurement) and syntactic (part-of-speech tagging) evaluations, respectively.",
}

@article{hill2015simlex,
  title={SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation},
  author={Hill, Felix and Reichart, Roi and Korhonen, Anna},
  journal={Computational Linguistics},
  volume={41},
  number={4},
  pages={665--695},
  year={2015}
}

@inproceedings{agirre-etal-2009-study,
    title = "A Study on Similarity and Relatedness Using Distributional and {W}ord{N}et-based Approaches",
    author = "Agirre, Eneko  and
      Alfonseca, Enrique  and
      Hall, Keith  and
      Kravalova, Jana  and
      Pa{\c{s}}ca, Marius  and
      Soroa, Aitor",
    booktitle = "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    month = jun,
    year = "2009",
    address = "Boulder, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N09-1003",
    pages = "19--27",
}

%%%%%%%%%%%%%% Retrofitting embeddings
@article{faruqui2014retrofitting,
  title={Retrofitting word vectors to semantic lexicons},
  author={Faruqui, Manaal and Dodge, Jesse and Jauhar, Sujay K and Dyer, Chris and Hovy, Eduard and Smith, Noah A},
  journal={NAACL},
  year={2015}
}

%%%%%%%%%%%%%% reasoning datasets 

@article{zellers2018swag,
  title={Swag: A large-scale adversarial dataset for grounded commonsense inference},
  author={Zellers, Rowan and Bisk, Yonatan and Schwartz, Roy and Choi, Yejin},
  journal={arXiv preprint arXiv:1808.05326},
  year={2018}
}

@article{talmor2018commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  journal={arXiv preprint arXiv:1811.00937},
  year={2018}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@inproceedings{levesque2012winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning},
  year={2012}
}

%%%%%%%%%%%%%%%%%%%% Graph neural nets 
@article{wu2019comprehensive,
  title={A comprehensive survey on graph neural networks},
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S},
  journal={arXiv preprint arXiv:1901.00596},
  year={2019}
}

@article{hamilton2017representation,
  title={Representation learning on graphs: Methods and applications},
  author={Hamilton, William L and Ying, Rex and Leskovec, Jure},
  journal={IEEE Data Engineering Bulletin, September 2017},
  year={}
}

@article{Sun2019vGraphAG,
  title={vGraph: A Generative Model for Joint Community Detection and Node Representation Learning},
  author={Fan-Yun Sun and Meng Qu and Jordan Hoffmann and Chin-Wei Huang and Jian Tang},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.07159}
}

%%%%%%%%%%%%%%%%%%%% learning algorithms 
@inproceedings{grover2016node2vec,
  title={node2vec: Scalable feature learning for networks},
  author={Grover, Aditya and Leskovec, Jure},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={855--864},
  year={2016},
  organization={ACM}
}

@inproceedings{Epasto:2019:SEE:3308558.3313660,
 author = {Epasto, Alessandro and Perozzi, Bryan},
 title = {Is a Single Embedding Enough? Learning Node Representations That Capture Multiple Social Contexts},
 booktitle = {The World Wide Web Conference},
 series = {WWW '19},
 year = {2019},
 isbn = {978-1-4503-6674-8},
 location = {San Francisco, CA, USA},
 pages = {394--404},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3308558.3313660},
 doi = {10.1145/3308558.3313660},
 acmid = {3313660},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graph embeddings, polysemous representations, representation learning},
} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{brunner2019validity,
  title={On the validity of self-attention as explanation in transformer models},
  author={Brunner, Gino and Liu, Yang and Pascual, Dami{\'a}n and Richter, Oliver and Wattenhofer, Roger},
  journal={arXiv preprint arXiv:1908.04211},
  year={2019}
}

@article{clark2019does,
  title={What Does BERT Look At? An Analysis of BERT's Attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal={arXiv preprint arXiv:1906.04341},
  year={2019}
}

@inproceedings{jawahar2019does,
  title={What Does BERT Learn about the Structure of Language?},
  author={Jawahar, Ganesh and Sagot, Beno{\^\i}t and Seddah, Djam{\'e}},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3651--3657},
  year={2019}
}

@Article{Kacmajor2019,
author="Kacmajor, Magdalena
and Kelleher, John D.",
title="Capturing and measuring thematic relatedness",
journal="Language Resources and Evaluation",
year="2019",
month="Mar",
day="27",
abstract="In this paper we explain the difference between two aspects of semantic relatedness: taxonomic and thematic relations. We notice the lack of evaluation tools for measuring thematic relatedness, identify two datasets that can be recommended as thematic benchmarks, and verify them experimentally. In further experiments, we use these datasets to perform a comprehensive analysis of the performance of an extensive sample of computational models of semantic relatedness, classified according to the sources of information they exploit. We report models that are best at each of the two dimensions of semantic relatedness and those that achieve a good balance between the two.",
issn="1574-0218",
doi="10.1007/s10579-019-09452-w",
url="https://doi.org/10.1007/s10579-019-09452-w"
}


@inproceedings{torabi-asr-etal-2018-querying,
    title = "Querying Word Embeddings for Similarity and Relatedness",
    author = "Torabi Asr, Fatemeh  and
      Zinkov, Robert  and
      Jones, Michael",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1062",
    doi = "10.18653/v1/N18-1062",
    pages = "675--684",
    abstract = "Word embeddings obtained from neural network models such as Word2Vec Skipgram have become popular representations of word meaning and have been evaluated on a variety of word similarity and relatedness norming data. Skipgram generates a set of word and context embeddings, the latter typically discarded after training. We demonstrate the usefulness of context embeddings in predicting asymmetric association between words from a recently published dataset of production norms (Jouravlev {\&} McRae, 2016). Our findings suggest that humans respond with words closer to the cue within the context embedding space (rather than the word embedding space), when asked to generate thematically related words.",
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}}

@misc{Wikiextractor2015,
  author = {Giusepppe Attardi},
  title = {WikiExtractor},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/attardi/wikiextractor}}
}

@article{miller1991contextual,
  title={Contextual correlates of semantic similarity},
  author={Miller, George A and Charles, Walter G},
  journal={Language and cognitive processes},
  volume={6},
  number={1},
  pages={1--28},
  year={1991},
  publisher={Taylor \& Francis}
}

@Book{cc:BrueggerMiligan:2018:SAGE-handbook-web-history,
  title        = "The {SAGE} Handbook of Web History",
  author       = "Brügger, Nils and Milligan, Ian",
  year         = "2019",
  URL          = "https://us.sagepub.com/en-us/nam/the-sage-handbook-of-web-history/book252251",
  publisher    = "SAGE Publications Limited",
  cc-author-affiliation = "Aarhus University, Denmark; University of Waterloo, Canada",
  cc-classes   = "web-science, web history"
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}


@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={ICLR 2020},
  year={2020}
}