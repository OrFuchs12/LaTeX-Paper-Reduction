\relax 
\citation{hochreiter1997long}
\citation{li2018towards}
\citation{li2018towards}
\citation{li2018towards}
\citation{greff2017lstm}
\citation{greff2017lstm}
\citation{harabagiu2004incremental}
\citation{kampffmeyer2019connnet}
\citation{battaglia2018relational}
\providecommand \oddpage@label [2]{}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:intro_example}{{1}{1}{An illustrative example of the input gate, the forget gate, and their correlation for part of a given sentence in sentiment classification datasets. Blue and Red bars denote the positive and negative correlations, respectively. (Left) The new context starts from the word "but", so the negative correlation imposes the large input gate value and the small forget gate value to focus on the current context. (Right) To infer the semantic meaning between the word "not" and "over-priced", the positive correlation occurs at "over-priced", so the positive correlation makes the input gate and the forget gate have large values. The high forget gate value denotes the importance of the previous context in our model.}{}{}}
\newlabel{fig:intro_example@cref}{{[figure][1][]1}{[1][1][]1}}
\citation{gal2016theoretically}
\citation{DBLP:journals/corr/abs-1904-09816}
\citation{chung2015recurrent}
\citation{serban2017hierarchical}
\citation{sru}
\citation{NIPS2018_8253}
\citation{greff2017lstm}
\citation{li2018towards}
\newlabel{fig:CIFGLSTM_cell}{{2a}{2}{CIFG-LSTM}{}{}}
\newlabel{fig:CIFGLSTM_cell@cref}{{[subfigure][1][2]2a}{[1][2][]2}}
\newlabel{sub@fig:CIFGLSTM_cell}{{a}{2}{CIFG-LSTM}{}{}}
\newlabel{sub@fig:CIFGLSTM_cell@cref}{{[subfigure][1][2]2a}{[1][2][]2}}
\newlabel{fig:G2LSTM_cell}{{2b}{2}{$G^{2}$-LSTM}{}{}}
\newlabel{fig:G2LSTM_cell@cref}{{[subfigure][2][2]2b}{[1][2][]2}}
\newlabel{sub@fig:G2LSTM_cell}{{b}{2}{$G^{2}$-LSTM}{}{}}
\newlabel{sub@fig:G2LSTM_cell@cref}{{[subfigure][2][2]2b}{[1][2][]2}}
\newlabel{eq:G2_ct}{{4}{2}{}{}{}}
\newlabel{eq:G2_ct@cref}{{[equation][4][]4}{[1][2][]2}}
\citation{jankowiak2018pathwise}
\citation{OLKIN2003407}
\citation{OLKIN2003407}
\citation{ARNOLD20111194}
\citation{dieng2018noisin}
\newlabel{fig:bBLSTM(5G)_prior_cell}{{3d}{3}{bBeta-LSTM(5G+p)}{}{}}
\newlabel{fig:bBLSTM(5G)_prior_cell@cref}{{[subfigure][4][3]3d}{[1][2][]3}}
\newlabel{sub@fig:bBLSTM(5G)_prior_cell}{{d}{3}{bBeta-LSTM(5G+p)}{}{}}
\newlabel{sub@fig:bBLSTM(5G)_prior_cell@cref}{{[subfigure][4][3]3d}{[1][2][]3}}
\newlabel{fig:our_cell_structure}{{3}{3}{The cell structure of our proposed models. The red and yellow circle denotes a cell state and gates, respectively. The blue circle represents random variables that follow the Gamma distribution. The input and the forget gates in bBeta-LSTM(5G) and bBeta-LSTM(5G+p) shares random variables, $u^{(3)},u^{(4)},u^{(5)}$ and prior $u^{(0)}$.}{}{}}
\newlabel{fig:our_cell_structure@cref}{{[figure][3][]3}{[1][2][]3}}
\newlabel{eq:U}{{7}{3}{}{}{}}
\newlabel{eq:U@cref}{{[equation][7][]7}{[1][3][]3}}
\newlabel{eq:u}{{8}{3}{}{}{}}
\newlabel{eq:u@cref}{{[equation][8][]8}{[1][3][]3}}
\newlabel{eq:3G_input_forget}{{10}{3}{}{}{}}
\newlabel{eq:3G_input_forget@cref}{{[equation][10][]10}{[1][3][]3}}
\newlabel{eq:5G_input}{{11}{3}{}{}{}}
\newlabel{eq:5G_input@cref}{{[equation][11][]11}{[1][3][]3}}
\newlabel{eq:5G_forget}{{12}{3}{}{}{}}
\newlabel{eq:5G_forget@cref}{{[equation][12][]12}{[1][3][]3}}
\citation{VAE2014}
\citation{blei2003latent}
\citation{sru}
\citation{wang2019rtransf}
\citation{cooijmans2016recurrent}
\citation{hdetach2019}
\newlabel{eq:log_marginal_likelihood}{{13}{4}{}{}{}}
\newlabel{eq:log_marginal_likelihood@cref}{{[equation][13][]13}{[1][4][]4}}
\newlabel{eq:ELBO}{{14}{4}{}{}{}}
\newlabel{eq:ELBO@cref}{{[equation][14][]14}{[1][4][]4}}
\newlabel{fig:prior_learning_corr_cr_mr}{{4}{4}{(Left) The sentiment classification accuracy of bBeta-LSTM variants with or without prior learning on a $0_{th}$ fold of CR dataset. (Right) The correlation of bBeta-LSTM(5G+p) on CR dataset.}{}{}}
\newlabel{fig:prior_learning_corr_cr_mr@cref}{{[figure][4][]4}{[1][4][]4}}
\citation{sru}
\newlabel{table:sentence}{{1}{5}{Test accuracies on sentence classification task.}{}{}}
\newlabel{table:sentence@cref}{{[table][1][]1}{[1][4][]5}}
\newlabel{fig:cr_gate_histogram}{{5}{5}{Histogram of input gate value on CR dataset. Our proposed model bBeta-LSTM(5G+p) shows the more flexible gate value than that of other models. CR dataset is used for the sentiment classification, and only a few words are important instead of whole words. As a result, the input gate in all models has a relatively higher portion of 0 than the portion of value 1. The bBeta-LSTM(5G+p) is more likely to have such a tendency, and it leads to better performance of bBeta-LSTM(5G+p) on CR dataset. }{}{}}
\newlabel{fig:cr_gate_histogram@cref}{{[figure][5][]5}{[1][4][]5}}
\citation{wang2019rtransf}
\citation{karpathy2015deep}
\citation{you2016image}
\citation{vinyals2015show}
\citation{xu2015show}
\citation{xu2015show}
\citation{yao2017boosting}
\citation{hdetach2019}
\citation{hdetach2019}
\newlabel{fig:case_study}{{6}{6}{Visualization of input and forget gates for each model and the correlation for bBeta-LSTM(5G+p). The sentence with the negative label is designed for the sentiment classification task, and the "but he losses his focus"$t=22 \sim 26$ is an important part. At time step 22 ("but"), the change of context occurs, and bBeta-LSTM(5G+p) has a large input gate and relatively small forget gate to handle the context change. At time step 25 ("his"), both input gate and forget gate have high values to propagate the information "losses his" efficiently. This is the result of a relatively large correlation value at time step 25, and this correlation helps to propagate the information through the model. }{}{}}
\newlabel{fig:case_study@cref}{{[figure][6][]6}{[1][5][]6}}
\newlabel{table:music}{{2}{6}{Negative log-likelihood on polyphonic music}{}{}}
\newlabel{table:music@cref}{{[table][2][]2}{[1][5][]6}}
\newlabel{table:mnist}{{3}{6}{Test error rates on MNIST}{}{}}
\newlabel{table:mnist@cref}{{[table][3][]3}{[1][5][]6}}
\newlabel{fig:pMNIST_qualitative}{{7}{6}{Average gradient norm, $\|{\frac  {\partial L_{ELBO}}{\partial c_{t}}}\|$ for loss $L_{ELBO}$ over each time step. Beta-LSTM and bBeta-LSTM(5G+p) considers long-term dependency relatively well because they have larger gradients for initial timesteps, (left). bBeta-LSTM(5G+p), which incorporates the prior distribution, shows a relatively stable validation error curve, and shows the lowest validation error (right).}{}{}}
\newlabel{fig:pMNIST_qualitative@cref}{{[figure][7][]7}{[1][6][]6}}
\citation{MSCOCO2014}
\citation{karpathy2015deep}
\citation{vinyals2015show}
\citation{xu2015show}
\citation{hdetach2019}
\newlabel{table:coco_results}{{4}{7}{Test performance on MS-COCO dataset for BLEU, METEOR, CIDEr, ROUGE-L and SPICE evaluation metric.}{}{}}
\newlabel{table:coco_results@cref}{{[table][4][]4}{[1][6][]7}}
\bibdata{BetaLSTM}
\bibstyle{aaai}
\gdef \@abspage@last{8}
