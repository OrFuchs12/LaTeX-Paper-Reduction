%File: anonymous-submission-latex-2024.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[smaller]{acronym}
\usepackage{xspace}
\usepackage{listings, xcolor}

\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{paralist}
\usepackage{fancyvrb}
\usepackage{xr}
\makeatletter


\newcommand*{\addFileDependency}[1]{% argument=file name and extension
\typeout{(#1)}% latexmk will find this if $recorder=0
% however, in that case, it will ignore #1 if it is a .aux or 
% .pdf file etc and it exists! If it doesn't exist, it will appear 
% in the list of dependents regardless)
%
% Write the following if you want it to appear in \listfiles 
% --- although not really necessary and latexmk doesn't use this
%
\@addtofilelist{#1}
%
% latexmk will find this message if #1 doesn't exist (yet)
\IfFileExists{#1}{}{\typeout{No file #1.}}
}\makeatother

\newcommand*{\myexternaldocument}[1]{%
\externaldocument{#1}%
\addFileDependency{#1.tex}%
\addFileDependency{#1.aux}%
}

\myexternaldocument{AAAI-main}


\newfloat{lstfloat}{htbp}{lop}
\floatname{lstfloat}{Listing}

\newcommand{\safe}{\textit{safe}}
\newcommand{\conflict}{\textit{conflict}}
\newcommand{\pre}{\textit{pre}}
\newcommand{\eff}{\textit{eff}}
\newcommand{\add}{\textit{add}}
\newcommand{\del}{\textit{del}}
\newcommand{\post}{\textit{post}}
\newcommand{\fail}{\textit{fail}}
\newcommand{\public}{\textit{public}}
\newcommand{\private}{\textit{private}}
\newcommand{\pol}{\ac{POL}\xspace}
\newcommand{\epol}{\ac{EPOL}\xspace}
\newcommand{\cmapbb}{\ac{CMAP-BB}\xspace}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem{observation}{Observation}

\newcommand{\tuple}[1]{\ensuremath{\left \langle #1 \right \rangle }}
\newcommand{\params}{\textit{params}}
\newcommand{\objects}{\textit{objects}}
\newcommand{\lifted}{\textit{lifted}}
\newcommand{\relevant}{\textit{relevant}}


\newcommand{\name}{\textit{name}}
\newcommand{\type}{\textit{type}}
\newcommand{\cnf}{\textit{CNF}}
\newcommand{\conj}{\textit{Conj}}
\newcommand{\realm}{\ensuremath{M^*}\xspace}
\newcommand{\liftf}{f^L}
\newcommand{\liftl}{l^L}
\newcommand{\lifta}{a^L}
\newcommand{\esam}{\textit{ESAM}\xspace}
\newcommand{\sgam}{\textit{SGAM}\xspace}
\newcommand{\bindings}{\textit{bindings}}
\newcommand{\iseff}{\text{IsEff}}
\newcommand{\ispre}{\text{IsPre}}

\acrodef{NO-OP}{No Operation}
\acrodef{CMAP-BB}{CMAP with Black-Box Agents}
\acrodef{MF-MAP}{Model-Free Multi-Agent Planning}
\acrodef{MA-SAM}{Multi-Agent Safe Action Model Learning}
\acrodef{SAM}{Safe Action Model Learning}
\acrodef{JAT}{Joint Action Trajectory}
\acrodef{LCA}{Lifted Collaborative Action}
% \newcommand{\noop}{\ac{NO-OP}\xspace}
\newcommand{\noop}{\textit{NO-OP}\xspace}
\newcommand{\sam}{\ac{SAM}\xspace}
\newcommand{\masam}{\ac{MA-SAM}\xspace}
\newcommand{\cmasam}{\textit{CMA-SAM}\xspace}
\newcommand{\mfmap}{\ac{MF-MAP}\xspace}
\newcommand{\jat}{\ac{JAT}\xspace}
\newcommand{\blmaa}{\ac{LCA}\xspace}
\newcommand{\blmaas}{\textit{LCAs}\xspace}


\newcommand{\roni}[1]{{\textcolor{red}{[Roni: #1]}}}
\newcommand{\argaman}[1]{{\textcolor{blue}{[Argaman: #1]}}}
\newcommand{\brendan}[1]{{\textcolor{orange}{[Brendan: #1]}}}


%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Safe Learning of Multi-Agent Action Models from Concurrent Observations - Supplementary Material}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,\\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz\equalcontrib,
    Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    1900 Embarcadero Road, Suite 101\\
    Palo Alto, California 94303-3310 USA\\
    % email address must be in roman text type, not monospace or sans serif
    proceedings-questions@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\section{Safety Proof}
\begin{observation}
%Let $M'=\{M'_i\}_{i=1}^k$ and $M=\{M_i\}_{i=1}^k$ be two multi-agent action models. An action model $M'$ is safe w.r.t $M$ if and only if for every agent $i$ it holds that $A'_i$ is safe w.r.t $A_i$.
A multi-agent action model $M'=\{M'_i\}_{i=1}^k$ is safe w.r.t $M=\{M_i\}_{i=1}^k$ iff for every agent $i$ it holds that $M'_i$ is safe w.r.t $M_i$.
\end{observation}
\begin{proof}
If $M$ is safe w.r.t $M'$, then it must satisfy the following equation for each state $s$:
\begin{equation}
\small
    \pre_{M'}(a)\subseteq s \rightarrow 
    \Big(\pre_M(a)\subseteq s \wedge 
    a_{M'}(s)=a_M(s)\Big)
    \label{eq:safe_action_model}
\end{equation}
The above holds for every joint action, including
joint actions in which only one agent acts and the rest of the actions are \noop. Thus, the safety of the single-agent action models is guaranteed. 
If the single-agent action models are safe, then for every joint action, the union of the preconditions of its constituent single-agent action according to $M'$ is a (possibly not strict) superset of the preconditions according to $M$. A similar argument applies to the effects as well.
\end{proof} 

\section{MA-SAM Theoretical Properties}
\begin{theorem}
\label{lemma:ma-sam}
The action model learned using \masam is safe w.r.t.\  the real action model (\realm), and the runtime is linear in the number of possible parameter bound literals and quadratic in the number of agents and action triplets. 
\end{theorem}
\begin{proof}
The learned preconditions of each action are a superset of their actual preconditions. Each single agent action learned by \masam can be safely applied with respect to the learned preconditions and effects. 
Thus, under the learned preconditions, we know for every literal whether applying the action adds it to the following state. 
Consequently, an action model containing the single agent actions is guaranteed to be safe with respect to \realm. 
In terms of runtime, \masam iterates over all action triplets and applies the inductive rules in Definition~\ref{def:lifted-ma-sam}. 
The time complexity of applying these rules on a given action triplet is linear in the number of agents and literals. 
The number of clauses in each CNF is at most linear in the number of agents and action triplets. 
Applying unit propagation on a CNF is linear in the total formula size. 
Thus, minimizing our CNFs is linear in the number of literals and quadratic in the number of agents and action triplets. 
Finally, computing the effects of the safe actions requires linear time in the number of actions, literals, and CNF clauses. 
Thus, the complexity of \masam is a low-order polynomial in the number of agents, actions, literals, and action triplets. 
\end{proof}

% \argaman{fix links between papers}

\section{Sample Complexity}
Let $D$ be a probability distribution over solvable MA-STRIPS problems in a given domain, 
and let $T(\Pi)$ be a probability distribution over trajectories corresponding to plans that solve 
a MA-STRIPS problem $\Pi$.  
$T(\Pi)$ may be produced, for example, by running a sound and complete planner on $\Pi$. 
For a given \mfmap problem $\tuple{\Pi, \mathcal{T}}$, we assume that $\Pi$ is obtained by sampling from $D$, 
and $\mathcal{T}$ is obtained by sampling $m$ problems $\Pi_1,\ldots,\Pi_m$ independently from $D$, and then sampling a single trajectory from every $T(\Pi_i)$, $i\in\{1,\ldots, m\}$. 

\begin{definition}[Sample complexity]
The sample complexity of an algorithm $X$ solving a \mfmap problem for a given $\epsilon,\delta\in (0,1)$ is defined as the minimum number of trajectories required to ensure that for any distributions $D$ and $T(\cdot)$, with probability at least $1-\delta$ the safe action model returned by $X$ enables solving $\Pi$ with probability at least $1-\epsilon$.  
\end{definition}
By ``$X$ enables solving $\Pi$'', we mean that there exists a solution to $\Pi$ that can be found with a complete planner using the action model returned by $X$. 
Based on Theorem 5 in~\cite{juba2021safe}, the sample complexity of SAM learning is
linear in the number of actions and state variables. 
In our context, the number of joint actions is exponential in the number of agents yielding an  upper bound on the sample complexity that is exponential in the number of agents as well. 
Unfortunately, the sample complexity can indeed be exponential in the number of agents that participate in individual concurrent actions in the training set:

%\paragraph{Sample Complexity}
%We turn to studying the sample complexity of solving \mfmap problems. We extend the learning formulation of \cite{stern2017efficientAndSafe} to the multi-agent setting as follows.
%\begin{definition}[Sample complexity of \mfmap]
%Fix a domain and suppose that there is a probability distribution $D$ on solvable MA-STRIPS problems in that domain. For each such problem $\Pi$ in $D$, we further suppose that there is a probability distribution $T(\Pi)$ on trajectories solving $\Pi$, for example, as would be produced by running a sound and complete planner on $\Pi$. Then for any given $m$, we can consider the \mfmap problem with a \emph{training set} of size $m$ obtained by sampling $m$ problems $\Pi_1,\ldots,\Pi_m$ independently from $D$, and putting $\mathcal{T}$ equal to the set of samples $T(\Pi_1),\ldots,T(\Pi_m)$, and then letting the \emph{test problem} $\Pi$ be sampled independently from $D$. The \emph{sample complexity} for a given $\epsilon,\delta\in (0,1)$ is the minimum size of a training set so that for some algorithm, for any distributions $D$ and $T(\cdot)$, with probability at least $1-\delta$ over the sampling of the training set, the obtained training set is adequate for the algorithm to produce a safe action model for the domain that solves the test problem with probability at least $1-\epsilon$.
%\end{definition}



\begin{theorem}
For all integers $A\geq 2$, $\kappa$, $k$, and $F\geq 2Ak$, and real numbers $\epsilon,\delta < 1/4$ there is a family of domains with $k$ agents, each with $A$ actions, and $F$ propositional fluents such that for distributions $T(\cdot)$ supported on trajectories in which at most $\kappa$ agents participate in each action (i.e., the other $k-\kappa$ take \noop), the sample complexity for $\epsilon$ and $\delta$ is at least $\Omega\left(\frac{1}{\epsilon}(FA^{\kappa}{k\choose\kappa}+\log\frac{1}{\delta})\right)$.
\end{theorem}

%The proof requires Stirling's formula (cf.\  \citet{robbins1955}):
%\begin{equation}
%    \sqrt{2\pi n}e^{n\ln n-n+\frac{1}{12n+1}} < n! < \sqrt{2\pi n}e^{n\ln n-n+\frac{1}{12n}}
%\end{equation}

\begin{proof}
The domains are as follows. For each agent $i=1,\ldots,k$, there is a set of $A$ fluents for agent $i$ such that for each $j$th fluent, one of the actions has that fluent as an effect, and this is the only action with that fluent as an effect. There is an additional set of $F-Ak$ fluents which we will refer to as {\em flags}. All of the actions have all of the flags as effects. The actions have no other effects.

Now consider the following distributions on problems and trajectories. The initial states of problems $\Pi$ in $D$ have all of the action fluents false, and all but one of the flags (uniformly at random) true. With probability $1-4\epsilon$, the goal is empty, and otherwise the goals specify that $\kappa$ of the fluents, each corresponding to distinct agents and chosen uniformly at random from such sets should be true, all of the other agent-action fluents should be false, and all of the flags should be true. $T(\Pi)$ then consists of trajectories with a single concurrent action; for the empty goal, all agents take \noop actions, and otherwise the set of $\kappa$ agents corresponding to the goal fluents take the actions corresponding to those fluents.

To obtain a success rate of $1-\epsilon$ on such problems, the safe action model must permit plans in which only the agents corresponding to the goal fluents act, and such that after the first action in which some of the agents take actions other than \noop, the state of the flag is known. Indeed, on the nonempty goals (which occur with probability $4\epsilon$) such actions must exist with probability at least $3/4$, or else the failure rate will exceed $\frac{1}{4}4\epsilon=\epsilon$. Since the action model is assumed to be safe, this means that $\mathcal{T}$, together with the MA-STRIPS rules, either entails that  at least one of the actions in that joint action has the flag as an effect, or entails that none of them have the flag as an effect. We note that the latter is impossible: since all of the training examples have all flags true in the post-states, the examples are certainly consistent with an action model in which all actions have the flags as an effect, so no safe action model can conclude that any flag will not be an effect.

On the other hand, consider any of the nonempty goals; if no example trajectory has included the joint action corresponding to precisely the set of $\kappa$ agent-action pairs mentioned in the goal with the relevant flag false in the pre-state, since these trajectories are either {\noop}s, or mention a different flag, or include at least one agent/action pair outside this set, the trajectories are consistent with an action model in which the relevant $\kappa$ agent-action pairs do not have the relevant flag as an effect, but all other agent-action pairs do have this flag as an effect (and all other flags are effects of all actions). Thus, we cannot determine the state of that flag after taking a joint action with any subset of these $\kappa$ actions. Since taking any action outside this set will set an agent-action fluent to true that should be false, and there is no way to set these fluents false, the goal cannot be achieved following any such action.

In summary, nonempty goals cannot be achieved unless we have previously observed an example trajectory in which the joint action consists of the $\kappa$ relevant agent-action pairs, with the relevant flag false in the pre-state. There are at least $F/2$ flags and $A^\kappa{k\choose \kappa}$ such joint actions, which we observe uniformly at random among the non-empty goals. The probability that we fail to observe such an example among $\frac{1}{8}FA^\kappa{k\choose \kappa}$ of the trajectories with non-empty goals is at least
\[
\left(1-\frac{2}{FA^\kappa{k\choose \kappa}}\right)^{\frac{1}{8}FA^\kappa{k\choose \kappa}}> 1/2
\]
since $1-x+x^2>e^{-x}$ for positive $x$ and $k,\kappa\geq 1$, $A\geq 2$, and $F\geq 4$. The expected number of such goals, in turn, is therefore at least $1/2$ of them by the linearity of expectation; the probability that more than $3/4$ are observed is less than $2/3$ by Markov's inequality. In turn, observing that the number of examples with non-empty goals out of $\frac{1}{16\epsilon}FA^{\kappa}{k\choose\kappa}$ exceeds $\frac{1}{8}FA^\kappa{k\choose \kappa}$ with probability at most $1/2$ by Markov's inequality, we see that if the training set includes fewer than $\frac{1}{16\epsilon}FA^{\kappa}{k\choose\kappa}$ examples, with probability greater than $1/2\cdot 2/3>\delta$ over the training set, any safe action model for the training set must fail to achieve the goal with probability greater than $\epsilon$. 

Furthermore, we observe that a training set of at least $\frac{1}{\epsilon}\ln\frac{1}{\delta}$ examples are also necessary, since if we draw fewer than this many examples, the probability that we never observe an example with a nonempty goal is at least $(1-8\epsilon)^{\frac{1}{\epsilon}\ln\frac{1}{\delta}}>e^{-\ln\frac{1}{\delta}}=\delta$. In such a case, no safe action model can take any actions, thus suffering a failure rate of at least $8\epsilon$. As $(x+y)/2\leq\max(x,y)$, we can therefore conclude that more than $\frac{1}{32\epsilon}(FA^{\kappa}{k\choose\kappa}+\ln\frac{1}{\delta})$ examples are necessary.
\end{proof}

%We remark that this bound is attained by a trivial reduction that treats each joint action as a distinct single-agent action, and uses the single-agent action model learning algorithm from prior work~\cite{stern2017efficientAndSafe,juba2021safe}. Indeed, furthermore, it shows that this algorithm has optimal worst-case sample complexity.

We can also interpret the lower bound construction in term of concurrency in a relational domain \cite{crosby2014single}. Suppose instead of propositional fluents, we have unary predicates in a domain with a single object. We then immediately obtain the following:
\begin{corollary}
\label{corollary:conflicting-actions-complexity}
For all integers $A\geq 2$, $\kappa$, $k$, and $F\geq 2Ak$, and real numbers $\epsilon,\delta < 1/4$ there is a family of domains with $k$ agents, each with $A$ actions, and $F$ relational fluents such that for distributions $T(\cdot)$ supported on trajectories in which at most $\kappa$ agents participate in actions involving any single object at a time, the sample complexity for $\epsilon$ and $\delta$ is at least $\Omega\left(\frac{1}{\epsilon}(FA^{\kappa}{k\choose\kappa}+\log\frac{1}{\delta})\right)$.
\end{corollary}




%%
%% The simplified version from the ICAPS submission appears below.
%%

% Cannot do any better
%\begin{theorem}
%The sample complexity of any \mfmap algorithm for $\epsilon,\delta < 1/4$ is at least
%$\Omega\left(\frac{1}{\epsilon}(|P|\prod_{i=1}^k |A_i|+\log\frac{1}{\delta})\right)$.
%\end{theorem}
%\begin{proof}
%For any $p\geq 2\prod_{i=1}^k |A_i|$, consider a domain in which for each agent $i=1,\ldots,k$ and action $a_j\in A_i$ 
%there if a fluent $f_{i,j}$ that is the effect of exactly one action. 
%The domain includes an additional set of $p-\prod_{i=1}^k |A_i|$ fluents (so $|P|=p$) and every action in the domain has all these fluents as an effect. We refer to the first type of fluents as the \emph{goal fluents} and the second type as the \emph{flag fluents}. 


%Now consider the following distributions on problems and trajectories. 
%The initial state of every problem sampled from $D$ has all goal fluents set to false and all but one of the flag fluents (uniformly at random) true. 
%With probability $1-4\epsilon$, the goal is empty.
%Otherwise, the goal includes setting all flag fluents to true, as well as an additional $k$ goal fluents, one per agent, chosen uniformly at random, that should be set to true. All other goal fluents must be set to false. 
%$T(\Pi)$ consists of trajectories with a single joint action; for the empty goal, all agents take \noop actions, and otherwise the $k$ agents take the actions corresponding to the $f_{i,j}$ goal fluents to be set true in the goal. 

%For any problem with a non-empty goal that we did not observe in the training set, the action model in which the corresponding actions do not set the missing flag to true and otherwise all actions set all flags to true is consistent with the training set. 
%Indeed, either the set of actions appears with a different flag to be set true (which it is, following those actions) or else the joint action includes at least one action distinct from the set of $k$ we need to achieve this goal, and then the flag would be set to true by that action. Therefore, no safe action model can permit taking the joint action needed to achieve the goal, and all other actions would reach a state in which some incorrect goal fluents are set to true and cannot be subsequently set to false. 

%Since the \noop goal only comprises $1-4\epsilon$ probability in the goal distribution, we need to observe at least a $3/4$ fraction of the possible goals for a safe action model to attain probability $1-\epsilon.$ But, there are $|P|\prod_{i=1}^k |A_i|$ goals, and in expectation, a sample of size $m$ only contains $4\epsilon m$ examples of these goals. We, therefore, need $\Omega\left(\frac{1}{\epsilon}|P|\prod_{i=1}^k |A_i|\right)$ examples; likewise, to even observe any of the nonempty goals with probability $1-\delta$, we need $\Omega\left(\frac{1}{\epsilon}\log\frac{1}{\delta}\right)$ examples, giving the claimed bound.
%\end{proof}
%Limitations on the allowed concurrency may reduce, to some extent, the sample complexity. 
%For example, if we assume that at most $\kappa<k $ agents can act concurrently, 
%then sample complexity reduces by replacing $k$ with ${\kappa}{k\choose\kappa}$. 

\section{Additional Experimental Actions}  

\begin{figure}[ht]
\begin{center}
\begingroup
    \fontsize{8pt}{8pt}\selectfont
\begin{Verbatim}[commandchars=\\\{\}]
(:action dummy-add-predicate-action
    :parameters (?agent - object)
    :precondition (and)
    :effect (and (dummy-additional-predicate)
    )
)

(:action dummy-del-predicate-action
    :parameters (?agent - object)
    :precondition (and)
    :effect (and (not (dummy-additional-predicate))
    )
)
\end{Verbatim}
\endgroup
\caption{The new actions added to each agent's domain. The actions receive the agent as a parameter and add or delete the new predicate.}
\label{list:new-actions-to-domain}
\end{center}
\end{figure}

In our experiments, we observed that the plans generated by the multi-agent planners resulted in mostly trivial joint actions. 
To challenge the learning algorithms, we added two new actions aiming that they will be executed concurrently with the original plan's actions. 
The added actions are presented in Figure~\ref{list:new-actions-to-domain}. We note that the added actions do not change the internal logic of the original plans as they do not affect any of the domain's fluents and only add or delete the newly created fluent $(dummy-additional-predicate)$.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Previous version w/o dependence on A etc. below

%\begin{theorem}
%For all sufficiently large even integers  $\kappa$ and $k\geq 16\kappa^2$, there is a family of domains with $k$ agents, each with two actions, and $k+1$ propositional fluents such that for some $\epsilon,\delta >0$, for distributions $T(\cdot)$ supported on trajectories in which at most $\kappa$ agents participate in each action (i.e., the other $k-\kappa$ take \noop), the sample complexity is at least $2^{\Omega(\kappa)}$.
%\end{theorem}
%
%The proof requires Stirling's formula (cf.\  \cite{robbins1955}):
%\begin{equation}
%    \sqrt{2\pi n}e^{n\ln n-n+\frac{1}{12n+1}} < n! < \sqrt{2\pi n}e^{n\ln n-n+\frac{1}{12n}}
%\end{equation}
%
%\begin{proof}
%The domains are as follows. For each agent $i=1,\ldots,k$, there is a fluent for agent $i$ such that both of the actions have that fluent as an effect, and these are the only actions with that fluent as an effect. One of these actions also has the final fluent, which we will refer to as the \emph{flag} as an effect, and the other does not. The actions have no other effects.
%
%Now consider the following distributions on problems and trajectories. The initial states of problems $\Pi$ in $D$ have all fluents false, and the goals specify that $\kappa$ of the fluents, each corresponding to distinct agents, should be true, as well as the flag. $T(\Pi)$ then consists of trajectories with a single concurrent action, in which the set of $\kappa$ agents corresponding to the goal fluents take actions from the following distribution: each agent chooses uniformly at random from its two actions, conditioned on at least one of the $\kappa$ agents choosing the action with the flag as an effect.
%
%To solve such problems, the safe action model must permit plans in which only the agents corresponding to the goal fluents act, and such that after the first action in which some of the agents take actions other than \noop, the state of the flag is known. Since the action model is assumed to be safe, this means that $\mathcal{T}$, together with the MA-STRIPS rules, either entails that  at least one of the actions in that joint action has the flag as an effect, or entails that none of them have the flag as an effect. We note that the latter is impossible: since all of the training examples have the flag as an effect, the examples are certainly consistent with an action model in which all actions have the flag as an effect, so no safe action model can conclude that the flag will not be an effect. We will show that the number of examples necessary to make such inferences with probability greater than $(4\kappa)^{-\kappa/4}$ is at least $2^{\kappa/2}$.
%
%We'll first upper bound the probability that the training set entails that for some fixed choice of actions for the $\kappa$ agents involved in the given goal, that at least one of these actions has the flag as an effect. Note that since any subset of these having the flag as an effect entails that the full set has this effect, this upper bounds the probability for all subsets of these actions as well. Furthermore, since the test goal is drawn independently from the training set, this will give a bound on the probability that the agent has any safe action that can possibly achieve the test goal.
%
%Indeed, we can bound the probability that in any of the training examples, more than $\kappa/2$ of the fluents in the goal are set as follows. Observe that if we consider the events corresponding to a fixed set of $\kappa/2$ agents being the first $\kappa/2$ (in some fixed ordering) out of the $\kappa$ agents participating in the example's action, these events cover the event that some subset of at least $\kappa/2$ of these agents participate. There are ${\kappa\choose \kappa/2}$ of these events and the probability of each of these events is, in turn, at most $\frac{{k-\kappa/2\choose \kappa/2}}{{k\choose \kappa}}$. Hence, the total probability is at most
%\begin{align*}
%    {\kappa\choose \kappa/2}\frac{{k-\kappa/2\choose \kappa/2}}{{k\choose \kappa}} &=
%    \frac{\kappa!(k-\kappa/2)!\kappa!(k-\kappa)!}{(\kappa/2)!^3(k-\kappa)!k!}
%    \end{align*}
%    Which by Stirling's formula, is at most
%\begin{align*}
%\sqrt{\frac{2k-\kappa}{\pi\kappa k}}&\exp(2\kappa \ln\kappa -2\kappa +\frac{1}{6\kappa}\\
%+&(k-\frac{\kappa}{2})\ln(k-\frac{\kappa}{2})-(k-\frac{\kappa}{2})+\frac{1}{12k-6\kappa}\\
%-&\frac{3}{2}\kappa\ln\frac{\kappa}{2}+\frac{3}{2}\kappa-\frac{3}{6\kappa+1}\\
%-&k \ln k +k -\frac{1}{12k+1})
%\end{align*}
%or, collecting terms and rewriting in asymptotic notation,
%\begin{align*}
%    \exp(&k\ln(1-\frac{\kappa}{2k})
%   -\frac{\kappa}{2}\ln\frac{k-\kappa/2}{8\kappa}
%    -\frac{1}{2}\ln\frac{\pi\kappa k}{2k-\kappa}
%    +O(\frac{1}{k})).
%\end{align*}
%Now using the fact that $\ln(1+x)\leq x$ for all $x$ and our relation $k\geq 16\kappa^2$, for sufficiently large $k$ this is at most
%\begin{align*}
%    \exp(-\frac{\kappa}{2}\ln\frac{k-\kappa/2}{(8/e)\kappa}+O(1/k))<\left(\frac{1}{8\kappa}\right)^{\kappa/2}.
%\end{align*}
%There are at most $2^\kappa=4^{\kappa/2}$ such sets of actions with these $\kappa$ effects, so the probability that we draw any of them is at most $\frac{1}{(2\kappa)^{\kappa/2}}$. In particular, the probability that no trajectory out of $\kappa^{\kappa/4}$ draws has such an example is at least
%\[
%(1-\frac{1}{(2\kappa)^{\kappa/2}})^{\kappa^{\kappa/4}} > 1/e^{(4\kappa)^{-\kappa/4}}>1-(4\kappa)^{-\kappa/4}.
%\]
%
%Suppose that we have no such examples in the training set. Note that the training set entails that the joint action $\hat{a}$ has the flag as an effect iff assuming that none of the actions in $\hat{a}$ has the flag as an effect leads to a contradiction. But, at most $\kappa/2$ out of the $\kappa$ actions in each joint action in the training set appear in $\hat{a}$; each example then (only) asserts that one of the (at least) $\kappa/2$ other actions in the joint action must have the flag as an effect. Thus, each example only rules out $2^{2k-\kappa/2}$ out of the $2^{2k}$ possible sets of effects on the flag for the $2k$ actions. Therefore at least $2^{\kappa/2}$ examples are necessary to entail that some joint action $\hat{a}$ has the flag as an effect for this goal.
%\end{proof}


%% Old version of lower bound with a parameter for A recorded below


%\begin{theorem}
%For all even integers $A$, $\kappa$, and $k\geq 16\kappa^2$, there is a family of domains with $k$ agents, each with $A$ actions, and $k(A/2)+1$ propositional fluents such that for some $\epsilon,\delta >0$, for distributions $T(\cdot)$ supported on trajectories in which at most $\kappa$ agents participate in each action (i.e., the other $k-\kappa$ take \noop), the sample complexity is at least $(kA)^{\Omega(\kappa)}$.
%\end{theorem}
%
%The proof requires Stirling's formula (cf.\  \cite{robbins1955}):
%\begin{equation}
%    \sqrt{2\pi n}e^{n\ln n-n+\frac{1}{12n+1}} < n! < \sqrt{2\pi n}e^{n\ln n-n+\frac{1}{12n}}
%\end{equation}
%
%\begin{proof}
%The domains are as follows. For each agent $i=1,\ldots,k$, there is a set of $A/2$ fluents for agent $i$ such that for each $j$th fluent, two of the actions have that fluent as an effect, and these are the only actions with that fluent as an effect. One of these actions also has the final fluent, which we will refer to as the \emph{flag} as an effect, and the other does not. The actions have no other effects.
%
%Now consider the following distributions on problems and trajectories. The initial states of problems $\Pi$ in $D$ have all fluents false, and the goals specify that $\kappa$ of the fluents, each corresponding to distinct agents, should be true, as well as the flag. $T(\Pi)$ then consists of trajectories with a single concurrent action, in which the set of $\kappa$ agents corresponding to the goal fluents take actions from the following distribution: each agent chooses uniformly at random from its two actions, conditioned on at least one of the $\kappa$ agents choosing the action with the flag as an effect.
%
%To solve such problems, the safe action model must permit plans in which only the agents corresponding to the goal fluents act, and such that after the first action in which some of the agents take actions other than \noop, the state of the flag is known. Since the action model is assumed to be safe, this means that $\mathcal{T}$, together with the MA-STRIPS rules, either entails that  at least one of the actions in that joint action has the flag as an effect, or entails that none of them have the flag as an effect. We note that the latter is impossible: since all of the training examples have the flag as an effect, the examples are certainly consistent with an action model in which all actions have the flag as an effect, so no safe action model can conclude that the flag will not be an effect. We will show that the number of examples necessary to make such inferences with probability greater than (...) is at least (...).
%
%We'll first upper bound the probability that the training set entails that for some fixed choice of actions for the $\kappa$ agents involved in the given goal, that at least one of these actions has the flag as an effect. Note that since any subset of these having the flag as an effect entails that the full set has this effect, this upper bounds the probability for all subsets of these actions as well.
%
%Indeed, we can bound the probability that in any of the training examples, more than $\kappa/2$ of the fluents in the goal are set as follows. Observe that if we consider the events corresponding to a fixed set of $\kappa/2$ agents being the first $\kappa/2$ (in some fixed ordering) out of the $\kappa$ agents participating in the example's action, these events cover the event that some subset of at least $\kappa/2$ of these agents participate. There are ${\kappa\choose \kappa/2}$ of these events and the probability of each of these events is, in turn, at most $\left(\frac{2}{A}\right)^{\frac{\kappa}{2}}\frac{{k-\kappa/2\choose \kappa/2}}{{k\choose \kappa}}$. Hence, the total probability is at most
%\begin{align*}
%    {\kappa\choose \kappa/2}\left(\frac{2}{A}\right)^{\frac{\kappa}{2}}\frac{{k-\kappa/2\choose \kappa/2}}{{k\choose \kappa}} &=
%    \left(\frac{2}{A}\right)^{\frac{\kappa}{2}}\frac{\kappa!(k-\kappa/2)!\kappa!(k-\kappa)!}{(\kappa/2)!^3(k-\kappa)!k!}
%    \end{align*}
%    Which by Stirling's formula, is at most
%\begin{align*}
%\left(\frac{2}{A}\right)^{\frac{\kappa}{2}}\sqrt{\frac{2k-\kappa}{\pi\kappa k}}&\exp(2\kappa \ln\kappa -2\kappa +\frac{1}{6\kappa}\\
%+&(k-\frac{\kappa}{2})\ln(k-\frac{\kappa}{2})-(k-\frac{\kappa}{2})+\frac{1}{12k-6\kappa}\\
%-&\frac{3}{2}\kappa\ln\frac{\kappa}{2}+\frac{3}{2}\kappa-\frac{3}{6\kappa+1}\\
%-&k \ln k +k %-\frac{1}{12k+1})
%\end{align*}
%or, collecting terms and rewriting in asymptotic notation,
%\begin{align*}
%    \left(\frac{2}{A}\right)^{\frac{\kappa}{2}}\exp(&k\ln(1-\frac{\kappa}{2k})
%    -\frac{\kappa}{2}\ln\frac{k-\kappa/2}{8\kappa}\\
%    &-\frac{1}{2}\ln\frac{\pi\kappa k}{2k-\kappa}
%    +O(1/k)).
%\end{align*}
%Now using the fact that $\ln(1+x)\leq x$ for all $x$ and our relation $k\geq 16\kappa^2$, for sufficiently large $k$ this is at most
%\begin{align*}
%    \left(\frac{2}{A}\right)^{\frac{\kappa}{2}}\exp(-\frac{\kappa}{2}\ln\frac{k-\kappa/2}{(8/e)\kappa}+O(1/k))<\left(\frac{1}{4A\kappa}\right)^{\kappa/2}.
%\end{align*}
%There are at most $2^\kappa=4^{\kappa/2}$ such sets of actions with these $\kappa$ effects, so the probability that we draw any of them is at most $\frac{1}{(A\kappa)^{\kappa/2}}$. In particular, the probability that no trajectory out of $(A\kappa)^{\kappa/2}$ draws has such an example is at least
%\[
%(1-\frac{1}{(A\kappa)^{\kappa/2}})^{(A\kappa)^{\kappa/2}} > 1/3.
%\]
%
%Suppose that we have no such examples in the training set. Note that the training set entails that the joint action $\hat{a}$ has the flag as an effect iff assuming that none of the actions in $\hat{a}$ has the flag as an effect leads to a contradiction. But, at most $\kappa/2$ out of the $\kappa$ actions in each joint action in the training set appear in $\hat{a}$; each example then (only) asserts that one of the (at least) $\kappa/2$ other actions in the joint action must have the flag as an effect. Thus, each example only rules out $2^{2\kappa-\kappa/2}$ out of the $2^{2\kappa}$ possible sets of effects on the flag for these $2\kappa$ actions. Therefore at least $2^{\kappa/2}$ examples are necessary to entail that some joint action $\hat{a}$ has the flag as an effect. (oops-- missing the A\kappa -- maybe put $A^{1/4}$ actions per fluent, $A^{3/4}$ fluents)
%\end{proof}




% statistics on the joint actions.
% # compile domains 
% # create test to see that we can compile a plan with two actions interacting with the same literal VVVV
% # create baseline MA-SAM
% # write paper.


% l1(o11,o12)
% l2(o21,o22)
% a(ax?,ay?,az?,aw?)
% b(bx?,by?)
% c(cx?,cy?)

% pre: {}
% transitions:
% {a(o11,o12,o21,o22), 
%  b(o11, o12), 
%  c(o21,o22)}
% post: { l1(o11,o12), l2(o21, o22) }


% CNF_l1 = [(a, l1(ax?,ay?) or (b, l1(bx?, by?))]
% CNF_l2 = [(a, l2(aw?,az?) or (c, l2(cx?, cy?))]
        
% CNF_l1(lx?,ly?) = 
%     [(a, ax?:lx?, ay?:ly?) or 
%      (b, bx?:lx?, by?:ly?))]
% CNF_l2 = [(a, l2(aw?,az?) or (c, l2(cx?, cy?))]

% Bounded lifted multi-agent action
% (a, b)[ax?,ay?,az?,aw?]
% {ax?:bx?, ay?:by?}


% Bounded lifted multi-agent action
% (a, b, c)[ax?,ay?,az?,aw?]
% {ax?:bx?, ay?:by?, az:?cx, aw?:cy?}

% Can_Infer_Literal_Value(BLMAA, CNF_l)
%   For each clause C in CNF_l
%       if all actions in C are in BLMAA and
%          for every parameter lp? in params(l)
%             equiv_set <- emptyset
%             for every action a in C that include lp?
%                  ap?<- the param of a bound to lp? 
%                  add ap? to equiv_set
%             if all action parameters in equiv_set are not bound to each other according to the BLMAA
%                 go to next clause
%          return true!
         
% Safe joint action?

% (a,b,c)[ax?,ay?,az?,aw?] = 
% {  }


%% The file kr.bst is a bibliography style file for BibTeX 0.99c
% \bibliography{aaai24}

\end{document}
