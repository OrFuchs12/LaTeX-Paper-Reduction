conflict and their effects also do not conflict. 
% the preconditions 
% ~\cite{crosby2014single}, which means 
% Formally, a set of literals $L$ is said to be \emph{well-defined} if no fluent $f\in P$ exists, such as $f\in L$ and $\neg f\in L$. 
% Thus, joint action $\hat{a}$ is said to be \emph{well-defined} if its preconditions and effects are composed of well-defined literals and the joint action does not violate the concurrency constraints.


% %
% In some domains, actions may not be independent and behave differently when executed concurrently. Boutilier and Brafman~\shortcite{boutilier2001partial} extended STRIPS to support interacting actions, and their extension was then formalized into an extension of PDDL 3.1~\shortcite{kovacs2012multi}. 


% ill-defined in some cases, e.g., when actions have incompatible effects~\cite{blum1997fast}.  considered two actions as interfering (or conflicting) if one deletes a precondition or an added effect of the other.

% %Boutilier and Brafman~
% Boutilier and Brafman~\shortcite{boutilier2001partial} extended STRIPS to support interacting actions, and their extension was then formalized into an extension of PDDL 3.1~\shortcite{kovacs2012multi}. 



% In this work, we limit our attention to cases where the preconditions and effects of concurrently executed single-agent actions are the union of the preconditions and effects of these single-agent actions~\shortcite{crosby2014single}. 
% Formally, a set of literals $L$ is said to be \emph{well-defined} if no fluent $f\in P$ exists, such as $f\in L$ and $\neg f\in L$. 
% Thus, joint action $\hat{a}$ is said to be \emph{well-defined} if its preconditions and effects are composed of well-defined literals and the joint action does not violate the concurrency constraints.



% %Boutilier and Brafman~
% Boutilier and Brafman~\shortcite{boutilier2001partial} extended STRIPS to support interacting actions, and their extension was then formalized into an extension of PDDL 3.1~\shortcite{kovacs2012multi}. 
% \cite{shekhar2020representing} formalized the notion of a \textit{collaborative action}, which is a set of single-agent actions whose effects, when performed concurrently, may differ from the union of the single-agent actions' effect comprising it. For example, the action $2push(a1, a2, b)$ presented in~\shortcite{shekhar2020representing} has the agents $a1$ and $a2$ push the box $b$. If each agent tries to push the box by itself, it won't move since it is too heavy. The box will only move if the agents push it simultaneously.

% Concurrent actions may be ill-defined in some cases. For example, when a subset of actions in a joint action have incompatible effects, \cite{blum1997fast} considered two actions as interfering (or conflicting) if one deletes a precondition or an added effect of the other. 
% To make sure that the joint effect remains consistent and that actions will not conflict, \cite{crosby2014single} extended the PDDL language to include the notion of a \emph{concurrency constraint} as defined by Boutilier and Brafman~\shortcite{boutilier2001partial}. There are two types of concurrency constraints, positive and negative. The first states that there are sets of single-agent actions that must be executed concurrently, and the second indicates that some actions cannot be executed concurrently. 






% Sample complexity

% \begin{definition}[Sample complexity]
% The sample complexity of an algorithm $X$ solving a \mfmap problem for a given $\epsilon,\delta\in (0,1)$ is defined as the minimum number of trajectories required to ensure that for any distributions $D$ and $T(\cdot)$, with probability at least $1-\delta$ the safe action model returned by $X$ enables solving $\Pi$ with probability at least $1-\epsilon$.  
% \end{definition}
% By ``$X$ enables solving $\Pi$'', we mean that there exists a solution to $\Pi$ that can be found with a complete planner using the action model returned by $X$. 
% Based on Theorem 5 in~\cite{juba2021safe}, the sample complexity of SAM learning is
% linear in the number of actions and state variables. 
% In our context, the number of joint actions is exponential in the number of agents yielding an  upper bound on the sample complexity that is exponential in the number of agents as well. 
% Unfortunately, the sample complexity can indeed be exponential in the number of agents that participate in individual concurrent actions in the training set:

% %\paragraph{Sample Complexity}
% %We turn to studying the sample complexity of solving \mfmap problems. We extend the learning formulation of \cite{stern2017efficientAndSafe} to the multi-agent setting as follows.
% %\begin{definition}[Sample complexity of \mfmap]
% %Fix a domain and suppose that there is a probability distribution $D$ on solvable MA-STRIPS problems in that domain. For each such problem $\Pi$ in $D$, we further suppose that there is a probability distribution $T(\Pi)$ on trajectories solving $\Pi$, for example, as would be produced by running a sound and complete planner on $\Pi$. Then for any given $m$, we can consider the \mfmap problem with a \emph{training set} of size $m$ obtained by sampling $m$ problems $\Pi_1,\ldots,\Pi_m$ independently from $D$, and putting $\mathcal{T}$ equal to the set of samples $T(\Pi_1),\ldots,T(\Pi_m)$, and then letting the \emph{test problem} $\Pi$ be sampled independently from $D$. The \emph{sample complexity} for a given $\epsilon,\delta\in (0,1)$ is the minimum size of a training set so that for some algorithm, for any distributions $D$ and $T(\cdot)$, with probability at least $1-\delta$ over the sampling of the training set, the obtained training set is adequate for the algorithm to produce a safe action model for the domain that solves the test problem with probability at least $1-\epsilon$.
% %\end{definition}



% \begin{theorem}
% For all integers $A\geq 2$, $\kappa$, $k$, and $F\geq 2Ak$, and real numbers $\epsilon,\delta < 1/4$ there is a family of domains with $k$ agents, each with $A$ actions, and $F$ propositional fluents such that for distributions $T(\cdot)$ supported on trajectories in which at most $\kappa$ agents participate in each action (i.e., the other $k-\kappa$ take \noop), the sample complexity for $\epsilon$ and $\delta$ is at least $\Omega\left(\frac{1}{\epsilon}(FA^{\kappa}{k\choose\kappa}+\log\frac{1}{\delta})\right)$.
% \end{theorem}

% %The proof requires Stirling's formula (cf.\  \citet{robbins1955}):
% %\begin{equation}
% %    \sqrt{2\pi n}e^{n\ln n-n+\frac{1}{12n+1}} < n! < \sqrt{2\pi n}e^{n\ln n-n+\frac{1}{12n}}
% %\end{equation}

% \begin{proof}
% The domains are as follows. For each agent $i=1,\ldots,k$, there is a set of $A$ fluents for agent $i$ such that for each $j$th fluent, one of the actions has that fluent as an effect, and this is the only action with that fluent as an effect. There is an additional set of $F-Ak$ fluents which we will refer to as {\em flags}. All of the actions have all of the flags as effects. The actions have no other effects.

% Now consider the following distributions on problems and trajectories. The initial states of problems $\Pi$ in $D$ have all of the action fluents false, and all but one of the flags (uniformly at random) true. With probability $1-4\epsilon$, the goal is empty, and otherwise the goals specify that $\kappa$ of the fluents, each corresponding to distinct agents and chosen uniformly at random from such sets should be true, all of the other agent-action fluents should be false, and all of the flags should be true. $T(\Pi)$ then consists of trajectories with a single concurrent action; for the empty goal, all agents take \noop actions, and otherwise the set of $\kappa$ agents corresponding to the goal fluents take the actions corresponding to those fluents.

% To obtain a success rate of $1-\epsilon$ on such problems, the safe action model must permit plans in which only the agents corresponding to the goal fluents act, and such that after the first action in which some of the agents take actions other than \noop, the state of the flag is known. Indeed, on the nonempty goals (which occur with probability $4\epsilon$) such actions must exist with probability at least $3/4$, or else the failure rate will exceed $\frac{1}{4}4\epsilon=\epsilon$. Since the action model is assumed to be safe, this means that $\mathcal{T}$, together with the MA-STRIPS rules, either entails that  at least one of the actions in that joint action has the flag as an effect, or entails that none of them have the flag as an effect. We note that the latter is impossible: since all of the training examples have all flags true in the post-states, the examples are certainly consistent with an action model in which all actions have the flags as an effect, so no safe action model can conclude that any flag will not be an effect.

% On the other hand, consider any of the nonempty goals; if no example trajectory has included the joint action corresponding to precisely the set of $\kappa$ agent-action pairs mentioned in the goal with the relevant flag false in the pre-state, since these trajectories are either {\noop}s, or mention a different flag, or include at least one agent/action pair outside this set, the trajectories are consistent with an action model in which the relevant $\kappa$ agent-action pairs do not have the relevant flag as an effect, but all other agent-action pairs do have this flag as an effect (and all other flags are effects of all actions). Thus, we cannot determine the state of that flag after taking a joint action with any subset of these $\kappa$ actions. Since taking any action outside this set will set an agent-action fluent to true that should be false, and there is no way to set these fluents false, the goal cannot be achieved following any such action.

% In summary, nonempty goals cannot be achieved unless we have previously observed an example trajectory in which the joint action consists of the $\kappa$ relevant agent-action pairs, with the relevant flag false in the pre-state. There are at least $F/2$ flags and $A^\kappa{k\choose \kappa}$ such joint actions, which we observe uniformly at random among the non-empty goals. The probability that we fail to observe such an example among $\frac{1}{8}FA^\kappa{k\choose \kappa}$ of the trajectories with non-empty goals is at least
% \[
% \left(1-\frac{2}{FA^\kappa{k\choose \kappa}}\right)^{\frac{1}{8}FA^\kappa{k\choose \kappa}}> 1/2
% \]
% since $1-x+x^2>e^{-x}$ for positive $x$ and $k,\kappa\geq 1$, $A\geq 2$, and $F\geq 4$. The expected number of such goals, in turn, is therefore at least $1/2$ of them by the linearity of expectation; the probability that more than $3/4$ are observed is less than $2/3$ by Markov's inequality. In turn, observing that the number of examples with non-empty goals out of $\frac{1}{16\epsilon}FA^{\kappa}{k\choose\kappa}$ exceeds $\frac{1}{8}FA^\kappa{k\choose \kappa}$ with probability at most $1/2$ by Markov's inequality, we see that if the training set includes fewer than $\frac{1}{16\epsilon}FA^{\kappa}{k\choose\kappa}$ examples, with probability greater than $1/2\cdot 2/3>\delta$ over the training set, any safe action model for the training set must fail to achieve the goal with probability greater than $\epsilon$. 

% Furthermore, we observe that a training set of at least $\frac{1}{\epsilon}\ln\frac{1}{\delta}$ examples are also necessary, since if we draw fewer than this many examples, the probability that we never observe an example with a nonempty goal is at least $(1-8\epsilon)^{\frac{1}{\epsilon}\ln\frac{1}{\delta}}>e^{-\ln\frac{1}{\delta}}=\delta$. In such a case, no safe action model can take any actions, thus suffering a failure rate of at least $8\epsilon$. As $(x+y)/2\leq\max(x,y)$, we can therefore conclude that more than $\frac{1}{32\epsilon}(FA^{\kappa}{k\choose\kappa}+\ln\frac{1}{\delta})$ examples are necessary.
% \end{proof}

% %We remark that this bound is attained by a trivial reduction that treats each joint action as a distinct single-agent action, and uses the single-agent action model learning algorithm from prior work~\cite{stern2017efficientAndSafe,juba2021safe}. Indeed, furthermore, it shows that this algorithm has optimal worst-case sample complexity.

% We can also interpret the lower bound construction in term of concurrency in a relational domain \cite{crosby2014single}. Suppose instead of propositional fluents, we have unary predicates in a domain with a single object. We then immediately obtain the following:
% \begin{corollary}
% For all integers $A\geq 2$, $\kappa$, $k$, and $F\geq 2Ak$, and real numbers $\epsilon,\delta < 1/4$ there is a family of domains with $k$ agents, each with $A$ actions, and $F$ relational fluents such that for distributions $T(\cdot)$ supported on trajectories in which at most $\kappa$ agents participate in actions involving any single object at a time, the sample complexity for $\epsilon$ and $\delta$ is at least $\Omega\left(\frac{1}{\epsilon}(FA^{\kappa}{k\choose\kappa}+\log\frac{1}{\delta})\right)$.
% \end{corollary}




% %%
% %% The simplified version from the ICAPS submission appears below.
% %%

% % Cannot do any better
% %\begin{theorem}
% %The sample complexity of any \mfmap algorithm for $\epsilon,\delta < 1/4$ is at least
% %$\Omega\left(\frac{1}{\epsilon}(|P|\prod_{i=1}^k |A_i|+\log\frac{1}{\delta})\right)$.
% %\end{theorem}
% %\begin{proof}
% %For any $p\geq 2\prod_{i=1}^k |A_i|$, consider a domain in which for each agent $i=1,\ldots,k$ and action $a_j\in A_i$ 
% %there if a fluent $f_{i,j}$ that is the effect of exactly one action. 
% %The domain includes an additional set of $p-\prod_{i=1}^k |A_i|$ fluents (so $|P|=p$) and every action in the domain has all these fluents as an effect. We refer to the first type of fluents as the \emph{goal fluents} and the second type as the \emph{flag fluents}. 


% %Now consider the following distributions on problems and trajectories. 
% %The initial state of every problem sampled from $D$ has all goal fluents set to false and all but one of the flag fluents (uniformly at random) true. 
% %With probability $1-4\epsilon$, the goal is empty.
% %Otherwise, the goal includes setting all flag fluents to true, as well as an additional $k$ goal fluents, one per agent, chosen uniformly at random, that should be set to true. All other goal fluents must be set to false. 
% %$T(\Pi)$ consists of trajectories with a single joint action; for the empty goal, all agents take \noop actions, and otherwise the $k$ agents take the actions corresponding to the $f_{i,j}$ goal fluents to be set true in the goal. 

% %For any problem with a non-empty goal that we did not observe in the training set, the action model in which the corresponding actions do not set the missing flag to true and otherwise all actions set all flags to true is consistent with the training set. 
% %Indeed, either the set of actions appears with a different flag to be set true (which it is, following those actions) or else the joint action includes at least one action distinct from the set of $k$ we need to achieve this goal, and then the flag would be set to true by that action. Therefore, no safe action model can permit taking the joint action needed to achieve the goal, and all other actions would reach a state in which some incorrect goal fluents are set to true and cannot be subsequently set to false. 

% %Since the \noop goal only comprises $1-4\epsilon$ probability in the goal distribution, we need to observe at least a $3/4$ fraction of the possible goals for a safe action model to attain probability $1-\epsilon.$ But, there are $|P|\prod_{i=1}^k |A_i|$ goals, and in expectation, a sample of size $m$ only contains $4\epsilon m$ examples of these goals. We, therefore, need $\Omega\left(\frac{1}{\epsilon}|P|\prod_{i=1}^k |A_i|\right)$ examples; likewise, to even observe any of the nonempty goals with probability $1-\delta$, we need $\Omega\left(\frac{1}{\epsilon}\log\frac{1}{\delta}\right)$ examples, giving the claimed bound.
% %\end{proof}
% %Limitations on the allowed concurrency may reduce, to some extent, the sample complexity. 
% %For example, if we assume that at most $\kappa<k $ agents can act concurrently, 
% %then sample complexity reduces by replacing $k$ with ${\kappa}{k\choose\kappa}$. 

% Notably, the sample complexity of \masam is unbounded, 
% because, in some cases, it cannot learn a safe, non-trivial action model for each agent. 
% For example, consider a pair of single-agent actions $a_1(?x_1)$ and $a_2(?x_2)$ that are always performed concurrently and parameterized by the same object. 
% In this case, it is impossible to disambiguate between the effects of $a_1$ and $a_2$ and create a safe action model for each agent. 

 
% Nevertheless, while the sample complexity of \masam is worse than SAM over joint actions, our experimental results below demonstrate that \masam is very effective for standard MA-STRIPS benchmarks. 
% On the other hand, SAM on joint actions is not a practical algorithm since when problems contain many joint actions, it does not learn much about the underlying single-agent actions.  



% \subsection{\masam Incompleteness Solution}
% Negative results: ma-sam incomplete
% While the action model returned by \masam is indeed safe, there are cases where it cannot learn a non-trivial safe action model, even with an infinite number of trajectories. For example, consider a pair of single-agent actions $a_1$ and $a_2$, that are always performed concurrently, and are always parameterized by the same object. In such cases, \masam would never be able to disambiguate their effects, and thus neither of them will be considered a safe action. %to use. % allows using either of these actions, as it will not 
% Possible approach: learn joint actions
% Next, we propose an extension of \masam that can learn an effective safe action model even in such cases. 