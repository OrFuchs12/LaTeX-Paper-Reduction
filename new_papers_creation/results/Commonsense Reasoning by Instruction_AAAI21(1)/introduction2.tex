\section{Introduction}
% \vspace{-1em}
Despite the remarkable success of artificial intelligence (AI) and machine learning in the last few decades, commonsense reasoning remains an unsolved problem at the heart of AI \cite{levesque2012winograd,davis2015commonsense,sakaguchi2019winogrande}. 
Common sense allows us humans to engage in conversations with one another and to convey our thoughts efficiently, without the need to specify much detail \cite{grice1975logic}. For example, if Alice asks Bob to ``wake her up early whenever it snows at night'' so that she can get to work on time, Alice assumes that Bob will wake her up only if it snows enough to cause traffic slowdowns, and only if it is a working day. Alice does not explicitly state these conditions since Bob makes such \emph{presumptions} without much effort thanks to his common sense.
%does not need to explicitly mention that it needs to snow enough to cause traffic slowdowns and that it needs to be a working day for Bob to wake her up early; Bob makes these presumptions without much effort thanks to his common sense. 
A study, in which we collected such {\bf if(state)-then(action)} commands
% (similar to the previous example)
from human subjects, revealed that humans often under-specify conditions in their statements; perhaps because they are used to speaking with other humans who possess the common sense needed to infer their more specific intent by making presumptions about their statement.
% relying on the presumptions that their fellow humans would make about the statements. 
% Computers are currently unable to make these presumptions, making it challenging for them to engage in natural sounding conversations with humans.

The inability to make these presumptions makes it challenging 
% is one of the main reasons why 
%Amos: you can save some space by replacing "is one of the main reasons why it" with "makes it challenging..."
% it is challenging 
for computers to engage in natural sounding conversations with humans. While conversational AI systems such as Siri, Alexa, and others are entering our daily lives, their conversations with us humans remains limited to a set of pre-programmed tasks.
% Now is the right time to address this problem since conversational agents such as Siri, Alexa, and others are entering our daily lives yet their conversations with us humans remains limited to a set of pre-programmed tasks. %Amos: also here you can save some space by replacing "now is the right time to address this problem since" with "While conversational..." and remove "but"
We propose that handling unseen tasks requires conversational agents to develop common sense. % conversational agents need to develop common sense to handle unseen commands. %If we want to have conversational agents that resemble intelligent conversational interactions, then our conversational agents should be able to understand and execute   %The reason is that our conversational agents currently do not have the technology to correctly understand and execute new out-of-domain commands. We propose that one of the main reasons for this lack of understanding is that conversational agents lack commonsense reasoning. 
% into our daily lives in the past few years makes addressing this problem a timely contribution. This is becoming a prominent problem for computers in the recent years since computers are increasing their conversational interactions with humans

% \tmcomment{this is the problem we focus on in this paper. In the next parag we can say the problem more precisedly (Tom's sentence in the doc)} 
% In an attempt to enable this, 
Therefore, we propose a new commonsense reasoning benchmark for conversational agents where the task is to
% relevant to
% We consider a specific type of common sense reasoning relevant to
% understanding the intent of
infer \emph{commonsense presumptions} in commands of the form ``If \textState~holds Then perform \textAction~Because I want to achieve \textGoal.'' The {\bf if-(state), then-(action)} clause arises when humans instruct new conditional tasks to conversational agents \cite{azaria2016instructable, labutov2018lia}. 
The reason for including the \textbf{because-(goal)} clause in the commands is that some presumptions are ambiguous without knowing the user's purpose, or goal. For instance, if Alice's goal in the previous example was to see snow for the first time, Bob would have presumed that even a snow flurry would be excuse enough to wake her up. Since humans frequently omit details when stating such commands, a computer possessing common sense
% our interest lies in how computers
should be able to infer the hidden \emph{presumptions}; that is, the additional unstated conditions on the If and/or Then portion of the command. Please refer to Tab.~\ref{tab:statement_stats} for some examples. %although the presumptions in the above example are more natural, other scenarios can also occur:
% \vspace{-0.4em}
% \begin{itemize}
%     \item[$\mathcal{A}$)] Whenever it snows at night, then wake me up early, because I want to get to work on time.
%     \vspace{-0.4em}
%     \item[$\mathcal{B}$)] Whenever it snows at night, then wake me up early, because I have never seen snow before.
% \end{itemize}
% \vspace{-0.3em}
% A computer possessing common sense should be able to presume that a snow flurry does not trigger scenario $\mathcal{A}$ but triggers scenario $\mathcal{B}$. 
% For the above example, although that interpretation is more natural it is possible to have another scenario.
%which specify the goal of each if-then statement.
% The following is an example of two of the statements in our data set.

% in scenario $\mathcal{A}$ the speaker does not want to wake up early on a weekend, whereas in scenario $\mathcal{B}$ the speaker wants to wake up early regardless of the day. 
% It should also be able to presume that 

%   The under-specified statements in our data set are annotated with the presumptions that humans would make to fill in the unspoken details (Tab.~\ref{tab:statement_stats}).

% We consider in particular
\input{statement_stats.tex}
% \vspace{-0.8em}
In this paper, in addition to the proposal of this novel task and the release of a new dataset to study it, we propose a novel initial approach that infers such missing presumptions, by
extracting %completing
a chain of reasoning that shows how the commanded \textAction will achieve the desired \textGoal when the \textState holds. Whenever any additional reasoning steps appear in this reasoning chain, 
% presumptions are required to complete this chain of reasoning, 
they are output by our system as assumed implicit presumptions associated with the command. For our reasoning method we propose a neuro-symbolic interactive, conversational approach, in which the computer combines its own commonsense knowledge with conversationally evoked knowledge provided by a human user. The reasoning chain is extracted using our neuro-symbolic theorem prover that learns sub-symbolic representations (embeddings) for logical statements, making it robust to variations of natural language encountered in a conversational interaction setting. 
%To the best of our knowledge, no work in the literature studies this aspect of commonsense reasoning.
% \tmcomment{we consider here one form of commonsense reasoning that is used to }

% We release the if-then statements collected in our study as a benchmark for evaluating machine common sense. 
% The goal of the because explanation is to help disambiguate the above two scenarios.
% intentions of the user? The data is annotated with the gold under-specifications. \tmcomment{say that the condition is under-specified and the annotations are gold annotations that indicate what the condition should be [intended conditions/implicit condition] -> define this early on and then use these}
% In contrast to our task design, almost all the commonsense reasoning benchmarks such as \cite{levesque2012winograd,sakaguchi2019winogrande,roemmele2011choice,mostafazadeh2017lsdsem} are designed in a multi-choice manner. It has been shown that some incorrect answers in these benchmarks could be ruled out due to biases that are easily detectable by language models \cite{trinh2018simple, sakaguchi2019winogrande} resulting in an over-estimation of machine commonsense. %Tab.~\ref{tab:statement_stats} shows examples of our data.
% The task that we propose has a more open domain and a harder structure \facomment{reword, why is this exactly?}

% using soft logical reasoning.
% \facomment{motivate why logic? Why not directly use natural language reasoning chains? e.g. why not do: \cite{clark2020transformers}}.
% Our neuro-symbolic reasoning system is
% We propose a neuro-symbolic commonsense reasoning framework that uncovers commonsense presumptions in a given statement. It has a soft logic theorem prover at its heart that extracts chains of reasoning given an input if-then-because statement. For example, given statement $\mathcal{A}$ it extracts the following reasoning chain: $(1)$ if it snows more than two inches, then there will be traffic, $(2)$ if there is traffic, then my commute time to work increases, $(3)$ if my commute time to work increases then I need to leave the house earlier to ensure I get to work on time. Using this reasoning chain, it presumes that the speaker probably needs to wake up early only when it snows more than two inches. % This reasoning chain fills in the underspecified details in the original statement. For example, the amount of snow needed to cause traffic slowdowns, which is not explicitly mentioned in the statement.

% The first challenge that our theorem prover addresses is dealing with variations of natural language. These variations result in different logical surface forms for semantically similar statements. For example, ``it is going to snow at night'' is semantically similar to ``the forecast is calling for snow tonight'' but these have very different logical representations. Our theorem prover learns sub-symbolic representations (embeddings) that reflect the similarity between such phrases.

% \tmcomment{make it clear that we are referring to conversation}
% and either query it or propose automated knowledge base completion methods for it.
% The second challenge in performing reasoning is that the computer needs to have access to commonsense knowledge about the world. Most of the literature on reasoning relies on extracting the largest possible snapshot of world knowledge. However, there is currently no technology that has the capacity to store and query the sum total of all human knowledge, making current knowledge bases incomplete. Mimicking a human's strategy of accumulating knowledge, we argue that it is necessary to equip reasoning engines with a conversational interaction strategy facilitating the extraction of just-in-time information needed for reasoning. The advent of conversational agents like Siri, Alexa and others into our daily lives has made leveraging conversational interactions, a realistic and plausible opportunity.
% \tmcomment{make it clear that we are conversing with the user and not crawling the web e.g.} 
% This is \tmcomment{remove} because there is currently no technology that has the capacity to store and query the sum total of all human knowledge.
\paragraph{Contributions}
This paper presents three main contributions. 1) We propose a benchmark task for commonsense reasoning in conversational agents and release a data set containing \textbf{if-(state), then-(action), because-(goal)} commands, annotated with commonsense presumptions.
% \tmcomment{gold under-specifications -> not clear}.
2) We present CORGI (COmmonsense ReasoninG by Instruction), a system that performs soft logical inference. CORGI uses our proposed neuro-symbolic theorem prover and applies it to extract a multi-hop reasoning chain that reveals commonsense presumptions. %full condition.
3) We equip CORGI with a conversational interaction mechanism that enables it to collect just-in-time commonsense knowledge from humans.
% when encountering missing background knowledge. 
Our user-study shows (a) the plausibility of relying on humans to evoke commonsense knowledge and (b) the effectiveness of our theorem prover, enabling us to extract reasoning chains for up to 45\% of the studied tasks\footnote{The code and data are available here: https://github.com/ForoughA/CORGI}.

% Commonsense reasoning refers to our ability of making sense of the world around us and reasoning about it in our everyday life. \hl{give an example}. The hardest problem in commonsense reasoning is that when we humans talk to one another, we make assumptions about each other's world knowledge and therefore, under-specify many terms knowing that the other party will understand it, and that if they don't they will just ask for more explanation. 

% For example, if a human tells another human ``'' Many of the failures of AI systems today can be tracked back to the lack of commonsense \cite{}. For example, \hl{give some examples}. Therefore, it is becoming more and more important to develop machine learning algorithms that are capable of doing commonsense reasoning and take care of some of the problems that is so obvious to us humans todate. \facomment{bad wording need to rephrase everything}

% We argue in this paper, that one way of approaching commonsense is developing reasoning systems that are capable of extracting multi-hop chains of inference, similar to how humans logically think in their every day lives when they encounter ``trivial to us but hard for computers'' scenarios. we could cite \cite{battaglia2018relational} here.

% A trivial method for this is to represent knowledge in terms of first order logic (FOL) and then do logical inference to extract these reasoning chains. Logic inference systems are powerful and were designed and optimized for this task, but one major drawback of logical inference is that it is hard to generalize to unseen logic statements. In order to deal with this flaw, we propose a neuro-symbolic logical inference algorithm that learns sub-symbolic representations for logic statements.

% Neuro-symbolic models have recently gained widespread attention \cite{lamb2020graph,rocktaschel2017end,evans2018learning,arabshahi2018combining}. The symbolic aspect of these models provide structure while their neural aspect provide flexibility and these models use the best of both worlds allowing them to generalize to unseen problems in a systematic way \cite{battaglia2018relational, lake2017generalization, arabshahi2018combining, allamanis2017learning} These models bring in a symbolic flavor to conventional neural networks. The result is that these models leverage the power of symbolic computing, which is a major research focus ever since the formation of artificial intelligence, and overcome its main challenge which is dealing with noise and uncertainty. In this paper, we propose a neuro-symbolic automated theorem prover for the problem of commonsense reasoning.

% Commonsense reasoning is a challenging open problem in Artificial Intelligence. Despite the efforts of researchers \cite{levesque2012winograd,bosselut2019comet,sakaguchi2019winogrande}, computers are still far from perfect when it comes to commonsense. We study commonsense reasoning in the context of natural language if-then statements. For example, ``If it snows at night then wake me up 30 minutes earlier''. If a human encounters this statement, they will probably think ``this person is likely concerned about the fact that if it snows a lot, then there will be traffic and that if there is traffic they will need to wake up earlier to account for traffic and get to work on time''. In this paper, we study the possibility of the computer extracting the same thought process as a human by reducing commonsense reasoning to first order logical inference. The result of this is a multi-hop reasoning chain that resembles a human's thought process when making sense of the encountered statement. 

% The first challenge is performing such a reduction is the fact that natural language is highly noisy and ambiguous. Specifically, semantically similar natural language statements could have completely different structures and surface forms making FOL inference fail at solving this task. Therefore, we propose a neuro-symbolic inference algorithm that learns embeddings for logical statements that allows the system to overcome this challenge.



% We collected a data set of if-then statements from humans that we release with this paper and can be used as a benchmark for evaluating commonsense reasoning abilities. An interesting insight we have added when we collect these statements was to ask humans for the reason they want to perform an if-then task. In order to show why this is important consider the following two statements: ``If it snows tonight then wake me up early because I don't want to be late to work.'', v.s. ``If it snows tonight then wake me up early because I have never seen snow before.'' Comparing these two statements reveals the very different implications of each resulting is very different reasoning chains. To make it concrete, statements have a general format of ``if state $S$ occurs then do action $A$ because I want to achieve goal $G$'' and commonsense reasoning is the task of identifying how performing action $A$ in state $S$ allows one to achieve goal $G$. We would like to note that these are free-form natural language spoken text, and there is no specific template according to which these were collected other than being tied to the state/action/goal structure.

