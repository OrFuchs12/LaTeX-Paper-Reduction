\section{Experiment Design}
% In this section, we provide the details of the model training and our experimental setup for testing the performance of CORGI.
The knowledge base, \KB, used for all experiments is a small handcrafted set of commonsense knowledge that reflects the incompleteness of SOTA KBs. See Tab.~\ref{tab:kb_examples} in the Appendix for examples and the code supplement to further explore our KB.
% as if-then rules, also known as horn clauses. 
\KB~includes general information about time, restricted-domains such as setting alarms and notifications, emails, and so on, as well as commonsense knowledge about day-to-day activities. \KB~contains a total of 228 facts and rules. Among these, there are 189 everyday-domain and 39 restricted domain facts and rules. We observed that most of the if-then-because commands require everyday-domain knowledge for reasoning, even if they are restricted-domain commands (see Table \ref{tab:dialog} for example). 
% Some examples of the facts and rules in the knowledge base are given below:

Our Neuro-Symbolic theorem prover is trained on proof traces (proof trees similar to Fig.~\ref{fig:prooftree}) collected by proving automatically generated queries to \KB~using sPyrolog\footnote{\url{https://github.com/leonweber/spyrolog}}. 
% Training queries were automatically generated from the predicates in \KB~(e.g. \prologTerm{isBefore(monday,?)}
% \kmcomment{TODO}\facomment{why is this a TODO? We have already done it}
% )% and proved using sPyrolog.
% that takes as input a similarity file crafted by the authors to bind semantically similar predicates and variables. 
$\mathbf{M}^{\text{rule}}$ and $\mathbf{M}^{\text{var}}$ are initialized randomly and with GloVe embeddings \cite{pennington2014glove}, respectively, where $m_1=256$ and $m_2=300$. Since \KB~is type-coerced (e.g. \prologTerm{Time}, \prologTerm{Location}, $\dots$), initializing the variables with pre-trained word embeddings helps capture their semantics and improves the performance. %rules-M1: 256, vars-M2: 300
% The embedding dimension of the variables is $300$ and the embedding dimension of the rules is $256$. 
The neural components of the theorem prover are implemented in PyTorch \cite{paszke2017automatic} and the prover is built on top of sPyrolog.%\footnote{Our code and data are available in the supplementary material}.
% [kmm - original text below:]
% Our rule embeddings were trained on proof traces collected from our knowledge base. The proof traces were collected by passing automatically-generated queries to sPyrolog. These queries were generated using the predicates in the knowledge base. The variable embedding matrix is initialized with GloVe embeddings \cite{pennington2014glove} and the rule embedding matrix is initialized randomly. Since the variable names refer to the variable type (e.g. \prologTerm{Time}, \prologTerm{Location}, $\dots$), it is important to initialize them with pre-trained word embeddings to capture their semantics. %rules-M1: 256, vars-M2: 300
% The embedding dimension of the variables is 300 and the embedding dimension of the rules is $256$. Our neural model and our soft reasoning system is implemented in PyTorch \cite{paszke2017automatic} and built on top of sPyroloy.
% Our knowledge base is a small hand crafted commonsense knowledge represented in terms of if-then rules, also known as horn clauses. This knowledge base includes general information about time and LIA-related contexts such as setting alarms and notifications, emails and so on, as well as open domain commonsense knowledge about day-to-day activities. This knowledge base consists of a total number of 228 facts and rules. Among these, there are 189 open-domain and 39 LIA-domain facts and rules. We observed that most of the statements require open-domain knowledge for reasoning, even if they are originally LIA-domain statements (see Figure \ref{fig:dialog} for example). Some examples of the facts and rules in the knowledge base are given below:
% \begin{itemize}
%     \item \prologTerm{isEarlierThan(Time1,Time2) :- isBefore(Time1,Time3), isEarlierThan(Time3,Time2).}
%     \item \prologTerm{isBefore(monday, tuesday).}
%     \item \prologTerm{notify(Person1, lia, Action1) :- email(Person1, Action1).}
% \end{itemize}
% [/kmm]

% \begin{align*}
%     \text{\prologTerm{isEarlierThan(Time1,Time2)}}  \text{\prologTerm{ :- isBefore(Time1,Time3),}}&  \\
%     \text{\prologTerm{isEarlierThan(Time3,Time2).}} & \\
%     \text{\prologTerm{isBefore(monday, tuesday).}}
% \end{align*}
\vspace{-0.5em}
\subsection{User Study}
% \input{results_table.tex}
\input{results_sidebyside.tex}
In order to assess CORGI's performance, % and evaluate the effectiveness of the user feedback as well as the learned rule embeddings and the proposed soft reasoning 
we ran a user study. We selected 10 goal-type if-then-because commands from the dataset in Table \ref{tab:statement_stats} and used each as the prompt for a reasoning task.
We had 28 participants
% \footnote{Participants were not necessarily the people from whom we collected the dataset in Table \ref{tab:statement_stats}. Although it would be best to interact with the same user that inputs the statement, we argue that it is reasonable to use other humans for the explanations. First, most of these statements require day-to-day commonsense knowledge, therefore most humans will be able to give the required commonsense knowledge to the system. For example, some are as simple as having an umbrella to not get wet (Figure \ref{fig:dialog}). \kmcomment{I'm not convinced we need this paragraph; did a reviewer want it?}
% Second, we would like to be able to assess the performance of the model on a fixed dataset for comparison purposes. \kmcomment{I don't know what this means}}
in the study, 4 of which were experts closely familiar with CORGI and its capabilities. The rest were undergraduate and graduate students with the majority being in engineering or computer science fields and some that majored in business administration or psychology. These users had never interacted with CORGI prior to the study (novice users).
% The participants included students in the United States and Israel. 
Each person was issued the 10 reasoning tasks, taking on average 20 minutes to complete all 10. 

% We use a small subset of the if/then/because statements of Table \ref{tab:statement_stats} containing 10 examples and run a user study to assess the effectiveness of the user's feedback in proving the statements' because \textGoal and evaluate the performance of our proposed ``soft'' inference method. 14 people participated in our study and out of the 14 people, 3 were experts closely familiar with CORGI and its capabilities.
Solving a reasoning task consists of participating in a dialog with CORGI  as the system attempts to complete a proof for the \textGoal of the current task; 
% If the because \textGoal is not in the knowledge base, CORGI asks how to tell whether the goal has been achieved. 
% The user's responses are converted to new candidate if-then rules to add to \KB
% If the body of the rule is not found in the knowledge base, CORGI asks a new question, and so on
see sample dialogs in Tab.~\ref{tab:dialog}. The task succeeds if CORGI is able to use the answers provided by the participant to construct a reasoning chain (proof) leading from the \textGoal to the \textState and \textAction.
% If successful, CORGI adds all the candidate if-then rules to the knowledge base. If the system is not able to complete a proof after asking the user 3 questions, CORGI gives up and fails, discarding the candidate rules.
We collected 469 dialogues in our study.

The user study was run with the architecture shown in Fig.~\ref{fig:model}. 
We used the participant responses from the study to run a few more experiments. We (1) Replace our theorem prover with an \emph{oracle} prover that selects the optimal rule at each proof step in Alg.~\ref{alg:inference} and (2) attempt to prove the \textGoal without using any participant responses (\emph{no-feedback}). Tab.~\ref{tab:user_study} shows the success rate in each setting. % (1) Replace our theorem prover with vanilla prolog which does \emph{hard unification},
% Figure \ref{fig:dialog} shows examples of a successful vs. failed interaction with CORGI from the study.


% The results of the study are given in Table \ref{tab:user_study}. The fractions in the table indicate the number of successful reasoning tasks vs the number of attempted tasks. We compare the performance of CORGI against two scenarios. Scenario no-feedback is when we do not ask the users for any feedback and attempt to prove the because \textGoal against the knowledge base. The oracle scenario is when our soft unification has an oracle to select the optimal rule at each step so that soft unification always succeeds when it should and fails when it should.

% \amoscomment{reasons why it makes sense to ask other people to explain someone else's statements 1. they can better explain because when someone is too deep in the conversation then they cant give good feedback 2. if you crowdsource you will ask someone else to explain. 3. the 'explainers' may be better familiar with the engine's capabilities and knowledge.
% Clearly there is an advantage in asking the user to explain herself as she understands best what she wants.}

% \amoscomment{people need to understand what G(X) means}

% \amoscomment{at least 15 people in the user study}