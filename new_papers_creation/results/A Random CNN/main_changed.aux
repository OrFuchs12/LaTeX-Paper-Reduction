\relax 
\bibstyle{aaai22}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\citation{bias:cohen:ICLR17}
\citation{cam:zhou:CVPR16,gradcam:ICCV17,psol:zhang:CVPR20}
\citation{scda:tip17,ddt:wei:pr19}
\citation{ILSVRC2012:russakovsky:IJCV15}
\citation{scda:tip17}
\citation{scda:tip17}
\citation{InfoNCE:arxiv2018}
\citation{moco:kaiming:CVPR20}
\citation{simclr:hinton:ICML20}
\citation{byol:grill:NIPS20}
\@LN@col{1}
\babel@aux{american}{}
\@LN@col{2}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:vis}{{1}{1}{Visualization of localization heatmaps using SCDA~\citep  {scda:tip17} for a \textit  {randomly initialized} ResNet-50. Best viewed in color when zoomed in.}{}{}}
\citation{lottery:frankle:ICLR19}
\citation{lotteryrecognition:girish:CVPR21,lotteryssl:chen:CVPR21,lotteryprove:malach:ICML20}
\citation{byol:grill:NIPS20}
\citation{deepimageprior:Dmitry:CVPR18}
\citation{gradcam:ICCV17,psol:zhang:CVPR20}
\citation{cam:zhou:CVPR16}
\citation{scda:tip17}
\citation{ddt:wei:pr19}
\citation{ILSVRC2012:russakovsky:IJCV15}
\citation{InfoNCE:arxiv2018}
\citation{simclr:hinton:ICML20}
\citation{moco:kaiming:CVPR20}
\citation{unmix:shen:arxiv20}
\citation{bsim:cgy:arxiv20}
\citation{mixup:ICLR18}
\citation{cutmix:yun:ICCV19}
\citation{cut-paste-learn:dwibedi:ICCV19,simple-copy-paste:quoc:arxiv2012}
\citation{resnet:he:CVPR16}
\citation{scda:tip17}
\citation{scda:tip17}
\@LN@col{1}
\@LN@col{2}
\newlabel{sec:method}{{}{2}{}{}{}}
\newlabel{sec:method1}{{}{2}{}{}{}}
\newlabel{sec:method2}{{}{2}{}{}{}}
\citation{supcon:khosla:nips20}
\citation{moco:kaiming:CVPR20}
\citation{scda:tip17}
\citation{cam:zhou:CVPR16}
\citation{cub200}
\citation{ILSVRC2012:russakovsky:IJCV15}
\citation{pytorch:NIPS19}
\citation{scda:tip17}
\citation{scda:tip17}
\citation{kaiminginit:he:ICCV15}
\citation{VOC:mark:IJCV10}
\@LN@col{1}
\newlabel{fig:network}{{2}{3}{Pipeline of Tobias SSL. Upper part: splitting foreground and background using a \textit  {randomly initialized} CNN. Lower part: applying Tobias augmentation into SSL.}{}{}}
\newlabel{Tk}{{3}{3}{}{}{}}
\@LN@col{2}
\newlabel{view2}{{5}{3}{}{}{}}
\newlabel{pcb}{{6}{3}{}{}{}}
\newlabel{sec:exp}{{}{3}{}{}{}}
\newlabel{sec:exp1}{{}{3}{}{}{}}
\citation{resnet:he:CVPR16}
\citation{cam:zhou:CVPR16}
\citation{vit:ICLR21}
\citation{faster-rcnn:ren:NIPS15}
\citation{coco:LinTY:ECCV14}
\citation{edgebox:pitor:ECCV14}
\citation{batchnorm:Ioffe:ICML15}
\newlabel{tab:loc-results}{{1}{4}{Comparisons of localization accuracy between ImageNet pretrained and randomly initialized CNNs on ImageNet and CUB-200. `\#ReLU' and `\#stages' represent the number of ReLU units and stages, respectively. `IN super.' stands for `ImageNet supervised'. We report the average accuracy and standard deviation of 3 trials for randomly initialized models.}{}{}}
\@LN@col{1}
\@LN@col{2}
\citation{alexnet:NIPS2012}
\citation{vgg:simonyan:ICLR15}
\citation{deepimageprior:Dmitry:CVPR18}
\citation{inceptionv3:Szegedy:CVPR16}
\citation{kaiminginit:he:ICCV15}
\citation{xavier}
\citation{mocov2:xinlei:arxiv2020}
\citation{simclr:hinton:ICML20}
\citation{S3L:cao:arxiv2021}
\@LN@col{1}
\newlabel{tab:loc-results-more}{{2}{5}{Localization accuracy of various CNNs on ImageNet and CUB-200. We report the average accuracy and standard deviation of 3 trials for randomly initialized models.}{}{}}
\@LN@col{2}
\newlabel{fig:scatter}{{3}{5}{Classification accuracy after training (PyTorch model zoo) versus localization accuracy (when randomly initialized) on ImageNet. The number in brackets represents the number of convolutions in the model (i.e., depth).}{}{}}
\newlabel{sec:exp2}{{}{5}{}{}{}}
\newlabel{sec:cub}{{}{5}{}{}{}}
\citation{mocov2:xinlei:arxiv2020}
\citation{VOC:mark:IJCV10}
\citation{FPN:kaiming:CVPR17}
\citation{mask-rcnn:he:ICCV17}
\citation{wu2019detectron2}
\citation{S3L:cao:arxiv2021}
\@LN@col{1}
\newlabel{tab:clean-cub200-result}{{3}{6}{Comparisons of pretraining details and accuracies (\%) on CUB-200. `N/A' means that pretraining are conducted on ImageNet instead of CUB-200 for ImageNet supervised models. `FT' is short for `fine-tuning'.}{}{}}
\newlabel{tab:coco-result}{{4}{6}{Object detection on PASCAL VOC trainval07+12 (default VOC metric $\text  {AP}_{50}$, COCO-style AP, and $\text  {AP}_{75}$).}{}{}}
\newlabel{sec:imagenet}{{}{6}{}{}{}}
\@LN@col{2}
\newlabel{tab:small-imagenet-result}{{5}{6}{Downstream object detection performance on VOC 07\&12 and linear evaluation accuracy on Tiny-IN-200 when pretrained on ImageNet subsets using ResNet-50. `\#imgs' (`\#eps') represent the number of images (epochs).}{}{}}
\newlabel{fig:ap-size}{{4}{6}{Performance of Tobias on Pascal VOC ($\text  {AP}_{75}$) with respect to different training data size.}{}{}}
\citation{unmix:shen:arxiv20}
\citation{byol:grill:NIPS20}
\@LN@col{1}
\newlabel{sec:exp3}{{}{7}{}{}{}}
\@LN@col{2}
\newlabel{tab:hyperparameter}{{6}{7}{Effect of hyper-parameter $p$. All settings are pretrained on IN-10k for 800 epochs using ResNet-50.}{}{}}
\newlabel{tab:transformation}{{7}{7}{Impact of progressively removing transformations. All pretrained on IN-10k for 800 epochs.}{}{}}
\@LN@col{1}
\@LN@col{2}
\bibdata{egbib}
\gdef \@abspage@last{8}
