\section{Background \& Related Work\label{sec:related}}

\subsection{Algorithmic Accountability}
As algorithmic approaches replace and supplant human decisions across society, researchers have pointed out the importance of holding algorithms accountable \citep{Gillespie2014,Diakopoulos2015,Garfinkel2017}. One way to do this is the ``algorithm audit,'' which derives its name and approach from longstanding methods in the social sciences designed to detect discrimination \citep{Sandvig2014}. For example, algorithm audits have exposed discrimination in image search algorithms \citep{Kay}, Google auto-complete \citep{Baker2013}, dynamic pricing algorithms \citep{Chen2016}, automated facial analysis \citep{Buolamwini2018}, and word embeddings \citep{Caliskan2017}. Of particular relevance to our work are audit studies focusing on news intermediaries which aggregate, filter, and sort news content from primary publishers.

\subsection{Auditing Intermediaries}
Examples of algorithmic news intermediaries include social media websites (e.g. Facebook, Twitter, reddit), search engines (e.g. Google, Bing), and news aggregation websites (e.g. Google News). On each of these platforms, algorithms select and sort content for millions of users, thus wielding significant power as algorithmic gatekeepers. This content moderation has attracted critical attention \citep{Gillespie2018}, and spurred some researchers to audit intermediaries and check for discrimination, diversity, or filter-bubble effects.

\subsubsection{Social Media}
In the case of social media, \citet{Bakshy} investigated Facebook's News Feed, finding that user choices (e.g., click history, visit frequency) played a stronger role than News Feed's algorithmic ranking when it came to helping users encounter ideologically cross-cutting content. A 2009 study of Twitter showed that trending topics were more than 85\% news-oriented \citep{Kwak2010}. However, because of the high churn rate, trending topics can exhibit temporal coverage bias depending on when users visit the site \citep{Chakraborty2015}.

\subsubsection{Web Search}
As the most popular platform for online search, Google has been the subject of numerous audit studies, some of which reveal discriminatory results. For example, Google's autocomplete feature was shown to exhibit gender and racial biases \citep{Baker2013}, and its image search was shown to systematically underrepresent women \citep{Kay}. Other audits show that user-generated content such as Stack Overflow, Reddit, and particularly Wikipedia play a critical role in Google's ability to satisfy search queries \cite{Vincent2019}

Several studies have investigated potential political bias in Google's search results \citep{Robertson2018,Diakopoulos,Epstein2017}, however, further research is needed to understand the extent and causes of apparent biases. For example, Google's search algorithms may increase exposure to particular news sources (with left-leaning audiences) due to freshness, relevance, or greater overall abundance of content on the web \citep{Trielli}.

Some studies have also investigated Google for creating virtual echo chambers, which may affect democratic processes. While concern has mounted over the search engine's filter-bubble effects \citep{Pariser}, studies have thus far found limited supporting evidence for the phenomenon \citep{Puschmann2018,Flaxman,Hannak2013,Robertson2018}. An analysis of more than 50,000 online users in the United States showed that the search engine can actually ``increase an individual's exposure to material from his or her less preferred side of the political spectrum'' \citep{Flaxman}. Still, some search results may vary with respect to location \citep{Kliman-Silver2015}, an effect that has the potential to create geolocal filter bubbles. %\citep{Hannak2017} examine 200 Google webs earch users and show that ``on average, 11.7\% of results show differences due to personalization.'' Similarly, \citep{Robertson2018} found minimal evidence for the filter bubble hypothesis.

\subsubsection{Google News}
As early as 2005, researchers zeroed in on Google's news system to assess potential bias in the platform \citep{Ulken2005,Schroeder2005}, showing that even shortly after its introduction, scholars were troubled by the platform's potential effects on journalism and the public at large. For example, the Associated Press was concerned that Google used their content without providing compensation \citep{Gaither2005}, and early on, Google News was suspected to have a conservative bias \citep{Ulken2005}.

Google News still attracts critical attention more than a decade later, but concerns have shifted towards the possibility of filter bubbles (as was the case with Google's search engine). Two studies in particular have addressed such concerns: \citet{Haim2018} tested for personalization with manually-created user profiles, and \citet{Nechushtai2019} did so with real-world users. The former study discovered ``minor effects'' on content diversity from both explicit personalization (from user-stated preferences) and implicit personalization (using inferences from observed online behavior). \citet{Nechushtai2019} showed that users with different political leanings and locations ``were recommended very similar news,'' but their study presented a separate concern: just five news outlets accounted for 49\% of the 1,653 recommended news stories. This source concentration highlights the multifaceted implications of news aggregators, which we return to later.


\subsubsection{Apple News}
Apple News has begun to attract the interest of various stakeholders in industry and research. The New York Times wrote about the app in October 2018 \citep{Nicas2018}, focusing on Apple's ``radical approach'' of using humans to curate content instead of just algorithms. Despite its growth, monetization on the platform has thus far proven difficult and drawn criticism \citep{Davies2017,Dotan2018}. Slate reports that it takes 6 million page views in the Apple News app to generate the same advertising revenue as 50,000 page views on its website -- a more than hundredfold difference \citep{Oremus2018}.

A study of Apple News' editorial curation choices in June 2018 analyzed tweets and email newsletters from the editors, finding that larger publishers appeared far more often \citep{Brown2018}. In a followup study, screen recordings captured the Top Stories section in the United Kingdom to collect 1,031 total articles, 75\% of which came from just six publishers \citep{Brown2018a}. This paper builds on and adds to these studies in two important ways. First, we design and use a method for \textit{fully automated data collection}, rather than relying on manual coding of screen recordings. This method allows us to collect both Top Stories and Trending Stories in the United States, whereas \citep{Brown2018a} collected only Top Stories in the United Kingdom. Second, we examine mechanical aspects of Apple News in addition to examining content. By investigating mechanisms such as adaptation and update frequency, we reveal several intriguing design choices and curation patterns within the app.