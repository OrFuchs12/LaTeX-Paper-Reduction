\relax 
\bibstyle{aaai22}
\citation{antol2015vqa,anderson2018bottom,wu2017visual,jang2017tgif,geng2021dynamic,chen2020counterfactual,ghosh2019generating}
\citation{dang2021hierarchical,antol2015vqa}
\citation{anderson2018bottom,johnson2015image,krishna2017visual,dornadula2019visual}
\citation{johnson2015image}
\citation{li2019know,li2019relation,pan2020spatio,geng2021dynamic}
\citation{geng2021dynamic,chatterjee2021visual,pan2020spatio,herzig2019spatio}
\citation{shamsian2020learning}
\citation{tung20203d,girdhar2019cater}
\citation{multiview}
\citation{ranftl2019towards,fu2018deep}
\citation{vaswani2017attention}
\citation{xiao2021next}
\citation{alamri2019audio}
\citation{teney2018tips,wu2017visual}
\citation{johnson2015image}
\citation{herzig2019spatio,wang2018non,jang2017tgif,tsai2019video,girdhar2019video}
\citation{pan2020spatio}
\citation{geng2021dynamic}
\citation{Jiang_Gao_Guo_Zhang_Xiang_Pan_2019}
\citation{fan2019heterogeneous}
\citation{bar2020compositional,rashid2020action,Wang2018videos}
\citation{ji2019action,cong2021spatial}
\citation{armeni20193d}
\citation{zhang2021exploiting,wu2021scenegraphfusion}
\citation{tung20203d}
\citation{choromanski2021graph}
\citation{tsai2019Transformer}
\citation{bello2021lambdanetworks}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pipeline}{{1}{2}{A schematic illustration of our proposed (2.5+1)D\xspace  video QA reasoning pipeline.}{}{}}
\citation{ren2015faster}
\citation{anderson2018bottom}
\citation{fu2018deep,li2020unsupervised}
\citation{ranftl2019towards}
\citation{carreira2017quo}
\newlabel{alg:ancestors}{{1}{4}{Identifying common ancestors for merging}{}{}}
\newlabel{eq:criterion}{{1}{4}{}{}{}}
\newlabel{eq:match}{{2}{4}{}{}{}}
\newlabel{eq:2}{{3}{4}{}{}{}}
\newlabel{eq:kernel}{{4}{4}{}{}{}}
\citation{xiao2021next}
\citation{alamri2019audio}
\citation{xiao2021next}
\citation{AVSD@DSTC7}
\citation{geng2021dynamic}
\newlabel{fig:h3.5d_txr}{{2}{5}{The architecture of the proposed Hierarchical (2.5+1)D-Transformer\xspace  for encoding (2.5+1)D\xspace  scene graphs. The left module in red (N$\times $) is the standard Transformer. }{}{}}
\newlabel{eq:4}{{5}{5}{}{}{}}
\newlabel{eq:htxr}{{6}{5}{}{}{}}
\citation{carreira2017quo}
\citation{radford2021learning}
\citation{geng2021dynamic}
\citation{jang2019video}
\citation{jiang2020reasoning}
\citation{le2020hierarchical}
\citation{geng2021dynamic}
\citation{le2019multimodal}
\citation{xiao2021next}
\citation{xiao2021next}
\citation{jang2019video}
\citation{gao2018motion}
\citation{le2020hierarchical}
\citation{fan2019heterogeneous}
\citation{jiang2020reasoning}
\citation{geng2021dynamic}
\citation{geng2021dynamic}
\citation{alamri2019audio}
\citation{hori2019end}
\citation{alamri2019audio}
\citation{le2019multimodal}
\citation{geng2021dynamic}
\newlabel{tab:sota_nextqa}{{1}{6}{NExT-QA: Comparisons to the state of the art. Results for the various competitive methods are taken from\nobreakspace  {}\citep  {xiao2021next}.}{}{}}
\citation{xiao2021next}
\citation{xiao2021next}
\citation{fan2019heterogeneous}
\citation{geng2021dynamic}
\citation{fan2019heterogeneous}
\citation{geng2021dynamic}
\citation{jiang2020reasoning}
\newlabel{tab:sota_avsdqa}{{2}{7}{AVSD-QA: Comparisons to the state of the art. The prior results are taken from\nobreakspace  {}\citep  {geng2021dynamic}.}{}{}}
\newlabel{tab:abl_nextqa_avsdqa}{{3}{7}{Ablation study on NExT-QA and AVSD-QA. Below, \emph  {Txr} is the standard Transformer and \emph  {I3D+FRCNN} is the averaged I3D and FRCNN features per frame (no graph), \emph  {V(2+1)D Txr}: without depth.}{}{}}
\newlabel{tab:nextqa-ablation}{{4}{7}{Ablation study on NExT-QA by removing modules in our pipeline.}{}{}}
\newlabel{tab:hier-ablation}{{5}{7}{Ablation study on NExT-QA using different spatio-temporal hierarchies defined by the kernel bandwidth $\sigma $.}{}{}}
\newlabel{tab:abl_compute}{{6}{7}{Computational benefits of the proposed approach. The numbers indicate the average number of graph nodes per video sequence in each dataset.}{}{}}
\citation{ranftl2019towards}
\citation{ranftl2019towards}
\citation{ranftl2019towards}
\citation{ranftl2019towards}
\bibdata{aaai22}
\newlabel{tab:sota_categories}{{7}{8}{Comparison of our answer selection on various categories in NExT-QA dataset against other recent methods. The numbers for competing methods are taken from\nobreakspace  {}\citep  {xiao2021next}.}{}{}}
\newlabel{fig:next-qa-more-quals}{{3}{8}{An example illustration of (2.5+1)D scene graphs produced by our method. The figure shows a video frame from the NExT-QA dataset, its pseduo-3D rendering, and the (2.5+1)D static and dynamic graphs computed on all frames of the video.}{}{}}
\newlabel{fig:quals}{{4}{8}{Qualitative responses: First two rows show the results on the NExT-QA dataset and the last two on the AVSD-QA dataset. We compare our results to those produced by HGA\nobreakspace  {}\citep  {fan2019heterogeneous} on NExT-QA, and STSGR\nobreakspace  {}\citep  {geng2021dynamic} on AVSD-QA datasets. }{}{}}
\newlabel{fig:quals_1}{{5}{9}{RGB images, depth images produced using\nobreakspace  {}\citep  {ranftl2019towards}, the depth rendered RGB images, the static scene graphs, and the dynamic scene graphs for the respective videos. See below for the questions and answers for these videos.}{}{}}
\newlabel{fig:quals_2}{{6}{10}{RGB images, depth images produced using\nobreakspace  {}\citep  {ranftl2019towards}, the depth rendered RGB images, the static scene graphs, and the dynamic scene graphs for the respective videos. See below for the questions and answers for these videos.}{}{}}
\newlabel{fig:next-qa-more-quals-x}{{7}{11}{Qualitative responses from the NExT-QA dataset for various types of questions.}{}{}}
\newlabel{fig:avsd-qa-more-quals-x}{{8}{12}{Qualitative responses from the AVSD-QA dataset for various types of questions.}{}{}}
\gdef \@abspage@last{13}
