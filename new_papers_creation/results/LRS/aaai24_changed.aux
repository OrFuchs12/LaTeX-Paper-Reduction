\relax 
\bibstyle{aaai24}
\citation{he2016resnet}
\citation{redmon2016you}
\citation{ronneberger2015u}
\citation{goodfellow2015FGSM,madry2018pgd}
\citation{dong2018boosting,lin2020nesterov}
\citation{dong2019evading,xie2019improving,wu2019skip,guo2020backpropagating}
\citation{wu2020boosting}
\citation{li2020learning}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:loss_landscape}{{1}{1}{The loss landscape of original and transformed surrogate model: corrugated vs. smooth. Transformed surrogate models offer more stable input gradients and make the generated AE more generalizable, enabling more potent attacks.}{}{}}
\citation{xie2019improving}
\citation{inkawhich2019feature}
\citation{dong2018boosting}
\citation{xiong2022stochastic}
\citation{szegedy2014intriguing}
\citation{dong2018boosting}
\citation{qin2022boosting}
\citation{xie2019improving}
\citation{dong2019evading}
\citation{lin2020nesterov}
\citation{wu2020towards}
\citation{wang2021admix}
\citation{wu2020boosting}
\citation{papernot2016limitations}
\citation{chen2020universal}
\citation{inkawhich2019feature}
\citation{liu2017delving}
\citation{gubri2022lgv}
\citation{li2023making}
\citation{wu2018understanding}
\citation{charles2019geometric,tramer2017space}
\citation{gulrajani2017improved}
\citation{wu2020towards}
\citation{liu2017delving}
\newlabel{sec:methodology}{{}{3}{}{}{}}
\newlabel{eq:1}{{1}{3}{}{}{}}
\newlabel{def:lip_continous}{{1}{3}{}{}{}}
\newlabel{alg}{{1}{3}{LRS-1 (using PGD as an example base)}{}{}}
\newlabel{eq:L1}{{4}{3}{}{}{}}
\newlabel{def:lip_smooth}{{2}{3}{}{}{}}
\citation{qin2022boosting}
\citation{madry2018pgd}
\citation{krizhevsky2009learning}
\citation{russakovsky2015imagenet}
\citation{dong2018boosting,dong2019evading,guo2020backpropagating,li2023making}
\citation{Huang2017densely}
\citation{Simonyan2015}
\citation{He2016}
\citation{Zagoruyko2016}
\citation{Xie2017aggregated}
\citation{Han2017}
\citation{He2016}
\citation{Simonyan2015}
\citation{He2016}
\citation{Szegedy2016}
\citation{Huang2017densely}
\citation{Sandler2018mobilenetv2}
\citation{Hu2018}
\citation{Xie2017aggregated}
\citation{Zagoruyko2016}
\citation{Liu2018}
\citation{Tan2019mnasnet}
\newlabel{tab:quick_exp}{{1}{4}{Attack success rates of adversarial examples crafted on CIFAR10 dataset using original and transformed surrogate model under $\ell _\infty $ constraint with $\epsilon =4/255$ and $\epsilon =8/255$, PGD serves as the backbone method. `*' denotes white-box attacks.}{}{}}
\newlabel{eq:L2}{{6}{4}{}{}{}}
\newlabel{sec:experiments}{{}{4}{}{}{}}
\citation{dong2019evading}
\citation{lin2020nesterov}
\citation{guo2020backpropagating}
\citation{wang2021admix}
\citation{huang2022transferable}
\citation{guo2022intermediate}
\newlabel{tab:comapre_imagenet}{{2}{5}{Attack success rates of SOTA transfer-based untargeted attacks on ImageNet using ResNet-50 as the surrogate model and PGD as the backend attack method, under the $\ell _\infty $ constraint with $\epsilon =8/255$. `*' denotes white-box attack.}{}{}}
\citation{yang2020closer}
\citation{chaudhari2019entropy,keskar2017on,foret2020sharpness}
\citation{finlay2018improved,zhang2022rethinking}
\newlabel{tab:combine_cifar10}{{3}{6}{Attack success rates by combining SOTA transfer-based untargeted attacks with our methods, on CIFAR-10 using DenseNet as the surrogate model and PGD as the backbone attack method, under the $\ell _\infty $ constraint with $\epsilon =4/255$. `*' denotes white-box attack.}{}{}}
\newlabel{eq:lip_emp}{{9}{6}{}{}{}}
\newlabel{tab:lip_const}{{4}{6}{Empirical local Lipschitz constant of surrogate model estimated via Eq.\nobreakspace  {}\eqref  {eq:lip_emp}. The constants of DenseNet and ResNet50 are evaluated on CIFAR10 and ImageNet, respectively.}{}{}}
\newlabel{fig:loss}{{2}{7}{The loss of surrogate model (DenseNet) and target model (ResNet18), w.r.t. PGD-generated AE. It reveals that LRS-transformed models demonstrate more robustness and enable more transferable attacks.}{}{}}
\newlabel{fig:ablation}{{3}{7}{Ablation studies on average ASR under different hyperparameters $h$ and $\lambda $, the performance gains are consistent in a wide range of hyper-parameter values.}{}{}}
\newlabel{sec:conclusion}{{}{7}{}{}{}}
\bibdata{aaai24}
\gdef \@abspage@last{8}
