\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Bertsekas(1982)}]{Bertsekas1982PNM}
Bertsekas, D.~P. 1982.
\newblock Projected Newton Methods for Optimization Problems with Simple Constraints.
\newblock \emph{SIAM Journal on Control and Optimization}, 20(2): 221--246.

\bibitem[{Cao et~al.(2017)Cao, Luo, Li, and Li}]{Cao2017}
Cao, Z.; Luo, C.; Li, W.; and Li, S. 2017.
\newblock Joint Copying and Restricted Generation for Paraphrase.
\newblock In \emph{Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence}, AAAI'17, 3152â€“3158. AAAI Press.

\bibitem[{Chowdhury, Zhuang, and Wang(2022)}]{Chowdhury_Zhuang_Wang_2022}
Chowdhury, J.~R.; Zhuang, Y.; and Wang, S. 2022.
\newblock Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 36(10): 10535--10544.

\bibitem[{Conneau and Lample(2019)}]{NEURIPS2019_c04c19c2}
Conneau, A.; and Lample, G. 2019.
\newblock Cross-lingual Language Model Pretraining.
\newblock In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d\textquotesingle Alch\'{e}-Buc, F.; Fox, E.; and Garnett, R., eds., \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc.

\bibitem[{Creutz(2019)}]{Creutz2019}
Creutz, M. 2019.
\newblock {Open subtitles paraphrase corpus for six languages}.
\newblock \emph{LREC 2018 - 11th International Conference on Language Resources and Evaluation}, (2005): 1364--1369.

\bibitem[{Dong et~al.(2021{\natexlab{a}})Dong, Luu, Ji, and Liu}]{dong2021towards}
Dong, X.; Luu, A.~T.; Ji, R.; and Liu, H. 2021{\natexlab{a}}.
\newblock Towards robustness against natural language word substitutions.
\newblock \emph{arXiv preprint arXiv:2107.13541}.

\bibitem[{Dong et~al.(2021{\natexlab{b}})Dong, Luu, Lin, Yan, and Zhang}]{dong2021should}
Dong, X.; Luu, A.~T.; Lin, M.; Yan, S.; and Zhang, H. 2021{\natexlab{b}}.
\newblock How should pre-trained language models be fine-tuned towards adversarial robustness?
\newblock \emph{Advances in Neural Information Processing Systems}, 34: 4356--4369.

\bibitem[{Duolingo(2020)}]{DVN/38OJR6_2020}
Duolingo. 2020.
\newblock {Data for the 2020 Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE)}.

\bibitem[{Federmann, Elachqar, and Quirk(2019)}]{federmann-etal-2019-multilingual}
Federmann, C.; Elachqar, O.; and Quirk, C. 2019.
\newblock Multilingual Whispers: Generating Paraphrases with Translation.
\newblock In \emph{Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)}, 17--26. Hong Kong, China: Association for Computational Linguistics.

\bibitem[{Foundation(2019)}]{wmt19translate}
Foundation, W. 2019.
\newblock ACL 2019 Fourth Conference on Machine Translation (WMT19), Shared Task: Machine Translation of News.

\bibitem[{Freitag et~al.(2020)Freitag, Foster, Grangier, and Cherry}]{freitag-etal-2020-human}
Freitag, M.; Foster, G.; Grangier, D.; and Cherry, C. 2020.
\newblock Human-Paraphrased References Improve Neural Machine Translation.
\newblock In \emph{Proceedings of the Fifth Conference on Machine Translation}, 1183--1192. Online: Association for Computational Linguistics.

\bibitem[{Gan and Ng(2019)}]{gan-ng-2019-improving}
Gan, W.~C.; and Ng, H.~T. 2019.
\newblock Improving the Robustness of Question Answering Systems to Question Paraphrasing.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, 6065--6075. Florence, Italy: Association for Computational Linguistics.

\bibitem[{Goodfellow, Shlens, and Szegedy(2015)}]{goodfellow2015explaining}
Goodfellow, I.~J.; Shlens, J.; and Szegedy, C. 2015.
\newblock Explaining and Harnessing Adversarial Examples.
\newblock arXiv:1412.6572.

\bibitem[{Guo et~al.(2019)Guo, Liao, Jiang, Zhang, Zhang, and Liu}]{guo2019zeroshot}
Guo, Y.; Liao, Y.; Jiang, X.; Zhang, Q.; Zhang, Y.; and Liu, Q. 2019.
\newblock Zero-Shot Paraphrase Generation with Multilingual Language Models.
\newblock arXiv:1911.03597.

\bibitem[{Hosking and Lapata(2021)}]{hosking-lapata-2021-factorising}
Hosking, T.; and Lapata, M. 2021.
\newblock Factorising Meaning and Form for Intent-Preserving Paraphrasing.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, 1405--1418. Online: Association for Computational Linguistics.

\bibitem[{Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone, De~Laroussilhe, Gesmundo, Attariyan, and Gelly}]{pmlr-v97-houlsby19a}
Houlsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.; De~Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and Gelly, S. 2019.
\newblock Parameter-Efficient Transfer Learning for {NLP}.
\newblock In Chaudhuri, K.; and Salakhutdinov, R., eds., \emph{Proceedings of the 36th International Conference on Machine Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, 2790--2799. PMLR.

\bibitem[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora}
Hu, E.~J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2021.
\newblock LoRA: Low-Rank Adaptation of Large Language Models.
\newblock arXiv:2106.09685.

\bibitem[{Kaushik et~al.(2021)Kaushik, Gain, Kortylewski, and Yuille}]{kaushik2021understanding}
Kaushik, P.; Gain, A.; Kortylewski, A.; and Yuille, A. 2021.
\newblock Understanding Catastrophic Forgetting and Remembering in Continual Learning with Optimal Relevance Mapping.
\newblock arXiv:2102.11343.

\bibitem[{Krippendorff(1970)}]{Krippendorff1970EstimatingTR}
Krippendorff, K. 1970.
\newblock Estimating the Reliability, Systematic Error and Random Error of Interval Data.
\newblock \emph{Educational and Psychological Measurement}, 30: 61 -- 70.

\bibitem[{Lester, Al-Rfou, and Constant(2021)}]{lester-etal-2021-power}
Lester, B.; Al-Rfou, R.; and Constant, N. 2021.
\newblock The Power of Scale for Parameter-Efficient Prompt Tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, 3045--3059. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics.

\bibitem[{Lewis et~al.(2020)Lewis, Ghazvininejad, Ghosh, Aghajanyan, Wang, and Zettlemoyer}]{NEURIPS2020_d6f1dd03}
Lewis, M.; Ghazvininejad, M.; Ghosh, G.; Aghajanyan, A.; Wang, S.; and Zettlemoyer, L. 2020.
\newblock Pre-training via Paraphrasing.
\newblock In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., \emph{Advances in Neural Information Processing Systems}, volume~33, 18470--18481. Curran Associates, Inc.

\bibitem[{Li and Liang(2021)}]{li-liang-2021-prefix}
Li, X.~L.; and Liang, P. 2021.
\newblock Prefix-Tuning: Optimizing Continuous Prompts for Generation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, 4582--4597. Online: Association for Computational Linguistics.

\bibitem[{Liu et~al.(2022)Liu, Ji, Fu, Tam, Du, Yang, and Tang}]{liu-etal-2022-p}
Liu, X.; Ji, K.; Fu, Y.; Tam, W.; Du, Z.; Yang, Z.; and Tang, J. 2022.
\newblock {P}-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, 61--68. Dublin, Ireland: Association for Computational Linguistics.

\bibitem[{Miyato et~al.(2018)Miyato, ichi Maeda, Koyama, and Ishii}]{miyato2018virtual}
Miyato, T.; ichi Maeda, S.; Koyama, M.; and Ishii, S. 2018.
\newblock Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning.
\newblock arXiv:1704.03976.

\bibitem[{Niu et~al.(2021)Niu, Yavuz, Zhou, Keskar, Wang, and Xiong}]{niu-etal-2021-unsupervised}
Niu, T.; Yavuz, S.; Zhou, Y.; Keskar, N.~S.; Wang, H.; and Xiong, C. 2021.
\newblock Unsupervised Paraphrasing with Pretrained Language Models.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, 5136--5150. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics.

\bibitem[{Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu}]{papineni-etal-2002-bleu}
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
\newblock {B}leu: a Method for Automatic Evaluation of Machine Translation.
\newblock In \emph{Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics}, 311--318. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics.

\bibitem[{Pereira et~al.(2020)Pereira, Liu, Cheng, Asahara, and Kobayashi}]{pereira-etal-2020-adversarial}
Pereira, L.; Liu, X.; Cheng, F.; Asahara, M.; and Kobayashi, I. 2020.
\newblock Adversarial Training for Commonsense Inference.
\newblock In \emph{Proceedings of the 5th Workshop on Representation Learning for NLP}, 55--60. Online: Association for Computational Linguistics.

\bibitem[{Shafahi et~al.(2019)Shafahi, Najibi, Ghiasi, Xu, Dickerson, Studer, Davis, Taylor, and Goldstein}]{NEURIPS2019_7503cfac}
Shafahi, A.; Najibi, M.; Ghiasi, M.~A.; Xu, Z.; Dickerson, J.; Studer, C.; Davis, L.~S.; Taylor, G.; and Goldstein, T. 2019.
\newblock Adversarial training for free!
\newblock In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d\textquotesingle Alch\'{e}-Buc, F.; Fox, E.; and Garnett, R., eds., \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc.

\bibitem[{Shen et~al.(2022)Shen, Liu, Jiang, and Shi}]{shen-etal-2022-evaluation}
Shen, L.; Liu, L.; Jiang, H.; and Shi, S. 2022.
\newblock On the Evaluation Metrics for Paraphrase Generation.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, 3178--3190. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics.

\bibitem[{Snover et~al.(2006)Snover, Dorr, Schwartz, Micciulla, and Makhoul}]{snover-etal-2006-study}
Snover, M.; Dorr, B.; Schwartz, R.; Micciulla, L.; and Makhoul, J. 2006.
\newblock A Study of Translation Edit Rate with Targeted Human Annotation.
\newblock In \emph{Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers}, 223--231. Cambridge, Massachusetts, USA: Association for Machine Translation in the Americas.

\bibitem[{Thompson and Post(2020{\natexlab{a}})}]{thompson-post-2020-automatic}
Thompson, B.; and Post, M. 2020{\natexlab{a}}.
\newblock Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 90--121. Online: Association for Computational Linguistics.

\bibitem[{Thompson and Post(2020{\natexlab{b}})}]{thompson-post-2020-paraphrase}
Thompson, B.; and Post, M. 2020{\natexlab{b}}.
\newblock Paraphrase Generation as Zero-Shot Multilingual Translation: Disentangling Semantic Similarity from Lexical and Syntactic Diversity.
\newblock In \emph{Proceedings of the Fifth Conference on Machine Translation}, 561--570. Online: Association for Computational Linguistics.

\bibitem[{Yang et~al.(2019)Yang, Zhang, Tar, and Baldridge}]{yang-etal-2019-paws}
Yang, Y.; Zhang, Y.; Tar, C.; and Baldridge, J. 2019.
\newblock {PAWS}-{X}: A Cross-lingual Adversarial Dataset for Paraphrase Identification.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, 3687--3692. Hong Kong, China: Association for Computational Linguistics.

\bibitem[{Zhang et~al.(2019)Zhang, Zhang, Lu, Zhu, and Dong}]{NEURIPS2019_812b4ba2}
Zhang, D.; Zhang, T.; Lu, Y.; Zhu, Z.; and Dong, B. 2019.
\newblock You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle.
\newblock In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d\textquotesingle Alch\'{e}-Buc, F.; Fox, E.; and Garnett, R., eds., \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc.

\bibitem[{Zhang et~al.(2020)Zhang, Kishore, Wu, Weinberger, and Artzi}]{zhang-etal-2020-bertscore}
Zhang, T.; Kishore, V.; Wu, F.; Weinberger, K.~Q.; and Artzi, Y. 2020.
\newblock BERTScore: Evaluating Text Generation with BERT.
\newblock In \emph{International Conference on Learning Representations}.

\end{thebibliography}
