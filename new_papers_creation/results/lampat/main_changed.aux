\relax 
\bibstyle{aaai24}
\citation{freitag-etal-2020-human}
\citation{NEURIPS2020_d6f1dd03}
\citation{gan-ng-2019-improving}
\citation{Cao2017}
\citation{thompson-post-2020-paraphrase}
\citation{thompson-post-2020-automatic}
\citation{federmann-etal-2019-multilingual}
\citation{thompson-post-2020-paraphrase}
\citation{NEURIPS2019_812b4ba2}
\citation{kaushik2021understanding}
\citation{hu2021lora}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:lampat-architecture}{{1}{1}{The training process of LAMPAT consists of multiple stages. Firstly, we create a synthetic parallel corpus using unsupervised monolingual data. Next, we utilize LoRA to effectively fine-tune our model. Finally, we obtain the self-supervised model LAMPAT through the utilization of Virtual Adversarial Training.}{}{}}
\citation{hu2021lora}
\citation{hu2021lora}
\citation{hu2021lora}
\newlabel{tab:fail_example_mt}{{1}{2}{While human can change the structure of the sentence to create paraphrase (in bold), paraphrasing model usually tends to replace words or slightly modify the syntax (in italic). In the worst case, paraphrasing model even changes the meaning of the sentence by using inappropriate word replacement (in underline).}{}{}}
\newlabel{section:lampat}{{}{2}{}{}{}}
\citation{miyato2018virtual}
\citation{NEURIPS2019_812b4ba2}
\newlabel{fig:detail-lampat-architecture}{{2}{3}{LAMPAT is showcased using actual inputs. Initially, an input text undergoes corruption by removing stopwords and shuffling. Then, a noise perturbation, denoted as $\delta $, is introduced into the text embedding to generate a paraphrase that exhibits lexical diversity. The transformer block is replicated $N$ times, with the Multi-Head Attention component decomposed into low-rank matrices for efficient fine-tuning. Lastly, LAMPAT is trained using virtual adversarial training, incorporating a two-component loss function: the reconstruction loss $\mathcal  {L}_{rec}$ and the virtual adversarial regularizer $\mathcal  {L}_{vadv}$.}{}{}}
\newlabel{eq:emperical_risk}{{1}{3}{}{}{}}
\newlabel{eq:vat_adv_emperical_risk}{{2}{3}{}{}{}}
\newlabel{eq:main_emperical_risk}{{3}{3}{}{}{}}
\citation{wmt19translate}
\citation{thompson-post-2020-paraphrase}
\citation{guo2019zeroshot}
\citation{guo2019zeroshot}
\citation{guo2019zeroshot}
\newlabel{alg:main_algo}{{1}{4}{Low-rank Adaptation Multilingual Paraphrasing using Adversarial Training.}{}{}}
\citation{thompson-post-2020-paraphrase}
\citation{yang-etal-2019-paws}
\citation{Creutz2019}
\citation{DVN/38OJR6_2020}
\citation{Chowdhury_Zhuang_Wang_2022}
\citation{papineni-etal-2002-bleu}
\citation{snover-etal-2006-study}
\citation{zhang-etal-2020-bertscore}
\citation{hosking-lapata-2021-factorising}
\citation{shen-etal-2022-evaluation}
\citation{niu-etal-2021-unsupervised}
\citation{thompson-post-2020-paraphrase}
\citation{Krippendorff1970EstimatingTR}
\citation{guo2019zeroshot}
\citation{guo2019zeroshot}
\citation{guo2019zeroshot}
\newlabel{tab:main_result}{{2}{5}{Multilingual paraphrase generation test results over 4 languages English, Spanish, Chinese and Russian from the work of DAE \citep  {guo2019zeroshot}.}{}{}}
\newlabel{tab:perf_inp_only}{{3}{5}{Multilingual paraphrase generation test results on our input-only evaluation dataset.}{}{}}
\citation{pmlr-v97-houlsby19a}
\citation{li-liang-2021-prefix}
\citation{lester-etal-2021-power}
\citation{hu2021lora}
\citation{liu-etal-2022-p}
\citation{Bertsekas1982PNM}
\newlabel{tab:perf_inp_ref}{{4}{6}{Multilingual paraphrase generation test results on our input-reference evaluation dataset.}{}{}}
\newlabel{tab:human_eval}{{5}{6}{Human evaluation results. SP: Semantic Preservation; LS: Lexical Similarity; F: Fluency. All the scores reported are the average value of 5 chosen languages.}{}{}}
\newlabel{sec:ablation_study}{{}{6}{}{}{}}
\citation{thompson-post-2020-paraphrase}
\citation{thompson-post-2020-automatic}
\citation{guo2019zeroshot}
\citation{NEURIPS2019_c04c19c2}
\citation{thompson-post-2020-paraphrase}
\citation{goodfellow2015explaining}
\citation{pereira-etal-2020-adversarial}
\citation{dong2021towards}
\citation{miyato2018virtual}
\citation{dong2021should}
\citation{NEURIPS2019_812b4ba2}
\citation{NEURIPS2019_7503cfac}
\citation{miyato2018virtual}
\newlabel{tab:ablation_study}{{6}{7}{ParaScore of different fine-tuning methods on 13 languages. The mean and standard deviation are the weighted mean and standard deviation in all 13 languages.}{}{}}
\newlabel{fig:avg_records_of_adv}{{3}{7}{The average ParaScore of each technique over 13 languages.}{}{}}
\bibdata{anthology,custom}
\gdef \@abspage@last{8}
