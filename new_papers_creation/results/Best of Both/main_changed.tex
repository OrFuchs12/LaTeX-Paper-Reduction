\def\year{2022}\relax
%File: formatting-instructions-latex-2022.tex
%release 2022.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{booktabs}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{xcolor}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}

\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
%\nocopyright
%
% PDF Info Is REQUIRED.
% For /Title, write your title in Mixed Case.
% Don't use accents or commands. Retain the parentheses.
% For /Author, add all authors within the parentheses,
% separated by commas. No accents, special characters
% or commands are allowed.
% Keep the /TemplateVersion tag as is

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
%  -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
%  -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai22.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
% \title{Best of Both Worlds: Toward Lightweight\\Multi-Hop Explanation with Syntactic and Semantic Methods}
% \title{Reasoning and Re-Ranking in Multi-Hop Explanation:\\
% Integrating Syntactic and Semantic Methods}
\title{Best of Both Worlds:\\A Hybrid Approach for Multi-Hop Explanation with Declarative Facts}
\author{
    Shane Storks\textsuperscript{\rm 1}\thanks{Work completed during an internship with Amazon Alexa AI.},
    Qiaozi Gao\textsuperscript{\rm 2},
    Aishwarya Reganti\textsuperscript{\rm 2},
    Govind Thattai\textsuperscript{\rm 2}
}
\affiliations{
    \textsuperscript{\rm 1}University of Michigan\\
    \textsuperscript{\rm 2}Amazon Alexa AI\\
    sstorks@umich.edu, \{qzgao, areganti, thattg\}@amazon.com
}



\begin{document}

\maketitle

\begin{abstract}
Language-enabled AI systems can answer complex, multi-hop questions to high accuracy, but supporting answers with evidence is a more challenging task which is important for the transparency and trustworthiness to users. Prior work in this area typically makes a trade-off between efficiency and accuracy; state-of-the-art deep neural network systems are too cumbersome to be useful in large-scale applications, while the fastest systems lack reliability. In this work, we integrate fast syntactic methods with powerful semantic methods for multi-hop explanation generation based on declarative facts. Our best system, which learns a lightweight operation to simulate multi-hop reasoning over pieces of evidence and fine-tunes language models to re-rank generated explanation chains, outperforms a purely syntactic baseline from prior work by up to 7\% in gold explanation retrieval rate.
\end{abstract}


\section{Introduction}

Efficient and natural human communication relies on implicit shared knowledge and underlying reasoning processes. Despite rapid progress in language-enabled AI agents for tasks like question answering and more, state-of-the-art systems still struggle to explain their decisions in natural language. To improve their interpretability and robustness, a number of multi-hop explanation generation and identification benchmarks based on large, unstructured corpora of facts have been created~\cite{mihaylov2018can,khot2020qasc,jhamtani-clark-2020-learning}. However, when generating explanation chains, powerful deep neural networks can be too cumbersome to use in large-scale applications, while the fastest systems lack reliability, as they depend on syntactic features and ignore semantic relations between concepts~\cite{banerjee2020knowledge,jhamtani-clark-2020-learning}. In this work, we present novel approaches to integrate efficient syntactic retrieval methods with flexible semantic modeling methods for multi-hop explanation.
Our methods simulate a multi-hop reasoning process from the retrieval and synthesis of evidence to re-ranking candidate explanations.

% Mention the different types of knowledge resources, e.g., no structure, semi structure, fully structured.

%\textcolor{blue}{QZ: There are two types of explanation: introspective explanation and justification explanation~\cite{biran2017explanation}. The former explains how a decision is made, and the later gathers evidence to support a decision. Therefore, the task we are focusing on is justification explanation. This might help us distinguish this work with a lot of existing papers (including \cite{yadav-etal-2021-want}), since most of them focus on first finding the supporting knowledge and then answer the question.}


\section{Related Work}

%\textcolor{teal}{shane: Contributing a few sentences about the multi-hop explanation for QA. Use them as you see fit.}

%\paragraph{Multi-hop explanation.}
Recent work has focused on different aspects of multi-hop reasoning for question answering and related natural language understanding tasks. One line of work has incorporated highly structured knowledge graphs into language understanding by
combining graphical methods with language models~\cite{lin-etal-2019-kagnet,ji-etal-2020-language,yasunaga-etal-2021-qa},
augmenting language model inputs with relational knowledge~\cite{zhang-etal-2019-ernie,chen-etal-2020-improving,xu-etal-2021-fusing},
and applying language models to relational knowledge to infer multi-hop reasoning paths through knowledge graphs \cite{wang-etal-2020-connecting}. Others have further explored training language models with semi-structured relational knowledge \cite{sap2019atomic,bosselut-etal-2019-comet,mostafazadeh-etal-2020-glucose,Hwang2020COMETATOMIC}, i.e., where nodes are natural language sentences rather than canonicalized concepts, to later use for generating multi-hop explanations in natural language \cite{shwartz-etal-2020-unsupervised,Bosselut2019DynamicKG}.

For generating multi-hop explanations from entirely unstructured corpora, other work has explored using multi-step syntactic information retrieval methods \cite{jhamtani-clark-2020-learning}, and modeling such corpora as knowledge graphs with relations induced by shared mentions of concepts between documents \cite{dhingra2020differentiable,lin-etal-2021-differentiable}. While the former approach lacks the ability to capture semantic relationships between evidence sentences, the latter demands high time and space complexity both in generating a graph from corpora of millions of facts, and in everyday uses of adding or removing facts from the corpus.
More recent work has used pre-trained word embeddings to add some lightweight semantic representation to syntactic evidence retrieval \cite{yadav-etal-2021-want}.
Unlike these approaches, we present a flexible and relatively lightweight pipeline to apply both syntactic and learned, contextualized semantic approaches in multi-hop explanation generation, including evidence retrieval, multi-hop reasoning over evidence, and re-ranking candidate explanations.

% NOTE for later: the Yadav paper uses more relaxed evaluation setting, so we can't compare results (looks for two facts separately)

%\textcolor{red}{The best way I can think of to distinguish us from \cite{yadav-etal-2021-want} is that I think they only use primarily syntactic approaches to gather evidence (then combine them with the language model reranker). I tried to cite them at the end of the previous paragraph.}
%\textcolor{blue}{QZ: Seems their method \cite{yadav-etal-2021-want} do include some sort of semantic representation, via calculating cosine similarity between tokens' GLoVe vector. Maybe we can emphasize our semantic approach is on sentence level, instead of only on token level.}

%\paragraph{Reasoning with unstructured knowledge.}
%DrKIT \cite{dhingra2020differentiable}
%DrFact \cite{lin-etal-2021-differentiable}

\section{Problem Statement}
In the research community, two types of explanation have been studied: introspective explanation and justification explanation~\cite{biran2017explanation}. The former explicates how a decision is made, and the latter gathers evidence to support a decision. In this study, we focus on the task of justification explanation. Specifically, we explore the problem of generating multi-hop explanations to support the answer to a natural language question, where the explanation chain is generated from an unstructured corpus of declarative facts. Unstructured natural language corpora are suitable knowledge resources for human-AI interaction, as humans can easily support reasoning by providing their own commonsense knowledge in short, natural language statements. This carefully restricted problem of explanation generation consists of two key challenges. First, we must solve the \textit{retrieval} task to gather candidate supporting evidence from the corpus. Second, we need to invoke a \textit{multi-hop reasoning} process to connect pieces of evidence to form the most valid explanation to justify the answer to the question.

% The general problem formulation is to select facts from a corpus to support the reasoning. Mention the advantages of such an approach over using something more structured - suitable for user input, etc.

% We restrict the problem to the aspect of multi-hop explanation generation from an unstructured corpus. Given a question and an answer, the task is to find supporting facts from a large corpus, and form them into a multi-hop explanation chain.

% Talk about challenges of this problem.

% \paragraph{Information retrieval.}
% To gather candidate supporting evidence to later form an explanation, we should use information retrieval approaches to identify facts in the corpus. Approaches may be syntactic, i.e., based primarily on matching of keywords, or semantic.
% Traditional approaches include TF-IDF and BM25~\cite{bm25}.

% \paragraph{Reasoning.}

\subsection{Datasets}
To explore this problem, we consider two datasets. First, the Question Answering via Sentence Composition (QASC) dataset provides about 10,000 multiple-choice science questions \cite{khot2020qasc}. QASC is a challenging problem, as each question requires composing two facts from a corpus of about 17 million declarative facts to connect the question and its correct answer.
For example, given the question ``\textit{Differential heating of air} can be harnessed for what?'' and correct answer ``\textit{electricity production},'' the answer can be explained by composing the facts ``\textit{Differential heating of air} produces wind'' and ``Wind is used for \textit{producing electricity},'' which connect the question and answer.
QASC includes a gold, human-curated 2-hop explanation from the corpus for each question-answer pair.

Meanwhile, the Explainable QASC (eQASC) dataset adds 10 automatically generated explanations for each question-answer pair, each of which are labeled by annotators as valid or invalid \cite{jhamtani-clark-2020-learning}.
% eQASC was generated using a simple syntactic multi-hop retrieval approach.
While the state-of-the-art accuracy on QASC has reached up to 90\%,\footnote{See \url{https://allenai.org/data/qasc}.} only 76\% of questions have any valid explanation chains in eQASC. This indicates that \textit{explaining} the answers to questions in QASC is a more challenging problem than answering them. This motivates us to further explore the problem of generating multi-hop explanations for QASC.

\section{Methods}
In our experiments toward multi-hop explanation generation, we consider syntactic and semantic multi-hop retrieval methods, then explore ways to re-rank retrieved explanations to reduce the pool of candidates.

\subsection{Syntactic Methods}\label{sec:syntactic}
Syntactic information retrieval methods enable quick searching of millions of documents.
eQASC was originally generated using ElasticSearch, \footnote{https://www.elastic.co/} a fast but primarily syntactic search engine based on keyword overlap. After indexing the QASC corpus facts into an ElasticSearch index, \citet{jhamtani-clark-2020-learning} used a simple procedure (shown in Figure~\ref{fig:syntactic_pipeline}) to generate a 2-hop explanation for each question-answer pair from QASC. First, query the corpus for $N=20$ candidate first facts. For each candidate first fact, query the corpus for $M=4$ candidate second facts, where each candidate second fact must contain a word that appears in the question-answer pair, and a word that appears in the first fact. The purpose of this restriction is to force the resulting chain of facts to connect concepts in the question and answer through intermediate concepts. Lastly, from the set product of all candidate first facts and all candidate second facts, select up to $K=10$ candidate explanation chains, ranked by the sum of retrieval scores from the ElasticSearch engine.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{syntactic_pipeline.pdf}
    %removedVspace
    \caption{Syntactic pipeline used to generate multi-hop explanations in eQASC. First, the question-answer (Q-A) pair is used to query the ElasticSearch index for $N$ candidate first facts, each of which is used to query it for $M$ candidate second facts. All candidate first and second facts are paired, and the top-scored $K$ chains are returned as explanations. }
    %removedVspace
    \label{fig:syntactic_pipeline}
\end{figure}

\paragraph{Expanding syntactic retrieval.}
This is a simple, fast approach to generate a large number of candidate explanations. To improve the likelihood of generating a valid explanation, we can expand and diversify the search results by increasing $N$, $M$, and $K$. Specifically, we increase each of them to 200.

\subsection{Semantic Methods}\label{sec:semantic}
Alternatively, semantic information retrieval methods can enable stronger meaning representation than syntactic methods with a trade-off of search speed. Typical approaches generate a semantic vector embedding for all documents in a corpus. They then generate a comparable embedding of the query, and use vector similarity measures to rank documents.

\paragraph{Dense passage retrieval.}
Dense passage retrieval (DPR) is a recent approach to semantic information retrieval which learns dual encoders for queries and documents \cite{karpukhin-etal-2020-dense}. They are trained such that the query and document encoders generate similar embeddings for semantically similar queries and documents. Similarity is measured by inner product of vectors, and is maximized for matching queries and documents, but minimized for irrelevant queries and documents.
We can then use the document encoder to index the facts in a corpus, and efficiently query the index using the embedding from the query encoder.

For each question-answer pair in QASC and the two facts in its gold explanation chain, we can train a dense passage retriever to generate similar embeddings for the question-answer pair (query) and these facts (documents), then encode all facts in the QASC corpus for search purposes. %It is worth noting that this learning problem is slightly more challenging than the original formulation of DPR, as we must learn a query embedding that matches the embeddings of multiple documents, and we must learn document embeddings that can match the embeddings of multiple queries. \textcolor{blue}{(QZ: not sure about this sentence)}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{semantic_pipeline.pdf}
    %removedVspace
    \caption{Proposed semantic explanation pipeline. Facts are encoded using a fact encoder (in blue) and stored in a dense index, while the question-answer pair is encoded by a query encoder (in yellow). Maximum inner product search (MIPS) is used to query the index for $N$ candidate first facts, which are each re-encoded (in green), then used to query the index again for $M$ candidate second facts. All candidate first and second facts are paired, and the top-scored $K$ chains are returned as explanations. }
    %removedVspace
    \label{fig:semantic expl pipeline}
\end{figure*}

\paragraph{Multi-hop reasoning.}
To generate a multi-hop explanation using this approach, we need to facilitate reasoning over the facts and queries. Given a question-answer pair from QASC, we first query the DPR index for {$N=5$} facts. To reduce error accumulation in generating the chain of facts, we then \textit{re-encode} each fact into a new query embedding incorporating the candidate fact and the original query.

The re-encoder is a lightweight feedforward network inspired by a similar fact-translating function proposed in \citet{lin-etal-2021-differentiable}. Given the embeddings $q_{QA}$ and $d_1$ for the question-answer pair query and first fact document respectively, we use the gold explanation chains from QASC to learn the re-encoder $g(q_{QA}, d_1)$. Specifically, if $d_1$ is the embedding of the first fact in the gold explanation chain, we maximize the inner product between the re-encoded output $q_{r}$ and the document embedding $d_2$ for the second gold fact. Next, we query the DPR index again using $q_r$ to obtain {$M=2$} candidate second facts. To reduce noise, we filter out any facts that mention no concepts in either the question or answer.
Lastly, from the set product of all candidate first facts and all candidate second facts, select up to $K=10$ candidate explanation chains, ranked by the sum of retrieval scores, i.e., inner products when querying the DPR index. Our semantic explanation pipeline is shown in full in Figure~\ref{fig:semantic expl pipeline}. It is worth noting that our lightweight re-encoder operation can extend to any number of hops.

\subsection{Re-Ranking Candidate Explanations}
Both of our syntactic and semantic multi-hop retrieval systems can quickly propose candidate explanation chains for questions in QASC. However, both approaches over-generate candidates, and the high number of candidates (i.e., up to 200) limits the practical usefulness of the systems to an end user. As such, we lastly propose a re-ranker for candidates based on large-scale, pre-trained language models \cite{devlin-etal-2019-bert,liu2019roberta}. Specifically, we use the gold explanation chains from QASC to fine-tune a language model to the classification task of whether or not a candidate explanation is valid for a question-answer pair. We then re-rank a pool of candidates based on the system's estimated likelihood that each explanation chain is valid, and keep only the top $K=10$ candidates for a direct comparison to the syntactic approach used to generate eQASC.

\section{Experimental Results}
We next apply these approaches to the task of selecting 2-hop reasoning chains for question-answer pairs in QASC, and directly compare our results to the original procedure to generate eQASC. We compare systems by their individual \textit{gold retrieval rate} on the validation set for QASC, i.e., the percentage of question-answer pairs for which the gold explanation chain from QASC was successfully reproduced.\footnote{As the ordering of facts in QASC explanation chains does not typically matter, the gold retrieval rate counts both the forward and reverse forms of gold explanation chains.} This serves as an indicator of the quality of generated explanations, as it suggests that generated explanations tend to look more like those curated from the corpus by humans.

\subsection{Expanded Syntactic Explanation}
As mentioned earlier, we first expanded the ElasticSearch-based approach used to generate eQASC by increasing the search hyperparameters $N$, $M$, and $K$ each to 200. Selected results from this are listed in Table~\ref{tab:expand syntactic}. By only increasing $K$ (i.e., the number of candidate explanation chains considered) to 200, the retrieval rate increases from 31.1\% to 37.0\%. When increasing $N$ and $M$ (i.e., the number of candidate first and second facts considered) also to 200, the retrieval rate further increases to 46.5\%, a net 15.4\% gain.

\begin{table}
    \centering
    \footnotesize
    \begin{tabular}{ccc|c}
        \toprule
        \textbf{N} & \textbf{M} & \textbf{K} & \textbf{Gold Retrieval Rate (\%)} \\\midrule
        20 & 4 & 10 & 31.1 \\
        20 & 4 & 200 & 37.0 \\
        200 & 200 & 200 & \textbf{46.5} \\
        \bottomrule
    \end{tabular}
    \normalsize
    \caption{Gold explanation chain retrieval rates for syntactic multi-hop retrieval with ElasticSearch on QASC validation set. The first row indicates the original search hyperparameters used to generate eQASC, while the last two rows increase hyperparameters to expand and diversify the search.}
    %removedVspace
    \label{tab:expand syntactic}
\end{table}

\subsection{Syntactic-Semantic Multi-Hop Explanation}
Next, we incorporate our semantic multi-hop retrieval process powered by DPR.

\paragraph{Training details.}
The dual encoders for DPR are learned starting from pre-trained \textsc{BERT}-base~\cite{devlin-etal-2019-bert}. The best encoders are selected based on the mean squared error between embeddings for matching question-answer pairs and facts on the QASC validation set. The batch size is fixed at 16, while learning rate and number of training epochs are selected based on a grid search.
%Our best DPR system achieves a validation mean squared error of 0.0446 when trained with a learning rate of 1e-4 for 13 epochs.
For the re-encoder, the training batch size, learning rate, and number of epochs are similarly selected based on a grid search, minimizing the mean squared error between the output re-encoded queries and target fact embeddings on the validation set.
%The best re-encoder achieves a validation loss of 0.0539 when trained on top of our best DPR instance with a batch size of 64 and learning rate of 1e-5 for 3 epochs.

\paragraph{Results.}
Table~\ref{tab:semantic} compares the gold retrieval rate of various combinations of the syntactic and semantic approaches for multi-hop retrieval on the QASC validation and testing sets.\footnote{When combining the syntactic and semantic approaches, we replace up to the lowest-ranked 25\% of syntactic candidate explanation chains with the top semantic candidate explanation chains.} Our results show that while using only the semantic candidate explanation chains leads to a 13.9\% gold retrieval rate at best, combining the expanded syntactic and semantic candidates gives us the best result of up to 51.1\% gold retrieval rate, outperforming the case where only the expanded syntactic candidates are considered. Thus, the semantic approach finds some of the missing gold explanations that the syntactic approach misses, suggesting that both syntactic and semantic approaches are needed for generating the best-quality explanations on QASC questions.

\begin{table}
    \centering
    \footnotesize
    \begin{tabular}{c|cc}
    \toprule
        \textbf{Approach}  & \multicolumn{2}{c}{\textbf{Gold Retrieval Rate (\%)}} \\
         & \textit{Validation} & \textit{Test} \\\midrule
        syntactic & 37.0 & 40.2 \\
        syntactic (exp.) & 46.5 & 49.3 \\
        semantic & 10.8 & 13.9 \\
        syntactic (exp.) + semantic & \textbf{49.9} & \textbf{51.1} \\
        \bottomrule
    \end{tabular}
    \normalsize
    \caption{Gold explanation chain retrieval rates (top $K=200$ candidates) for combinations of multi-hop retrieval approaches on QASC. Syntactic refers to the second result from Table~\ref{tab:expand syntactic}, while syntactic (exp.) refers to the expanded third result. Semantic refers to the previously introduced DPR-based multi-hop retrieval approach.}
    %removedVspace
    \label{tab:semantic}
\end{table}


\subsection{LM Re-Ranking}
While our results improve the gold retrieval rate by a wide margin, recall that our multi-hop retrieval approaches for QASC increase the number of candidate explanation chains $K$ to 200. Such a large set of candidates is not useful in practice, as a human user would have to sort through a cumbersome number of explanations in order to judge the machine's understanding of the question and answer. As such, we lastly present our experiments on \textit{re-ranking} candidate explanation chains, which enables us to truncate our results to $K=10$ top candidate explanation chains without massive performance drops, and consequently compare our approach directly to the original approach used to generate eQASC.


\begin{table}
    \centering
    \footnotesize
    \begin{tabular}{c|c|cc}
    \toprule
        \textbf{Retrieval Approach}  & \textbf{Re-Ranker} &\multicolumn{2}{c}{\textbf{Gold RR (\%)}} \\
         & & \textit{Val.} & \textit{Test} \\\midrule
        syntactic & -- & 31.1 & 34.1 \\\midrule
        syntactic (exp.) & \textsc{BERT} & 36.3 & 34.0 \\
        syntactic (exp.) + semantic & \textsc{BERT} & {36.4} & {34.1} \\\midrule
        % syntactic & -- & -- & -- \\
        syntactic (exp.) & \textsc{RoBERTa} & 37.9 & 36.2 \\
        syntactic (exp.) + semantic & \textsc{RoBERTa} & \textbf{38.1} & \textbf{36.4} \\
        \bottomrule
    \end{tabular}
    \normalsize
    \caption{Gold explanation chain retrieval rates (RR; top $K=10$ candidates) for combinations of multi-hop retrieval approaches on QASC, re-ranked by fine-tuned language models. Syntactic refers to the original approach used to generate eQASC, while syntactic (exp.) refers to the expanded third result from Table~\ref{tab:expand syntactic}. Semantic refers to our proposed DPR-based multi-hop retrieval approach.}
    %removedVspace
    \label{tab:reranking}
\end{table}

\paragraph{Training details.}
Using the re-ranking approach described earlier, we fine-tune the \textsc{BERT}~\cite{devlin-etal-2019-bert} and \textsc{RoBERTa}~\cite{liu2019roberta} %(\textcolor{red}{Not sure if we used RoBERTa-large or base? It's RoBERTa base})
pre-trained language models.\footnote{For both models, we use the ``base'' form which has 12 hidden layers, a hidden dimension of 768, and 12 attention heads.} Models are trained with a 1:2 ratio of gold and invalid explanation chains, with 3 unique invalid explanation chains randomly sampled from ElasticSearch results per gold explanation chain (forward and reverse forms). Models are selected based on instances achieving the highest top-1 gold retrieval rate, i.e., proportion of question-answer pairs where the gold explanation chain is ranked highest, on the QASC validation set similarly redistributed in this way.
%Batch size is fixed at 128 (the maximum power of 2 that could fit in GPU memory), while learning rate and number of training epochs are selected based on a grid search. Our best \textsc{BERT} model achieves 88.7\% validation top-1 gold retrieval rate when trained with a learning rate of 5e-5 for 3 epochs.

%\textcolor{red}{If the RoBERTa fine-tuning is not consistent with BERT, we can remove some of these unnecessary details.}

\paragraph{Results discussion.}
Table~\ref{tab:reranking} compares the final gold retrieval rates for the top $K=10$ re-ranked candidates from various approaches. While the original syntactic approach for generating eQASC achieves a respective 31.1\% and 34.1\%
% \textcolor{red}{(this number does not agree with Table 3. Is it 34.1 or 34.4?)}
gold retrieval rate on the validation and testing sets, our expanded syntactic approach achieves up to 37.9\% and 36.2\% gold retrieval rate with \textsc{RoBERTa}. Again with \textsc{RoBERTa}, our syntactic-semantic multi-hop retrieval achieves the best results of 38.1\% and 36.4\% on the validation and testing sets, respectively, exceeding the baselines.
After narrowing down from 200 candidate explanations to 10 with the re-ranker, we retain up to a 7.0\% net improvement of gold retrieval rate compared with the baseline. Given that the net gain was 13.9\% with 200 candidate explanations, one future direction is to improve the re-ranker performance, so that we can retain more of this improvement.
% While our best approach achieves up to a net 7.0\% gain over the baseline syntactic approach used to generate eQASC, much of our performance gain was lost after re-ranking.
To achieve this, one option is to revisit the re-ranker training, which did not incorporate negative examples proposed by DPR, and may experience generalization issues.
% This is possibly because the negative examples proposed by DPR were not incorporated in re-ranker training, which may cause generalization issues.
% Future work may further explore the re-ranking problem to better preserve the retrieval rates from the larger candidate pools explored earlier.




\section{Conclusion}
In this work, by utilizing a small amount of ground truth supervision, we explored approaches to improve the generation of multi-hop explanations from a corpus of declarative facts. We show that both fast, syntactic methods and slow, semantic methods are useful for gathering relevant evidence for explanation.
To facilitate multi-hop reasoning from one piece of evidence to the next, we had some success in using a lightweight feedforward re-encoder, as opposed to state-of-the-art graph-based approaches that consume too much time and memory for practical online use.
As many of our approaches over-generate candidate explanations, we lastly explored using pre-trained language models to re-rank and filter candidates. Our results suggest this is a significant challenge, and future work may further explore this problem.


Similique corporis atque fugiat consequatur non minima, alias ab vitae recusandae suscipit,
\bibliography{main}

\end{document}