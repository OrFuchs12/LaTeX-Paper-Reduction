%!TEX root = ./SelfMGNN.tex

In the technical appendix, we provide further details for the proof and derivations as well as the experiments.

\section{A. On the Attentional Aggregation}
In this section, we first start with attentional aggregation in the Euclidean space, 
and then elaborate on the necessary operations in the $\kappa$-stereographic model $\mathcal M$ and generalize the attentional aggregation in the Euclidean space to the $\kappa$-stereographic model.
Finally, we prove the \emph{Theorem 1 ($\kappa$-left-matrix-multiplication as aggregation)}.

\subsection{Attentional aggregation in the Euclidean space}
In this part, we show that attentional aggregation is a linear combination of the  feature vectors with the learnable weights and left-matrix-multiplication essentially performs the attentional aggregation.

%The attentional aggregation is the summation of feature vectors with the learnable weights. 
Given a target node $i$ and its neighbor node set $\mathcal N_i$, 
the encoding of the target $\boldsymbol z_i$  is updated as 
$\sum\nolimits_j A_{ij}\boldsymbol z_j$,
where $j \in \Omega$ and $\Omega =\mathcal N_i \cup i$. We add a self-loop in particular to keep the information of the node itself, and $A_{ij}$ denotes the attentional weight to be learned \cite{WuPCLZY21,velickovic2018graph}.
Obviously, the attentional aggregation can be rewritten as a linear combination  $\boldsymbol L(\cdot, \cdot)$. 
Specifically, the linear combination is defined as $\boldsymbol L(c(A_{ij}), \boldsymbol z_j)=\sum\nolimits_j c(A_{ij})\boldsymbol z_j$,
where $c(A_{ij})$ is the coefficient function to output a real coefficient scaling the corresponding $\boldsymbol z_j$. We have $c(A_{ij})=A_{ij}$ for the attentional aggregation above.

Let us consider left-matrix-multiplication of $\mathbf Z\in \mathbb R^{N \times D}$ by $\mathbf A\in \mathbb R^{N \times N}$, i.e., we have $\mathbf A\mathbf Z \in \mathbb R^{N \times D}$.  
The $i^{th}$ row of $(\mathbf A\mathbf Z)$ is given as follows:
\begin{equation}
A_{i1}\boldsymbol z_1+A_{i2}\boldsymbol z_2 + \cdots + A_{ij}\boldsymbol z_j+ \cdots +A_{iN}\boldsymbol z_N,
\end{equation}
where $ \boldsymbol z_j$ is the $j^{th}$ row of $\mathbf Z$.
In fact, the row-wise left-matrix-multiplication is given by the linear combination $(\mathbf A\mathbf Z)_{i \bullet}=\boldsymbol L(c(A_{ij}), \boldsymbol z_j)$ with $\mathbf A$, performing the attentional aggregation.
Thus, we can give the lemma as follows:
\newtheorem*{lemma}{Lemma 1 (Left-matrix-multiplication as attentional aggregation)}
\begin{lemma}
Given $\mathbf{Z}\in \mathbb R^{N \times D}$ holding encoding vectors in its row and weights $\mathbf{A}\in \mathbb R^{N \times N}$, 
the left-matrix-multiplication of $\mathbf{Z}$ by $\mathbf{A}$ updates encoding vectors in $\mathbf{Z}$ with attentional aggregation.
\end{lemma}

\subsection{Operations in the $\boldsymbol \kappa$-stereographic model}
In this part, we first review the notion of \emph{midpoint} in the $\kappa$-stereographic model, and then generalize the Euclidean left-matrix-multiplication  to the \emph{$\kappa$-left-matrix-multiplication} in the $ \kappa$-stereographic model with the \emph{midpoint}.

The midpoint in the Euclidean space is intuitive, however, it is nontrivial in the $\kappa$-stereographic model as the manifold is curved.
We give the definition of the (weighted) midpoint in the $\boldsymbol \kappa$-stereographic model as follows:
\newtheorem*{def2}{Definition 1 (Midpoint in the $\boldsymbol \kappa$-stereographic model)} 
\begin{def2}
Given a set of  $\kappa$-stereographic vectors $\{\mathbf{x}_{i}\}_{i=1}^n$, and weights $\boldsymbol{\alpha} \in \mathbb R^n$,
the weighted midpoint in the $\kappa$-stereographic model is calculated via $\mathbf{mid}_{\kappa}\left( \{\mathbf{x}_{i}\}_{i=1}^n ; \boldsymbol{\alpha}\right)$ as follows:
\begin{equation}
%\resizebox{0.91\hsize}{!}{$
\mathbf{mid}_{\kappa}\left( \{\mathbf{x}_{i}\}_{i=1}^n ; \boldsymbol{\alpha}\right)=\frac{1}{2} \otimes_{\kappa} \left(\sum_{i=1}^{n} \frac{\alpha_{i} \lambda_{\mathbf{x}_{i}}^{\kappa}}{\sum_{j=1}^{n} \alpha_{j} (\lambda_{\mathbf{x}_{j}}^{\kappa}-1)} \mathbf{x}_{i}\right),
%$}
\end{equation}
where  $\lambda_{\mathbf{x}_{j}}^{\kappa}=4\left(1+\kappa||\mathbf x||_2^2\right)^{-2}$ is the conformal factor.
\end{def2}
\noindent Note that, the midpoint in the $\kappa$-stereographic model is essentially a linear combination regulated with a $\kappa-$scaling.

With the geometry of the $\kappa$-stereographic model, we give the definition  of $\kappa$-left-matrix-multiplication following \citet{BachmannBG20}, the generalization of left-matrix-multiplication in the Euclidean space, below:
\newtheorem*{def0}{Definition 2 ($\boldsymbol \kappa$-left-matrix-multiplication)} 
\begin{def0}
Given $\mathbf{Z}\in \mathbb R^{N \times D}$ holding $\kappa$-stereographic vectors in its row and weights $\mathbf{A}\in \mathbb R^{N \times N}$, 
the $\kappa$-left-matrix-multiplication of $\mathbf{Z}$ by $\mathbf A$ is defined as follows:
\begin{equation}
\left(\mathbf{A} \boxtimes_{\kappa} \mathbf{Z}\right)_{i \bullet}:= A \otimes_{\kappa} \mathbf{mid}_{\kappa}\left( \{ \mathbf{Z}_{i \bullet}\}_{i=1}^n ; \mathbf{A}_{i \bullet}\right),
\end{equation}
where $A=\sum_{j} \mathbf A_{i j}$, $\mathbf{mid}_{\kappa}$ denotes midpoint in the $\kappa$-stereographic model.
\end{def0}

\subsection{Attentional Aggregation in  $\boldsymbol \kappa$-stereographic Model}
In this part, we show that the $\kappa$-left-matrix-multiplication performs the attentional aggregation in $\kappa$-stereographic model.
In other words, we prove \textbf{Theorem 1} in the subsection of \emph{attentional aggregation layer} in our paper.

With the formal definition of the linear combination, we rewrite the  \textbf{Theorem 1} equivalently as follows:
\newtheorem*{thm1}{Theorem 1 ($\boldsymbol \kappa$-left-matrix-multiplication as attentional aggregation)}
\begin{thm1}
Let rows of $\mathbf{H}$ hold the encoding $\boldsymbol z_{\mathcal M_i}$, (linear transformed by $\mathbf{W}$)
and $\mathbf{A}$ hold the attentional weights,
the  $\kappa$-left-matrix-multiplication $\mathbf{A} \boxtimes_{\kappa} \mathbf{H}$ performs the attentional aggregation over the rows of $\mathbf{H}$, i.e., 
$\mathbf{A} \boxtimes_{\kappa} \mathbf{H}$ is the row-wise linear combination of  $\mathbf{H}$ with respect to attentional weight $\mathbf{A}_{ij}$: 
\begin{equation}
(\mathbf{A} \boxtimes_{\kappa} \mathbf{H})_{i \bullet}=\boldsymbol L(c_{\text{stereo}}(\mathbf{A}_{ij}), \mathbf{h}_{j}),
\end{equation}
% \begin{equation}
% (\mathbf{A} \boxtimes^{\kappa} \mathbf{H})_{i \bullet}=\oplus^{\kappa}_{j\in \Psi}(\mathbf{A}_{ij} \otimes^{\kappa} \mathbf{H}_{i \bullet}),
% \end{equation}
where  $\mathbf{h}_{j}$ is the $j^{th}$ row of $\mathbf H$, $j$ enumerates the index set $\Psi$, $\Psi=i \cup \mathcal N_i$ and $\mathcal N_i$ is the neighbors of $i$ on the graph. $c_{\text{stereo}}(\cdot)$ is the function to output the coefficient in the $\kappa$-stereographic model.
\end{thm1}
\begin{proof}
Recall the design of the (intra-component) attentions in our paper:
$\mathbf A$ is given as $\hat{\mathbf A}+ \mathbf I$,
where $\hat{\mathbf A}$ is filled with softmax values in its row, and $\mathbf I$ is the identity matrix to keep the initial information of the node itself.
That is, we have the row sum, $A=2$.
Then, with the definitions of \emph{$\kappa$-left-matrix-multiplication} and \emph{midpoint in the $\kappa$-stereographic model}, we give the derivation as follows:
\begin{equation}
\begin{aligned}
   &(\mathbf{A} \boxtimes_{\kappa} \mathbf{H})_{i \bullet}\\
=&A \otimes_{\kappa} \mathbf{mid}_{\kappa}\left( \{ \mathbf{h}_j\}_{j=1}^n ; \{\mathbf A_{ij}\}_{j=1}^n\right)\\
=&2\otimes_{\kappa} \frac{1}{2} \otimes_{\kappa} \left(\sum_{j=1}^{n} \frac{\mathbf A_{ij} \lambda_{\mathbf{h}_{j}}^{\kappa}}{\sum_{l=1}^{n} \mathbf A_{il} (\lambda_{\mathbf{h}_{l}}^{\kappa}-1)} \mathbf{h}_{j}\right)\\
=&\sum_{j=1}^{n} \frac{\mathbf A_{ij} \lambda_{\mathbf{h}_{j}}^{\kappa}}{\sum_{l=1}^{n} \mathbf A_{il} (\lambda_{\mathbf{h}_{l}}^{\kappa}-1)} \mathbf{h}_{j}\\
=&\boldsymbol L(c_{\text{stereo}}(j), \mathbf{h}_{j}),\\
\end{aligned}
\end{equation}
where the coefficient function is given as $c_{\text{stereo}}(j)=\frac{1}{C}\mathbf A_{ij} \lambda_{\mathbf{h}_{j}}^{\kappa}$, and $C=\sum_{l=1}^{n} \mathbf A_{il} (\lambda_{\mathbf{h}_{l}}^{\kappa}-1)$.
As shown above, with the well-designed attention mechanism, 
we eliminate the $\kappa$-scaling and make the $\kappa$-left-matrix-multiplication to be the linear combination with respect to the learnable attentions, 
performing the attentional aggregation.
\end{proof}


 \begin{table}
    \centering
    \setcounter{table}{1}
          \caption{The statistics of  the datasets.}
    \begin{tabular}{ p{1.5cm}<{\centering} |   p{2cm}<{\centering}  p{2cm}<{\centering}   p{1.2cm}<{\centering}}
      \toprule
\textbf{Dataset}&   \#(Node)   &  \#(Links) &   \#(Labels)      \\
\toprule
\textbf{Citeseer}& $\ \ 3,327$ & $\ \ \ \ 4,732$ & $6$ \\
\textbf{Cora}      & $\ \ 2,708$ & $\ \ \ \ 5,429$ & $7$ \\
\textbf{Pubmed}& $19,717$ & $\ \ 44,338$ & $3$ \\
\textbf{Amazon}& $13,381$ & $245,778$ & $10$ \\
\textbf{Airport}  & $\ \ 1,190$ & $\ \ 13,599$ & $4$ \\
      \bottomrule
    \end{tabular} 
        \label{statistics}
  \end{table}

\section{B. Experimental Details}
In this section, we give further experimental details, including data \& code  and implementation notes, in order to enhance the \emph{reproducibility}.
\subsection{Data and Code}
\subsubsection{Data} The datasets used in this paper are publicly available, i.e., Citeseer, Cora, Pubmed, Amazon and Airport. We briefly describe these datasets as follows:
\begin{itemize}
   \item Citeseer , Cora and Pubmed are the widely used citation networks, where nodes represent papers, and edges represent citations between them.
   \item The Amazon is a co-purchase graph, where nodes represent goods and edges indicate that two goods are frequently bought together. 
   \item The Airport is an air-traffic graph,  where nodes represent airports and edges indicate the traffic connection between them.
\end{itemize}
We list the statistics of  the datasets in Table \ref{statistics}. 


\subsubsection{Code} We submit the source code of an instance implementation of \textsc{SelfMGNN} in a ZIP named \emph{Code}, and will publish the source code after acceptance.  
% \subsection{Running Environment}
% All experiments were conducted on the a CentOS server with two $11$G Nvidia RTX 2080Ti and $128$G RAM.
\subsection{Implementation Notes}
In \textsc{SelfMGNN}, we stack the attentive aggregation layer twice to learn the  component embedding. 
We employ a two-layer MLP$_\kappa$ in the Riemannian projector to reveal the Riemannian views for the self-supervised learning. 
In the experiments, we set the weight $\gamma$ to be $1$, i.e., the single-view and cross-view contrastive learning are considered to have the same importance.
The grid search is performed over the learning rate in $[0.001, 0.003, 0.005, 0.008, 0.01]$ as well as the dropout probability in $[0, 0.8]$ with the step size of $0.1$.

For all the comparison model, we perform a hyper-parameter search on a validation set to obtain the best results, 
and the $\kappa$-GCN is trained with positive curvature in particular to evaluate the representation ability of the spherical space.
We set the dimensionality to be $24$ for all the models for the fair comparison.
Note that, in \textsc{SelfMGNN}, the component space can be set to arbitrary dimensionality, whose curvature and importance are learned from the data, and thereby we  construct a mixed-curvature space of  any dimensionality, matching the curvatures of any datasets.
%\subsection{Parameter Settings}


% Use \bibliography{yourbibfile} instead or the References section will not appear in your paper

