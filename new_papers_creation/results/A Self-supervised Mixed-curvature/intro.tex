%!TEX root = ./SelfMGNN.tex

\section{Introduction}

Graph representation learning \cite{cui2018survey,hamilton2017inductive} shows fundamental importance in various applications, 
such as link prediction and node classification \cite{kipf2016semi}, 
and thus receives increasing attentions from both academics and industries.
%We discuss the limitations of prio,r works in the following aspects:
Meanwhile, we have also observed great limitations with the existing graph representation learning methods in two major perspectives, 
which are described as follows:

%Uni-curvature:
\noindent \textbf{\emph{Representation Space}}:
Most of existing methods ignore the complexity of real graph structures, and limit the graphs in a single \emph{constant-curvature} representation space \cite{GuSGR19}.
Such methods can only work well on particular kinds of structure that they are designed for.
% The (zero-curvature) Euclidean space has been the workhorse for graph representation learning \cite{}.
% Recently, the Riemannian space, such as hyperbolic or spherical space, 
% has gained increasing attentions by providing better representations for certain types of data, 
For instance, the constant negative curvature hyperbolic space is well-suited for graphs with hierarchical or tree-like structures \cite{HGNN}.
The constant positive curvature spherical space is especially suitable for data with cyclical structures, e.g., triangles and cliques \cite{BachmannBG20},
and the zero-curvature Euclidean space for grid data \cite{WuPCLZY21}.
% For better representing general-structured graphs, it calls for a \emph{mixed curvature} representation space in fact.
% The motivation is intuitive:
However, graph structures in reality are usually mixed and complicated rather than uniformed, in some regions hierarchical, while in others cyclical \cite{papadopoulos2012popularity,ravasz2003hierarchical}.
Even more challenging, the curvatures over different hierarchical or cyclical regions can be different as will be shown in this paper.
%Thus, it calls for  a \emph{mixed-curvature} space to match the wide variety of graph structures for providing more promising representations.
In fact, it calls for a new representation space to match the wide variety of graph structures, and we seek spaces of \emph{mixed-curvature} to provide better representations.
% Self-supervised:
% Self-supervised graph representation learning is a more favorable choice in many cases due to the freedom from labels, particularly when we intend to take advantage from a large scale unlabeled graph in the wild.

%training GNNs in existing approaches usually requires a certain form of supervision.
\noindent \textbf{\emph{Learning Paradigm}}:
Learning graph representations usually requires abundant supervision label information \cite{velickovic2018graph,HGCN}.
Labels are usually scarce in real applications, 
and undoubtedly, labeling graphs is expensive—manual annotation or paying for permission, and is even impossible to acquire because of the privacy policy.
Fortunately, the rich information in graphs provides the potential for \emph{self-supervised learning}, i.e., learning representations without labels \cite{DBLP:journals/corr/abs-2006-08218}. 
Self-supervised graph representation learning is a more favorable choice, 
particularly when we intend to take the advantages from the unlabeled graphs in real applications.
% Recently, a few attempts \cite{VelickovicFHLBH19,HassaniA20,QiuCDZYDWT20} show that \emph{contrastive learning}, i.e., learning graph representations by contrasting graphs with the congruent or discongruent counterpart, is a promising method.
% %However, to the best of knowledge, none of existing Riemannian GNNs equip the ability of self-supervised learning. 
Recently, contrastive learning \cite{VelickovicFHLBH19,QiuCDZYDWT20}
emerges as a successful method for the graph self-supervised learning.
However, existing self-supervised methods, 
to the best of our knowledge, 
cannot be applied to the mixed-curvature spaces due to the intrinsic differences in the geometry.


To address these aforementioned limitations, 
we take the first attempt to study the \emph{self-supervised graph representation learning in the mixed-curvature space} in this paper. 

To this end, we present a novel \textbf{Self}-supervised \textbf{M}ixed-curvature \textbf{G}raph \textbf{N}eural \textbf{N}etwork, named \textbf{\textsc{SelfMGNN}}. %referred to as \textsc{SelfMGNN}.
%To address the first limitation, we model the graphs in a representation space of mixed curvature.
To address the first limitation, 
we propose to learn the representations in a \emph{mixed-curvature space}.
Concretely, we first construct a mixed-curvature space via the Cartesian product of multiple Riemannian—hyperbolic, spherical and Euclidean—component spaces, jointly enjoying the strength of different curvatures to match the complicated graph structures.
%The mixed-curvature space has multiple hyperbolic (spherical) components with learnable curvatures.  
%We utilize the $\kappa$-stereographic model to give the unified formalism of an arbitrary hyperbolic or spherical component spaces. %, i.e., a positive $\kappa$ for spherical, a negative $\kappa$ for hyperbolic, and the limit at zero for Euclidean.
Then, we introduce hierarchical attention mechanisms for learning and fusing representations in the product space.
In particular, we design an intra-component attention for the learning within a component space
and an inter-component attention for the fusing across component spaces.
To address the second limitation, 
we propose a novel \emph{dual contrastive approach} to enable the self-supervisd learning.
The constructed mixed-curvature space %not only matches the complicate structures of graphs, 
actually provides multiple Riemannian views for contrastive learning.
Concretely, we first introduce a Riemannian projector to reveal these views, i.e., hyperbolic, spherical and Euclidean views. 
Then, we introduce the \emph{single-view} and \emph{cross-view contrastive learning}. % are performed simultaneously.
In particular, we utilize a well-designed Riemannian discriminator to 
contrast positive and negative samples in the same Riemannian view  (i.e., the single-view contrastive learning)
and concurrently contrast between different Riemannian views (i.e., the cross-view contrastive learning).
In the experiments, we study the curvatures of real graphs and show the advantages of allowing multiple positive and negative curvature components for the first time,  
%graph in the mixed-curvature spacedecomposition of the Cartesian product space for self-supervised GNNs for the first time and 
demonstrating the superiority of \textsc{SelfMGNN}.
% to capture the complicated graph structures without labels.

Overall, our main contributions are summarized below:
\begin{itemize}
\item \emph{Problem}: To the best of our knowledge, this is the first attempt to study the self-supervised graph representation learning in the mixed-curvature space.
\item \emph{Model}: This paper presents a novel \textsc{SelfMGNN} model, where hierarchical attention mechanisms and dual contrastive approach are designed for self-supervised learning in the mixed-curvature space, allowing multiple hyperbolic (spherical) components with distinct curvatures.  
%\item \emph{Experiments}: Extensive experiments show the superiority of \textsc{SelfMGNN}. This paper is the first to show the advantage of allowing multiple positive (negative) curvature components for graph in the mixed-curvature space, demonstrating that the curvatures over the different hierarchical (spherical) regions of a graph can be different.  
\item \emph{Experiments}: Extensive experiments show the curvatures over different hierarchical (spherical) regions of a graph can be different.  \textsc{SelfMGNN} captures the complicated graph structures without labels and outperforms the state-of-the-art baselines. 

\end{itemize}

