%File: anonymous-submission-latex-2023.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai23}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}

\usepackage{amsmath}
\usepackage{multirow}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
% \floatstyle{ruled}
% \newfloat{listing}{tb}{lst}{}
% \floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2023.1)
}

\usepackage{amsthm}

\newtheorem{theorem}{Theorem}
\newtheorem{observation}{Observation}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}

\usepackage{paralist}
\usepackage{xspace}
\newcommand{\tuple}[1]{\ensuremath{\left \langle #1 \right \rangle }}
\newcommand{\pre}{\textit{pre}}
\newcommand{\params}{\textit{params}}
\newcommand{\eff}{\textit{eff}}
\newcommand{\name}{\textit{name}}
\newcommand{\type}{\textit{type}}
\newcommand{\cnf}{\textit{CNF}}
\newcommand{\conj}{\textit{Conj}}
\newcommand{\true}{\textit{true}}
\newcommand{\false}{\textit{false}}
\newcommand{\unobserved}{\textit{?}}
\newcommand{\realm}{\ensuremath{M^*}\xspace}
\newcommand{\liftf}{F}
\newcommand{\liftl}{L}
\newcommand{\lifta}{A}
\newcommand{\pisam}{\textit{PI-SAM}\xspace}
\newcommand{\sam}{\textit{SAM}\xspace}
\newcommand{\sgam}{\textit{SGAM}\xspace}
\newcommand{\bindings}{\textit{bindings}}
\newcommand{\iseff}{\textit{IsEff}}
\newcommand{\ispre}{\textit{IsPre}}
\usepackage{xcolor}
\newcommand{\brendan}[1]{{\textcolor{red}{[Brendan: #1]}}}
\newcommand{\hai}[1]{{\textcolor{orange}{[Hai: #1]}}}
\newcommand{\roni}[1]{{\textcolor{green}{[Roni: #1]}}}



% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{1} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai23.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Learning Safe Action Models with Partial Observability}
\author{Submission \#5826}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name,\textsuperscript{\rm 1}
    Second Author Name, \textsuperscript{\rm 2}
    Third Author Name \textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1} Affiliation 1\\
    \textsuperscript{\rm 2} Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}

% One of the main methods for solving planning problems is by  in the AI literature is to model 
% Domain-independent planning algorithms are a primary tool to require an action model 

% Powerful domain-independent planners have been developed to solve various types of planning problems. 
% These planners often require a model of the acting agent's actions, given in some planning domain description language. 
% Yet obtaining such an action model is a notoriously hard task. 
% This task is even more challenging in mission-critical domains, where a trial-and-error approach to learning how to act is not an option. 
% In such domains, the action model used to generate plans must be \emph{safe}, in the sense that plans generated with it must be applicable and achieve their goals. 
% The challenge of learning safe action models for planning has been recently addressed for domains in which states are sufficiently described with Boolean variables. 
% In this work, we go beyond this limitation and propose the \nsam algorithm. 
% \nsam runs in time that is polynomial in the number of observations and, under certain conditions, is guaranteed to return safe action models. 
% We analyze its worst-case sample complexity, which may be intractable for benchmark domains. Empirically, however, \nsam can quickly learn a safe action model that can solve most problems in the domain. 


% In reinforcement learning, learning a domain's action model is a difficult problem, especially under partial observability. [roni: out of context, no flow]
A common approach for solving a planning problem is to model them in a formal language, such as the Planning Domain Definition Language (PDDL), and then apply a general purpose planning algorithm that accepts problems given in that language. Modeling a planning problem in a formal planning language, however, is a notoriously hard task. To address this, several algorithms for learning planning models from observations have been proposed. In many real-world domains, the collected observations are only partially observable, which makes learning models for such domains even more challenging. Prior works on learning planning models under partial observability exist, but none of them guarantee the learned model is safe. In this paper, we propose the two algorithms for learning safe action models from partially observable observations. The first algorithm runs in polynomial time and under a certain set of assumptions has a tractable sample complexity for guaranteeing probabilistic completeness. The second algorithm returns a stronger safe action model that maps the given planning problem to a contingent planning problem. Finally, we present experiments on common IPC domains comparing our approach to FAMA, a state-of-the-art algorithm in action model learning. 

% fixed some grammar 

% Under common settings, the agent has all information about the environment. 
% However, in many real-world problems, the planning domains are often partially observable, i.e. the states of the world are not completely observed. 
% That makes the learning of those domains even more challenging. 
% There are existing work on learning domain's action model under partial observability. 
% Nevertheless, none of them can guarantee safety. 
% In this paper, we propose a polynomial time algorithm to learn safe action models for partially observable domains under an identified set of assumptions. 
% We provide proofs of safety and sample complexity of the algorithm. 
% We also propose a stronger safe algorithm for general cases, and map it to a contingent planning problem. 
% Finally, we present experiments on common IPC domains comparing our approach to FAMA, 
% a state of-the-art algorithm in action model learning. 


% We learn the strongest safe model, and map it to a contingent planning problem.
% We identify a set of assumptions under which it is possible to learn a classical planning action model in polynomial time that is guaranteed to be probabilistically complete.  

% 2 branches: with the assumption - without the assumption -> compare in practice
\end{abstract}

\section{Introduction} 

% What is domain-independent planning
Classical planning, i.e., planning in a discrete, deterministic, and fully observable environment is a useful abstraction for solving many planning problems, and has been researched extensively in the Artificial Intelligence (AI) community. Powerful and well-documented classical planners such as Fast Downward~\cite{helmert2006fast} are available and have been developed and refined over the years.


% The challenge modeling % Current approaches: FAMA and SAM. One supports partial observability, the other supports safety.
In order to use these planners, however, one must first model the problem at hand in a formal language, such as the Planning Domain Definition Language (PDDL). This is a not an easy task. 
Therefore, several approaches to learning a PDDL model from observations have been proposed~\cite{aineto2019learning,stern2017efficientAndSafe,juba2021safe,cresswell2013acquiring,wu2007arms}. 
A prominent example is FAMA~\cite{aineto2019learning}, which is a state of the art algorithm for learning a PDDL action model from observations. A major advantage of FAMA is that it is able to learn an action model even if the given observations are not fully observable, i.e., only a subset of the actions and state variables are observed. 

% Safety is cool
A major disadvantage of FAMA and most planning model learning algorithms, is that they do not provide any guarantee on the performance of the learned model. That is, plans generated with the learned model may not be executable or may fail to achieve their intended goals. 
SAM Learning~\cite{stern2017efficientAndSafe,juba2021safe,juba2022learning,mordoch2022collaborative} is a recently introduced family of learning algorithms that provide \emph{safety} guarantees over the learned action model: any plan generated with the action model they returned is guaranteed to be executable and achieve the intended goals. SAM Learning for classical planning domains runs in polynomial time, has a reasonable sample complexity, and, of course, is guaranteed out output a safe action model. However, SAM Learning is limited to learning from fully observed trajectories. 

% Our contributions
\roni{This needs to be revised to remove the relation between PI-SAM and bounded concealment}
In this paper, we present the first work to address safe action model learning from partially observed trajectories. 
We propose two algorithms for learning safe action models in partially observed domains. 
The first algorithm, PI-SAM, extends SAM \cite{juba2021safe} to support partially observable domains satisfying a \textit{bounded concealment assumption}, adapted from prior work on learning from partial observations by Michael~\shortcite{michael2010partialObservability}. 
We theoretically show that PI-SAM is safe, has polynomial running time, and guarantees probabilistic completeness with a tractable sample complexity. The second algorithm, EPI-SAM, is a more general version, which can work on domains without the \textit{bounded concealment assumption}. We prove that EPI-SAM returns the strongest safe action model, which then can be mapped to a contingent planning problem. Finally, we empirically demonstrate the performance of each algorithm through experiments on common domains from the International Planning Competition (IPC) \cite{ipc}. 





% Our contribution: the first work to address safe planning with partially observable trajectories. 

% Negative results, and the bounded concealment property. PI-SAM

% EPI-SAM, link to contingent planning

% Experimental results are good


% The rest of the paper is structured as follows. We start with the background (related work) and definition of the problem. We then present our two algorithm for learning safe action models on domains with partial observability. Next, we report experiments that illustrate the performance of these two algorithms. Finally, we give our conclusions and sketch some possible directions for future work. \roni{We don't really need this for a conference paper}

\section{Background and Problem Definition} 

% \subsection{Classical Planning}
% Planning domain and an action model
A classical planning \textbf{domain} is defined by a tuple 
$\tuple{F,A}$ where $F$ is a set of Boolean state variables, also known as fluents, 
and $A$ is a set of actions. A \emph{literal} in this context is either a fluent $f\in F$ or its negation $\neg f$.  
A \emph{state} is a complete assignment of values to all fluents, i.e., $s:F\rightarrow\{\true, \false\}$. 
A \emph{partial state} is an assignment of values to some (possibly all) of the fluents. 
For a fluent $f$ and a partial state $s$, we denote by $p[f]$ the value assigned to $f$ according to $p$. 
A partial state $p$ is consistent with a partial state $p'$
if every fluent $f$ assigned in $p$ is also assigned in $p'$ 
and $p[f]=p'[f]$. 
An action $a\in A$ is defined by a tuple $\tuple{\name(a), \pre(a), \eff(a)}$ where 
$\name(a)$ is a unique identifier of the action
and $\pre(a)$ and $\eff(a)$ are partial states that specify the preconditions and effects of $a$, respectively. 
% Basic planning axioms
An action $a$ is \emph{applicable} in a state $s$ if $\pre(a)$ is consistent with $s$. 
If $a$ is applicable in $s$, then \emph{applying} in $s$ results in a state $a(s)$ where
for every fluent $f \in F$: 
(1) if $f$ is assigned in $\eff(a)$ then $\eff(a)[f]=a(s)[f]$, 
(2) otherwise, $s[f]=a(s)[f]$. 
A sequence of actions $a_1,\ldots a_n$ is applicable in a state $s$ if 
$a_1$ is applicable in $s$ and for every $i=2,..,n$ 
$a_i$ is applicable in $a_{i-1}(\cdots a_1(s)\cdots)$. 
The result of applying such a sequence of actions in a state $s$ is 
the state $a_n(\cdots a_1(s)\cdots)$. 

% Planning problem
A classical planning \textbf{problem} is defined by a tuple $\tuple{F,A,I,G}$ 
where $\tuple{F,A}$ is a domain, $I$ is the initial state, and $G$ is a partial state representing  the goal we aim to achieve.  
A state $s$ is called a goal state if $G$ is consistent with $s$. 
A \textbf{solution} to a planning problem is a \emph{plan}, which is a sequence of actions that can be applied in $I$ and results in a state $s_G$ if applied to $s_I$ results in a goal state. 
Such a sequence of actions is called a \emph{plan}. 
A \emph{trajectory} is an alternating sequence of states and actions $s_0,a_1,\ldots,a_n,s_n$ 
such that $a_i$ is applicable in $s_{i-1}$ and $s_i=a_i(s_{i-1})$, for every $i=1,\ldots,n$. 
Classical planning domains and problems are often described in a \emph{lifted} manner, where fluents and actions are parameterized over objects. 
For ease of presentation we describe our work in the grounded manner described above but our work fully supports a lifted domain representation. 

%\subsection{Contingent Planning and Imperfect Observations}
\emph{Contingent planning} is a generalization of classical planning in which action effects may be conditioned over some unknown state variable~\cite{majercik2003contingent,hoffmann2005contingent,albore2009translation,brafman2012multi}. 
There are two main differences between classical planning and contingent planning. 
The first difference is that instead of an initial state $I$, a contingent planning problem accepts a formula $\varphi_I$ over the set of fluents that defines a set of possible initial states. 
The second difference is that the effects of some actions include observing the values of some fluents. 
Contingent planning algorithms output a \emph{plan tree}, which is an annotated tree 
where nodes are labelled with actions and edges are labelled with observations. 
Nodes labelled with actions that do not provide any observation will have a single outgoing edge labelled with a null observation. 
A (strong) solution to a contingent planning problem is a plan tree that guarantees a goal state is eventually found in every branch. 



\subsection{Learning Action Models} 
An \emph{action model} of a planning domain $\tuple{F,A}$ is the set of actions $A$ including their names, preconditions, and effects. 
Many algorithms have been proposed for learning action models from a given set of trajectories~\citep{cresswell2013acquiring,yang2007learning,aineto2019learning,juba2021safe}. 
Algorithms from the LOCM family~\cite{cresswell2011generalised, cresswell2013acquiring} learn action models by analyzing observed action sequences and constructing finite state machine that capture how actions change the states of objects in the world. 
The FAMA algorithm~\citep{aineto2019learning} translates the problem of learning an action model to a planning problem, where every solution to this planning problem is an action model consistent with the available observations. As noted above, FAMA work even partially observable observations. 
The \sam learning family of algorithms~\cite{stern2017efficientAndSafe,juba2021safe,juba2022learning,mordoch2022collaborative}
guarantee that the action model they return is \emph{safe}. Most algorithms from this family have a tractable running time and reasonable sample complexity to ensure a probabilistic form of completeness, but rely on perfect observability of the given observations. 
% However, it relies on perfect observability of the given set of trajectories. We relax this requirement and propose the first safe action model learning algorithm that is able to learn from partially observed trajectories. \roni{removed this to avoid repetition from the introduction}
% [Roni: removed this to avoid repetition from the introduction]some of the actions, states, or state variables in the given set of trajectories are hidden.
%FAMA~\citep{aineto2019learning}, a state of the art action model learning algorithm, translate the problem of learning an action model to a planning problem. This planning problem searches the space of possible action models that are consistent with the available observations. Importantly, FAMA is specifically designed to be able to handle partial observability of the given set of trajectories. That is, FAMA can work even when some of the actions, states, or state variables in the given set of trajectories are hidden. However, FAMA does not provide any formal guarantee that the learned action model is \emph{safe}, in the sense that using it will produce actionable plans that achieve their intended goals. 

% To fill this gap, the \sam learning family of algorithms~\cite{stern2017efficientAndSafe,juba2021safe,juba2022learning,mordoch2022collaborative}
% guarantee that the action model they return is \emph{safe}. 
% \sam learning has a tractable running time and reasonable sample complexity to ensure a probabilistic form of completeness. 
% However, it relies on perfect observability of the given set of trajectories. We relax this requirement and propose the first safe action model learning algorithm that is able to learn from partially observed trajectories. \roni{removed this to avoid repetition from the introduction}


% \subsection{Imperfect Observation Model}
\subsection{Problem Definition}

% Observation function
To capture the type of partially observability we consider in this work formally, we define an observation function $O$ that accepts a state $s$ in a fully observable trajectory and outputs a partial state. 
The \emph{partial view} of a fully observable trajectory $T=(s_0,a_1,\ldots,a_n,s_n)$ 
induced by an observation function $O$, denoted $O(T)$, is the trajectory created by applying $O$ to every state in $T$, i.e., $O(T)=\left(O(T,s_0),a_1,\ldots,a_n,O(T,s_n)\right)$. A partially observed trajectory $T'$ is consistent with a fully observable trajectory $T$ if for every literal $l$ at every state $s$, $s_{T'}(l) = s_{T}(l)$ or $s_{T'}(l)$ is not observed.
We denote by $s(l)=\unobserved$ the fact that the literal $l$ is not observed at state $s$. 


Finally, we can define the type \emph{safe model-free planning} problem we address in this work. 
The problem-solver is tasked with solving a classical planning problem $\Pi=\tuple{F,A,I,G}$. 
The action model $A$, however, is not given to the problem solver. 
Instead, it is given a set of partial views of trajectories created by 
observing the execution of plans that solve other problems in the same domain. 
A solution to our safe model-free planning problem is a safe plan. 
Safety here refers to the requirement that the returned plan must be a plan for the  underlying planning problem $\Pi$, that is, a sequence of actions that are applicable in $I$ according to the real action model $A$ and ends up in a goal state. 




% Scope
In this work, we make the following simplifying assumptions. 
Actions have deterministic effects. 
we assume that actions' preconditions and effects are conjunctions of literals, as opposed to more complex logical statements, and we do not currently consider conditional effects of actions. 
In addition, we assume that the observation function used to create the partially observable trajectories is noise-free, that is, the partial state returned by applying the observation function on a state $s$ is consistent with $s$. 
These assumptions are reasonable when planning in digital/virtual environments, such as video games, or environments that have been instrumented with reliable sensors, such as warehouses designed to be navigated by robots~\cite{li2020lifelong}. 






% \section{Partial Information Safe Action Model Learning}
\section{Partial Information SAM Learning}
Following prior work~\cite{stern2017efficientAndSafe,juba2021safe}, the general approach we take in this work is to first learn an action model from the given trajectories and then use a planner to solve the given planning problem. 
To ensure safety, we aim to learn an action model that is \emph{safe}. 
\begin{definition}[Safe Action Model]
An action model $\hat{A}$ is safe with respect to an action model $A$ 
if for every action $a\in\hat{A}$ and every state $s$ it holds that 
if $a$ is application in $s$ according to $\hat{A}$ then it is also applicable according to $A$ and if for every goal $g$, if the plan achieves $g$ under $\hat{A}$, it also achieves $g$ in $A$. 
\end{definition}
Learning a safe action model is particularly difficult in our setting, due to the partial observability of the available trajectories. 
%In the remainder of this work deals with how to learn such a safe action model from observations. 

\subsection{General Negative Results}
Without any restrictions on the observation function $O$, ensuring that a non-trivial action model is safe is impossible. 
To see this, consider a type of observation function that always hides the value of some fluent $f$. 
Since we never observe the value of $f$, then for every action $a$ we can never be certain if either $f=\true$ is a precondition, $f=\false$ is a precondition, or neither.
Thus, we can never have a safe action model that allows action $a$ to be applicable. 
This example highlights that some assumption over $O$ must be made in order to allow learning a safe action model. 


\subsection{Bounded Concealment Assumption}

In our first approach to learning safe action models, we extend the SAM learning algorithm \cite{juba2021safe} to cope with partially observable domains. For this approach, we make the following assumption, adapted from Michael's theory of learning from partial information~\cite{michael2010partialObservability}:

\begin{definition}[Bounded Concealment Assumption]
The {\em $\eta$-bounded concealment assumption} is the following property of an observation function and environment:
For every literal that is not a precondition of an action, when that action is taken and the literal is false, then the corresponding fluent is observed in both the pre- and post-states with probability at least $\eta$.
%\begin{compactitem}
%\item If a literal is not a precondition of an action, and the literal is false when the action is taken, the fluent is observed with probability at least $\eta$.
%we will observe the action taken with the negation of that literal with some non-negligible probability. 
%\item If a literal is an effect of an action and the action is taken when the literal is false in the pre-state, its fluent is observed in both the pre- and post-states with probability at least $\eta$. 
%before and after the action is taken with some non-negligible probability. 
%\item In contrast, if a literal is not an effect of an action and that action is taken, we observe its fluent in the post-state with probability at least $\eta$.
%\end{compactitem}
%some non-negligible probability. 
\end{definition}

\paragraph{Random Masking}
An example of partial observations that satisfy the \textit{bounded concealment assumption} is obtained from an observation function called \textit{random masking}.
\begin{definition}[Random Masking]
A random masking process $O_\eta$ is a function that maps a complete trajectory $T$ to a consistent partially observed trajectory $T' = O_{\eta}(T)$ such that for every state $s$ in the trajectory $T$, each literal $l$ is independently observed with probability $\sqrt{\eta}$. 
\end{definition}
\noindent
Note that each literal is observed in both the pre- and post-states with probability $\sqrt{\eta}\cdot\sqrt{\eta}=\eta$ on each transition, so this observation function has $\eta$-bounded concealment.

Under the \textit{bounded concealment assumption}, we are able to extend the SAM learning algorithm \cite{juba2021safe} to partially observed domains. The core of our algorithm is based on the following generalization of the basic SAM learning rules~\cite{stern2017efficientAndSafe,juba2021safe}:
% \paragraph{Learning Rules for Partially Observable Domains}
\begin{observation}\label{obs:pi-sam-learning-rules}
For any action triplet $\tuple{s, a, s'}$% it holds that
\begin{itemize}
    \item Rule 1 [not a precondition]. If we observe an unmasked literal $\neg l \in s$, $l$ is not a precondition of action $a$. %Remove $l$ from $\pre(a)$. [Roni: this is already part of the PI-SAM algorithm, not the rules]
    
    \item Rule 2 [not an effect]. If we observe an umasked literal $\neg l \in s$, $l$ is not an effect of action $a$.
    
    \item Rule 3 [an effect]. If we observe an unmasked literal $l \in s'/s$, $l$ is an effect of action $a$. 
    %Add $l$ to $\eff(a).$ [Roni: {Sthis is already part of the PI-SAM algorithm, not the rules]
\end{itemize}
   

\end{observation}



% \subsection{Partial Information SAM (PI-SAM) Learning Algorithm}
\subsection{SAM Learning with Partial Information}
We present our Partial Information SAM (PI-SAM) learning algorithm in Algorithm \ref{alg:pisam}. For every action $a$ observed in some trajectories, we first assume that it has no effect and its preconditions consist of all possible parameter-bound literals. For every state-action triplet $\tuple{s, a, s'}$ that contains action $a$, we remove from the precondition of $a$ every literal $l$ which its negation is unmasked and observed \roni{What is the difference between ``unmasked'' and ``observed'' in our context? I think it is the same, and we should go over the paper and use only one term} in the pre-state $s$ (Rule 1 in Observation \ref{obs:pi-sam-learning-rules}). We also add to the effect of $a$ every literal that is unmasked \roni{in both $s$ and $s'$, right?} and holds in the post-state $s'$ but not the pre-state $s$ (Rule 3 in Observation \ref{obs:pi-sam-learning-rules}). 
\roni{We should update this to also say that we only remove a precondition if its literal is observed in both pre and post states, to fix the bug we had in the AAAI submission}

\begin{algorithm}[t]
\small
\DontPrintSemicolon
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Partially Observed Trajectories $\mathcal{T}$}
\Output{($\pre$, $\eff$) for a safe action model}
\BlankLine
    \ForEach{action $a$}{
        $\eff(a)\gets\emptyset$ \\ 
        $\pre(a)\gets $ all parameter-bound literals \nllabel{line:init_pre_pi} \\
        \ForEach{transition $\tuple{s, a, s'}$}{
            \ForEach{literal $\ell$ that is unmasked in $s$ and $s'$}{
                % \lIf{$\neg\ell\in s$ and $\ell\in \pre(a)$}{ Roni: commented this because I'd like to stay in one line in the PDF per command. Feel free to revert if this annoys you
                \lIf{$\neg\ell\in s$}{
                Remove $\ell$ from $\pre(a)$
                }
                \lIf{$\ell\in s'\setminus s$}{
                Add $\ell$ to $\eff(a)$
                }
            }
            
        }
    }
    Return $\tuple{\pre, \eff}$

\caption{Partial Information SAM Learning Algorithm (PI-SAM)}\label{alg:pisam}
\end{algorithm}

\subsection{Theoretical Properties}
PI-SAM has several appealing theoretical properties: it returns a safe action model, its running time is polynomial in the number of observed transitions, literals, and actions, and it only requires a tractable number of samples to guarantee that future solvable problems will be solvable with the learned action model.


%% NOTE: Safety only holds whp because it depends on observing the results of the effects in addition to the preconditions. We could modify it slightly to obtain unconditional safety, but (a) these weren't the experiments we ran, (b) we are short on time, and (c) we are also short on space.
\paragraph{Safety Property}
\begin{theorem}\label{safe-pisam-thm} The PI-SAM Learning Algorithm (Algorithm ~\ref{alg:pisam}) creates a safe action model. 
\end{theorem}
\begin{proof}
PI-SAM (Algorithm~\ref{alg:pisam})  initializes every action with no effects, and preconditions consisting of all possible literals. During the main loop (lines 5-9 in Algorithm \ref{alg:pisam}), the algorithm never removes any literal of an action's preconditions that is an actual precondition (because they are observed violated), and only adds literals that are actual effects to its effects (because the literal is observed to switch to true). Thus, for every action, the preconditions returned by PI-SAM is a superset of its true preconditions, and the effects returned by PI-SAM is a subset of its true effects. Finally, since each effect $\ell$ is added when $\neg\ell$ is observed in the pre-state, causing $\ell$ to be removed from the preconditions, if $\ell$ is not added then $\ell$ must remain a precondition. Thus, PI-SAM returns an action model $M$ such that every action which is applicable to $M$ must also be applicable in the true action model $M^*$, and every effect of $a$ in $M^*$ is true in the post-state of $a$ in $M$ whenever $a$ satisfies $\pre_M(a)$. 
\end{proof}

\paragraph{Time Complexity}

Let $arity(\liftf,t)$ and $arity(\lifta,t)$ be the number of type-$t$ parameters of the lifted fluent $\liftf$ and action $\lifta$, and $\mathcal{T}(a)$ denotes the set of triplets of trajectories in $\mathcal{T}$ with action $a$. %, respectively.
%Let $\eta$ be the the \textit{random masking} probability.

\begin{theorem}
Given a set of trajectories $\mathcal{T}$, PI-SAM learning (Algorithm~\ref{alg:pisam}) runs in time 
\begin{small}
\[\mathcal{O}\Big(\sum_{a\in \mathcal{A}}|\mathcal{T}(a)|\sum_{f\in\mathcal{F}}\prod_{t\in T}arity(a,t)^{arity(f,t)}\Big)\]
\end{small}
\end{theorem}

\begin{proof}
For every action $a\in \mathcal{A}$, PI-SAM iterates over all state-action triplets in $\mathcal{T}(a)$. There are $arity(a,t)^{arity(f,t)}$ ways to bind the parameters. 
%Additionally, for each literal, PI-SAM needs to observe it unmasked in both the pre-state and post-state of a triplet to add it as an effect. 
%Since at each state, each literal has $\eta$ chances of being observed, it adds a factor of $\frac{1}{\eta^2}$ to the running time.
%% Once the trajectories are fixed there is no probability involved.
Thus, in total, the running time is $\mathcal{O}\Big(\sum_{a\in \mathcal{A}}|\mathcal{T}(a)|\sum_{f\in\mathcal{F}}\prod_{t\in T}arity(a,t)^{arity(f,t)}\Big)$.
\end{proof}

\paragraph{Sample Complexity Analysis}

Let $\mathcal{P}_D$ be a probability distribution over solvable planning problems in a domain $D$. 
%That is, $\mathcal{P}_D$ is a probability distribution over pairs $\tuple{s_I, s_G}$ where $s_I$ is a state, $s_G$ is a goal condition that is achievable from $s_I$. 
Let $\mathcal{T}_D$ be a probability distribution over pairs $\tuple{P, T}$ 
given by drawing a problem $P$ from $\mathcal{P}(D)$, 
using a sound and complete planner to generate a plan for $P$, 
and setting $T$ to be the trajectory from following this plan.\footnote{%
The planner need not be deterministic.}
\begin{theorem}\label{complexity-pisam-thm}
Under $\eta$-bounded concealment, given 
\begin{equation*}
    \small
m \geq \frac{1}{\epsilon \cdot\eta} (2\ln 3\sum_{\substack{f\in\mathcal{F}\\a\in\mathcal{A}}}\prod_{t\in T}arity(a,t)^{arity(f,t)} + \ln \frac{1}{\delta})
\end{equation*}
trajectories sampled from $\mathcal{T}_D$, 
The PI-SAM Learning Algorithm (Algorithm~\ref{alg:pisam}) returns a safe action model $M_\pisam$ such that with probability at least $1-\delta$, a problem drawn from $\mathcal{P}_D$ is not solvable with $M_\pisam$ with probability at most $\epsilon$.
\end{theorem}
\noindent
\emph{Sketch of Proof.} 
The PI-SAM algorithm (Algorithm \ref{alg:pisam}) applies the same learning rules as the SAM learning algorithm \cite{juba2021safe}, except that it ignores literals that are masked in the pre- or post-state of a transition. Thus, the numbers of state-action triplets required by PI-SAM is inversely proportional to the probability $\eta$ for which the observation function satisfies bounded concealment. 
Since the SAM algorithm requires $m \geq L = \frac{1}{\epsilon} (2\ln 3\sum_{\substack{f\in\mathcal{F}\\a\in\mathcal{A}}}\prod_{t\in T}arity(a,t)^{arity(f,t)} + \ln \frac{1}{\delta})$ samples (Theorem 5 \cite{juba2021safe}), PI-SAM requires $ m \geq \frac{1}{\eta}L$ samples. 


%\section{Extended Partial Observability SAM (EPI-SAM) Learning Algorithm}
\section{Extended PI-SAM (EPI-SAM)}

PI-SAM algorithm (Algorithm \ref{alg:pisam}) can only work on domains that satisfy the \textit{bounded concealment assumption}. When the assumption does not hold, PI-SAM can't guarantee that it can learn any effect after a bounded number of state-action triplets since PI-SAM needs to see a literal unmasked in both the pre-state and post-state in a triplet in order to add it as an effect. To address this issue, we propose the Extended PI-SAM (EPI-SAM) learning algorithm, which describes the safe action model in form of a set of Conjunctive Normal Form (CNF) formulas $CNF_\eff(l)$ for each literal $l$, 
and a conjunction $Pre(a)$ for each action $a$. 
\roni{not important now, but I think we should replace the above with an example showing that PI-SAM is not utilizing all the information that it can use, and not go for the bounded concealment argument}

\subsection{EPI-SAM Learning: Effects - Preconditions}
The EPI-SAM learning algorithm, presented in Algorithm \ref{alg:episam}, can be split in two separate parts: learning the effects and learning the preconditions. 
The first part (lines 1-9 in Algorithm \ref{alg:episam}) creates a Conjunctive Normal Form (CNF) formula for each literal $l$, denoted $CNF_{\eff}(l)$, which contains all possible effect clauses (i.e., atoms of the form $\iseff(l,a) $ that specify whether literal $l$ is an effect of action $a$) that we can write about literal $l$. 
$CNF_{\eff}(l)$ is first initialized as empty. For each sequence of states in the trajectory during which the fluent in literal $l$ is not observed and $l$ is true at the end of the sequence, there are two kinds of clauses we can write about $l$. First, if literal $l$ was $false$ at the beginning state $s_0$, $l$ must be an effect of some action $a_i$ (line 4-5). Second, if $\neg l$ was an effect of any action $a_j$ (which guarantee $l$ is $\false$ in state $s_j$, then some action $a_k$ for $k > j$ must have had $l$ as an effect (line 6-7). Finally, for each action $a$, we also add to the CNF a mutual exclusion clause for literal $l$ (line 8-9).   
\roni{I think we're missing in the explanation and the pseudo-code the PI-SAM rule that identifies literals that are not an effect. 
That is, we should start the creation of $CNF_\eff(l)$ by identifying the set of actions that we know do not have $l$ as an effect (PI-SAM rule 2). 
Then remove these actions from the trajectories considered when creating $CNF_\eff(s)$. This relates to the new preconditions algorithms, which I think can be viewed as re-running EPI-SAM effect learning on a modified set of trajectories that assumed $l$ is a precondition of $a$.}


The second part (lines 10-24 in Algorithm \ref{alg:episam}) learns the precondition $pre(a)$ (as a conjunction) for each action $a$. Initially, $pre(a)$ represents that all possible literals are preconditions. For each sequence of states in the trajectory that end at $a$, there are two ways that literal $l$ can be known to be true. Either $l$ was true at the beginning state $s_0$ and was not an effect of any other action in the sequence (which would switch it off), or  $l$ was an effect at some action $a_i$ and wasn't switched off by a later action (line 15- 23). The algorithm only removes $\neg l$ from the precondition $pre(a)$ if any of the above conditions are true. Since it is a Disjunctive Normal Form (DNF) formula, we first negate it then combine with the CNF effect clauses created from part 1 and use refutation to check if the combined formula is \textit{satisfiable}. If it is \textit{satisfiable}, we leave $\neg l$ as a precondition. Otherwise, we remove $\neg l$ from the precondition $\pre(a)$ (line 24-25). 

\paragraph{Safe planning with contingent planner}
The EPI-SAM algorithm (Algorithm \ref{alg:episam}) returns an action model in which the effects are represented by CNF formulas. 
Classical planners are not designed to accept such an action model.
Instead, we solve the given classical planning problem with our action model by mapping it a \emph{contingent planning problem}, and solving it with any off-the-shelf contingent planner. In our work, we use the Contingent Planner \cite{albore2009translation}. The procedure to convert EPI-SAM's output to an appropriate input to the Contingent Planner is presented in the supplementary material. 

% Algorithm \ref{alg:tocontingent} describes how to do this mapping, i.e., how to convert a classical planning problem (without its action model), and the output from EPI-SAM, to an appropriate input to the Contingent Planner \cite{albore2009translantion}. 



\begin{algorithm}[t]
\small
\DontPrintSemicolon
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Partially Observed Trajectories $\mathcal{T}$}
\Output{CNF representation of effects $CNF_{\eff}(l)$ for each literal $l$ and precondition $\pre(a)$ for each action $a$}
\BlankLine
    \ForEach{ literal $l$}{
        $CNF_{\eff}(l) \gets \emptyset$\\
        % \ForEach{trajectory $\{s_0, a_1, s_1, ..., a_k, s_k\}\in\mathcal{T}$ where $s_0(l) = false$ and $s_k(l) = true$}{
        \ForEach{\roni{maximal} sequence $\{s_0, a_1, s_1, \ldots, a_k, s_k\}\subseteq \mathcal{T}_i\in\mathcal{T}$ 
        where $s_k(l) = true$, $s_1(l),\ldots,s_{k-1}(l)$ unobserved}{
            \uIf{$s_0(l) = false $}{
                $CNF_{\eff}(l) \gets CNF_{\eff}(l) \wedge (\iseff(l, a_1)  \vee   \iseff(l, a_2) \vee .. \vee \iseff(l, a_k))$
            }
        
            \ForEach{$j=1$ to $k-1$}{
                $CNF_{\eff}(l) \gets CNF_{\eff}(l) \wedge (\neg\iseff(\neg l, a_j) \vee \iseff(l, a_{j+1}) \vee\ldots\vee \iseff(l, a_k)) $
            }
        }
        \ForEach{action $a$}{
            $CNF_{\eff}(l) \gets CNF_{\eff}(l) \wedge (\neg \iseff(l, a) \vee \neg \iseff(\neg l, a))$
        }
    } 
    \lForEach{ action $a$}{$\pre(a)\gets$ all literals $l$}
    \ForEach{ literal $l$, action $a$}{
        $\mathcal{T}_{l,a}\gets\emptyset$\\
        \ForEach{trajectory     $T\in\mathcal{T}$}{
            Initialize $T_{l,a}\gets T$\\
            \ForEach{step $t$ with $a_t=a$}{
                \lIf{$s_{t-1}(l)$ is false}{
                Delete $l$ from $\pre(a)$ and skip to the next $(l,a)$ pair.\nllabel{episam-easy-pre-delete}
                }
                \lElse{Set $s_{t-1}(l)$ true in $T_{l,a}$.}
            }
            Put $T_{l,a}$ in $\mathcal{T}_{l,a}$
        }
        \While{Some $a'$ occurs in $\tuple{s_1,a',s'_1}$ and $\tuple{s_2,a',s'_2}$ in trajectories in $\mathcal{T}_{l,a}$ with observed $s'_1(l)\neq s'_2(l)$}{
        \nllabel{episam-action-delete-loop}
            \lIf{For some $\tuple{s,a',s'}$ in $\mathcal{T}_{l,a}$ $s(l)$ with observed $s(l)\neq s'(l)$}{
                Delete $l$ from $\pre(a)$ and skip to the next $(l,a)$.\nllabel{episam-hard-pre-delete}
            }
            \lElse{Delete all occurrences of $a'$ from $\mathcal{T}_{l,a}$\nllabel{episam-action-delete}}
        }
\iffalse{
%%
%% OLD SAT-BASED ALGORITHM BELOW
%%
        $\pre(a)\gets $ all parameter-bound literals \nllabel{line:init_pre_epi} \\
        $\varphi_{l,a}\gets \emptyset$\\
        \ForEach{trajectory $T=\{s_0, a_1, s_1, ..., a_k, s_k\}\in\mathcal{T}$ where $a_k = a$}{
            $i \gets k$\\
            $\varphi_{l,a,T}\gets \emptyset$ \roni{Should we initialize $\varphi$ here or only once per action and literal}\\
            \While{$i>0$}{
                \uIf{$l$ is observed in $s_{i-1}$}{
                    $\varphi_{l,a,T} \gets \varphi_{l,a,T} \lor (\neg \iseff(\neg l,a_i) \wedge \neg \iseff(\neg l, a_{i+1}) \wedge\cdots\wedge \neg \iseff(\neg l, a_{k-1}))$\\
                    break\\
                }
                \uElseIf{$\neg l$ is observed in $s_{i-1}$}{
                    break
                }
                \Else{
                    $\varphi_{l,a,T} \gets \varphi_{l,a,T} \lor (\iseff( l,a_i) \wedge \neg \iseff(\neg l, a_{i+1}) \wedge\cdots\wedge \neg \iseff(\neg l, a_{k-1}))$\\
                    $i \gets i-1 $\\
                }
            }
            $\varphi_{l,a}\gets ??$\\
        }
        \uIf{ $\neg \varphi_{l,a,T} \wedge CNF_{\eff}(l) $ is not satisfiable}{
            Remove $\neg l$ from $\pre(a)$
        }
%%%
%%% END OLD SAT-BASED ALGORITHM
%%%

        % If $\exist T$ such that $\neg \varphi_{l,a,T} \wedge CNF_{\eff}(l)$ is not SAT, 
        % then $\neg l$ cannot be a precondition of $a$. 
        % CNF_\eff = M1 or M2
        % T1 l was not true before a if M1 is true
        % T2 l was not true before a if M2 is true
        % \uIf{ $\neg \varphi_{l,a,T} \wedge CNF_{\eff}(l) $ is not satisfiable}{
        %     Remove $\neg l$ from $\pre(a)$
        % }
}\fi
    }
    % \ForEach{ literal $l$, action $a$}{
    %     $\pre(a)\gets $ all parameter-bound literals \nllabel{line:init_pre_epi} \\
    %     \ForEach{trajectory $\{s_0, a_1, s_1, ..., a_k, s_k\}\in\mathcal{T}$ where $a_k = a$}{
    %         $i \gets k$\\
    %         $\varphi\gets \emptyset$ \roni{Should we initialize $\varphi$ here or only once per action and literal}\\
    %         \While{$i>0$}{
    %             \uIf{$l$ is observed in $s_{i-1}$}{
    %                 $\varphi \gets \varphi \lor (\neg \iseff(\neg l,a_i) \wedge \neg \iseff(\neg l, a_{i+1}) \wedge\cdots\wedge \neg \iseff(\neg l, a_{k-1}))$\\
    %                 break\\
    %             }
    %             \uElseIf{$\neg l$ is observed in $s_{i-1}$}{
    %                 break
    %             }
    %             \Else{
    %                 $\varphi \gets \varphi \lor (\iseff( l,a_i) \wedge \neg \iseff(\neg l, a_{i+1}) \wedge\cdots\wedge \neg \iseff(\neg l, a_{k-1}))$\\
    %                 $i \gets i-1 $\\
    %             }
    %         }
    %     }
    %     \uIf{ $\neg \varphi \wedge CNF_{\eff}(l) $ is not satisfiable}{
    %         Remove $\neg l$ from $\pre(a)$
    %     }
    % }

\caption{Extended PI-SAM Learning Algorithm (EPI-SAM)}\label{alg:episam}
\end{algorithm}




% \begin{algorithm}[t]
% \small
% \DontPrintSemicolon
% \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
% \Input{Partially-observed trajectories $\mathcal{T}$}
% \Output{CNF representation of effects $CNF_{\eff}(l)$ for each literal $l$, or $\false$ if no action model is consistent with $\mathcal{T}$}
% \BlankLine
%     \tcc{Init $CNF_\eff$ with the PI-SAM rules}
%     \ForEach{fluent $f$}{
%         $CNF_{\eff}(f) \gets \emptyset$\\
%     }
%     \ForEach{transition $\tuple{s,a,s'}$}{
%         \ForEach{fluent $f$}{
%             \lIf{$s'(f)=\true$}{
%                 Add $\neg \iseff(a, \neg f)$ to $CNF_{\eff}(f)$\\
%                 \lIf{$s(f)=\false$}{
%                     Add $\iseff(a,f)$ to $CNF_{\eff}(f)$
%                 }
%             }
%             \lIf{$s'(f)=\false$}{
%                 Add $\neg \iseff(a, f)$ to $CNF_{\eff}(f)$\\
%                 \lIf{$s(f)=\true$}{
%                     Add $\iseff(a,\neg f)$ to $CNF_{\eff}(f)$
%                 }
%             }
%         }
%     }
%     %     unmasked in $s'$ and $s'(\ell)=\true$}{
%     %         Add $\neg \iseff(a,\neg \ell)$ to $CNF_{\eff}(\ell)$\\
%     %         \lIf{$\ell$ unmasked in $s$ and $s'(\ell)\neq s(\ell)$}{
                
%     %         }
%     %     }
%     %     \ForEach{literal $\ell$ unmasked in $s'$ and $s$}{
%     %     }
%     % }
%     % \ForEach{literal $\ell\in\eff_{M_{safe}}(a)$}{
%     %     Add $\iseff(a,\ell)$ to $CNF_{\eff}$\\
%     % }
    
%     % % PI-SAM effects rules tuple
%     % \ForEach{literal $\ell$}{
%     %     % \tcc{$A_{\neg\ell}$ contains all actions that are guaranteed to not have $\ell$ as an effect}
%     %     \tcc{All $a$ where $\iseff(a,\ell)$ must be false}
%     %     $A_{\neg\ell}\gets \{a| \exists \tuple{s,a,s'}$ where $s'(\ell)=\false \}$\\
%     %     $CNF_{\eff}(\ell) \gets \emptyset$\\
%     %     \ForEach{maximal sequence $\{s_0, a_1, s_1, \ldots, a_k, s_k\}\subseteq \mathcal{T}_i\in\mathcal{T}$ 
%     %     where $s_k(l) = \true$, $s_1(l),\ldots,s_{k-1}(l)$ unobserved}{
%     %         \uIf{$s_0(l) = \false $}{
%     %             $CNF_{\eff}(l) \gets CNF_{\eff}(l) \wedge (\iseff(l, a_1)  \vee   \iseff(l, a_2) \vee .. \vee \iseff(l, a_k))$
%     %             % If l changed from false to true, then l must be an effect of at least one action in the sequence
%     %         }
        
%     %         \ForEach{$j=1$ to $k-1$}{
%     %             $CNF_{\eff}(l) \gets CNF_{\eff}(l) \wedge (\neg\iseff(\neg l, a_j) \vee \iseff(l, a_{j+1}) \vee\ldots\vee \iseff(l, a_k)) $
%     %         }
%     %     }
%     %     \ForEach{action $a$}{
%     %         $CNF_{\eff}(l) \gets CNF_{\eff}(l) \wedge (\neg \iseff(l, a) \vee \neg \iseff(\neg l, a))$
%     %     }
%     % }
% \caption{Generalized EPI-SAM Effects Learning)}\label{alg:episam-effects}
% \end{algorithm}


% Input: Pre(.), CNF$_{eff}$, 
% Output: Contingent PDDL domain
% Input: CNF for literal
% Output: relevant oneof statements in the problem definition


% Link to contignent planner stuff:
% https://www.upf.edu/web/ai-ml/clg-contingent-planner

% \begin{algorithm}[t]
% \small
% \DontPrintSemicolon
% \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
% \Input{Original problem $\Pi=\tuple{F,A,I,G}$, 
% $Pre(a)$ for every action $a$,
% $CNF_{\eff}(l)$ for every literal $l$}
% \Output{Contingent planning problem $\Pi'$}

% \ForEach{action $a$}{Create $obs_a$}
% \ForEach{literal $l$}{Create $obs_l$}
% \ForEach{pair of action and literal}{Create a fluent \texttt{(is-eff $obj_a$, $obj_l$)}} 

% % Stuff to add to the domain file
% \ForEach{observed action $a$}{
%     Add an action to the domain with $Pre(a)$ as its preconditions\\
%     \ForEach{literal $l$}{
%         Add a conditional effect of the form:
%         \texttt{(when (is-eff $obj_a$ $obj_l$) $l$)}
%     }
% }

% % Stuff to add to the problem file
% %The problem contains all fluents from the original problem and the following fluents:
% \ForEach{literal $l$}{
%     \ForEach{clause $ C=(\sigma_1 \iseff(a_1,l) \vee \sigma_2 \iseff(a_2,l) \vee ... \vee \sigma_k \iseff(a_k,l)) \in \varphi$ ($\sigma_i$ is identity or $\neg$)}{
%     %-- i.e., if it is identity, no symbol appears for $\sigma_i$; if it's negation, ``not'' appears for $\sigma_i$ below
%         Add to problem file:\\
%         $\left(\textit{oneof}~~ \sigma_1 \iseff(a_1,l) 
%         ~\sigma_2 \iseff(a_2,l)
%         \cdots
%         ~\sigma_k \iseff(a_k,l) \right)$
%         }
%     }
% \caption{Action Model to Contingent PDDL}\label{alg:tocontingent}
% \end{algorithm}



% For a literal $l$, action $a$, and trajectory T=$\tuple{s_0,\ldots a_k}$ where $a_k=$, the corresponding $\varphi$ formula 
% describes all cases where $l$ was true before $a_k$ 
% So, it should be something like this: \varphi_{l,a,T}
% varph_{l,a}=\vee_T \varphi_{l,a,T}



\paragraph{Theoretical Properties}
The key property is that the formulas $CNF_{\eff}(l)$ created by EPI-SAM characterize the possible action models that are consistent with our trajectories:
\begin{lemma}\label{lem:cnf-char}
At line 10 in Algorithm \ref{alg:episam}, for every action model consistent with the set of partially observed trajectories $\mathcal{T}$, the assignment obtained by setting $\iseff(l,a)$ to true if $l$ is an effect of $a$ for each literal $l$ and action $a$ is a satisfying assignment to $CNF_{\eff}(l)$, and conversely, for any satisfying assignment to $CNF_{\eff}(l)$, the corresponding action model is an action model consistent with the trajectories $\mathcal{T}$.
\end{lemma}
\noindent
{\em Sketch of proof.}
We consider a CNF, $CNF_{\mathcal{T}}$, that has variables of the form $\iseff(l,a)$, $\ispre(l,a)$, and $State(l,t,h)$, with the last literal representing that the literal $l$ is satisfied at the $t$th step of the trajectory $h\in\mathcal{T}$. The clauses are obtained from clausal encodings of the STRIPS axioms, instantiated at each step of each trajectory in $\mathcal{T}$:
\begin{compactenum}
\item $\neg \ispre(l,a_t)\vee State(l,t-1,h)$
\item $\neg \iseff(l,a_t)\vee State(l,t,h)$
\item $\neg State(l,t-1,h)\vee \iseff(\neg l,a_t)\vee State(l,t,h)$
\end{compactenum}
with $State(l,t,h)$ replaced by true or false when $l$ is observed true or false, respectively, at step $t$ in $h$. It is immediate that satisfying assignments to $CNF_{\mathcal{T}}$ correspond to encodings of action models and the complete trajectories those action models would yield, given the values observed in the trajectories of $\mathcal{T}$. We note that the clauses of this formula each only contain literals for a single fluent, so the formula is satisfiable iff the formulas $CNF_{\mathcal{T}}(l)$ for each literal $l$ are satisfiable.

We then use the refutation-completeness of resolution to reduce the problem to identifying the clauses that may appear in resolution refutations. The $\ispre(a,l)$ literals, appearing only negatively, cannot appear in a refutation, and the literals $State(l,t,h)$ must be eliminated, but these only appear in consecutive instances of the second and third type of clauses where the literal $l$ is unobserved; and only the third clause has the negative literal. Reordering the applications of the resolution rule on these literals to the beginning of the proof, we see that we must create clauses that correspond to consecutive runs of unobserved literals using the resolution rule on the third type of clause for each step, beginning with either an observed literal or with using the second type of clause to eliminate the first $State(l,t,h)$ literal. These are, respectively, the clauses of $CNF_{\eff}(l)$ created on lines 4--5 and 6--7. See the supplemental material for details.
\iffalse{
%% Full proof below punted to supplemental
\begin{proof}
We first consider the following CNF encoding of the possible trajectories that could yield the observations appearing in $\mathcal{T}$: let $CNF_{\mathcal{T}}$ be a formula with variables $\iseff(l,a)$ for each literal $l$ and action $a$, $\ispre(l,a)$ for each literal $l$ and action $a$, and $State(l,t,h)$ for each literal $l$, trajectory $h\in\mathcal{T}$, and $t=1,\ldots,k$ where $k$ is the length of $h$. The clauses of $CNF_{\mathcal{T}}$ are obtained from clausal encodings of the STRIPS axioms as follows: For each $l$ and $a_t$ at step $t$ of some trajectory $h\in\mathcal{T}$, include
\begin{compactenum}
\item $\neg \ispre(l,a_t)\vee State(l,t-1,h)$ if $l$ is not observed at step $t-1$ or $\neg \ispre(l,a_t)$ if $l$ is observed false.
\item $\neg \iseff(l,a_t)\vee State(l,t,h)$, if $l$ is not observed at step $t$, or $\neg \iseff(l,a_t)$ if $l$ is observed false at step $t$, and
\item $\neg State(l,t-1,h)\vee \iseff(\neg l,a_t)\vee State(l,t,h)$ if $l$ is not observed at either step $t$ or $t-1$, $\neg State(l,t-1,h)\vee \iseff(\neg l,a_t)$ if $l$ is not observed at $t-1$ and observed false at $t$, and $\iseff(\neg l,a_t)\vee State(l,t,h)$ if $l$ is observed true at $t-1$ and not observed at $t$.
\end{compactenum}
We also include the mutual exclusion axioms $\neg \iseff(l,a)\vee \neg \iseff(\neg l,a)$ and $\neg State(l,0,h)\vee \neg State(\neg l,0,h)$ for each unobserved literal $l$.
Observe that the claim holds for $CNF_{\mathcal{T}}$ if we additionally set the variables $State(l,t,h)$ to true if $l$ would be true in the corresponding fully-observed trajectory at step $t$ (and conversely): indeed, these are encodings of precisely the action models that obey the STRIPS axioms and corresponding trajectories. We also note that the clauses of $CNF_{\mathcal{T}}$ each only use variables corresponding to a single fluent, so the set of satisfying assignments correspond to products of assignments to the formulas $CNF_{\mathcal{T}}(l)$ that only include the clauses using the same fluent as $l$. (Note that the STRIPS axioms ensure that the post-state is determined uniquely given an assignment to the pre-state.)

We now show that the satisfying assignments to each $CNF_{\eff}(l)$ have the corresponding property for the literal $l$. Recall that resolution is refutation complete, so it suffices to show that for any resolution refutation of a CNF formula $\varphi\wedge CNF_{\mathcal{T}}(l)$, where $\varphi$ (like $CNF_{\eff}(l)$) only contains variables $\iseff(l,a)$, a corresponding refutation of $\varphi\wedge CNF_{\eff}(l)$ exists. 

We first observe that $\ispre(l,a)$ only appears negatively in all clauses, and hence clauses containing these variables cannot be used in any refutation. Similarly, the initial state variables $State(l,0,h)$ only appear negatively and hence we cannot use the mutual exclusion clauses for the state variables either.

Second, observe that to obtain a refutation using the clauses of $CNF_{\mathcal{T}}(l)$, the variables $State(l,t,h)$ must be eliminated, but this variable only appears in a negative literal in the third type of clause (these do not appear in $\varphi$ by assumption), created for step $t+1$ of trajectory $h\in\mathcal{T}$. (And in the mutual exclusion axiom for unobserved variables in the initial state.) The variable only appears positively in the clauses created for step $t$ of the second and third type. Observe that we can rewrite the proof (possibly increasing its size) so that these applications of the resolution rule occur first.

We now claim that the clauses resulting from the final application of the resolution rule on a variable $State(l,t,h)$ appear in $CNF_{\eff}(l)$, which will prove the lemma. Indeed, we can only apply the resolution rule to some run of consecutive clauses of the third type in which the literal $l$ was not observed, ending with a step $t+\Delta$ where $l$ was observed, and beginning with either eliminating $State(l,t-1,h)$ using the clause $\neg \iseff(l,a_{t-1})\vee State(l,t-1,h)$ (created on lines 6--7), or with a clause corresponding to a step $t$ where $l$ was observed in step $t-1$ (created on lines 4--5). 
\end{proof}
}\fi
\begin{theorem}[Safety]\label{thm:episam-safe}
The contingent planning formulation created by EPI-SAM (Algorithm \ref{alg:episam}) is safe.
\end{theorem}
\begin{proof}
We first claim that if Algorithm \ref{alg:episam} omits $l$ from $\pre(a)$, then some trajectory in $\mathcal{T}$ must have had $l$ false in the pre-state of $a$, and hence $l$ could not have been in $\pre_{M^*}(a)$ for the true action model; it therefore will follow that whenever a state $s$ satisfies $\pre(a)$, it satisfies $\pre_{M^*}(a)$ and is therefore applicable.

It is immediate that this holds for any $l$ that is deleted in line \ref{episam-easy-pre-delete}. 
Otherwise, Algorithm \ref{alg:episam} creates a set of trajectories $\mathcal{T}_{l,a}$ in which we assume that $l$ was true whenever $a$ was taken; 
observe that $l$ can be deleted iff there does not exist a set of effects on the fluent for $l$ consistent with $\mathcal{T}_{l,a}$. \roni{I don't fully follow this condition. Do you mean that $l$ can be deleted iff there is no action model consistent with $\mathcal{T}_{l,a}$?} \brendan{Roughly so -- except that only the portions of the action model that have effects on l's fluent are relevant/considered, and the question is then whether or not there is an action model (restricted to this fluent) for which l could be the precondition. We consider each literal (hence, fluent) separately.}
Now, note that line \ref{episam-action-delete} only deletes actions that cannot have an effect on this fluent, as any subsequent (deleted) actions also could not have had an effect on the fluent, and the fluent then occurs with two different values at the next observation. 
\roni{This is where I get lost a bit. It seems there's an invariant you're claiming here but I don't get it}\brendan{Yes, I omitted a bit here. Strictly speaking what you have is inductively that for all of the deleted actions in the trajectory, none of those actions could have had an effect on the literal. Supposing that this is true, then any action that appears immediately prior to such sequences of actions, if it has an effect, must set the fluent equal to the next observed value in the trajectory since no subsequent action can change it. But now, if an action would need to set the fluent to two different values (in different portions of the trajectories), then since any action effect would only set the fluent to one value, the action could not had an effect on the fluent, so we can add this action to that sequence, i.e., delete/ignore all occurrences of the action in the trajectories, since the action cannot change the fluent we are examining.}
Therefore, in particular, if we encounter an action for which this fluent had a different value in the pre-state that cannot have an effect on the fluent, no such action model can exist and so we can delete the literal on line \ref{episam-hard-pre-delete}. \roni{I got lost here also. How in the pseudo-code do you ensure that the action (I guess $a'$) cannot have an effect on the fluent?}\brendan{See above. We only delete actions that can't have an effect on the fluent. Hopefully it's clearer now.}
Now, on the other hand, if the loop on line \ref{episam-action-delete-loop} terminates, any action at the end of a run of states where the fluent is unobserved must be followed by states where the fluent takes at most one value. Then we could assign the effect to that action that sets the fluent to that value, and thus obtain a consistent action model with $\mathcal{T}_{l,a}$. Thus, in this case, $l$ could be a precondition of $a$. The algorithm continues to the next iteration of the loop and does not consider this literal again, thus it remains in $\pre(a)$ upon termination.

Finally, we note that the contingent plan must achieve the goal with any setting of the $\iseff$ variables that is consistent with $CNF_{\eff}$. By Lemma~\ref{lem:cnf-char}, we see that this means that in particular the goal is achieved with the assignment corresponding to the real action model. Thus, the EPI-SAM action model is indeed safe.
%%
%% OLD SAT-BASED ANALYSIS
%%
%%Consider the formula $CNF_{\mathcal{T}}(l)$ created in Lemma~\ref{lem:cnf-char}: $\ispre(l,a)$ can be set to true in a satisfying assignment iff for every step $t$ where $a=a_t$, we have $State(l,t-1,h)$ true and $State(\neg l,t-1,h)$ false (equiv., $\neg State(\neg l,t-1,h)$ true).  (Equivalently, these are the clauses that can be derived using the unit clause $\ispre(l,a)$.) Thus, by the refutation completeness of resolution, we can refute this collection of unit clauses with $CNF_{\mathcal{T}}(l)$ iff $l$ cannot be a precondition of $a$.
%%
%%Again, the $State$ variables must be eliminated, and the (other) $\ispre$ variables cannot be used in a refutation because they only appear negatively. Again, recall that the $State$ variables only appear negatively in clauses of the third type in $CNF_{\mathcal{T}}$. Thus, in addition to the clauses constructed in $CNF_{\eff}(l)$, these new literals (only) allow us to obtain clauses for each run of steps in the trajectory where $l$ is not observed, either starting with a clause of the third type where $l$ is observed true (created in lines 16--17), or using a clause of the second type to eliminate the initial state literal (in line 22). By Lemma~\ref{lem:cnf-char}, this resulting formula can be refuted with $CNF_{\eff}(l)$ iff $l$ cannot be a precondition of $a$.
%%
%%Because we only remove literals from the preconditions when this formula is refuted, in any plan constructed with the EPI-SAM action model, whenever an action is taken it must satisfy the preconditions of the real action. Finally, we note that the contingent plan must achieve the goal with any setting of the $\iseff$ variables that is consistent with $CNF_{\eff}$. Again, by Lemma~\ref{lem:cnf-char}, we see that this means that in particular the goal is achieved with the assignment corresponding to the real action model. Thus, the EPI-SAM action model is indeed safe.
\end{proof}
%\emph{Sketch of proof (incomplete)}
%In the first part (line 1-9 in Algorithm \ref{alg:episam}), EPI-SAM creates for every literal a CNF that captures all possible clauses that we can write about literal $l$. If literal $l$ was an effect at some action in the trajectories, it must be represented by a clause in the CNF. In the second part (line 10-25 in Algorithm \ref{alg:episam}), the algorithm initializes that for every action, its preconditions contain all possible literals. For each action, the algorithm removes from its preconditions only literal such that there is no possible assignment that satisfies the CNF that the algorithm can infer about that literal. Thus, it won't remove any literal that is an actual precondition of an action.  
% \begin{definition}[Strength of Action Models]
% If there exists a trajectory that is consistent with $M'$ but not with $M$, then we say that $M$ is weaker than $M'$.
% If no such trajectory exists then we say that $M$ is at least as strong as $M'$. 
% \label{def:weakness}
% \end{definition}

% \begin{theorem}[The Strength of SGAM Learning]
% Let $M_{SGAM}$ be the action model created by SGAM learning given the set of trajectories $\mathcal{T}$. 
% $M_{SGAM}$ is at least as strong as any action model $M'$ that is safe and consistent with $\mathcal{T}$. 
% \label{thm:sam-learning-complete-grounded}
% \end{theorem}
% \begin{proof}
% Consider an action model $M'$, which is safe and consistent with $\mathcal{T}$. % and safe w.r.t.\ \realm. 
% Let $a$ be an action and $s$ be a state such that $a$ is applicable in $s$ according to $M'$, i.e., $\pre_{M'}(a)\subseteq s$. 

% Since $M'$ is safe w.r.t.\ \realm, then 
% $\pre_{\realm}(a)\subseteq s$

% and $a_{M'}(s)=a_{\realm}(s)$. 
% By construction of $M_\sgam$, if a literal $l$ is a precondition of $a$ according to $M_\sgam$, 
% then it has appeared in the pre-state of all action triplets in $\mathcal{T}(a)$. 
% Thus, there exists a consistent action model in which $l$ is a precondition of $a$ 
% and this action model may be the real model. 
% Therefore, since $M'$ is safe it follows that $\pre_{M_\sgam}(a) \subseteq \pre_{M'}(a)$, 
% and thus $a$ is applicable in $s$ according to $M_\sgam$, 
% i.e., $\pre_{M_\sgam}(a)\subseteq s$. 
% Since $M_\sgam$ is safe, %it follows that
% $a_{M_\sgam}(s)=a_{\realm}(s)=a_{M'}(s)$. %, as required. 
% Thus, every trajectory consistent with  $M'$ will also be consistent with $M_\sgam$.
% \end{proof}

\begin{theorem}[Strength]
The contingent planning formulation returned by the EPI-SAM learning algorithm (Algorithm \ref{alg:episam}) is the strongest safe action model.
\end{theorem}
\begin{proof}
In the proof of Theorem~\ref{thm:episam-safe}, we noted that literals are deleted from $\pre(a)$
%the formula we created can be refuted iff $\ispre(l,a)$ can be refuted with $CNF_{\mathcal{T}}$, i.e., 
iff $l$ cannot be a precondition of $a$ in any action model consistent with the trajectories in $\mathcal{T}$. Thus, suppose that some action model allows action $a$ to be taken in a state $s$ that does not satisfy the preconditions constructed by EPI-SAM. Then there is some $l$ that is a precondition of $a$ for EPI-SAM that is false in $s$. But by the above,
%$\ispre(l,a)$ may be set to true in some satisfying assignment of $CNF_{\mathcal{T}}$ -- i.e., 
$l$ may be a precondition of $a$ in the true action model, so the other action model is not safe. 

Similarly, suppose that there is a plan under the other action model that is allowed by the EPI-SAM action model, but for which EPI-SAM does not achieve the goal. This means (by Lemma~\ref{lem:cnf-char}) that there was some action model consistent with $\mathcal{T}$ under which the goal was not achieved. Again, the other action model is therefore not safe.
\end{proof}

%\emph{Sketch of proof (incomplete)}
%Let $CNF_\eff=\bigwedge_l CNF_\eff(l)$. 
%Observation 1: The effects part of every action model that is consistent with the observations is represented by a satisfying assignment to $CNF_\eff$.
%Observation 2: Every satisfying assignment to $CNF_\eff$ represents the effects part of an action model that is consistent with the observations. 
%Proof of observation 1+2:
%Here we need to say something about how it is Ok to learn the effects before the preconditions. 
%Observation 3: If EPI-SAM removed a literal $l$ from $pre(a)$ then $l$ isn't a precondition in any consistent action model.  
%Observation 4: If EPI-SAM did not remove a literal $l$ from $pre(a)$ then there exists a consistent action model in which $l$ is in $pre(a)$.  


\paragraph{Time Complexity}
\brendan{Note: while it's linear time for propositional domains so long as we can create a clause in unit time, it might actually be quadratic in the lifted domains since the same parameter bound literal could correspond to different ground literals at different points in the trajectory. We could suppress the difference by just saying ``polynomial time'' and leaving the exact exponent to the reader, and this would also save us from needing to talk about data structures.}
\begin{theorem}
Given a set of trajectories $\mathcal{T}$, the EPI-SAM learning (Algorithm~\ref{alg:episam}) runs in time 
\begin{small}
\[\mathcal{O}\Big(\sum_{a\in \mathcal{A}}|\mathcal{T}(a)|\sum_{f\in\mathcal{F}}\prod_{t\in T}arity(a,t)^{arity(f,t)}\Big)\]
\end{small}
\end{theorem}
\begin{proof}
Observe that in each loop, each literal in each step of each trajectory is examined $O(1)$ times---in the first loop we create one clause for each step, and it is set to true or deleted at most once in the second loop. 

We can perform the bookkeeping in the second loop in linear time overall by suitable data structures: we maintain a linked list over the occurrences of a given action, all with a reference to a common structure for the action that records which settings of $l$ appear in the post-state, and we record each of the unobserved runs of a literal with a linked list. Then checking if an action should be deleted takes $O(1)$ time and deleting the occurrences of an action takes $O(1)$ time per occurrence. The data structures likewise take $O(1)$ time per each occurrence of a literal to initialize.
\end{proof}
We remark that the data structures described above are essentially similar to the data structures enabling unit propagation to be performed in linear time; indeed, a SAT encoding of the problem would be solved by standard SAT solvers in linear time.
%EPI-SAM (Algorithm \ref{alg:episam}) consists of two parts: extracting all possible effect clauses for each literal as a CNF, and running the SAT solver to determine the preconditions for each action. The first part runs in polynomial time in the number of literals and total number of state-action triplets. The second part, however, depends on the running time of the SAT solver, which cannot be polynomial time if it is complete (unless P=NP). This is inherent:
%\begin{theorem}
%The following problem is NP-complete: given a set of partially observed trajectories $\mathcal{T}$, action $a$, and literal $l$, decide whether $l$ is a precondition of $a$ in some action model consistent with $\mathcal{T}$.
%\end{theorem}
%\noindent
%{\em Sketch of proof.}
%The problem is clearly in NP. We reduce 3SAT to the problem by creating an action for each literal and a trajectory for each clause in a domain with a single fluent $f$, ending in an additional action. We also create a trajectory for each literal ensuring that one literal in each pair has $f$ as an effect. The additional action does not have $f$ as a precondition iff $f$ is false in one of these trajectories, which occurs only if the formula is unsatisfiable.

\section{Experiments}

\begin{table}[t]
\centering
\small
\begin{tabular}{l|c|c|c|c}
\hline
            &               \#&             \# &        max &       max  \\
        Domain    & lifted  & lifted & arity & arity \\
            &fluents&actions&fluents&actions  \\ 
\hline

Blocks      & 5                 & 4                 &2                  &2                 \\
Depot      & 6                 & 5                 &4                  &2                    \\
Ferry       & 5                 & 3                 &2                  &2              \\
Floortile   & 10                & 7                 &2                  &4               \\
Gripper     & 4                 & 3                 &2                  &3              \\
Hanoi       & 3                 & 1                 &2                  &3                \\
Npuzzle     & 3                 & 1                 &2                  &3               \\
Parking     & 5                 & 4                 &2                  &3                  \\
Sokoban     & 4                 & 2                 &3                  &5                 \\
Transport      & 5                 & 3                 &2                  &5                 \\
\hline
\end{tabular}
\caption{Description of the domains used in our experiments.}
\label{tab:domains}
\end{table}


% \begin{table}[t]
% \centering
% % \small
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{|l|l|c|c|c|c|c|c|c|c|c|c|} 
% \hline
% \multirow{2}{*}{Domain} & \multirow{2}{*}{A} & \multicolumn{5}{c|}{$\eta = 0.3$}                                   & \multicolumn{5}{c|}{$\eta = 0.1$}                                     \\ 
% \cline{3-12}
%                           &                         & $|\mathcal{T}|$ & P(\pre) & R(\pre) & P(\eff) & R(\eff) & $|\mathcal{T}|$ & P(\pre) 
%               & R(\pre) 
%               & P(\eff) 
%               & R(\eff)  \\ 
% \hline
% \multirow{2}{*}{Blocks} & S                     & 2               & \textbf{1.00}           & 0.90           & 1.00          & \textbf{0.95}          & 3               & \textbf{1.00}           & 0.80          & \textbf{1.00}           & 0.85            \\ 
%                         & F                       & 2               & 0.90           & 0.90          &  1.00         & 0.89          & 3               & 0.90           &  \textbf{0.83}         &  0.90         &   0.85         \\ 
% \hline

% \multirow{2}{*}{Depot} & S                     & 3               & \textbf{1.00}           & 0.85          & \textbf{1.00}          & 1.00        & 4               & \textbf{1.00}            &  0.80         &  \textbf{1.00}          & 1.00           \\ 
%                         & F                       & 3               &  0.80          & 0.85          &0.90           &1.00           & 4               & 0.80           & 0.80          & 0.90           & 1.00           \\ 
% \hline

% \multirow{2}{*}{Ferry} & S                     & 2               & \textbf{1.00}           &    1.00       & 1.00          & 1.00          & 2               & \textbf{1.00}           &  0.90         &  \textbf{1.00}          &   0.90         \\ 
%                         & F                       & 2               & 0.85           & 1.00          & 1.00          & 1.00          & 2               &  0.75          &  \textbf{1.00}         &  0.85          & \textbf{1.00}           \\ 
% \hline

% \multirow{2}{*}{Floortile} & S                     & 5              &  \textbf{1.00}          &  \textbf{0.87}         & \textbf{1.00}          & \textbf{0.85}          & 7               & \textbf{1.00}           & 0.85          & \textbf{1.00}           & 0.85           \\ 
%                         & F                       & 5               & 0.83           & 0.80          & 0.77          & 0.80          & 7               & 0.87           & 0.85          &  0.80          &  0.85          \\ 
% \hline

% \multirow{2}{*}{Gripper} & S                     & 4               & 1.00            &1.00           & 1.00          &1.00           & 4               &   1.00         &   1.00        &   1.00         &   1.00         \\ 
%                         & F                       & 4               & 1.00           & 1.00          & 1.00          &1.00           & 4               &  1.00          & 1.00          & 1.00           &  1.00          \\ 
% \hline

% \multirow{2}{*}{Hanoi} & S                     & 1               & \textbf{1.00}           & 1.00          &  1.00         & 1.00          & 1               & \textbf{ 1.00}          & 1.00          &  1.00          &  1.00          \\ 
%                         & F                       & 1               & 0.85           & 1.00          & 1.00          & 1.00          & 1               &  0.80          & 1.00          &  1.00          & 1.00           \\ 
% \hline

% \multirow{2}{*}{Npuzzle} & S                     & 1               & 1.00           & 1.00          &1.00           & 1.00          & 1               &  \textbf{1.00}          &   1.00        &   1.00         &  1.00          \\ 
%                         & F                       & 1               & 1.00           & 1.00          & 1.00          & 1.00          & 1               & 0.80           &1.00           & 1.00           &  1.00          \\ 
% \hline

% \multirow{2}{*}{Parking} & S                     & 5               & \textbf{1.00}           & 0.85          & 1.00          & 1.00          & 6               & \textbf{1.00}           & 0.80          & \textbf{1.00}           &  1.00          \\ 
%                         & F                       & 5               & 0.83           & 0.85          & 1.00          &  1.00         & 6               & 0.80           & \textbf{0.85}          &  0.89          &  1.00          \\ 
% \hline

% \multirow{2}{*}{Sokoban} & S                     & 2               & 1.00           &1.00           &1.00           &1.00           & 2               &1.00            & 1.00          & 1.00           & 1.00           \\ 
%                         & F                       & 2               &1.00            & 1.00          & 1.00          &1.00           & 2               &  1.00          &1.00          & 1.00           &  1.00          \\ 
% \hline

% \multirow{2}{*}{Transport} & S                     & 3               & \textbf{1.00}           & \textbf{0.83}          &\textbf{1.00}           & 0.90          & 4               &  \textbf{1.00}          & 0.80          &  \textbf{1.00}          & 0.90           \\ 
%                         & F                       & 3               &  0.75          &0.80           & 0.80          &0.90           & 4               & 0.80           & 0.80          &  0.83          &  0.90          \\ 
% \hline
% \end{tabular}
% }
% \caption{Empirical precision and recall of PI-SAM and FAMA under random masking probability $\eta = 0.1$ and $\eta = 0.3$}
% \label{tab:prec-rec}
    
% \end{table}

\begin{table*}[t]
\centering
% \small
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|l|c|c|c|c|c|c|c|c|c|c|} 
\hline
\multirow{2}{*}{Domain} & \multirow{2}{*}{Algorithm} & \multicolumn{5}{c|}{$\eta = 0.3$}                                   & \multicolumn{5}{c|}{$\eta = 0.1$}                                     \\ 
\cline{3-12}
                          &                         & $|\mathcal{T}|$ & P(\pre) & R(\pre) & P(\eff) & R(\eff) & $|\mathcal{T}|$ & P(\pre) 
              & R(\pre) 
              & P(\eff) 
              & R(\eff)  \\ 
\hline
\multirow{2}{*}{Blocks} & PI-SAM                     & 2               & \textbf{1.00}           & 0.90           & 1.00          & \textbf{0.95}          & 3               & \textbf{1.00}           & 0.80          & \textbf{1.00}           & 0.85            \\ 
                        & FAMA                       & 2               & 0.90           & 0.90          &  1.00         & 0.89          & 3               & 0.90           &  \textbf{0.83}         &  0.90         &   0.85         \\ 
\hline

\multirow{2}{*}{Depot} & PI-SAM                     & 3               & \textbf{1.00}           & 0.85          & \textbf{1.00}          & 1.00        & 4               & \textbf{1.00}            &  0.80         &  \textbf{1.00}          & 1.00           \\ 
                        & FAMA                       & 3               &  0.80          & 0.85          &0.90           &1.00           & 4               & 0.80           & 0.80          & 0.90           & 1.00           \\ 
\hline

\multirow{2}{*}{Ferry} & PI-SAM                     & 2               & \textbf{1.00}           &    1.00       & 1.00          & 1.00          & 2               & \textbf{1.00}           &  0.90         &  \textbf{1.00}          &   0.90         \\ 
                        & FAMA                       & 2               & 0.85           & 1.00          & 1.00          & 1.00          & 2               &  0.75          &  \textbf{1.00}         &  0.85          & \textbf{1.00}           \\ 
\hline

\multirow{2}{*}{Floortile} & PI-SAM                     & 5              &  \textbf{1.00}          &  \textbf{0.87}         & \textbf{1.00}          & \textbf{0.85}          & 7               & \textbf{1.00}           & 0.85          & \textbf{1.00}           & 0.85           \\ 
                        & FAMA                       & 5               & 0.83           & 0.80          & 0.77          & 0.80          & 7               & 0.87           & 0.85          &  0.80          &  0.85          \\ 
\hline

\multirow{2}{*}{Gripper} & PI-SAM                     & 4               & 1.00            &1.00           & 1.00          &1.00           & 4               &   1.00         &   1.00        &   1.00         &   1.00         \\ 
                        & FAMA                       & 4               & 1.00           & 1.00          & 1.00          &1.00           & 4               &  1.00          & 1.00          & 1.00           &  1.00          \\ 
\hline

\multirow{2}{*}{Hanoi} & PI-SAM                     & 1               & \textbf{1.00}           & 1.00          &  1.00         & 1.00          & 1               & \textbf{ 1.00}          & 1.00          &  1.00          &  1.00          \\ 
                        & FAMA                       & 1               & 0.85           & 1.00          & 1.00          & 1.00          & 1               &  0.80          & 1.00          &  1.00          & 1.00           \\ 
\hline

\multirow{2}{*}{Npuzzle} & PI-SAM                     & 1               & 1.00           & 1.00          &1.00           & 1.00          & 1               &  \textbf{1.00}          &   1.00        &   1.00         &  1.00          \\ 
                        & FAMA                       & 1               & 1.00           & 1.00          & 1.00          & 1.00          & 1               & 0.80           &1.00           & 1.00           &  1.00          \\ 
\hline

\multirow{2}{*}{Parking} & PI-SAM                     & 5               & \textbf{1.00}           & 0.85          & 1.00          & 1.00          & 6               & \textbf{1.00}           & 0.80          & \textbf{1.00}           &  1.00          \\ 
                        & FAMA                       & 5               & 0.83           & 0.85          & 1.00          &  1.00         & 6               & 0.80           & \textbf{0.85}          &  0.89          &  1.00          \\ 
\hline

\multirow{2}{*}{Sokoban} & PI-SAM                     & 2               & 1.00           &1.00           &1.00           &1.00           & 2               &1.00            & 1.00          & 1.00           & 1.00           \\ 
                        & FAMA                       & 2               &1.00            & 1.00          & 1.00          &1.00           & 2               &  1.00          &1.00          & 1.00           &  1.00          \\ 
\hline

\multirow{2}{*}{Transport} & PI-SAM                     & 3               & \textbf{1.00}           & \textbf{0.83}          &\textbf{1.00}           & 0.90          & 4               &  \textbf{1.00}          & 0.80          &  \textbf{1.00}          & 0.90           \\ 
                        & FAMA                       & 3               &  0.75          &0.80           & 0.80          &0.90           & 4               & 0.80           & 0.80          &  0.83          &  0.90          \\ 
\hline
\end{tabular}
}
\caption{Empirical precision and recall of PI-SAM and FAMA under random masking probability $\eta = 0.1$ and $\eta = 0.3$}
\label{tab:prec-rec}
    
\end{table*}


\begin{table}[t]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|c|c|c}
    \hline
    \multirow{2}{*}{Domain} & \multicolumn{3}{c}{\# triplets}  \\
    \cline{2-4}
                        & SAM ($\eta = 1.0$) & PI-SAM ($\eta = 0.3$)    & PI-SAM ($\eta = 0.1$)                 \\
    \hline
    Hanoi                    & 1    & 4               &     38       \\
    Npuzzle                    & 1    & 4             &     36         \\
    Ferry                    & 4    & 15              &    42          \\
    Gripper                    & 5    & 18            &    55           \\
    Sokoban                    & 6    & 22            &     64         \\
    \hline
    \end{tabular}
    }
    \caption{\# of transitions needed to learn the preconditions}
    \label{tab:pisam_sam}
\end{table}

\begin{table}[t]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|l|c|c|c|c}
    \hline 
    Domain & Algorithm & P (\pre) & R (\pre) & P (\eff) & R (\eff) \\
    \hline
    \multirow{2}{*}{Blocks}    & EPI-SAM   &1.00 &0.90 & 1.00 & 0.95           \\
                           & PI-SAM    &1.00 &0.90 & 1.00 & 0.95          \\
    \hline                       
    \multirow{2}{*}{Depot}     & EPI-SAM   & 1.00 &0.85 & 1.00 & 1.00          \\
                           & PI-SAM    & 1.00 &0.85 & 1.00 &1.00           \\
    \hline
    \multirow{2}{*}{Floortile} & EPI-SAM   &1.00 & \textbf{0.89} & 1.00 & \textbf{0.90  }         \\
                           & PI-SAM    &1.00 & 0.87 & 1.00 & 0.85           \\
    \hline
    \multirow{2}{*}{Parking}   & EPI-SAM   &1.00 &0.85 & 1.00 &1.00           \\
                           & PI-SAM    &1.00 &0.85 & 1.00 &1.00           \\
    \hline
    \multirow{2}{*}{Transport} & EPI-SAM   &1.00 & \textbf{0.85} & 1.00  & \textbf{0.92 }          \\
                           & PI-SAM    &1.00 & 0.83 & 1.00  & 0.90                \\
    \hline

                                
    \end{tabular}
    }
    \caption{Empirical precision and recall of EPI-SAM under random masking probability $\eta = 0.3$}
    \label{tab:episam}
\end{table}


To evaluate our algorithms' performance in practice, we perform experiments on common domains from the IPC\cite{ipc}, which the feature descriptions are shown in Table \ref{tab:domains}. For each domain, we generated the problems using the generator provided by the IPC learning tracks and solved them using their true model in an off-the-shelf planner to obtain the trajectories. We then produced partially observable trajectories using \textit{random masking}. At each state, each literal is randomly masked with probability $1-\eta$ (i.e. each literal has $\eta$ chances of being observed). By definition, these partially observable trajectories satisfy the \textit{$\eta^2$-bounded concealment assumption}. They were then split into state-action triplets and passed to our algorithms to obtain the safe action model. 

% \begin{table}[t]
% \centering
% \small
% \begin{tabular}{l|c|c|c|c}
% \hline
%             &               \#&             \# &        max &       max  \\
%         Domain    & lifted  & lifted & arity & arity \\
%             &fluents&actions&fluents&actions  \\ 
% \hline

% Blocks      & 5                 & 4                 &2                  &2                 \\
% Depot      & 6                 & 5                 &4                  &2                    \\
% Ferry       & 5                 & 3                 &2                  &2              \\
% Floortile   & 10                & 7                 &2                  &4               \\
% Gripper     & 4                 & 3                 &2                  &3              \\
% Hanoi       & 3                 & 1                 &2                  &3                \\
% Npuzzle     & 3                 & 1                 &2                  &3               \\
% Parking     & 5                 & 4                 &2                  &3                  \\
% Sokoban     & 4                 & 2                 &3                  &5                 \\
% Transport      & 5                 & 3                 &2                  &5                 \\
% \hline
% \end{tabular}

% \caption{Description of the domains used in our experiments.}
% \label{tab:domains}
% \end{table}


% \paragraph{Metrics} To measure the performance of each algorithm, we computed the \textit{empirically-based precision-recall} for \textit{preconditions} and \textit{effects} defined as follows: 
% \begin{equation}\small
% precision = \frac{TP}{TP + FP},~
% recall = \frac{TP}{TP + FN}
% \end{equation}
% where, for \textit{preconditions}:
% \begin{compactitem}
%     \item TP (true positives): number of transitions that are valid according to both the obtained action model and the true action model.
%     \item FP (false positives): number of transitions that are valid according to  the obtained action model but not the true action model.
%     \item TN (true negatives): number of transitions that are invalid according to both the obtained action model and the true action model.
%     \item FN (false negatives:): number of transitions that are valid according to   the true action model but not the obtained action model.
% \end{compactitem}
% and for \textit{effects}: 
% \begin{compactitem}
%     \item TP (true positives): number of transitions in which similar literals switch values in both the obtained action model and the true action model.
%     \item FP (false positives): number of transitions in which exists a literal that switches value in the obtained action model but not the true action model.
%     \item TN (true negatives): number of transitions in which similar literals don't switch values in both the obtained action model and the true action model.
%     \item FN (false negatives:): number of transitions in which exists a literal that switches value in the true action model but not the obtained action model.
% \end{compactitem}
% Unlike the \textit{syntactic-based precision-recall}, which is computed based on the number of overlapping literals between the obtained action model and the true model, our \textit{empirically-based precision-recall} is computed using the number of actual valid transitions according to both the obtained action model and the true model. Thus, it can clearly indicate how safe the learned action model is (the higher the precision, the safer the learned model; at $precision = 1$, the obtained model is as safe as the true model).

\paragraph{Metrics} 
A common approach to comparing action models is by computing the precision and recall of the learned action model with respect to which literals appear in the real action model. However, this syntactic measure has three limitations. First, it requires the evaluated action models to use the same fluents and action names. Second, it gives the same ``penalty'' for every mistake in the learned model. Third, and most severely, domains may have distinct but semantically-equivalent action models. For example, in Npuzzle, we could have a precondition that the tile we are sliding into the empty position is not an empty position. This precondition is not necessary, as there is only ever one empty position in any puzzle. Thus, either formulation of the domain is adequate for planning purposes, but a syntactic measure of correctness will penalize one of the two formulations. Instead, we introduce and use 
\textit{empirically-based precision and recall} measures, which are based on comparing the number of transitions that are valid or invalid according to the learned action model ($\hat{A}$) and the true action model ($A$). 
Similar to the standard precision and recall measures, our empirical precision and recall measures are defined according to the number of true/false positives/negatives (TP,FP,TN,FN): 
\begin{equation}\small
precision = \frac{TP}{TP + FP},~
recall = \frac{TP}{TP + FN}
\end{equation}
but TP, FP, TN, and FN are computed differently, based on analyzing a set of transitions sampled from observed trajectories.  
\paragraph{For preconditions} 
TP are the number of transitions that are valid according to both $\hat{A}$ and $A$. 
FP are the number of transitions that are valid according to $\hat{A}$ but not $A$.
TN are the number of transitions that are invalid according to $\hat{A}$ and $A$. 
FN are the number of transitions that are valid according to $A$ and but not $\hat{A}$. 
\paragraph{For effects} 
TP are the number of transitions in which similar literals switch values in both $\hat{A}$ and $A$.
FP are the number of transitions in which exists a literal that switches value in $\hat{A}$ but not in $A$. 
TN are the number of transitions in which similar literals do not switch values in $\hat{A}$ and in $A$. 
FN are the number of transitions in which exists a literal that switches value in $A$ but not in $\hat{A}$.
% Let $a_X(s)$ denote the state reached by perfoming $a$ at $s$ according to action model $X$. 
% TP are the number of transitions in which $a_{\hat{A}}(s)=a_A(s)$. 
% FP are the number of transitions in which $\exists l\in s$ where $s[l]\neq \hat{A}}(s)$=a_A(s)$. 
% TP are the number of transitions in which $a_{\hat{A}}(s)=a_A(s)$. 
% TP are the number of transitions in which $a_{\hat{A}}(s)=a_A(s)$. 
% FP are the number of transitions that are valid according to $\hat{A}$ but not $A$.
% TN are the number of transitions that are invalid accord. %ing to $\hat{A}$ and $A$. 
% FN are the number of transitions that are valid according to $A$ and but not $\hat{A}$. 

% \begin{compactitem}
%     \item TP (true positives): number of transitions in which similar literals switch values in both the obtained action model and the true action model.
%     \item FP (false positives): number of transitions in which exists a literal that switches value in the obtained action model but not the true action model.
%     \item TN (true negatives): number of transitions in which similar literals don't switch values in both the obtained action model and the true action model.
%     \item FN (false negatives:): number of transitions in which exists a literal that switches value in the true action model but not the obtained action model.
% \end{compactitem}
% Unlike the \textit{syntactic-based precision-recall}, which is computed based on the number of overlapping literals between the obtained action model and the true model, our \textit{empirically-based precision-recall} is computed using the number of actual valid transitions according to both the obtained action model and the true model. Thus, it can clearly indicate how safe the learned action model is (the higher the precision, the safer the learned model; at $precision = 1$, the obtained model is as safe as the true model).

\paragraph{Results and Discussion}
We ran experiments with two random masking probabilities $\eta$, $0.1$ and $0.3$.
For each domain, we computed the \textit{empirical precision} (P) and \textit{recall} (R) separately for the preconditions (\pre) and effects (\eff). As a baseline, we compared our algorithms to FAMA \cite{aineto2019learning}, a modern algorithm for learning action models under partial observability. Table \ref{tab:prec-rec} lists the results of our experiments, averaged over three independent runs.
As we mentioned, FAMA has no safety guarantee. Meanwhile, our PI-SAM algorithm (Algorithm \ref{alg:pisam}) always returns a conservative model since it only removes a literal from the preconditions if its negation is observed (unmasked) in the pre-state, and adds a literal to the effect if it is unmasked and switches value between the pre-state and post-state. Thus, in terms of \textit{empirical precision}, PI-SAM (Algorithm \ref{alg:pisam}) has the advantage of achieving $1.0$ for both preconditions and effects while FAMA does not always manage this. In terms of \textit{empirical recall}, PI-SAM tends to perform better under lower masking probability (high $\eta$), while FAMA tends to obtain higher recall under higher masking probability (low $\eta$). 
%Nevertheless, the differences are not significant.
% <<- Significant in a statistical sense? omitting for now unless we can confirm.

% \begin{table*}[t]
% \centering
% \small
% %\resizebox{\textwidth}{!}{
% \begin{tabular}{|l|l|c|c|c|c|c|c|c|c|c|c|} 
% \hline
% \multirow{2}{*}{Domain} & \multirow{2}{*}{Algorithm} & \multicolumn{5}{c|}{$\eta = 0.3$}                                   & \multicolumn{5}{c|}{$\eta = 0.1$}                                     \\ 
% \cline{3-12}
%                           &                         & $|\mathcal{T}|$ & P(\pre) & R(\pre) & P(\eff) & R(\eff) & $|\mathcal{T}|$ & P(\pre) 
%               & R(\pre) 
%               & P(\eff) 
%               & R(\eff)  \\ 
% \hline
% \multirow{2}{*}{Blocks} & PI-SAM                     & 2               & \textbf{1.00}           & 0.90           & 1.00          & \textbf{0.95}          & 3               & \textbf{1.00}           & 0.80          & \textbf{1.00}           & 0.85            \\ 
%                         & FAMA                       & 2               & 0.90           & 0.90          &  1.00         & 0.89          & 3               & 0.90           &  \textbf{0.83}         &  0.90         &   0.85         \\ 
% \hline

% \multirow{2}{*}{Depot} & PI-SAM                     & 3               & \textbf{1.00}           & 0.85          & \textbf{1.00}          & 1.00        & 4               & \textbf{1.00}            &  0.80         &  \textbf{1.00}          & 1.00           \\ 
%                         & FAMA                       & 3               &  0.80          & 0.85          &0.90           &1.00           & 4               & 0.80           & 0.80          & 0.90           & 1.00           \\ 
% \hline

% \multirow{2}{*}{Ferry} & PI-SAM                     & 2               & \textbf{1.00}           &    1.00       & 1.00          & 1.00          & 2               & \textbf{1.00}           &  0.90         &  \textbf{1.00}          &   0.90         \\ 
%                         & FAMA                       & 2               & 0.85           & 1.00          & 1.00          & 1.00          & 2               &  0.75          &  \textbf{1.00}         &  0.85          & \textbf{1.00}           \\ 
% \hline

% \multirow{2}{*}{Floortile} & PI-SAM                     & 5              &  \textbf{1.00}          &  \textbf{0.87}         & \textbf{1.00}          & \textbf{0.85}          & 7               & \textbf{1.00}           & 0.85          & \textbf{1.00}           & 0.85           \\ 
%                         & FAMA                       & 5               & 0.83           & 0.80          & 0.77          & 0.80          & 7               & 0.87           & 0.85          &  0.80          &  0.85          \\ 
% \hline

% \multirow{2}{*}{Gripper} & PI-SAM                     & 4               & 1.00            &1.00           & 1.00          &1.00           & 4               &   1.00         &   1.00        &   1.00         &   1.00         \\ 
%                         & FAMA                       & 4               & 1.00           & 1.00          & 1.00          &1.00           & 4               &  1.00          & 1.00          & 1.00           &  1.00          \\ 
% \hline

% \multirow{2}{*}{Hanoi} & PI-SAM                     & 1               & \textbf{1.00}           & 1.00          &  1.00         & 1.00          & 1               & \textbf{ 1.00}          & 1.00          &  1.00          &  1.00          \\ 
%                         & FAMA                       & 1               & 0.85           & 1.00          & 1.00          & 1.00          & 1               &  0.80          & 1.00          &  1.00          & 1.00           \\ 
% \hline

% \multirow{2}{*}{Npuzzle} & PI-SAM                     & 1               & 1.00           & 1.00          &1.00           & 1.00          & 1               &  \textbf{1.00}          &   1.00        &   1.00         &  1.00          \\ 
%                         & FAMA                       & 1               & 1.00           & 1.00          & 1.00          & 1.00          & 1               & 0.80           &1.00           & 1.00           &  1.00          \\ 
% \hline

% \multirow{2}{*}{Parking} & PI-SAM                     & 5               & \textbf{1.00}           & 0.85          & 1.00          & 1.00          & 6               & \textbf{1.00}           & 0.80          & \textbf{1.00}           &  1.00          \\ 
%                         & FAMA                       & 5               & 0.83           & 0.85          & 1.00          &  1.00         & 6               & 0.80           & \textbf{0.85}          &  0.89          &  1.00          \\ 
% \hline

% \multirow{2}{*}{Sokoban} & PI-SAM                     & 2               & 1.00           &1.00           &1.00           &1.00           & 2               &1.00            & 1.00          & 1.00           & 1.00           \\ 
%                         & FAMA                       & 2               &1.00            & 1.00          & 1.00          &1.00           & 2               &  1.00          &1.00          & 1.00           &  1.00          \\ 
% \hline

% \multirow{2}{*}{Transport} & PI-SAM                     & 3               & \textbf{1.00}           & \textbf{0.83}          &\textbf{1.00}           & 0.90          & 4               &  \textbf{1.00}          & 0.80          &  \textbf{1.00}          & 0.90           \\ 
%                         & FAMA                       & 3               &  0.75          &0.80           & 0.80          &0.90           & 4               & 0.80           & 0.80          &  0.83          &  0.90          \\ 
% \hline
% \end{tabular}
% %}
% \caption{Empirical precision and recall of PI-SAM and FAMA under random masking probability $\eta = 0.1$ and $\eta = 0.3$}
% \label{tab:prec-rec}
    
% \end{table*}

We also compared the the number of state-action triplets used by the PI-SAM algorithm and the SAM algorithm \cite{juba2021safe} to correctly learn the preconditions (i.e., \textit{precision} and \textit{recall} $= 1.0$). Here, we ran the SAM algorithm \cite{juba2021safe} on the same trajectories without random masking on a subset of the domains above in which PI-SAM achieved \textit{precision} and \textit{recall} $=1.0$ for the preconditions ($\eta = 0.3$). The results are shown in Table \ref{tab:pisam_sam}. As we observed, the number of state-action triplets required by PI-SAM scales inversely with the random masking probability $\eta$, which verifies the tightness of the bound in Theorem \ref{complexity-pisam-thm}. 

% \begin{table}[t]
%     \centering
%     \resizebox{\columnwidth}{!}{
%     \begin{tabular}{l|c|c|c}
%     \hline
%     \multirow{2}{*}{Domain} & \multicolumn{3}{c}{\# triplets}  \\
%     \cline{2-4}
%                         & SAM ($\eta = 1.0$) & PI-SAM ($\eta = 0.3$)    & PI-SAM ($\eta = 0.1$)                 \\
%     \hline
%     Ferry                    & 4    & 15              &    42          \\
%     Gripper                    & 5    & 18            &    55           \\
%     Hanoi                    & 1    & 4               &     38       \\
%     Npuzzle                    & 1    & 4             &     36         \\
%     Sokoban                    & 6    & 22            &     64         \\
    
%     \hline
%     \end{tabular}
%     }
%     \caption{Number of state-action triplets needed to learn the preconditions}
%     \label{tab:pisam_sam}
% \end{table}

We also investigated the advantage of EPI-SAM (Algorithm \ref{alg:episam}) on the other subset of the domains (\{Blocks, Depot, Floortile, Parking, Transport\}). These are the more complicated domains (more fluents and actions) in which neither PI-SAM nor FAMA learned the preconditions exactly. EPI-SAM does not necessarily determine the state of fluents, so we used unit propagation to compute a subset of effects that must hold, and used the resulting action model instead of a contingent planner. The results are recorded in Table \ref{tab:episam}. EPI-SAM (Algorithm \ref{alg:episam}) performed as well as PI-SAM (Algorithm \ref{alg:pisam}) on the Blocks, Depot, and Parking domains and achieved slightly better results on the Floortile and Transport domains.  Note that EPI-SAM also has the advantage of providing guarantees on domains that don't satisfy the \textit{bounded concealment assumption}, which is not exploited here.  

% \begin{table}[t]
%     \centering
%     \resizebox{\columnwidth}{!}{
%     \begin{tabular}{l|l|c|c|c|c}
%     \hline 
%     Domain & Algorithm & P (\pre) & R (\pre) & P (\eff) & R (\eff) \\
%     \hline
%     \multirow{2}{*}{Blocks}    & EPI-SAM   &1.00 &0.90 & 1.00 & 0.95           \\
%     \cline{2-6}
%                           & PI-SAM    &1.00 &0.90 & 1.00 & 0.95          \\
%     \hline                       
%     \multirow{2}{*}{Depot}     & EPI-SAM   & 1.00 &0.85 & 1.00 & 1.00          \\
%     \cline{2-6}
%                           & PI-SAM    & 1.00 &0.85 & 1.00 &1.00           \\
%     \hline
%     \multirow{2}{*}{Floortile} & EPI-SAM   &1.00 & \textbf{0.89} & 1.00 & \textbf{0.90  }         \\
%     \cline{2-6}
%                           & PI-SAM    &1.00 & 0.85 & 1.00 & 0.87           \\
%     \hline
%     \multirow{2}{*}{Parking}   & EPI-SAM   &1.00 &0.85 & 1.00 &1.00           \\
%     \cline{2-6}
%                           & PI-SAM    &1.00 &0.85 & 1.00 &1.00           \\
%     \hline
%     \multirow{2}{*}{Transport} & EPI-SAM   &1.00 & \textbf{0.85} & 1.00  & \textbf{0.92 }          \\
%     \cline{2-6}
%                           & PI-SAM    &1.00 & 0.83 & 1.00  & 0.90                \\
%     \hline

                                
%     \end{tabular}
%     }
%     \caption{Empirical precision and recall of EPI-SAM under random masking probability $\eta = 0.3$}
%     \label{tab:episam}
% \end{table}

\section{Conclusion and Future Work}
In this paper, we propose two algorithms for learning safe action models in domains with partial observability. The first algorithm, PI-SAM, extends SAM learning algorithm \cite{juba2021safe} to partially observable domains and runs in polynomial time based on the \textit{bounded concealment assumption}. The second algorithm, EPI-SAM, provides guarantees without the assumption, but has intractable worst-case time complexity. We theoretically and empirically show the strengths and weaknesses of both algorithms. In practice, depending on the specific domain (i.e. whether it satisfies the \textit{bounded concealment assumption} or not), we can choose either PI-SAM or EPI-SAM to optimize between performance and running time. For future work, we plan to add experiments on domains without the \textit{bounded concealment assumption} to evaluate the performance of the EPI-SAM algorithm. We also aim to extend safe action model learning to more complicated domains, such as domains with stochastic effects, numeric state variables, etc. 

% For future work, we plan to run experiments on domains without the \textit{bounded concealment assumption} to investigate the effectiveness of the EPI-SAM algorithm. We will also add experiments to compare the performance of PI-SAM, EPI-SAM, and FAMA in terms of the percentage of problems solved when using an off-the-shelf planner to solve for the plan with the action models returned by each algorithm. Besides that, we aim to extend safe action model learning to more complicated domains. For example, domains in which the execution is also partially observable (i.e. the underlying domain is a partially observable non-deterministic (POND) domain or a POMDP), domains with stochastic effects, numeric state variables, etc. 
% \roni{@Hai: If we're tight with space, should cut this section by about a half}

% \appendix
% \section{Reference Examples}
% \label{sec:reference_examples}

% \nobibliography*
% Formatted bibliographies should look like the following examples. You should use BibTeX to generate the references. Missing fields are unacceptable when compiling references, and usually indicate that you are using the wrong type of entry (BibTeX class).

% \paragraph{Book with multiple authors~\nocite{em:86}} Use the \texttt{@book} class.\\[.2em]
% \bibentry{em:86}.

% \paragraph{Journal and magazine articles~\nocite{r:80, hcr:83}} Use the \texttt{@article} class.\\[.2em]
% \bibentry{r:80}.\\[.2em]
% \bibentry{hcr:83}.

% \paragraph{Proceedings paper published by a society, press or publisher~\nocite{c:83, c:84}} Use the \texttt{@inproceedings} class. You may abbreviate the \emph{booktitle} field, but make sure that the conference edition is clear.\\[.2em]
% \bibentry{c:84}.\\[.2em]
% \bibentry{c:83}.

% \paragraph{University technical report~\nocite{r:86}} Use the \texttt{@techreport} class.\\[.2em]
% \bibentry{r:86}.

% \paragraph{Dissertation or thesis~\nocite{c:79}} Use the \texttt{@phdthesis} class.\\[.2em]
% \bibentry{c:79}.

% \paragraph{Forthcoming publication~\nocite{c:21}} Use the \texttt{@misc} class with a \texttt{note="Forthcoming"} annotation.
% \begin{quote}
% \begin{footnotesize}
% \begin{verbatim}
% @misc(key,
%   [...]
%   note="Forthcoming",
% )
% \end{verbatim}
% \end{footnotesize}
% \end{quote}
% \bibentry{c:21}.

% \paragraph{ArXiv paper~\nocite{c:22}} Fetch the BibTeX entry from the "Export Bibtex Citation" link in the arXiv website. Notice it uses the \texttt{@misc} class instead of the \texttt{@article} one, and that it includes the \texttt{eprint} and \texttt{archivePrefix} keys.
% \begin{quote}
% \begin{footnotesize}
% \begin{verbatim}
% @misc(key,
%   [...]
%   eprint="xxxx.yyyy",
%   archivePrefix="arXiv",
% )
% \end{verbatim}
% \end{footnotesize}
% \end{quote}
% \bibentry{c:22}.

% \paragraph{Website or online resource~\nocite{c:23}} Use the \texttt{@misc} class. Add the url in the \texttt{howpublished} field and the date of access in the \texttt{note} field:
% \begin{quote}
% \begin{footnotesize}
% \begin{verbatim}
% @misc(key,
%   [...]
%   howpublished="\url{http://...}",
%   note="Accessed: YYYY-mm-dd",
% )
% \end{verbatim}
% \end{footnotesize}
% \end{quote}
% \bibentry{c:23}.

% \vspace{.2em}
% For the most up to date version of the AAAI reference style, please consult the \textit{AI Magazine} Author Guidelines at \url{https://aaai.org/ojs/index.php/aimagazine/about/submissions#authorGuidelines}

% % Use \bibliography{yourbibfile} instead or the References section will not appear in your paper
% \nobibliography{aaai23}


\bibliography{aaai23}

% \section{Acknowledgments}
% AAAI is especially grateful to Peter Patel Schneider for his work in implementing the original aaai.sty file, liberally using the ideas of other style hackers, including Barbara Beeton. We also acknowledge with thanks the work of George Ferguson for his guide to using the style and BibTeX files --- which has been incorporated into this document --- and Hans Guesgen, who provided several timely modifications, as well as the many others who have, from time to time, sent in suggestions on improvements to the AAAI style. We are especially grateful to Francisco Cruz, Marc Pujol-Gonzalez, and Mico Loretan for the improvements to the Bib\TeX{} and \LaTeX{} files made in 2020.

% The preparation of the \LaTeX{} and Bib\TeX{} files that implement these instructions was supported by Schlumberger Palo Alto Research, AT\&T Bell Laboratories, Morgan Kaufmann Publishers, The Live Oak Press, LLC, and AAAI Press. Bibliography style changes were added by Sunil Issar. \verb+\+pubnote was added by J. Scott Penberthy. George Ferguson added support for printing the AAAI copyright slug. Additional changes to aaai23.sty and aaai23.bst have been made by Francisco Cruz, Marc Pujol-Gonzalez, and Mico Loretan.

% \bigskip
% \noindent Thank you for reading these instructions carefully. We look forward to receiving your electronic files!


%TODO RONI: UNDERSTAND FROM GUY, CAN CONTINGENT PLANNINGET AS INPUT A CNF DESCRIBING THE EFFECTS? IS THERE MORE THAN ONEOFF


\end{document}


























\begin{lemma}\label{lem:cnf-char}
At line 10 in Algorithm \ref{alg:episam}, for every action model consistent with the set of partially observed trajectories $\mathcal{T}$, the assignment obtained by setting $\iseff(l,a)$ to true if $l$ is an effect of $a$ for each literal $l$ and action $a$ is a satisfying assignment to $CNF_{\eff}(l)$, \roni{shouldn't it be as follows: any action model consistent with the trajectories satisfies these CNFs?} and conversely, for any satisfying assignment to $CNF_{\eff}(l)$, the corresponding action model is an action model consistent with the trajectories $\mathcal{T}$.
\end{lemma}
% \begin{lemma}\label{lem:cnf-char}
% Let $\mathcal{T}$ be a set of partially observable trajectories, 
% and let $A_\sam=\left(\{\cnf_\ell\}_\ell, \{\pre_a\}_a \right)$ be the output of EPI-SAM when given $\mathcal{T}$. 
% Every action model consistent with $\mathcal{T}$ 
% must satisfy $\{\cnf_\ell\}_\ell$ and for every action $a$
% and every action model that satisfies $\{\cnf_\ell\}_\ell$ is consistent with $\mathcal{T}$. 
% \end{lemma}
\noindent
{\em Sketch of proof.}
We consider a CNF, $\cnf_{\mathcal{T}}$, that has variables of the form $\iseff(l,a)$, $\ispre(l,a)$, and $State(l,i,T)$, representing that 
$l$ is a precondition of $a$, 
$l$ is an effect of $a$, 
and $l=\true$ in the $i^{th}$ state of trajectory $T$, respectively.  
The clauses for $\cnf_\mathcal{T}$ are obtained from clausal encodings of the STRIPS axioms, instantiated at each step of each trajectory in $\mathcal{T}$:
\begin{enumerate}
\item $\neg \ispre(l,a_i)\vee State(l,i-1,T)$
\item $\neg \iseff(l,a_i)\vee State(l,i,T)$
\item $\neg State(l,i-1,T)\vee \iseff(\neg l,a_i)\vee State(l,i,T)$
\end{enumerate}
with $State(l,i,T)$ replaced by true or false when $l$ is observed true or false, respectively, at step $i$ in $T$. 
It is immediate that satisfying assignments to $\cnf_{\mathcal{T}}$ correspond to encodings of action models and the complete trajectories those action models would yield, given the values observed in the trajectories of $\mathcal{T}$. We note that the clauses of this formula each only contain literals for a single fluent, so the formula is satisfiable iff the formulas $\cnf_{\mathcal{T}}(l)$ for each literal $l$ are satisfiable.

We then use the refutation-completeness of resolution to reduce the problem to identifying the clauses that may appear in resolution refutations. The $\ispre(a,l)$ literals, appearing only negatively, cannot appear in a refutation, and the literals $State(l,i,T)$ must be eliminated, but these only appear in consecutive instances of the second and third type of clauses where the literal $l$ is unobserved; and only the third clause has the negative literal. Reordering the applications of the resolution rule on these literals to the beginning of the proof, we see that we must create clauses that correspond to consecutive runs of unobserved literals using the resolution rule on the third type of clause for each step, beginning with either an observed literal or with using the second type of clause to eliminate the first $State(l,i,T)$ literal. These are, respectively, the clauses of $CNF_{\eff}(l)$ created on lines 4--5 and 6--7. See the supplemental material for details.\roni{I am not super happy with this proof. Don't we need a claim about the preconditions?} 
\iffalse{
%% Full proof below punted to supplemental
\begin{proof}
We first consider the following CNF encoding of the possible trajectories that could yield the observations appearing in $\mathcal{T}$: let $CNF_{\mathcal{T}}$ be a formula with variables $\iseff(l,a)$ for each literal $l$ and action $a$, $\ispre(l,a)$ for each literal $l$ and action $a$, and $State(l,t,h)$ for each literal $l$, trajectory $h\in\mathcal{T}$, and $t=1,\ldots,k$ where $k$ is the length of $h$. The clauses of $CNF_{\mathcal{T}}$ are obtained from clausal encodings of the STRIPS axioms as follows: For each $l$ and $a_t$ at step $t$ of some trajectory $h\in\mathcal{T}$, include
\begin{compactenum}
\item $\neg \ispre(l,a_t)\vee State(l,t-1,h)$ if $l$ is not observed at step $t-1$ or $\neg \ispre(l,a_t)$ if $l$ is observed false.
\item $\neg \iseff(l,a_t)\vee State(l,t,h)$, if $l$ is not observed at step $t$, or $\neg \iseff(l,a_t)$ if $l$ is observed false at step $t$, and
\item $\neg State(l,t-1,h)\vee \iseff(\neg l,a_t)\vee State(l,t,h)$ if $l$ is not observed at either step $t$ or $t-1$, $\neg State(l,t-1,h)\vee \iseff(\neg l,a_t)$ if $l$ is not observed at $t-1$ and observed false at $t$, and $\iseff(\neg l,a_t)\vee State(l,t,h)$ if $l$ is observed true at $t-1$ and not observed at $t$.
\end{compactenum}
We also include the mutual exclusion axioms $\neg \iseff(l,a)\vee \neg \iseff(\neg l,a)$ and $\neg State(l,0,h)\vee \neg State(\neg l,0,h)$ for each unobserved literal $l$.
Observe that the claim holds for $CNF_{\mathcal{T}}$ if we additionally set the variables $State(l,t,h)$ to true if $l$ would be true in the corresponding fully-observed trajectory at step $t$ (and conversely): indeed, these are encodings of precisely the action models that obey the STRIPS axioms and corresponding trajectories. We also note that the clauses of $CNF_{\mathcal{T}}$ each only use variables corresponding to a single fluent, so the set of satisfying assignments correspond to products of assignments to the formulas $CNF_{\mathcal{T}}(l)$ that only include the clauses using the same fluent as $l$. (Note that the STRIPS axioms ensure that the post-state is determined uniquely given an assignment to the pre-state.)

We now show that the satisfying assignments to each $CNF_{\eff}(l)$ have the corresponding property for the literal $l$. Recall that resolution is refutation complete, so it suffices to show that for any resolution refutation of a CNF formula $\varphi\wedge CNF_{\mathcal{T}}(l)$, where $\varphi$ (like $CNF_{\eff}(l)$) only contains variables $\iseff(l,a)$, a corresponding refutation of $\varphi\wedge CNF_{\eff}(l)$ exists. 

We first observe that $\ispre(l,a)$ only appears negatively in all clauses, and hence clauses containing these variables cannot be used in any refutation. Similarly, the initial state variables $State(l,0,h)$ only appear negatively and hence we cannot use the mutual exclusion clauses for the state variables either.

Second, observe that to obtain a refutation using the clauses of $CNF_{\mathcal{T}}(l)$, the variables $State(l,t,h)$ must be eliminated, but this variable only appears in a negative literal in the third type of clause (these do not appear in $\varphi$ by assumption), created for step $t+1$ of trajectory $h\in\mathcal{T}$. (And in the mutual exclusion axiom for unobserved variables in the initial state.) The variable only appears positively in the clauses created for step $t$ of the second and third type. Observe that we can rewrite the proof (possibly increasing its size) so that these applications of the resolution rule occur first.

We now claim that the clauses resulting from the final application of the resolution rule on a variable $State(l,t,h)$ appear in $CNF_{\eff}(l)$, which will prove the lemma. Indeed, we can only apply the resolution rule to some run of consecutive clauses of the third type in which the literal $l$ was not observed, ending with a step $t+\Delta$ where $l$ was observed, and beginning with either eliminating $State(l,t-1,h)$ using the clause $\neg \iseff(l,a_{t-1})\vee State(l,t-1,h)$ (created on lines 6--7), or with a clause corresponding to a step $t$ where $l$ was observed in step $t-1$ (created on lines 4--5). 
\end{proof}
}\fi



            % \ForEach{$T\in\mathcal{T}_{a,\ell}$}{
            %     \For{$i=1,\ldots,|T|$}{
            %         \If{$T.a_i=a'$}{
            %             \textbf{Propagate}($T$,$i$,$\ell$)\\
            %             \eIf{$T.s_{i-1}[\ell]\neq T.s_{i}[\ell]$}{
            %                 Remove $\ell$ from $\pre(a)$ \nllabel{episam-hard-pre-delete}\\
            %                 Continue to the next $(a,\ell)$ pair\nllabel{line:propagate-end}
            %             }{
            %                 \textbf{Remove}($\tuple{s,a',s'}$,$\mathcal{T}_{a,\ell}$)\\
            %             }    
            %         }
            %     }
            % }
            
            % \ForEach{$\tuple{s,a',s'}$ in $\mathcal{T}_{a,\ell}$}{
            %     \lIf{$T.s_{i-1}[\ell]=\unobserved$}{
            %                 $T.s_{i-1}[\ell]\gets T.s_{i}[\ell]$
            %     }
            %     \lIf{$T.s_{i}[\ell]=\unobserved$}{
            %         $T.s_{i}[\ell]\gets s[\ell]$
            %     }
            %     \eIf{$T.s_{i-1}[\ell]\neq T.s_{i}[\ell]$}{
            %         Remove $\ell$ from $\pre(a)$ \nllabel{episam-hard-pre-delete}\\
            %         Continue to the next $(a,\ell)$ pair\nllabel{line:propagate-end}
            %     }{
            %         \textbf{Remove}($\tuple{s,a',s'}$,$\mathcal{T}_{a,\ell}$)\\
            %     }
                
            % }
            % % Implement the assumption that $a'$ is irrelevant to l\\
            % \ForEach{$T\in\mathcal{T}_{a,\ell}$}{
            %     \For{$i=1,\ldots,|T|$}{
            %         \If{$T.a_i\in A_{irr}$}{
            %             \lIf{$T.s_{i-1}[\ell]=\unobserved$}{
            %                 $T.s_{i-1}[\ell]\gets T.s_{i}[\ell]$
            %             }
            %             \lIf{$T.s_{i}[\ell]=\unobserved$}{
            %                 $T.s_{i}[\ell]\gets s[\ell]$
            %             }
            %             \uIf{$T.s_{i-1}[\ell]\neq T.s_{i}[\ell]$}{
            %                 Remove $\ell$ from $\pre(a)$ \nllabel{episam-hard-pre-delete}\\
            %                 Continue to the next $(a,\ell)$ pair\nllabel{line:propagate-end}
            %             }
            %         }
            %     }
            % }
            % --------------------
            %     \nllabel{episam-action-delete-loop}
            % % Implement the assumption that $a'$ is irrelevant to l\\
            % Add $a'$ to $A_{irr}$ \\
            % \ForEach{$T\in\mathcal{T}_{a,\ell}$}{
            %     \For{$i=1,\ldots,|T|$}{
            %         \If{$T.a_i\in A_{irr}$}{
            %             \lIf{$T.s_{i-1}[\ell]=\unobserved$}{
            %                 $T.s_{i-1}[\ell]\gets T.s_{i}[\ell]$
            %             }
            %             \lIf{$T.s_{i}[\ell]=\unobserved$}{
            %                 $T.s_{i}[\ell]\gets s[\ell]$
            %             }
            %             \uIf{$T.s_{i-1}[\ell]\neq T.s_{i}[\ell]$}{
            %                 Remove $\ell$ from $\pre(a)$ \nllabel{episam-hard-pre-delete}\\
            %                 Continue to the next $(a,\ell)$ pair\nllabel{line:propagate-end}
            %             }
            %         }
            %     }
            % }
            % \For{$i=0,\ldots,$
            % \ForEach{$\tuple{s,a'',s'}$ in $\mathcal{T}_{a,\ell}$ \nllabel{line:propagate-start}}{
            %     \If{$a''\in A_{irr}$}{
            %         \lIf{$s[\ell]=\unobserved$}{
            %             $s[\ell]\gets s'[\ell]$
            %         }
            %         \lIf{$s'[\ell]=\unobserved$}{
            %             $s'[\ell]\gets s[\ell]$
            %         }
            %         \uIf{$s[\ell]\neq s'[\ell]$}{
            %             Remove $\ell$ from $\pre(a)$ \nllabel{episam-hard-pre-delete}\\
            %             Continue to the next $(a,\ell)$ pair\nllabel{line:propagate-end}
            %         }
            %     }
            % }
        }
    }