\def\year{2020}
%File: main.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai20}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{subfig}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{graphicx}  % DO NOT CHANGE THIS
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{mathtools}
%\usepackage{pcatcode}


\usepackage{amsthm}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
%\newtheorem{definition}{Definition}[section]

\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS

\newtheorem{example}{Example}

\newcommand{\commentout}[1]{}
\newcommand{\eliran}[1]{\textbf{[\color{red}ELIRAN:#1]}}
\newcommand{\ronen}[1]{\textbf{[\color{blue}RONEN:#1]}}
\newcommand{\guy}[1]{\textbf{[\color{orange}GUY:#1]}}
%\newcommand{\begin{example}}[1]{\textbf{[\color{purple}RunningExample:#1]}}

\newcommand{\cbp}[0]{Collaborative Box-Pushing}
\newcommand{\mitg}[0]{Meet In The Grid}
\newcommand{\crs}[0]{Cooperative Rock-Sampling}
\newcommand{\macor}[0]{Multi-Agent Corridor}

\newcommand{\cact}[1]{{\em CActions$_#1$}}
\newcommand{\pcact}[1]{{\mathit{PCA}_#1}}
\newcommand{\eff}{\mathit{eff}}

\newcommand{\Tau}{\mathrm{T}}

%\nocopyright
%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
 \pdfinfo{
/Title (A Factored Approach To Solving Dec-POMDPs)
/Author (Eliran Abdoo, Ronen I. Brafman, Guy Shani)
} %Leave this	
% /Title ()
% Put your actual complete title (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case
% Leave the space between \Title and the beginning parenthesis alone
% /Author ()
% Put your actual complete list of authors (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case. 
% Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands, 
% remove them. 

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{caption} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \natbib} -- This package is specifically forbidden -- use the following workaround:
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai20.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\setlength\titlebox{2.5in} % If your paper contains an overfull \vbox too high warning at the beginning of the document, use this
% command to correct it. You may not alter the value below 2.5 in
\title{A Factored Approach To Solving Dec-POMDPs }
%Your title must be in mixed case, not sentence case. 
% That means all verbs (including short verbs like be, is, using,and go), 
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\author{Eliran Abdoo, Ronen I Brafman, Guy Shani}
\begin{document}

\maketitle

\begin{abstract}
Dec-POMDPs model planning problems under uncertainty and partial observability for a distributed team of cooperating agents planning together but executing their plans in a distributed manner. This problem is
very challenging computationally (NEXP-Time Complete) and consequently, exact methods have difficulty scaling up. In this paper we present a heuristic approach for solving certain instances of Factored Dec-POMDP. Our approach reduces the joint planning problem to multiple single agent POMDP planning problems.
First, we solve a centralized version of the Dec-POMDP, which we call the team problem, where agents have a shared belief state. Then, each agent individually plans to execute its part of the team plan. Finally, the different solutions are aligned to achieve synchronization. Using this approach we are able to solve larger Dec-POMDP problems, limited mainly by the abilities of the underlying POMDP solver.
\end{abstract}




\section{Introduction}

Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are a popular model for planning in stochastic environments under uncertainty with partial observability by a distributed team of agents \cite{DECPOMDPART}. In this model, a team of agents attempts to maximize the team's cumulative reward where each agent has only partial information about the state of the system during execution.  The team can plan together in a centralized manner prior to acting, but during execution each agent is aware of its own observations only. Communication is possible only through explicit communication actions, if these are available. 

To achieve their common goal agents must coordinate their actions in two ways: First, as in single agent problems, actions must be coordinated sequentially. That is, current actions must help steer the system later towards states in which greater reward will be possible. For example, to be rewarded for shipping a product, it must first be assembled. 
Second, agents may need to coordinate their simultaneous actions because their effects are dependent, e.g., a heavy box can only be pushed if two agents push it simultaneously. 

Our focus is on centralized off-line planning for distributed execution. That is, offline, a solver with access to the complete model must generate a policy for each agent. An agent's policy specifies which action must be taken as a function of the agent's history of
actions and observations. Such policies can be represented by a {\em policy graph} where nodes are labeled by actions, and edges are labeled by observations. Online, each agent executes its own policy independently of the other agents.
The difficulty lies in generating policies that provide sufficient coordination, even though each agent senses different observations at run-time, and thus the agent's beliefs over which states are possible are typically different. 

Dec-POMDPs are notoriously hard to solve -- they are NEXP-Time hard~\cite{DECPOMDPCOMP}, implying that only the smallest toy problems are optimally solvable.
However, many approximate methods for solving Dec-POMDPs have been proposed, with steady progress. Some of these methods generate solutions with bounds on
their optimality \cite{GMAAICE,MBDP,DICEPS}, and some are heuristic in nature~\cite{JESP}. However, current methods typically do not scale to state spaces with more than a few hundreds of states.


In this paper we describe a heuristic approach for solving Dec-POMDPs that scales to much larger state spaces. The key idea is to solve a Dec-POMDP by
solving multiple POMDPs. First, we solve a POMDP obtained by assuming that all agents have the same belief state. That is, that any observation by one agent is immediately available to the other agents. We refer to this as the {\em team POMDP}. The solution of the team
POMDP can be represented by a policy graph --- the {\em team policy graph.} . It provides us with a skeleton for the solution of the Dec-POMDP, specifying what each agent needs to provide for the team. Naturally, this policy is not executable by the agents, because agents cannot condition their actions on the observations of other agents in the real world.

Hence, in the next stage, we let each agent solve a POMDP in which it is rewarded for behaving following the specification in the team policy. This leads to the generation of a policy tree for each agent. 
These policy trees are often not well synchronized. In the last step we synchronize the policy trees by delaying the actions of agents to improve the probability of good coordination. 

\guy{we experiment with more than one problem - need to fix this}\eliran{not sure I understand your meaning. Do you mean that we need to experiment with more than one?}
We implemented our algorithm and tested it on several configurations of a benchmark problem \cbp which is a variation of the Cooperative Box Pushing problem. We show that the algorithm manages to scale well beyond
current Dec-POMDP solvers.
One of the main properties of the domain, is that agents policies are only loosely coupled. That is, the need for actions that affect state components that are relevant to all agent, is sparse. That sparsity allows for each agent to independently construct a plan that consists mostly of its own private actions without requiring it to consider the other agents' behavior. This allows us to achieve good decentralized policies even when achieving the goal requires many steps, compared to planning directly over the Dec-POMDP model.






\section{Background}

We now provide needed background on POMDPs, Dec-POMDPs, their factored representation, and policies. We also introduce the concept of private and public variables and actions in Dec-POMDPs.


\subsection{POMDPs}

A POMDP is a model for single-agent sequential decision making under uncertainty and partial observability.
Formally, it is a tuple $P=\langle S, A, T, R, \Omega, O, \gamma, h, b_0 \rangle$, where:
\begin{itemize}
\item
$S$ is the set of states. The future is independent of the past, given the current state.
\item
$A$ is the set of actions. An action may modify the state and/or
provide information about the current state.
\item
$T: S \times A \rightarrow \prod(S)$ is the state transition function.  $T(s, a, s')$ is the probability of transitioning to $s'$ when applying $a$ in $s$. 
\item
$R:S \times A \times S \rightarrow \mathbb{R}$  is the immediate reward function. $R(s,a, s')$ is the reward obtained after performing $a$ in $s$  and reaching $s'$. 
\item
$\Omega$ is the set of observations. An observation is obtained by the agent following an action, and provides some  information about the world.
\item
$O:S \times A \rightarrow \prod (\Omega)$ is the observation function, specifying the likelihood of sensing a specific observation following an action. $O(s', a, o)$ is the probability of observing $o\in \Omega$ when performing $a$ and \emph{reaching} $s'$. 
\item
$\gamma \in (0,1)$ is the discount factor, quantifying the relative importance of immediate rewards vs.~future rewards.
\item
$h$ is the planning horizon --- the amount of actions that an agent executes before terminating. The horizon may be infinite.
% \eliran{Open issue - included infinity as our algorithm outputs a policy for infinite horizon, where we state this?}
% \ronen{Do we really plan for an infinite horizon? Perhaps you mean unbounded horizon?}\eliran{I'm not sure actually. The policies we output are for infinite horizons (there's an edge for every possible observation of the node's action), but perhaps it is because we plan for enough steps so that the policy graph closes, and then it might be for an unbounded horizon. I'll look into it}
%\eliran{we said it can be considered as "unbounded horizon", should we drop the $\infty$ sign?}

\item
$b_0\in \prod(S)$ is a distribution over $S$ specifying the probability distribution over the initial state.
\end{itemize}

For ease of representation, we assume that agent actions are either sensing actions or non-sensing actions. An agent that applies a non-sensing action always receives the observation {\em null-obs}. In addition, we also assume that every action has 
an effect that we consider as the successful outcome, while all other effects are considered failures. We later explain how this assumption can be omitted in the relevant parts.

Often, the state space $S$ is structured, i.e., it consists of assignments to some set of variables $X_1,\ldots X_k$, and the observation space $\Omega$ is
also structured, consisting of a set of observation variables $W_1,\ldots, W_d$. 
Thus, $S=Dom(X_1)\times\cdots\times Dom(X_k)$ and
$\Omega = Dom(W_1)\times\cdots\times Dom(W_d)$. 
In that case, $\tau$, $O$, and $R$ can be represented compactly by, e.g., a dynamic Bayesian network~\cite{BAYESNETWORK}. Formats such as RDDL~\cite{RDDL} and POMDPX~\cite{POMDPX}
exploit factored representations to specify POMDPs compactly.

\begin{example}
Consider a simple Box-Pushing in a 2 tile grid. The left tile is marked by \emph{L} and the right tile by \emph{R}. The agent begins in the left tile. There is a single box, that may intially be in any one of the tiles. The agent can either move, sense its current tile or push a box from its current tile. Both move and push can be done in any direction --- left and right. The agent's goal is to push the box to the right tile.
The state is composed of 2 variables: the location of the agent and the location of the box. Each variable can take one of two values: \emph{L} or \emph{R}.
The sense action returns an observation telling whether there's a box in the agent's tile, while the move and push actions are non-sensing actions always returning {\em null-obs}.
The push action has a success probability of 0.8.
\end{example}

A solution to a POMDP can be formed as a {\em policy}, assigning to each history of actions and observation ({\em $AO$-history}) the next action to execute. 
Such a policy is often represented using a {\em policy tree} or, more generally, a {\em policy graph} (also called a finite-state controller). 

A policy graph $G=(V,E)$ is a directed simple graph, in which each vertex is associated with an action, and each edge is associated with an observation.
For every edge $v\in V$ and every observation $o\in\Omega$ exactly one edge emanates from $v$ with the label $o$.
The graph has a single root which acts as its entry point. Every $AO$-history $h$ can be associated with some path from the root to some vertex $v$,
and the action labelling $v$ is the action that the policy associates with $h$.

Finally, using a policy graph to direct the agent on the problem produces a {\em trace} -- an execution trajectory. A trace $T$ of length $l$ is a sequence of quintuplets $e_i = (s_i, a_i, s'_i, o_i, r_i)$, namely \emph{steps}, that occurred during a possible policy execution where: $s_i$ is a state in step $i$ and $s_0$ is the initial state;
$a_i$ is the action taken in step $i$;
    $s'_i$ is the result of applying $a_i$ in $s_i$;
    $o_i$ is the observation received after taking $a_i$ and reaching $s'_i$; and is the reward received for taking the $a_i$ in $s_i$ and reaching $s'_i$. Clearly, $\forall i$ such
    that $0\leq i \leq l-1$, we have $s'_i=s_{i+1}$.




\subsection{Dec-POMDP}

A Dec-POMDP models problems where there are $n>1$ acting agents.. 
These agents are part of a team, sharing the same reward, but they act in a distributed manner,
sensing different observations. Thus, their information state is often different. 
Formally, a Dec-POMDP for $n$ agents is a tuple  $P=(S, A=\bigcup_{i=1}^{n}{\{A_i\}}, T, R, \Omega=\bigcup_{i=1}^{n}{\{\Omega_i\}},  O, \gamma, h, {\{I_i\}}_{i=1}^{n})$, where:
\begin{itemize}
\item
$S,\gamma,h,b_0$ are defined as in a POMDP.
\item
$A_i$ is the set of actions available to agent $i$. We assume that $A_i$ contains a special {\em no-op} action, which does not change the state of the world, and does not provide any informative observation. 
$A=A_1 \times A_2 \times .. \times A_n$ is the set of joint actions. On every step each agent $i$ chooses an action $a_i \in A_i$ to execute, and all agents execute their actions jointly. $\langle a_1,...,a_n \rangle$ is known as a joint action. We often treat the
single-agent action $a_i$ as a joint-action, with the understanding that it refers to the joint-action $\langle$no-op,$\ldots,a_i,\ldots,$no-op$\rangle$
\item
$T:S \times A \rightarrow \prod(S)$  is the transition function. Transitions are specified for joint actions, that is, $T(s, \langle a_1,...,a_n \rangle, s')$ is the probability of transitioning from state $s$ to state $s'$ when each agent $i$ executes action $a_i$.
\item
$R:S \times A \times S \rightarrow \mathbb{R}$  is the reward function. Rewards are also specified over joint actions.
\item
$\Omega = \Omega_1 \times \Omega_2 \times .. \times \Omega_n$ is the set of joint observations. We assume $\Omega_i$ contains a special {\em null-obs} observation, which is the observation received when applying a non-sensing action.
\item
$O:S \times A \rightarrow \prod_{i=1..n}(\Omega_i)$  is the observation function, specified over joint actions. $O(s',\langle a_1,...,a_n \rangle,\langle o_1,...,o_n \rangle)$ is the probability that when all agents execute $\langle a_1,...,a_n \rangle$ jointly and reach $s'$, each agent $i$ observes $o_i$.
\item
$\gamma$  is the discount factor.
\item
$h$ is the horizon.
\item
$b_0 \in \prod(S)$ is a distribution over $S$ specifying the probability that each agent begin its execution in each state. In principle, different agents may have different initial belief states, but
we make the (common) assumption that the initial belief state is identical. 
\end{itemize}

\begin{example}
We now take the previous example and extend it to a Dec-POMDP by adding an agent at the right tile and a second box. The agents are denoted by \emph{Agent1} and \emph{Agent2} and the boxes by \emph{Box1}, and \emph{Box2}. \emph{Box1} must reach the right tile, and \emph{Box2} must reach the left tile.
\end{example}

As in the case of POMDPs, Dec-POMDPs can also be represented in a factored manner \cite{FDECPOMDP}, although most work to date uses the flat-state representation.
%(e.g., \cite{GMAAICE}, \cite{JESP}).
We add the notion of \emph{observation variables}, which capture the observation value of each agent following an action. Each observation variable is denoted by $\omega_i$ which takes values in $\Omega_i$, and represents the observation of agent $i$.

\begin{example}
In our example, the state is now composed of 4 state variables: the location of each box -- $(X_{B1}, X_{B2})$ -- and the location of each agent -- $(X_{A1}, X_{A2})$. In addition, there are two observation variables -- $(\omega_1, \omega_2)$.
\end{example}

An important element of a factored specification of Dec-POMDPs is a compact formalism for specifying joint-actions. If there are $|A|$ actions in the domain, then, in principle, there are $O(|A|^n)$ possible joint actions. Specifying all joint actions explicitly is unrealistic for large domains. 

In many problems of interest we may expect
that most actions will not interact with each other. A pair of actions $a\in A_i$, $a' \in A_j$ is said to be non interacting, if their effects when applied jointly (in the same joint action) is the union of their effects when applied separately.
Thus, our specification language focuses on specifying
the effects of single-agent actions and specific
combinations of single-agent actions that interact with each other, which we refer to as {\em collaborative} actions~\cite{IMAP}.
For a more detailed discussion of the compact specification of joint-actions, see~\cite{QDECPOMDPPLAN2}. 

\begin{example}
We alter our example further by introducing a collaborative action. To do so, we need to convert one of the boxes to a "heavy" box - a box that requires both agents to push it. We will convert \emph{Box1} to such box. Both agents now have also the option to apply a \emph{collaborative-push} action in any specified direction. If both agents apply that action to push \emph{Box1} while in the same tile with it, the box will transit.
\end{example}

Finally, a solution to a Dec-POMDP is a set of policies $\rho_i$, one for each agent. It maps action-observation sequences of this agent to actions in $A_i$.
As in POMDPs, these policies can be specified using a policy graph for each agent. The policy graph for agent $i$ associates nodes with actions in $A_i$
and edges with observations in $\Omega_i$. 
%\eliran{add an example decentralized policy graph? I think it could be fairly understood when showing the policies in the alignment part, the idea is very straightforward after all, two graphs that are performed simultaneously}



\subsection{Public and Private Actions and Variables}

Public variables are state variables that several agents manipulate directly, while private variables are manipulated by a single agent only. The concept of \emph{private} and \emph{public} (or \emph{local} and \emph{global}) variables~\cite{FACTOREDPLAN} has been used extensively
in work on privacy-preserving multi-agent planning (e.g., \cite{PRIVACYPLAN}) and, more recently in work on solving qualitative variants of Dec-POMDPs~\cite{QDECPOMDPPLAN1,QDECPOMDPPLAN2}. We now explain how we extend these concepts to factored Dec-POMDPs. The definitions below are making some simplifying assumptions and should be viewed as heuristics for helping our algorithm focus on relevant variables. These definitions are based on the notions of
preconditions and effects, as used in classical planning.

\guy{Perhaps this should also be in the form of formal definitions}

Let $a\in A_i$ be an action of agent $i$. As noted, we identify $a_i$ with the joint action $(\mbox{no-op},\ldots, a_i,\ldots,\mbox{no-op})$.
We say that a state variable $X_i$ is an {\em effect} of $a\in A_j$ if there is some state $s$ for which there is a positive probability that the value 
$X_i$ changes following $a$.
\guy{Below - should be "observation", right?}\eliran{instead of effect? no. an observation variable is an effect of a sensing actions}\guy{This makes no sense - the term "effect" is reserved to changes in the world. Why would you want to say that an observation is an effect?}\eliran{Just to make the definitions uniform for state and observation variables. We can change it to observation and then change the definition of relevant variables to union of influencers, effects and observations}
Similarly, we say that observation variable $\omega_i$ is an {\em effect} of $a\in A_j$ if there exists a state $s$ such that there is a positive probability
of observing $\omega_i \neq$ {\em null-obs} when $a$ is executed and $s$ is reached. We denote the effects of $a$ by $\eff(a)$.

We say that a state variable $X_i$ is an
{\em influencer} of action $a$ if the action behaves differently for different values of $X_i$. That is, if there are two states $s_1,s_2$ that differ only in the value of $X_i$ such that $R(s_1,a,s')\neq R(s_2,a,s')$, or $T(s_1,a,s')\neq T(s_2,a,s')$ for some state $s'$, or $O(s_1,a,o)\neq O(s_2,a,o)$ for some observation $o$.
We denote the influencers of $a$ by
$inf(a)$.
We refer to the union of the influencers and effects of $a$ as
the {\em relevant} variables of $a$,
denoted $rel(a)$.
%\ronen{This is problematic. Perhaps this is related to the footnote. For example, suppose that p influences a only if q has a certain value, and similary for q. That is, the effect of a changes only if both p and q are true.}
%\eliran{Why is that problematic? If this is the case, then on states where we fix q to be true, we can see the effect of $a$ when transitioning between p=false and p=true, and vice versia.}

%We associate with each action $a$ the set of variables it can affect or it is affected by, which we refer to as  {\em context}$(a)$. The effect could take place through the transitions, rewards, or observations associated with $a$. {\em context}$(a)$ is the union of two sets, the preconditions of $a$ denoted by {\em pre(a)} and the objectives of $a$ denoted by {\em obj(a)}. 

As explained above, we expect that most actions do not interact. In that case, it is straightforward to get the post-action distribution for their combination from the specification of the single-agent actions they contain. But some actions may behave differently when applied jointly with other actions. We refer to a combination of single-agent actions whose conditional effects are not the union of the single-agent actions as $\emph{collaborative}$ actions. 

We say that $X_i$ is an effect of a collaborative action $a$ that consists of single agent actions
$a_{i_1},\ldots,a_{i_k}$ (and {\em no-ops} for the non-collaborating agents) if $X_i$ is an effect of $a$, as defined above, if $X_i$ has a positive probability of changing following an application of $a$.
\guy{Why do we have influencer again? Should be effects, no?}\eliran{No, the definitions of effects and influencers are similar for collaborative actions, except for the fact that $a$ is defined differently - multiple actions and no-ops instead of one action and no-ops. I thought of just stating this instead of repeating the definition, but Ronen removed the comment so I think he prefered to leave it as is.}
We also say that $X_i$ is an influencer of $a$ if there are two states $s_1,s_2$ that differ only in the value of $X_i$ such that $R(s_1,a,s')\neq R(s_2,a,s')$ or $T(s_1,a,s')\neq T(s_2,a,s')$ for some state $s'$, or $O(s_1,a,o)\neq O(s_2,a,o)$ for some observation $o$.
Finally, the {\em relevant} variables of a collaborative actions $a$ is the union of its influencers and effects.
%\ronen{Why are the effects of a collaborative action only those that are not of its single-agent actions? It is also possible that  some other variable will affect a collaborative action}
%\eliran{Updated -In second thought it seems quite simple. In term of effects and influencers, we only care about variables that fit the same definitions as with the single action, regardless of the single actions themselves. A variable can be an influencer or an effect of one of the single action but not of the collaborative actions, and vice versia. This is due to the fact that we don't define a collaborative action's effect through its factors nor defining its factors' effects as a derivation of its own effect.}
%
%\ronen{If the picture is more complicated, then we need to say something. I do think there are complications.As I noted above}

%\footnote{A complete treatment of the subtleties of this issue is beyond the scope of this paper. The above definition will be sufficient for our purpose.}

We say that a variable $X_i$ is an \emph{effect} of agent $j$, denoted,
$\eff(j)$, if  $X_i\in \eff(a)$ for some $a\in A_j$.
%\eliran{note the addition,in the code relevants are only objectives}

We can now define the concept of {\em private} and {\em public} variables. $X_i$ is {\em private} to agent $j$ if $X_i$, if it an effect of agent $j$ but it is not an effect of any agent $k\neq j$. If $X_i$ is an effect of more than one agent, we say that $X_i$ is {\em public}. 
An action $a\in A_j$ is public if at least one of its relevant variables, $X_i$ is public. A collaborative action is always public.

\begin{example}
In our running example, $X_{B1}$ and $X_{B2}$ are both public variables, as they are effects of both agents' push actions.
$X_{A1}$, $X_{A2}$ are private variables of $Agent1$ and $Agent2$ respectively, as they are the effects of each respective agent's move actions. The same holds for $\omega_1$ and $\omega_2$ with respect to the sensing actions.
Move and sense actions are private actions, while the push actions are public.
\end{example}





\section{FDMAP - Factorized Distributed MAP}
\guy{Factorized? Factored?}

We now describe our approach for producing policy graphs for agents acting in a Dec-POMDP. First, we generate a team solution using a POMDP for the centralized problem. Next, we generate single agent POMDPs in which each agent attempts to fulfill its part in the team solution. Finally, the policies obtained from the single agent POMDPs are synchronized. 

\commentout{
Given the input factored Dec-POMDP problem $P$, we first generate the team POMDP $P_{team}$. $P_{team}$ is identical to $P$, ignoring the underlying multi-agent structure. That is, the actions are the joint actions and the observations are the joint observation, viewed as applied 
and observed by a single agent. Equivalently, this can be viewed as a Dec-POMDP in which all observations are communicated accurately and instantaneously to all agents.}

Given the input factored Dec-POMDP problem $P$, we first generate the team POMDP $P_{team}$. We solve $P_{team}$ using an off-the-shelf POMDP solver -- we used SARSOP \cite{SARSOP} -- and output the team policy. We then use the team policy to produce traces, which are simulations of the team policy over the team problem. Using the traces, we project the team problem with respect to each agent.

First, for each agent, we extract from the traces a set of public actions and the context in which they were applied, which we call a {\em contexted actions}. The context captures the conditions under which the action achieves the same desired effects as in the trace.
Then, we associate a reward with each such contexted action. Using these contexted actions and their rewards, together with the factored Dec-POMDP, we  generate
one single-agent problem for each agent. The dynamics of each single-agent problem is similar to that of the Dec-POMDP, except that some variables are projected away. The reward associated with the contexted action is designed so that agents will be rewarded for acting in a manner similar to its
behavior in the team solution.

Finally, we process the single-agent policies and align them to try and ensure that actions are properly synchronized when they are executed in a decentralized manner. 
%The high-level pseudo-code is described below. We described the first step (generating $P_{team}$) above. 
In the rest of this section we explain the steps that follow the
generation of the team solution in more detail.

%\begin{algorithm}
%\caption{GenerateAgentPolicies \eliran{keep this?}}
%\begin{algorithmic}[tbph]
%\State Input: $P$, $\alpha$, $p_{team}$
%\State $P_{team} \gets \Call{Centralize}{P}$
%\State {\em Traces} $\gets \Call{ProduceTraces}{P_{team}, \alpha, p_{team}}$
%\State {\em RawSAPolicies}$ \gets \Call{ProjectAndSolve}{P_{team},{\mathit %Traces}}$
%\State {\em SAPolicies} $\gets \Call{ProcessAndAlign}{\mathit{RawSAPolicies}}$
%\State {return {\em SAPolicies}}
%\end{algorithmic}
%\end{algorithm}


\subsection{Producing the Traces}


\guy{I will go through this once you finalize your discussion below. Quite frankly, given that our approach is heuristic, and will not guarantee anything, I see no point in this discussion.}

Having generated the team problem, $P_{team}$, we 
must specify three hyper-parameters: a pair of confidence parameters $\alpha, \beta$ and a precision parameter $\epsilon_{team}$. We generate an $\epsilon_{team}$-optimal solution
to $P_{team}$ using an off-the-shelf POMDP solver.
Then we generate sufficiently many traces so that the distance between the
empirical distribution of initial states in the traces and the initial belief state is less then $\alpha$,
as measured using ???.
%An $\epsilon$-optimal solution is a solution whose value differs by at most $\epsilon$ from the value of an optimal solution.
%Both parameters should be specified by the user, where $\epsilon_{team}$ should be small enough so that the team policy achieves the user's requirements - the final decentralized policy can only be as good as the team policy.

\eliran{Open issue - capturing every initial state is both very heuristic and quite weak -- should we change that?}
\ronen{I agree. I think what you want is that the distance between the
distribution of initial states in your trace and the initial belief state is
sufficiently small. E.g., in terms of KL divergence or some other distance measure. This is related to the error of estimating a multinomial distribution by sampling. I think the bound you are using can be adjusted to that,
e.g., some norm.}
\eliran{I think that the problem here is not only about producing enough traces to capture the initial state correctly, but to also produce enough so that we capture all possible scenarios, an issue which is not tied only to the number of possible initial states - yet we can extract some measure from the team policy graph for it. Anyway, for now, the following seems more formal than the previous version, but we can replace it with an iterative version that uses KL-Divergence, what do you think?}
\ronen{Yes. The issue is whether we can generate the most likely traces. This is what we should say. I'm not sure how to bound this with an unbounded horizon. I think you should say that this number of samples guarantees that the distribution of initial states is similar to the true one, but that ultimately, you want to sample the more likely traces.}
\eliran{Wrote two ways to approach it - the second uses concentration bound. Which do you prefer?}
\eliran{Option 1}
Formally, we want to select the number of traces $n_t$ to be such that $Pr(\cup_{s_0\in supp(b_0)}(|\frac{n_{s_0}}{n_t}-Pr(b_0=s_0)|>\beta)) \leq 1-\alpha$, where $n_{s_0}$ is the number of traces that begin with the initial state $s_0$.
Using the union bound and the Chebyshev bound, we get:
\begin{align*}
    Pr(\cup_{s_0\in supp(b_0)}(|\frac{n_{s_0}}{n_t}-Pr(b_0=s_0)|>\beta)) \leq \\
    \sum_{s_0\in supp(b_0)}\frac{Pr(b_0=s_0) \cdot (1-Pr(b_0=s_0))}{n_t \cdot \beta^2}
\end{align*}
We pick $n_t$ such that the bound is less than $1-\alpha$.

\eliran{Option 2}
We use the theorem presented in \cite{KLDIV} to pick the number of traces $n_t$, so that the KL-Divergence between the sampled distribution of the initial states and the initial belief state is small. $b_0$ is the multinomial distribtuion of the initial belief state, and we denote by $T_0$ the sampled distribution, and by $k$ the number of initial states, namely the support set of $b_0$. Using the theorem, for every $n_t > \frac{k-1}{\beta}$ we have:
\begin{align*}
    Pr(KL(T_0 || b_0) \geq \beta) \leq e^{-n_t \cdot \beta} \left( \frac{e \beta n_t}{k-1}\right)^{k-1}
\end{align*}
Hence we need to pick a large enough $n_t$ so the condition $n_t > \frac{k-1}{\beta}$ holds and the right-hand side of the inequality above is smaller than $1-\alpha$, our confidence parameter. We get:
\begin{align*}
    % e^{-n_t \cdot \beta} \left( \frac{e \beta n_t}{k-1}\right)^{k-1} &\leq 1-\alpha \\
    % n_t > \frac{k-1}{\beta}&
    &n_t \cdot \beta - ln(n_t)\cdot (1-k) \geq \\ &(k-1)\cdot(1+ln(\frac{\beta}{k-1})) -ln(1-\alpha)
\end{align*}

%\begin{algorithm}
%\caption{ProduceTraces \eliran{keep this?}}
%\begin{algorithmic}[tbph]
%\State Input: $P_{team}$, $\alpha$, $\epsilon_{team}$
%\State {\em{TeamPolicy} $\gets \Call{POMDPSolver}{P_{team}, \epsilon_{team}}$}
%\State {$n_t \gets \Call{NumTracesRequired}{P, \alpha}$}
%\State {\em{Traces} $\gets \Call{Simulate}{\mathit{TeamPolicy}, n_t}$}
%\State {return \em{Traces}}
%\end{algorithmic}
%\end{algorithm}

\begin{example}
\guy{Reading this, I think that you mean that box1 always starts at the left cell, while box2 may or may not start at the right cell. This was not clear in the problem description, and I revised it. Please check it again.}\eliran{no sorry, I meant that box1 and box2 start in the left and right cells respectively. fixed that}
In our example, solving the team problem would result in the following policy graph. 
$Agent2$ starts by pushing $Box2$ to the left, and then senses whether it had succeeded. It then moves left to assist $Agent1$ to push the heavy $Box1$ to the right.

Next, we use the policy graph to produce the traces. Different
traces will differ by the number of pushes each agent performs until success.
Table~\ref{tbl:Traces} shows two possible traces.
Recall that the state is composed of 4 state variables: $(X_{A1}, X_{A2}, X_{B1}, X_{B2})$, where each variables can take values in \emph{(L, R)}. The actions' names will be denoted by the action name ($M$ for move, $P$ for push, $CP$ for collaborative push and $S$ for sense),
followed by the direction for move and push actions ($L$, $R$), and sub-scripted by the target box for sense and push actions ($B1$, $B2$).
We also denote the null-obs by $\phi$


\guy{In both traces agent 2 first pushes and then senses. Why is that? Should be the other way around.}\eliran{relates to the previous points, fixed that}
% \begin{enumerate}
%     \item \emph{(L,R,L,R),IDLE,Agent2-Push-Left-Box2}
%     \item \emph{null-obs, null-obs}
%     \item \emph{(L,R,L,L),IDLE, Agent2-Sense-Box2}
%     \item \emph{null-obs, no}
%     \item \emph{(L,R,L,L),IDLE, Agent2-Move-Left}
%     \item \emph{null-obs, null-obs}
%     \item \emph{(L,L,L,L),Agent1-CPush-Right-Box1,Agent2-CPush-Right-Box1}
%     \item \emph{null-obs, null-obs}
%     \item \emph{(L,L,R,L),Agent1-Sense-Box1, IDLE}
%     \item \emph{no, null-obs}
% \end{enumerate}
\begin{table}[ht]
    \centering
    \scriptsize
    \begin{tabular}{|c||c|c|c|c||c|c||c|c|}
    \hline
     &$X_{A1}$&$X_{A2}$&$X_{B1}$&$X_{B2}$&$a_1$&$a_2$&$\omega_1$&$\omega_2$\\ \hline
    1 &L&R&L&R&IDLE&$PL_{B2}$&$\phi$& $\phi$\\
    2 &L&R&L&L&IDLE&$S_{B2}$&$\phi$& no\\
    3 &L&R&L&L&IDLE&$ML$&$\phi$&$\phi$\\
    4 &L&L&L&L&$CPR_{B1}$&$CPR_{B1}$&$\phi$&$\phi$\\
    5   &L&L&R&L&$S_{B1}$& IDLE&no&$\phi$\\ \hline
    \hline\hline
     %&$X_{A1}$&$X_{A2}$&$X_{B1}$&$X_{B2}$&$a_1$&$a_2$&$\omega_1$&$\omega_2$\\ \hline
    1 &L&R&L&R&IDLE&$PL_{B2}$&$\phi$& $\phi$\\
    2 &L&R&L&L&IDLE&$S_{B2}$&$\phi$& no\\
    3 &L&R&L&L&IDLE&$ML$&$\phi$&$\phi$\\
    4 &L&L&L&L&$CPR_{B1}$&$CPR_{B1}$&$\phi$&$\phi$\\
    5 &L&L&L&L&$S_{B1}$& IDLE&yes&$\phi$\\
    6 &L&L&L&L&$CPR_{B1}$&$CPR_{B1}$&$\phi$&$\phi$\\
    7 &L&L&R&L&$S_{B1}$& IDLE&no&$\phi$\\ \hline
    \end{tabular}
    \caption{An example of two traces}
    \label{tbl:Traces}
\end{table}
% \begin{enumerate}
%     \item \emph{(L,R,L,R),IDLE , Agent2-Push-Left-Box2}
%     \item \emph{null-obs, null-obs}
%     \item \emph{(L,R,L,L),IDLE, Agent2-Sense-Box2}
%     \item \emph{null-obs, no}
%     \item \emph{(L,R,L,L),IDLE, Agent2-Move-Left}
%     \item \emph{null-obs, null-obs}
%     \item \emph{(L,L,L,L),Agent1-CPush-Right-Box1,Agent2-CPush-Right-Box1}
%     \item \emph{null-obs, null-obs}
%     \item \emph{(L,L,L,L),Agent1-Sense-Box1, IDLE}
%     \item \emph{\textbf{yes}, null-obs}
%     \item \emph{(L,L,L,L),Agent1-CPush-Right-Box1,Agent2-CPush-Right-Box1}
%     \item \emph{null-obs, null-obs}
%     \item \emph{(L,L,R,L),Agent1-Sense-Box1, IDLE}
%     \item \emph{no, null-obs}
% \end{enumerate}
\end{example}


\subsection{Extracting Contexted Actions}
\guy{I rewrote the section how I think it should be. The old section remains below. I see no point in defining context and projected context, contexted actions and projected contexted actions. I also did not understand in the example why pushing box 2 should care about box 1 and vice versa. Please read, and then let's discuss in zoom.}

We seek a policy for each agent in which the agent's public actions executions are identical to those that appear in the team plan. That is, the agent should execute the same public actions it executes in the team plan and in the same contexts.
To generate such a policy, we define an appropriate reward function for each agent that encourages the agent to execute the public actions in the team plan in the same context.

The context of an action must capture the conditions under which the policy chooses the specific action to be executed. We can associate the context with a specific state, but this is too restrictive, as the state might contain various variables that are irrelevant to the action. It is preferable to define a less restrictive context that generalizes to all states where the action achieves the same effects. 


\begin{definition}
The {\em context} of an action $a$ is the set of values $\langle x_{j_1},...,x_{j_k} \rangle$ for the influencers $\langle X_{j_1},...,X_{j_k} \rangle$ of $a$.
\end{definition}

\begin{definition}
A {\em contexted action} is a pair $\langle c,a \rangle$, where $a$ is an action and $c$ is the context of $a$, such that there exists a trace $t$ and an index $i$, where $t_i=\langle s,a,\omega \rangle$, and $c$ contains the values that $s$ assigns to the influencers of $a$. A contexted action is said to be a \emph{public} contexted action, if $a$ is a public action.
\end{definition}

We extract the contexted actions for agent $i$, denoted  \cact{i} from the traces.
For each trace $t$, we identify all the public actions in $t$. For each such public action $a$ executed in a state $s$, we identify the context $c$ --- the values that $s$ assigns to the influencers of $a$. In many cases, even though an action $a$ is executed in two different states $s_1,s_2$ in the traces, the context is identical, as we are interested only in the values of the influencers in $s_1$ and $s_2$.
We focus on the \emph{public} actions because the projected single agent problems are designed to plan execute these actions only in their appropriate context.


\begin{example}
Returning to our Box-Pushing example: we find the following public contexted actions of $Agent2$ in the traces:
\begin{itemize}
    \item \emph{(L,R,L,R),$PL_{B2}$}
    \item \emph{(L,L,L,L),$CPR_{B1}$}
\end{itemize}


We construct the \cact{2} for agent 2 by taking the values of the influencers of the actions $PL_{B2}$ and $CPR_{B1}$. The influencers of $PL_{B2}$ are $X_{A2}$ and $X_{B1}$. The influencers of $CPR_{B1}$ are  $X_{A1}$, $X_{A2}$ and $X_{B2}$. 

This results in the following contexted actions:
\begin{itemize}
    \item $\langle X_{A2}=R, X_{B2}=R\rangle ,PL_{B2}$
    \item $\langle X_{A1}=L, X_{A2}=L, X_{B1}=L\rangle ,CPR_{B1}$
\end{itemize}
\end{example}

We next describe how the single-agent problems are constructed, given the contexted actions.


\subsection{Extracting Contexted Actions - OLD}

We seek a policy for each agent in which the agent's public actions executions are identical to those that appear in the team plan. That is, the agent should execute the same public actions it executes in the team plan and in the same contexts.
To generate such a policy, we define an appropriate reward function for each agent that encourages the agent to execute the public actions in the team plan in the same context.
First, we must define the concept of the context in which an action is executed.
% We must define appropriate conditions for rewarding the agent when it executes an action from the team plan, which we
%call the \emph{projected context}. 

The context of an action must capture the conditions under which the policy chooses the specific action to be executed. We can associate the context with a specific state, but this is too restrictive, as the state might contain various variables that are irrelevant to the action. It is preferable to define a less restrictive context that generalizes to all states where the action achieves the same effects - this is what we address to as a projection of the context. We now define {\em context}, {\em contexted-action}, and the projection of a context.
\begin{definition}
A {\em context} $c$ is a non-empty set of states $\{s_1,..., s_n\}\subseteq S$. Note that a context can also represent a single state.\eliran{Guy, do you think we should just give the definition in the factored setting and remove the one mentioned above?}
As we deal with factored state space $S=X_1,...,X_k$, we can represent a context by specifying values only for a subset of factors. That is, $c=(x_1,..., x_k)$ where $x_i\in X_i \cup \{*\}$, where $*$ stands for a wildcard.
Given a state $s=(x'_1,...,x'_k)\in S$, we say that $s\in c$ if $\forall 1\leq i\leq k. x_i=* \vee x'_i=x_i$
\end{definition}

\begin{definition}
Given a context represented in a factored manner, $c=(x_1,..., x_k)$, we can project it onto a subset of variables $V \subseteq \{X_1,...X_k\}$ by setting $\forall v\in \overline{V}. v=*$.
\end{definition}
When projecting a context onto a set of variables, we create a new, broader context, by ignoring the value of any variable that is not part of this set.

\guy{we just said that states is not what we want, so why do we have a definition of state?}\eliran{I changed it to be context, and stated the context is composed of that single state}

\guy{Why not more formally:}
\begin{definition}
A {\em contexted action} is a pair $\langle c,a \rangle$, such that there exists a trace $t$ and an index $i$, where $t_i=\langle s,a,\omega \rangle$, and $c=\{s\}$ is a single state context. A contexted action is said to be a \emph{public} contexted action, if $a$ is a public action.
\end{definition}


\guy{This is a mess, and certainly not the way you want it. You are trying to explain your thinking process, but this is not what a paper looks like. You need to provide a clear definition for a contexted action. I think that you want a formal definition using the concept of relevant variables. Then, you can describe the "Algorithm" for obtaining these contexted actions. Please write it, and then I will go through it.}
We define the projection of a contexted action with respect to an agent:
\begin{definition}
Given a contexted action $\langle c,a \rangle$ and the agent $i$ such that $a \in A_i$, we define its corresponding {\em projected contexted-action} (PCA) by a new contexted action $PCA(\langle c,a \rangle) = \langle c',a \rangle$, where $c'$ is the projection of $c$ onto $rel(a) \cup \eff(i)$.
\end{definition}

We now specify how we extract the projected contexted actions for each agent. For agent $i$, we denote with \cact{i}
the set of all the public contexted actions in the traces that contain an action of our agent.
We are interested in the \emph{public} actions only since the private objectives of the agent are left untouched when transitioning to the single-agent problem.
Next, we define the projected contexted actions of the agent, denoted by $\pcact{i}$,
as $\pcact{i} = \{ PCA(\langle c,a \rangle)|\langle c,a \rangle \in \cact{i}\}$.

By that we obtain a set of PCAs, each consisting of
a the public action executed by the agent, with a projected context.
Later on, we will associate a reward to each of these PCAs. Notice that   different contexted actions may yield identical PCAs.
\begin{example}
Returning to our Box-Pushing example: we find the following \emph{public contexted} actions of $Agent2$ in the traces:
\begin{itemize}
    \item \emph{(L,R,L,R),$PL_{B2}$}
    \item \emph{(L,L,L,L),$CPR_{B1}$}
\end{itemize}

We construct the PCAs for\emph{Agent2} by projecting the contexts with respect to \emph{Agent2} and its actions. The relevant variables (influencers + effects) of $PL_{B2}$ are $X_{A2}$ and $X_{B1}$. The effects of \emph{Agent2} are both $X_{B1}$ and $X_{B2}$, as it can push both boxes. Therefore, the context is projected onto the variable $X_{A2}, X_{B1}, X_{B2}$. The location of \emph{Agent1} is taken out of the context. 
For the collaborative push, we also add $X_{A1}$ to the projected context, as it is an influencer of the collaborative push.

This results in the following PCAs:
\begin{itemize}
    \item \emph{(*,R,L,R),$PL_{B2}$}
    \item \emph{(L,L,L,L),$CPR_{B1}$}
\end{itemize}
\end{example}

We will now describe how the single-agent problems are constructed, given the PCAs and their rewards.
How the reward values are computed will be described afterwards. We will use $Rewards_i$ to refer to the set
of reward values associated with $\pcact{i}$.

%\ronen{the commented out part below should move to the next example, after you explain how you define the rewards}\eliran{done - its in the Example 9 section}




\subsection{Single Agent Projection}


Our next step is to define a factored POMDP $P_i$ for each agent $i$. $P_i$ is designed to incentivize the agents to execute the contexted actions of $i$ in the appropriate context.
The actions of other agents are used to "simulate" some of the behaviors of the other agents -- behaviors that eventually enable the agent to carry out its own actions.

The set of state variables of $P_i$ is the complete set of state variables of the problem.

$P_i$ contains both actions of agent $i$ as well as actions of other agents. This allows agent $i$ to simulate in its policy the behaviors of other agents.
For public actions, we restrict $P_i$ to the set of public actions that appeared in the traces. That is, if a public action did not appear in any trace, then it is not a part of $P_i$. 
In addition, $P_i$ contains only sensing actions of $i$, ignoring all sensing actions of other agents. 

For each private action $a$ of another agents, $P_i$ contains a deterministic version of $a$. Here, we use the assumption that each action has a known desired effect, and the deterministic version of $a$ always achieves this desired effect. This determinization is done mainly to simplify $P_i$, allowing us to scale to larger problems. We can avoid this determinization, at the cost of a more complicated $P_i$.

We now design the reward function of $P_i$. The rewards incentivize the execution of the contexted actions in their appropriate context, and also discourages the agent from applying them outside their context. By penalizing public actions that are applied out of context and rewarding those that are applied in context, we effectively force PCAs to be applied only in an appropriate order.

To identify an appropriate order we use the set of traces $T$. We define a partial order $\prec_T$ over the contexted actions such that $ca_i \prec_T ca_j$ if in all traces $ca_j$ never appears before $ca_i$. \guy{I do not understand (from the text later) what happens if there are cycles - just assume no order?}\eliran{we just consider the first ordering that appeared. If we find two PCAs that created a cycle, we can't say which should come first} We now iterate over all the contexted actions in reverse topological order of $\prec_T$.

To execute a context action the agent may need to execute first a sequence of other actions, designed to achieve the appropriate influencers values. We design the rewards so that no sequence of non-rewarding actions would cost more than the contexted action reward.




\begin{algorithm}
\caption{ProjectAndSolve}
\begin{algorithmic}[tbph]
\State Input: $P_{team}$, $Traces$
\For {all agent $i$}
\State $\pcact{i},\mathit{Rewards_i} \gets \Call{ExtractPCAs}{P_{team}, Traces, i}$
\State {$\mathit{SAProblem_i}$ $\gets \Call{Project}{P_{team}, \pcact{i}, Rewards_i}$}
\State {$\epsilon_i \gets \Call{GetSAPrecision}{\mathit{MaxTraceLength}, \gamma}$}
\State {$\mathit{RawSAPolicy_i} \gets \Call{POMDPSolver}{\mathit{SAProblem_i}, \epsilon_i}$}
\EndFor
\State {return $\mathit{RawSAPolicy_1}, \mathit{RawSAPolicy_2},..., \mathit{RawSAPolicy_n}$}
\end{algorithmic}
\end{algorithm}



\subsection{Single Agent Projection - OLD}
Our next step is to define a POMDP for each agent. This POMDP will contain some of agent $i$'s action and some actions of other agents,
as well as appropriate rewards for applying its own actions in a
projected context. 
%Given the PCAs and their rewards, we construct single-agent problems that reward each agent for following its role in the team plan, as well as its own private objectives. The single agent POMDP of agent $i$ contains some actions of $i$, and some actions of other agents. 
The actions of other agents are used to "simulate" some of the behaviors of the other agents -- behaviors that eventually enable the agent to carry out its own actions.

The single-agent POMDP $P_i$ for agent $i$ is obtained by modifying $P_{team}$. In terms of
as follows:
\ronen{it is not clear what are the variables/states in this problem}\eliran{not 100\% sure what you mean. the state variables and states remain the same. Added to point 2 the fact that we also remove other agents observation variables.}
\ronen{state it positively: the state variables are...} 
\eliran{done}
\begin{enumerate}
\item Remove from the problem any \emph{public} action that does not appear in the traces, regardless of the agent applying it. As public actions alter facts in the world that are relevant to more than one agent, we  forbid the application of a public action that was not part of the team solution.
\item Remove sensing actions and variables of other agents because the agent does
not have access to their results. The state variables remain the same as in the team problem, and are left untouched.
\item The \emph{private} actions of other agents are transformed into deterministic actions that succeed with probability 1.
As private actions change only private variables, and an agent cannot sense other agents' private variables, this change allows the agent to progress the world towards states in which it must act more easily.
If we cannot assume that actions have a clear success outcome, we can omit this step. This will result in larger, more complicated single-agent policies.
%\ronen{omit what - making them probability 1?}\eliran{yes}

\item Add penalties for the application of all remaining \emph{public} actions in contexts that did not appear in the team plan - whether of the agent or of the other agents. We want to discourage an agent from applying public actions out of their context
in the team plan - as in terms of public action, it must conform to the team plan in order to stay coordinated with its co-agents. Moreover, positive reward cycles may occur~\cite{RLCYCLES}, which must be dealt with so to prevent the agent from trying to endlessly trying to achieve one of the sub-goals. The penalty is chosen to be $-1\cdot\max_{r\in Rewards_i}\{r\} \cdot|\pcact{i}|$, as an upper bound on the sum of rewards that can be achieved from applying PCAs. This ensures that no undesirable contexted action is worth applying, even in exchange for applying all the PCAs. There is no penalty for other agents' PCAs
(i.e., contexted-actions in $\bigcup_{j\neq i} \pcact{j}$). We want
to allow the agent to simulate other agents' PCAs in order to plan the execution of its own actions at appropriate times.
%\ronen{this is not clear. do we allow other agents action only in their context, or anywhere?}\eliran{only in their context, hope it is clear now}
\item Add the rewards for $\pcact{i}$ (explained later) according to $Rewards_i$
This reward will override the above penalty, but only when the action is applied in its projected context.
\item 
Remove rewards related to public variables the agent
can achieve. The goal of the single-agent POMDP is to imitate the team policy, not compute an alternative solution.
% Therefore, given a public variable $X_i$, two states $s, s'$ that differ only on $X_i$, and an action $a\in A_i$ such that $R(s, a, s'')\neq R(s', a, s'')$, or $R(s'', a, s) \neq R(s'', a, s)$, we set any of the involved \emph{positive} rewards to 0.
Therefore, given a public state variable $X_i$, an action $a \in A_i$ , a source state $s$, an outcome state $s'$ that differ from $s$ on $X_i$ and is reachable from $s$ using $a$ such that $R(s, a, s')\neq R(s, a, s)$, we set $R(s,a,s') = R(s,a,s') - \textit{max}\left(0, R(s,a,s') - R(s,a,s)\right)$. That way we cancel any positive reward that is achieved via the alternation of $X_i$.
% we set any of the involved \emph{positive} rewards to 0. Since we are dealing with varying reward functions, we need to make sure we don't reduce reward that is not specific to the alternation. To do so, we define
% $cost(a)=min_{s, s'\in S}R(s,a,s')$ and $cost(s,s')=min_{a\inA_i}R(s,a,s')$. These represent the constant 
\ronen{this is a bit problematic. What if we have negative rewards, too?
For example, undesirable states. What if we cannot reach the same state s'' from s and s'?}
\eliran{I still think there could be a problem here, for example if the model's reward function is always negative, and "rewards" are just inducing lower penalty, but perhaps in this case the reward function could be altered by adding a constant to all entries. Should we attend to that now or leave it with the current reformulation?}
\ronen{then say that you are assuming it. It is true that you can always add a constant value to the reward.}\eliran{I think I've found a better way to define it, without assuming anything on the reward function. Is it ok?}
\end{enumerate}

After applying these changes, we obtain the single-agent problem $P_i$. %Before solving it, we need to specify a precision hyperparameter that determines how close to optimal the policy should be. In oppose to the team problem case, we can pick this parameter wisely to ensure that if the output policy reaches the precision, it must satisfy our goals.
Ideally, the solution to $P_i$ should be sufficiently optimal so that %$\epsilon_i$-optimal to guarantee that if the policy is $\epsilon_i$-optimal, then 
no PCA will be ignored by the policy. Yet, the policy might fail to satisfy it for certain scenarios, for example where actions fail far beyond their failure expectancy. The optimality criteria for the problem $P_i$ is denoted by $\epsilon_i$. In the next section we specify how this parameter is chosen.
%\ronen{what do you mean by "on average"}\eliran{refactored it}

\begin{example}
%\ronen{this is strange that you are using agent 1 now, after you computed the PCA for agent 2}\eliran{changed that to agent2}
We now construct Agent2's single-agent problem. We denote the PCAs and their respective rewards with $\{pca_1:r_1, pca_2: r_2\}$. We follow the projection stages one by one:
\begin{enumerate}
    \item As the push action is the only public action in the problem, we remove all push actions except for the ones that are observed in the traces: Agent-1-CPush-Right-Box1, Agent-2-Push-Left-Box2 and Agent-2-CPush-Right-Box1. Notice that we keep Agent1's CPush-Left action, as we might need to simulate it.
    \item We remove the sensing action of Agent1, as well as its observation variable.
    \item We don't have any non-deterministic private actions, so no actions are turned to deterministic.
    \item We add a penalty of $-2\cdot max\{r_1, r_2\}$ to the remaining public actions, applied in any context except for the PCA's contexts.
    \item We add the rewards ${r_1,  r_2}$ to their corresponding PCAs.
    \item The rewards for pushing the boxes to the target tiles are set to 0 - we reward the agent only for doing its public action in context.
\end{enumerate}
\end{example}

Finally, we solve each single-agent problem $P_i$ using our off-the-shelf POMDP solver, SARSOP, resulting in a $\epsilon_i$-optimal policy for each agent. The generated policies will likely
contain non-sensing actions of other agents, which require
some handling, explained later on.

\begin{algorithm}
\caption{ProjectAndSolve}
\begin{algorithmic}[tbph]
\State Input: $P_{team}$, $Traces$
\For {all agent $i$}
\State $\pcact{i},\mathit{Rewards_i} \gets \Call{ExtractPCAs}{P_{team}, Traces, i}$
\State {$\mathit{SAProblem_i}$ $\gets \Call{Project}{P_{team}, \pcact{i}, Rewards_i}$}
\State {$\epsilon_i \gets \Call{GetSAPrecision}{\mathit{MaxTraceLength}, \gamma}$}
\State {$\mathit{RawSAPolicy_i} \gets \Call{POMDPSolver}{\mathit{SAProblem_i}, \epsilon_i}$}
\EndFor
\State {return $\mathit{RawSAPolicy_1}, \mathit{RawSAPolicy_2},..., \mathit{RawSAPolicy_n}$}
\end{algorithmic}
\end{algorithm}


\subsection{Rewards and Precision in the Projection}

After we compute the PCAs for the agent, we need to specify their respective rewards. The rewards should encourage the agent to perform the team plan actions in their corresponding contexts, in the same order they appeared in the team solution. 
To do so, we exploit the discount factor, which makes it beneficial for the planner to apply higher rewarding actions earlier. Hence, we need to order the PCAs, and assign higher rewards to actions that appear earlier in the order.

The main thing to notice is that the planner might need to insert costly actions that precede a PCA to achieve some needed influencer for it. This may lead to a scenario where it is more beneficial to apply the PCAs in a different order or even not to apply a PCA at all, as the costs might be higher than the reward of the PCAs.

The first problem, applying PCAs in a different orders, is automatically taken care of because effects are part of the
projected context. 
%Public actions are responsible for altering the agent's objectives, 
By penalizing public actions that are applied out of context and rewarding those that are applied in context, we effectively force PCAs to be applied only in an appropriate order - assuming that one of the ways of getting from one context to the next is preferable over others. When this assumption doesn't hold and there are "ties" between different ways of progression, we break these ties via small changes in the rewards we assign to the PCAs, as we explain later on.
%\ronen{not 100\% sure of this}\eliran{tried to make it clearer}

The second problem of making the PCAs worth applying, is handled via the rewards we assign to each PCA. 
%We want to make sure that in the projected problem it is beneficial for the agent to apply a sequence of PCAs that will achieve its desired goals. Since we no longer pursue the team goals, the reward function does not necessarily make these application beneficial, and therefore we insert artificial rewards.
%
%To do so, we tackle the problem more locally, and 
We design the rewards so that no sequence of non-rewarding actions would "cancel" the need for applying the PCA it precedes, that is - would cost more than the PCA's reward.

To compute the rewards, we need to first determine an order over the PCAs we extracted for the agent.
We go over the traces and construct a \emph{directed acyclic graph} of PCAs, ensuring it is acyclic by not adding edges that form cycles.
We start with an empty graph, and go over all traces. Let $l$ be a trace.
%\ronen{define event or change the term}\eliran{we defined event earlier on, in the definition of trace, but I now changed it to "step" instead, feels more intuitive. Should we use the same exact notation as in the definition of "step"? it seems unnecessarily formal here}
For each step $e_j$ in the trace, we compute the PCA of its contexed action with respect to agent $i$. We denote this PCA with $pca_j$.
Then, we add a vertex that represents $pca_j$, assuming it doesn't already exist. We add an edge between the vertex corresponding to $pca_{j-1}$ and $pca_j$.
This implies that the current action cannot appear before the one preceding it.
Once we went over all traces and constructed the graph, we compute a topological order
for it, giving us a sequence of PCAs $(\pi_1, a_1),..., (\pi_q, a_q)$, where the first instance corresponds to the action that appeared earliest.

\eliran{Not Implemented - Replace sequence with sets representing the levels of the DAG}
We begin by associating a reward for the last PCA in the sequence, namely the \emph{base} reward, and then iteratively calculate the reward for each preceding PCA.
The base reward is chosen so that applying the PCA will be beneficial in any possible scenario. Recall that we need to avoid a scenario where many costly actions are required to reach a state satisfying the context of the action, so that their total cost surpasses the reward, making it non-beneficial to apply.


%Next, we describe the formulas that are used to compute to rewards. In these calculations, we assume that each action has a success probability. Following that, we describe how we can we calculate the rewards without assuming these success probabilities exist.

Recalling that we assume each action has one successful outcome,
let $MaxCost$ be the maximal negative reward received by an action, that appeared in the traces.
%\ronen{Why do we care about negative reward after the action?}
%\eliran{sorry, just a bad formulation - I meant the negative reward caused by an action}
Let $MinSP$ be the minimal success probability of all actions in the problem, and $MinPCASP$ the minimal success probability of the actions appearing in the PCAs. Let $MTL$ be the Maximal Trace Length we produced, and $MPG$ be the Maximal Public Gap -- the maximal number of \emph{private} actions that precede a public action in the traces. Let $sp_i$ denote the success probability of action $a_i$. We set $\epsilon > 0$ to some arbitrary positive real. We compute $r_q$, the base reward:
\begin{equation}
\label{eqn:rq}
   r_q = \frac{\sum_{i=MTL - MPG}^{MTL}(\frac{MaxCost}{MinSP} \cdot \gamma^{i-1})+ \epsilon}{\gamma^{MTL-1}\cdot MinPCASP} 
\end{equation}%
The numerator is an upper bound on the expected discounted cost we would pay before applying the last PCA. The denominator amplifies that cost to be beneficial when scheduled as the last action in the policy.

%Equation~\ref{eqn:rq} ensures that the planner will always find the application of $a_q$ in the context $\pi_q$ beneficial, and thus will insert it to the policy.

After calculating the base reward for action $a_q$, 
%which is set as the reward of $(\pi_q, a_q)$, 
we can calculate the rewards associated with the rest of the PCAs in the sequence. Since the application order of the PCAs is enforced using the context, we simply need to perform tie breaking for cases in which several PCAs may be applied, so that the agents would produce coordinated solutions.
To do so, we set $r_i=(1+\epsilon)\cdot r_{i+1}$, where again $\epsilon$ can be an arbitrarily small positive real, and $r_i$ is
the reward associated with $a_i$ in context $\pi_i$.


% To ensure that the order is maintained, using the same intuition as with the base reward, we compute the reward for $(\pi_i, a_i)$ so that the planner will always prefer to apply $a_i$ in $\pi_i$ before applying $a_{i+1}$ in $\pi_{i+1}$. 
% An interesting cases arises when the failure probability of $a_i$ is small. Then, after applying the PCA once, it may be better for the planner to assume that the PCA succeeded and apply $a_{i+1}$, without any form of verification, causing the order to break.
% \guy{I do not understand why we enforce "verification" - if it is beneficial to observe, the planner should do so, and if not, then not. Such "vitrification" should appear in the team plan to begin with, assuming that they are needed.}
% \eliran{Even if the verification appears in the team plan, the artificial rewards we add to the public actions would cause the planner to postpone this verification to a later stage and violate the order, as it would be more beneficial for him to apply the public actions as soon as possible. I hope that later on we would find other ways to design the reward function so that the team plan behavior could be more naturally preserved.}\guy{I disagree. Rewards are given for executing an action at a state. If the planner is unsure of the state, it will observe, if need be.}\eliran{Open Issue - I understand your meaning. The problem is that in the code, we still didn't implement the projection so that it would be only with respect to the variables relevant to the agent, but with respect to the action's context. This point is very crucial, though I think that both options (projecting onto context and onto relevant variables) would yield good results for now. Let's discuss this as well}
% We set $r_i=\gamma \cdot \frac{r_{i+1}}{1-sp_i} \cdot sp_{i+1} + \epsilon$ \eliran{explain further?}
% and by that we ensure that regardless of whether $a_i$ fails or not, it would still be beneficial to verify that and apply $a_i$ again if necessary, before applying $a_{i+1}$. We compute $r_i$ until we reach $r_1$, and by that associating a reward to each of the agent's PCAs.

When defining $MinSP$ in the base reward calculation, we assumed that each action has a clear success probability. If we do not want to assume that actions have a unique successful outcome, we can instead incorporate into each PCA, in addition to its projected context and action, a contexted minimal probability. That is, for each PCA we consider all its possible effects under contexts that match the projected-context of the PCA, and take the minimal probability among all of these effects' probabilities.
%\ronen{the above is not clear}\eliran{is it ok?}

To compute the $\epsilon_i$, the optimality parameter from the
POMDP's solution, we use a concept similar to the base reward calculation, but now we rely on the observed MaxTraceLength to capture the total number of costly action without amplifying their cost with their success probability.
\begin{align*}
\label{eqn:rq}
   \epsilon_i = \gamma^{MTL-1}\cdot r_q \cdot MinPCASP \\
   -\sum_{i=MTL - MPG }^{MTL}(MaxCost \cdot \gamma^{i-1})
\end{align*}
This is purely a heuristic choice.
\eliran{It is heuristic, but yields the best results on all problem configurations. Should we leave it as is for the workshop paper?}
\ronen{For now, yes}.

\begin{example}
We go back to our example to demonstrate how these rewards are calculated.
First, recall that our only non-deterministic action is the push action, which has a success probability of 0.8, hence $MinSP=MinPCASP=0.8$. In addition, the action costs are -1, -10, -3 and -2 for sense, move, push and cpush respectively. Hence $MaxCost$ is set to 10. We also set $\gamma=0.99$, $\epsilon=0.01$.
Taking our two traces, we have $MTL=7$ and $MPG=2$


As mentioned previously, we have two PCAs when considering Agent1's problem.
We construct the DAG from the traces and extract the topological order, which yields the following order between the two PCAs.
\begin{enumerate}
    \item $pca_1$ = \emph{(*, L, L, R), Agent-2-Push-Left-Box2}
    \item $pca_2$ = \emph{(L, L, L, L), Agent-2-CPush-Right-Box1}
\end{enumerate}


We calculate the base reward, $r_2$:
\begin{equation}
     r_2 = \frac{\sum_{i=7-2}^{7}(\frac{10}{0.8} \cdot \gamma^{i-1})+ \epsilon}{\gamma^{7-1}\cdot 0.8} = 47.36
\end{equation}
To calculate $r_1$ we just amplify by $1+\epsilon$ and get $r_1=47.83$

Considering the PCAs and their rewards, \emph{Agent2} will be rewarded with $r1$ for pushing \emph{Box2} to the left regardless of \emph{Agent1}'s position, but to push $Box1$ it will need to reason about $Agent1$'s position - although in a very shallow manner as it is a private variable of $Agent1$.
Notice that the contexts of both PCAs, enforces $pca_2$ to be applied only once \emph{Box2} is pushed left - meaning after $pca_1$.


Next we calculate the precision of \emph{Agent2}'s single-agent problem: \begin{equation}
     \epsilon_2 = \gamma^{7-1}\cdot r_2 \cdot 0.8 - \sum_{i=7-2}^{7}(10 \cdot \gamma^{i-1}) = 7.14
\end{equation}
\end{example}

\subsection{Policy Adjustment and Alignment}

We run the planner on each of the single agent projections that we generate, constructing a set of single agent policy graphs for the projections. We now adapt the policies to apply to the joint problem. 

First, the projection of agent $i$ contains actions of other agents that must be removed. We traverse through the policy graph and replace every action of another agent by its child. As we do not allow sensing actions of other agents, there is always a single child to actions of other agents.

We now align the policies to increase the synchronization of actions of different agents to occur in the same order as in the team plan. An action $a_1$ of agent 1 that generates an influencer for an action $a_2$ of agent 2 should be executed before $a_2$. Also, collaborative actions should be executed at the same time by all participating agents.  
%While we cannot guarantee this, because agents have different belief states, we can try to improve the probability that such synchronization will occur.

Given stochastic action effects, we cannot guarantee that the policies will be well correlated. It is desirable that we will maximize the probability of synchronization, but currently, we only offer a heuristic approach that empirically improves synchronization probability.

For each public action $a$ in the team plan we select a simple path from the root to $a$. The {\em identifier} of the action is the sequence of public actions along the simple path. Public actions in individual agent policy graphs that share the same identifier are assumed to be identical in all graphs.

For a public action $a$ that has the same identifier in all agent graphs, let $l$ be the length of the longest simple path to the action in all policy graphs, including private actions. In any graph where the length is less than $l$, we add {\em no-op} actions prior to $a$ to delay its execution.

We use an iterative process --- we begin with the action with the shortest identifier (breaking ties arbitrarily), and delay its execution where needed using {\em no-op}s. Then, we move to the next action, and so forth.



Finally, we handle the problem of a potential ``livelock'' between \emph{collaborative} actions. Consider a scenario where two agents need to perform a non-deterministic collaborative action whose effect can be directly sensed. To do so, after executing the action, both agents perform a \emph{sensing} action that senses the effect of that collaborative action.
In executions, the agents may be unsynchronized,  applying the collaborative and sensing actions in an alternating manner, where one agent performs the collaborative action while the other performs the sensing action, causing them to enter a livelock. To handle that, given a collaborative action with $n$ collaborating agents, we modify the graph so that every collaborative action that is part of a cycle is repeated by every agent for $n$ times instead of just once. This way, a livelock can never occur.

\guy{I think that the above is a sufficient explanation to the synchronization phase}




Given a policy graph of agent $i$, we consider the actions of any other agent as \emph{foreign} actions with respect to that policy graph. The policy adjustment process for agent $i$ can be divided into four steps:
\begin{enumerate}
    \item Remove all the \emph{private} actions of agents other than $i$ from agent $i$'s policy graph.
    \item Insert \emph{no-op} actions into the graph to synchronize it, as much as possible, with other agents' policy graphs. This is the alignment procedure. \guy{this is the complex part. how is it done?}
    \item Convert all \emph{public} actions of  agents other than $i$ to no-ops. \guy{why not remove them?}
    \item Handle potential \emph{collaborative} actions issue that might occur, in which slight unsynchronization leads the collaborating agents to enter an infinite loop.
\end{enumerate}

Each single-agent policy contains only a superficial plan for the other agents, if at all, and therefore its actions need to be aligned with respect to the true policies of other agents. 
Since we are dealing with non-deterministic actions, it is impossible to perform an alignment in which all action are perfectly coordinated. Instead, we expect the alignment to reduce the required postponement of actions, but currently, we do not guarantee a minimal waiting time.

We alter the graphs by inserting \emph{no-op} actions, so that corresponding public action vertices will appear in the same depth in all graphs, thus preventing the agents from applying actions prematurely.
To define what corresponding vertices are, we use the notion of an \emph{identifier} of a public action vertex. The identifier of a public action vertex in the policy graph is the
sequence of public actions that appear along the \emph{simple} path from the root to the vertex.
\eliran{Not specified - several identifiers}
\ronen{??}
\eliran{There might be several simple paths to the root, in which case we max on all identifiers. Back then you said we can omit it to keep things simple, leave it as is?}

The sequence of public actions represents the team-relevant actions that took place prior to the current action, and serves to  summarize the sequence of events. As the identifier is not agent-specific, it allows us to look for it in other policy graphs and determine which vertices represent similar sequences of events, namely corresponding vertices.

We now describe the algorithm in detail.
The alignment is an iterative process. We are given policy graphs $G_1,...,G_n$ which are the output of the last iteration, and initially set to be the raw policy graphs. In each iteration, we align every graph with respect to all other graphs, and stop once convergence is reached. The process is applied iteratively as some alignment might need to propagate through iterations. Algorithm~\ref{} provides the high-level pseudo-code of a single iteration.

Recall that in a policy graph, each vertex represents an action and each edge represents an observation. From a vertex that represents a non-sensing action, there is a single outgoing edge that represents the \emph{null-obs} observation. From a vertex representing a sensing action there is one outgoing edge for each possible observation.

We now describe the alignment of a single policy graph. For each public action $a$ in the policy graph $G_i$ of agent $i$, we identify this action in the graphs of other agents.
Then, we postpone $a$'s execution by inserting no-op actions, so that $a$ occurs at the same time in all policy graphs, given the maximal path to $a$ in all policy graphs.

We traverse $G_i$ using breadth-first search. For a vertex $v\in G_i$ representing a public action of agent $i$, we extract the $identifier$ of $v$. Having the identifier of $v$, we traverse all other graphs $G_j$, and in each we search for a vertex $v'$ that matches the identifier. $v'$ matches the identifier if there's a simple path from the root of $G_j$ to it that contains the identifier.
%\ronen{do you ignore no-ops?}\eliran{yes, they are private actions and hence are not part of the identifier}
\eliran{Not specified - exhaustive match, more precise}
If we found such $v'$ that matches $v$, we set the {\em no-op} amount of $G_j$ that is required for $v$, denoted by $m_j$, to be the length of the \emph{longest simple path} from the root to $v'$. Else we set $m_j$ to 0.
Once we calculated $m_j$ for all $j\neq i$, we need to determine the final amount of {\em no-ops} added to $v$ in $G_i$. First, we consider $max_{j\neq i}\{m_j\}$. However, the maximal requirement does not take into account {\em no-ops} that were already added to preceding vertices in $G_i$, or the depth of $v$ itself, and must be
corrected by subtracting from it a \emph{compensation term} which is the sum of the \emph{longest simple path} from the root to $v$ and the \emph{minimal} {\em no-op} amount added to any of the predecessors of $v$. Overall, we set the {\em no-op} amount of $v$ to be $max_{j\neq i}\{m_j\} - \mathit{CompensationTerm}$.

Having computed the  {\em no-op} amount for each of the public action vertices in $G_i$, we insert them to it by appending the required number of consecutive {\em no-op} vertices \emph{prior} to the public action vertex, thus postponing it.
In each iteration of the algorithm we apply this procedure to all policy graphs, one by one, and halt upon convergence.

\eliran{Not Implemented - Running with concurrent BFSs}

After the alignment procedure has converged, we proceed to the the third step. Unlike the case of \emph{private} actions that were previously eliminated, \emph{public} actions of other agents were left in the graphs to guide the alignment process. Following the alignment, we convert them into \emph{no-ops}, and by that make the agent wait while other agents are performing their public actions.

Finally, in the fourth step, we handle the problem of a potential ``livelock''
between \emph{collaborative} actions. Consider a scenario where two agents need to perform a non-deterministic collaborative action whose effect can be directly sensed. The action is costly so they must apply it the minimal necessary number of times. To do so, following every application, they perform a \emph{sensing} action that senses the effect of that collaborative action.
Because actions are non-deterministic, the alignment process is
not guaranteed to lead to synchrnized actions online.
Thus, in 
executions, the agents may be unsynchronized. 
and will be applying the collaborative and sensing actions in an alternating manner, where one agent performs the collaborative action while the other performs the sensing action, causing them to enter a livelock. To handle that, given a collaborative action with $n$ collaborating agents, we modify the graph so that every collaborative action that is part of a cycle is repeated by every agent for $n$ times instead of just once. This way, a livelock can never occur.

\begin{example}
+Figures~\ref{Fig:Alignment}(a) and (b) show \emph{Agent1}'s policy graph before and after the alignment and adjustments procedure.

\begin{figure}
    \centering
    \subfloat[Before]{\includegraphics[width=0.25\textwidth,height=\textheight,keepaspectratio]{BPNEW-2x1_2A_1H_1L_REPLACE_TEAM_agent1_001AMP_COLOR.dot.png}}
      \hfill
    \subfloat[After]{\includegraphics[width=0.45\textwidth,height=\textheight,keepaspectratio]{BPNEW-2x1_2A_1H_1L_REPLACE_TEAM_ALIGNED_exactalign_postv3_repeatcol_pre_agent1_COLOR.dot.png}}
\caption{Write Caption Here}
\label{Fig:Alignment}
\end{figure}

\begin{itemize}
    \item We remove \emph{Agent1}'s simulation of \emph{Agent2}'s left movement.
    \item The alignment algorithm inserts a {\em no-op} following \emph{Agent1}'s first sensing action, as the collaborative push in \emph{Agent2} occurs in depth 4.
    \item We convert the first simulated push of \emph{Agent2} to a {\em no-op}.
    \item We duplicate the collaborative push to avoid a live-lock. This way, even if \emph{Agent2} fails with his first push, it could still synchronize with \emph{Agent1} by entering the collaborative-push loop which ensures no live-lock can occur.
\end{itemize}
\end{example}


\begin{algorithm}
\caption{Alignment Iteration}
\begin{algorithmic}[tbph]
\State Input: PolicyGraphs $G_1, ..., G_M$
\For{$G_i,  i\in\{1, ..., M\}$}
	\State {$\mathit{NoopsReqs} \gets \mathit{VertexToIntMapping}$}
      \State {$\mathit{CurrBFS} \gets \Call{BFS}{G_i}$}
      \While {$\mathit{CurrBFS.queue}$ is not empty}
	\State {$v \gets \mathit{CurrBFS.queue.pop}$}
	\State {$a \gets v.action$}
	\If {$a$ is public action}
	\State {$\mathit{identifier} \gets \Call{GetIdentifier}{v}$}
	\State {$\mathit{MaxNoop} \gets 0$}
	\For {$G_j,  j\in\{1, ..., M\}\setminus\{i\}$}
	\State {$\mathit{CurrNoop} \gets \Call{NoopReq}{G_j, \mathit{identifier}}$}
	\State {$\mathit{MaxNoop} \gets max(\mathit{MaxNoop}, \mathit{CurrNoop})$}
	\EndFor
	\State {$\mathit{NoopsReqs}[v] \gets \mathit{MaxNoop} - \mathit{CompensationTerm}$}
	\EndIf
	\EndWhile
	\State {$G_i' \gets \Call{AddNoops}{G_i, \mathit{NoopsReqs}}$}
\EndFor
\State {return $G_1', ..., G_M'$}
\end{algorithmic}
\end{algorithm}

\section{Empirical Study}
We provide experimental results while focusing on large planning horizons on large scale problems.
The experiments were conducted on a variation of Cooperative Box Pushing, which we specify in the next subsection.
We compare our algorithm with two Dec-POMDP solvers, GMAA-ICE \cite{GMAAICE} and JESP \cite{JESP}, using MADP-tools. \cite{MADP}.
We evaluate FDMAP on a Windows machine with 4 cores of and 8GB of memory.
We evaluate both GMMA-ICE and JESP on a Linux machine with 4 cores and 8GB of memory.


\subsection{\cbp}
We present a variation of the well known Cooperative Box Pushing domain, in which the need shifts from a good local policy for each agent that relies mostly on short responses to observations, to a decentralized policy that relies on good coordination between agents and requires larger planning horizons to be accomplished. A grid tile can contain any number of agents and boxes, and each agent has both move and push actions in each of the four possible directions, as well as a sensing action for a box and a no-op action.
An agent (or agents), can push a box only when positioned in the same grid with it.
All action except for the push actions are deterministic.
Light boxes can be pushed by a single agent, while heavy boxes can only be pushed by the collaborative push of two agents.
The goal of the agents is to move the boxes to the target tile, located at the upper left corner of the grid.
Initially, each box can appear in either the target tile (making it invaluable to handle) or the lower right tile, with equal probability.
Each action has a cost (except for no-op), and a reward is given for pushing a box to the target tile. The costs are 5 for move, 1 for sense, 30 for push, 20 for collaborative push (per agent), and the reward is 500. In addition, there's a penalty of 500 for pushing a box \emph{out} of the target tile to avoid abuse.
The rewards were chosen so that it would be more beneficial to sense rather than blindly push.
A domain instance of $m$ tiles with $n$ agents $l$ light boxes and $h$ heavy boxes, has $m^{n+l+h}$ states, $(5\cdot(k+1)+4h\cdot(n-1))^n$ actions and $3^n$ observations. 

\subsection{Domain Configurations}
We demonstrate results on six configurations of \cbp. Each box can equally start at either the top-left or bottom-lower corner, so we only specify the initial location of each agent:
\begin{enumerate}
    \item 1 by 3 grid, 1 light box and 2 agents located at tiles (1,2) and (1,3)
    \item 2 by 2 grid, 1 light box and 2 agents located at tiles (1,2) and (2,2)
    \item 2 by 2 grid, 2 light boxes and 2 agents located at tiles (1,2) and (2,2)
    \item 2 by 2 grid, 3 light boxes and 2 agents located at tiles (1,2) and (2,2)
    \item 2 by 3 grid, 2 light boxes and 3 agents located at tiles (1,2), (1,3) and (2,3)
    \item 2 by 3 grid, 3 light boxes and 3 agents located at tiles (1,2), (1,3) and (2,3)
    \item 3 by 3 grid, 1 light box and 2 heavy boxes, 2 agents located at tiles (1,3) and (3,1). In this configuration we also increase the reward and penalty for pushing a box in and out of the target tile to 1000, so that it will remain beneficial for the agents to push all boxes.
\end{enumerate}

\begin{table}
\caption{\guy{Better put these numbers at the first row of each table.}}\eliran{what do you wanna do for configurations with no table?}
\begin{center}
    \begin{tabular}{||c|c|c|c|c||}
         \hline
         \multicolumn{5}{||c||}{\cbp \ Configurations} \\
         \hline
         Index & Name & $|S|$ & $|A|$ & $|\Omega| $ \\ 
         \hline
         1 & BP-13201 & 27 & 100 & 9 \\
         \hline
         2 & BP-22201 & 64 & 100 & 9 \\
         \hline
         3 & BP-22202 & 256 & 225 & 9 \\
         \hline
         4 & BP-22203 & 1024 & 400 & 9 \\
         \hline
         5 & BP-23302 & 7776 & 3375 & 27 \\ 
         \hline
         6 & BP-23303 & 46656 & 8000 & 27 \\
         \hline
         7 & BP-33221 & 59049 & 324 & 9 \\
         \hline
    \end{tabular}
\end{center}
\end{table}

\subsection{Comparison Setting}
We compared FDMAP with GMAA-ICE and DP-JESP. For GMAA-ICE and DP-JESP, the horizon stands for the planning horizon. Since FDMAP calculates a policy for an infinite horizon, the horizon parameter specifies the number of steps the policy was evaluated under.
The value metric for GMAA-ICE and DP-JESP specifies the policy value. In FDMAP we measured the average discounted accumulated reward of 1000 simulations, where in each simulation the policy was run for the number of steps specified in the horizon column. The last horizon prefixed with Max is the maximal horizon measured for reaching the goal state in all simulations.

GMAA-ICE and DP-JESP were given with 4000 seconds limit to solve each $\langle\textit{configuration, horizon}\rangle$ pair, FDMAP was given with the same amount to solve the problem once and output an infinite horizon solution, and specifically 15 minutes for each agent to solve its own problem. For the hardest problem configuration (BP-33221), we also present results for larger agent time limit, as 15 minutes weren't sufficient for reaching the target precision.
The times shown for GMAA-ICE do not include the problem loading. FDMAP times do not include writing SARSOP policies and graphs to disk, as they are highly dependent on hardware quality and can effectively remain in memory throughout the whole process.
% \eliran{explain that the process is done in separated steps and should be unified?}\ronen{Not essential now. EVentually, you should have code that is usable by others.}

In both GMAA-ICE and DP-JESP, configurations BP-23302, BP-23303 and BP-33221 could not be parsed in the time given. Therefore we provide comparisons only for the first three configurations, BP22201 and BP22202. For the latter three we provide the maximal and average number of steps it took FDMAP to reach a goal state - excluding instances in which the initial state is a goal state.

In DP-JESP, X marks a general timeout. In GMAA-ICE we mark two different timeout options - FF refers to failure of finding a full policy for the required horizon, where FH refers to an earlier stage timeout when computing the heuristic function.

\begin{center}
    \begin{tabular}{||c|c|c|c|c|c|c||}
         \hline
         \multicolumn{7}{||c||}{BP-13201 $|S|=27, |A|=100, |\Omega|=9$} \\
         \hline
         Horizon & \multicolumn{2}{|c|}{DP-JESP} & \multicolumn{2}{|c|}{GMAA-ICE} & \multicolumn{2}{|c||}{FDMAP}\\ 
         \hline
         & Time & Value & Time & Value & Time & Value \\
         \hline
         3 & & & & & 1.622 & -27.661 \\
         \hline
         4 & & & & & " & -31.589 \\
         \hline
         5 & & & & & " & -36.138 \\ 
         \hline
         6 & & & 6812.62 & 124.91 & " & 133.003 \\
         \hline
         7 & & & & & " & 136.299 \\
         \hline
         8 & & & & & " & \\
         \hline
         Max=16 & & & & & " & 185.958 \\
         \hline
    \end{tabular}
\end{center}

\begin{center}
    \begin{tabular}{||c|c|c|c|c|c|c||}
         \hline
         \multicolumn{7}{||c||}{BP-22201 $|S|=64, |A|=100, |\Omega|=9$} \\
         \hline
         Horizon & \multicolumn{2}{|c|}{DP-JESP} & \multicolumn{2}{|c|}{GMAA-ICE} & \multicolumn{2}{|c||}{FDMAP}\\ 
         \hline
         & Time & Value & Time & Value & Time & Value \\
         \hline
         3 & 14.11 & 0 & 2.08 & 0 & 1.36 & -26.958 \\
         \hline
         4 & 1279.06 & 80 & 368.54 & 76.64 & " & -32.213 \\
         \hline
         5 & X & - & 366.58 & 108.74 & " & -36.484 \\ 
         \hline
         6 & X & - & FF & - & " & 126.072 \\
         \hline
         7 & X & - & FF & - & " & 137.302 \\
         \hline
         8 & X & - & FF & - & " & 158.98 \\
         \hline
         Max=14 & X & - & FF & - & " & 178.316 \\
         \hline
    \end{tabular}
\end{center}

\begin{center}
    \begin{tabular}{||c|c|c|c|c|c|c||}
         \hline
         \multicolumn{7}{||c||}{BP-22202 $|S|=256, |A|=225, |\Omega|=9$} \\
         \hline
         Horizon & \multicolumn{2}{|c|}{DP-JESP} & \multicolumn{2}{|c|}{GMAA-ICE} & \multicolumn{2}{|c||}{FDMAP}\\ 
         \hline
         & Time & Value & Time & Value & Time & Value \\
         \hline
         3 & 262.28 & 0 & 50.16 & 0 & 3.88 & -27.91 \\
         \hline
         4 & X & - & FF & - & " & -32.56 \\
         \hline
         5 & X & - & FF & - & " & -48.14 \\ 
         \hline
         6 & X & - & FF & - & " & -48.48 \\
         \hline
         7 & X & - & FF & - & " & 181.65 \\
         \hline
         8 & X & - & FF & - & " & 199.38 \\
         \hline
         Max=23 & X & - & FF & - & " & 356.08 \\
         \hline
    \end{tabular}
\end{center}

\begin{center}
    \begin{tabular}{||c|c|c|c|c|c|c||}
         \hline
         \multicolumn{7}{||c||}{BP-22203 $|S|=1024, |A|=400, |\Omega|=9$} \\
         \hline
         Horizon & \multicolumn{2}{|c|}{DP-JESP} & \multicolumn{2}{|c|}{GMAA-ICE} & \multicolumn{2}{|c||}{FDMAP}\\ 
         \hline
         & Time & Value & Time & Value & Time & Value \\
         \hline
         3 & 3146.58 & 0 & 1856.81 & 0 & 21.01 & -14.07 \\
         \hline
         4 & X & - & FF & - & " & -14.84 \\
         \hline
         5 & X & - & FF & - & " & -19.78 \\ 
         \hline
         6 & X & - & FH & - & " & -45.91 \\
         \hline
         7 & X & - & FH & - & " & -49.39 \\
         \hline
         8 & X & - & FH & - & " & 49.80 \\
        \hline
         Max=35 & X &  & FH & - & " & 518.02 \\
         \hline
    \end{tabular}
\end{center}

\begin{center}
    \begin{tabular}{||c|c|c|c|c|c||}
         \hline
         \multicolumn{6}{||c||}{FDMAP}\\ 
         \hline
         Problem & MaxSteps & AvgSteps & Time & Value & \%Wins\\
         \hline
         BP-23302 & 40 & 14 & 92.70 & 243.91 & 100 \\
         \hline
         BP-23303 & 43 & 21 & 1799.94 & 353.82 & 98 \\
         \hline
         BP-33221 & 124 & 37 & 2962.29 & 8.64 & 95 \\
         \hline
         BP-33221 & 105 & 35 & 5379.32 & 345.56 & 100 \\
         1800secs  & & & & & \\
         \hline
    \end{tabular}
\end{center}
\section{Related Work}
QDec-POMDPs~\cite{QDECPOMDP} are qualitative version of Dec-POMDPS that tackle a conceptually simpler, more structured, model. In QDec-POMDPs 
non-determinism replaces stochastic uncertainty. The model is factored (i.e., described at the level of state variables rather than states), and actions 
are described using preconditions and non-determinstic effects. Although QDec-POMDPs are also NEXT-Time hard, recent work in the area that leverages heuristic-search planners, has been able to scale up to much larger domains (e.g., box pushing on a grid of size 24,  12 agents and 12 boxes, implying a state space of $24^{24}$)
albeit, under the assumptions that actions are deterministic.

Dec-POMDP algorithms.

\section{Conclusion}

\section{Future Research}
\eliran{its more of a sketch for now so we don't forget what we've talked about}
There are two directions in which the solver can be improved -  scalability and solution quality.
The use of online planners instead of offline ones can greatly improve the scale of solvable problems. The changes in terms of algorithm architecture are minor, as we merely need to be able to produce the single agent policy graphs using an online solver. \ronen{In fact, it seems we could use an RL algorithm here to generate a policy for each agent, as we can simulate as many traces as we wish.
In fact, we could use the RL algorithm to solve the team POMDP. This would generate the needed traces as well. We would learn incrementally
both the team solution and the single-agent solution. Using this idea, we could probably scale up to very large problems. In fact, we can use this
approach to do MA RL. If we can do the projections. What we need is a simulator that let's us control all the agents at once.}
In terms of solution quality, we would want to use more principled methods of reward shaping, that come from the worlds of reinforcement learning, in order to define the reward heuristic. We would want to achieve both the properties we already achieved using our current heuristic, and to still be able to optimize the expected discounted reward of the problem.
Also, in order to fully achieve that, the alignment algorithm will need to include some form of confidence aspect, where we no longer ignore cycles but rather look at non-simple paths and try to increase the certainty about the world's state. 

\bibliography{bibilography.bib}
\bibliographystyle{aaai}
\end{document}