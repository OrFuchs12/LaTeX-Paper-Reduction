\def\year{2020}\relax
%File: main.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai20}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{graphicx}  % DO NOT CHANGE THIS
\usepackage{xcolor}
\usepackage{amssymb}

\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS


\newcommand{\commentout}[1]{}
\newcommand{\eliran}[1]{\textbf{[\color{red}ELIRAN:#1]}}
\newcommand{\ronen}[1]{\textbf{[\color{blue}RONEN:#1]}}
\newcommand{\guy}[1]{\textbf{[\color{orange}GUY:#1]}}

\newcommand{\cbp}[0]{Collaborative Box-Pushing}
\newcommand{\mitg}[0]{Meet In The Grid}
\newcommand{\crs}[0]{Cooperative Rock-Sampling}
\newcommand{\macor}[0]{Multi-Agent Corridor}

\newcommand{\cact}[1]{{\em CActions$_#1$}}
\newcommand{\pcact}[1]{{\em \textit{PCAS$_#1$}}}

\newcommand{\Tau}{\mathrm{T}}

%\nocopyright
%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
 \pdfinfo{
/Title (A Factored Approach To Solving Dec-POMDPs)
/Author (Eliran Abdoo, Ronen I. Brafman, Guy Shani)
} %Leave this	
% /Title ()
% Put your actual complete title (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case
% Leave the space between \Title and the beginning parenthesis alone
% /Author ()
% Put your actual complete list of authors (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case. 
% Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands, 
% remove them. 

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{caption} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \natbib} -- This package is specifically forbidden -- use the following workaround:
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai20.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\setlength\titlebox{2.5in} % If your paper contains an overfull \vbox too high warning at the beginning of the document, use this
% command to correct it. You may not alter the value below 2.5 in
\title{A Factored Approach To Solving Dec-POMDPs }
%Your title must be in mixed case, not sentence case. 
% That means all verbs (including short verbs like be, is, using,and go), 
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\author{Eliran Abdoo, Ronen I Brafman, Guy Shani}
 \begin{document}

\maketitle

\begin{abstract}
Dec-POMDPs model planning problems under uncertainty and partial observability for a distributed team of cooperating agents planning together but executing their plans in a distributed manner. This problem is
very challenging computationally (NEXP-Time Complete) and consequently, exact methods have difficulty scaling up. In this paper
we present a heuristic approach for solving certain instances of Factored Dec-POMDP that tries to reduce the problem of planning in Dec-POMDPs to multiple
problems of planning in a POMDP. First, we solve a team version of the Dec-POMDP in which agents have a shared belief state, and then, each agent attempts to solve the problem of executing its part of the team plan. Finally, the different solutions are aligned to improve synchronization. Using this approach we are able to solve larger Dec-POMDP problems, limited only by the abilities of the underlying POMDP solver.
\end{abstract}




\section{Introduction}
Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are the most popular model for planning in stochastic environments under uncertainty with partial observability by a distributed team of agents~\cite{}. In this model, a team of agents attempts to maximize the team's cumulative reward where each agent has only partial information about the state of the system during execution. That is, each agent is aware of it's own observations only. Communication is possible only through explicit communication actions, if these are available.  

To achieve their common goal agents must coordinate their actions in two ways: First, as in single agent problems, actions must be coordinated sequentially. That is, current actions must help steer the system towards states in which greater reward will be possible,
e.g., to be rewarded for shipping a product, it must first be assembled. 
Second, agents may need to coordinate their simultaneous actions because their effects are dependent, e.g., a heavy box can only be pushed if two agents push it simultaneously. 

As with previous work in this area, our focus is on centralized off-line planning for distributed execution. That is, offline, a solver with access to the complete model must generate a policy for each agent. An agent's policy specifies which action must be taken as a function of the agent's history of
actions and observations. Such policies can be represented by a {\em policy graph} where nodes are labeled by actions, and edges are labeled by observations. Online, each agent executes it's own policy independently of the other agents.
The difficulty lies in generating policies that provide sufficient coordination, even though each agent may make different observations at run-time, and so the agent's beliefs over which states are possible are typically different. 

Dec-POMDPs are notoriously hard to solve -- they are NEXP-Time hard~\cite{} implying that only the smallest toy problems are optimally solvable.
However, many approximate methods for solving Dec-POMDPs have been proposed, with steady progress. Some of these methods generate solutions with bounds on
their optimality~\cite{}, and some are heuristic in nature~\cite{} \eliran{citations GMAA-ICE, DICEPS, JESP(heuristic), MBDP?}. However, current methods typically do not scale to state spaces with more than a few hundreds of states.


In this paper we describe a heuristic approach for solving Dec-POMDPs that scales up to much larger state spaces. The key idea is to solve a Dec-POMDP by
solving multiple POMDPs. First, we solve a POMDP obtained by assuming that all agents have the same belief state. That is, that any observation by one agent is immediately available to the other agents. We refer to this as the {\em team POMDP}. The solution of the team
POMDP can be represented by a policy graph --- the {\em team policy graph.} . It provides us with a skeleton for the solution of the Dec-POMDP, specifying what each agent needs to provide for the team. Naturally, this policy is not executable by the agents, because agents cannot condition their actions on the observations of other agents in the real world.

Hence, in the next stage, we let each agent solve a POMDP in which it is rewarded for behaving in a manner similar to the specification in the team policy. This leads to the generation of a policy tree for each agent. 
These policy trees are not well synchronized with each other, and in the last step, we synchronize them by delaying the actions of agents to
improve the probability of good coordination. 

We implement our algorithm and test it on several configurations of a benchmark problem \cbp which is a variation of the Cooperative Box Pushing problem. We show that the algorithm manages to scale well beyond
current Dec-POMDP solvers. \eliran{show property in experiments - portion of public actions}
One of the main properties of the domain, is that agents policies are only loosely coupled. That is, the need for actions that affect state components that are relevant to all agent, is sparse. That sparsity allows for each agent to independently construct a plan that consists mostly of its own private actions without requiring it to consider the other agents' behavior. This allows us to achieve good decentralized policies even when achieving the goal requires many steps, compared to planning directly over the Dec-POMDP model.

\section{Background}

In this section we provide needed background on POMDPs, Dec-POMDPs, their factored representation,
and policies. We also introduce the concept of private and public variables and actions in Dec-POMDPs.

\subsection{POMDPs}


A POMDP is a model for single-agent sequential decision making under uncertainty and partial observability.
Formally, it is a tuple $P=\langle S, A, T, R, \Omega, O, \gamma, h, b_0 \rangle$, where:
\begin{itemize}
\item
$S$ is the set of states. Each state captures all the relevant information for making decisions.
\item
$A$ is the set of actions. An action modifies the state of the world and may provide information about the current state.
\item
$T: S \times A \rightarrow \prod(S)$ is the state transition function.  $T(s, a, s')$ is the probability of transitioning to $s'$ when applying $a$ in $s$. 
\item
$R:S \times A \times S \rightarrow \mathbb{R}$  is the immediate reward function. $R(s,a, s')$ is the reward obtained after performing $a$ in $s$  and reaching $s'$. 
\item
$\Omega$ is the set of observations. An observation is obtained by the agent following an action, and provides some limited information about the world.
\item
$O:S \times A \rightarrow \prod (\Omega)$ is the observation function, specifying the likelihood of sensing a specific observation following an action. $O(s', a, o)$ is the probability of observing $o\in \Omega$ when performing $a$ and \emph{reaching} $s'$. 
\item
$\gamma \in (0,1)$ is the discount factor, modeling the higher importance of immediate rewards over future rewards.
\item
$h\in\mathbb{N}\cup\{\infty\}$ is the planning horizon --- the amount of actions that an agent executes before terminating. \eliran{Open issue - included infinity as our algorithm outputs a policy for infinite horizon, where we state this?}
\ronen{Do we really plan for an infinite horizon? Perhaps you mean unbounded horizon?}\eliran{I'm not sure actually. The policies we output are for infinite horizons (there's an edge for every possible observation of the node's action), but perhaps it is because we plan for enough steps so that the policy graph closes, and then it might be for an unbounded horizon. I'll look into it}

\item
$b_0\in \prod(S)$ is a distribution over $S$ specifying the probability that the agent begins the execution in each state.
\end{itemize}

\eliran{start running example}


We assume that agent actions are either sensing actions or non-sensing actions. An agent that applies a non-sensing action always receives the observation {\em null-obs}. If all actions in the joint action are sensing actions or {\em no-op}s, then the state does not change. In addition, we assume for simplicity that every action has a success probability, that is, it has an effect with constant probability that we consider as the successful outcome, while all the other effects are considered failures. We will specify how this assumption can be omitted in the relevant parts.

Often, the state space $S$ is structured, i.e., it consists of assignments to some set of variables $X_1,\ldots X_k$, and the observation space $\Omega$ is
also structured, consisting of a set of observation variables $W_1,\ldots, W_d$. 
Thus, $S=Dom(X_1)\times\cdots\times Dom(X_k)$ and
$\Omega = Dom(W_1)\times\cdots\times Dom(W_d)$. 

In that case, $\tau$, $O$, and $R$ can be represented compactly by, e.g., using a dynamic Bayesian network~\cite{}. Formats such as RDDL~\cite{} and POMDPX~\cite{} exploit factored representations to specify POMDPs compactly.

A solution to a POMDP is called a policy. In general, a policy assigns to each history of actions and observation ({\em $AO$-history}) the next action to execute. 
Such a policy is often represented using a {\em policy tree} or, more generally, a {\em policy graph} (also called a finite-state controller). 
A policy graph $G=(V,E)$ is a directed simple graph, in which each vertex is associated with an action, and each edge is associated with an observation.

For every edge $v\in V$ and every observation $o\in\Omega$ exactly one edge emanates from $v$ with the label $o$.
The graph has a single root which acts as its entry point. Every $AO$-history $h$ can be associated with some path from the root to some vertex $v$,
and the action labelling $v$ is the action that the policy associates with $h$.

Finally, a policy graph can be run on the problem and produce an execution trajectory (trace for short). A trace $T$ of length $l$ is a sequence of quintuplets $e_i = (s_i, a_i, s'_i, o_i, r_i)$, namely \emph{events}, that occurred during a possible policy execution where:
\begin{itemize}
    \item $s_i$ is a state in step $i$ and $s_0$ is the initial state.
    \item $a_i$ is the action taken in step $i$
    \item $s'_i$ is the result of applying
    $a_i$ in $s_i$.
    \item $o_i$ is the observation received after taking $a_i$ and reaching $s'_i$.
    \item $r_i$ is the reward received for taking the $a_i$ in $s_i$ and reaching $s'_i$.
\end{itemize}
$\forall 0\leq i \leq l-1$, $s'_i=s_{i+1}$.

\subsection{Dec-POMDP}

A Dec-POMDP extends POMDP to problems where there are $n$ acting agents for some $n>1$. 
These agents are part of a team, sharing the same reward, but they act in a distributed manner,
sensing different observations. Thus, their information state can be different. 
Formally, a Dec-POMDP for $n$ agents is a tuple  $P=(S, A=\bigcup_{i=1}^{n}{\{A_i\}}, T, R, \Omega=\bigcup_{i=1}^{n}{\{\Omega_i\}},  O, \gamma, h, {\{I_i\}}_{i=1}^{n})$, where:
\begin{itemize}
\item
$S$ is the set of all states. Each state captures all the relevant information for making decisions. 
\item
$A_i$ is the set of actions available to agent $i$. We assume that $A_i$ contains a special {\em no-op} action, which does not change the state of the world, and does not provide any informative observation.
$A=A_1 \times A_2 \times .. \times A_n$ is the set of joint actions. On every step each agent $i$ chooses an action $a_i \in A_i$ to execute, and all agents execute their actions jointly. $\langle a_1,...,a_n \rangle$ is known as a joint action.
\item
$T:S \times A \rightarrow \prod(S)$  is the transition function. Transitions are specified for joint actions, that is, $T(s, \langle a_1,...,a_n \rangle, s')$ is the probability of transitioning from state $s$ to state $s'$ when each agent $i$ executes action $a_i$.
\item
$R:S \times A \times S \rightarrow \mathbb{R}$  is the reward function. Rewards are also specified over joint actions.
\item
$\Omega = \Omega_1 \times \Omega_2 \times .. \times \Omega_n$ is the set of joint observations. We assume $\Omega_i$ contains a special {\em null-obs} observation, which is the observation received when applying a non-sensing action.
\item
$O:S \times A \rightarrow \prod_{i=1..n}(\Omega_i)$  is the observation function, specified over joint actions. $O(s',\langle a_1,...,a_n \rangle,\langle o_1,...,o_n \rangle)$ is the probability that when all agents execute $\langle a_1,...,a_n \rangle$ jointly and reach $s'$, each agent $i$ observes $o_i$.
\item
$\gamma$  is the discount factor.
\item
$h\in\mathbb{N}\cup\{\infty\}$ is the horizon.
\item
$b_0 \in \prod(S)$ is a distribution over $S$ specifying the probability that each agent begin its execution in each state. In principle, different agents may have different initial belief states, but
we make the (common) assumption that the initial belief state is identical. 
\end{itemize}


\guy{running example}

As in the case of POMDPs, Dec-POMDPs can also be represented in a factored manner \cite{}, although most work to date uses the flat-state representation \cite{}.\eliran{citations}
An important element of a factored specification of Dec-POMDPs is a compact formalism for specifying joint-actions. If there are $|A|$ actions in the domain, then, in principle, there are $O(|A|^n)$ possible joint actions. Specifying all joint actions explicitly is unrealistic for large domains. 

In practice, we may expect
that most actions will not interact with each other. A pair of actions $a\in A_i$, $a' \in A_j$ is said to be non interacting, if their effects when applied jointly (in the same joint action) is identical to their effects when applied separately.
Thus, our specification language focuses on specifying
the effects of single-agent actions and specific
combinations of single-agent actions that interact with each other, which we refer to as {\em collaborative} actions~\cite{}. For a more detailed discussion of the issue
of compact specification of joint-actions, see~\cite{}. 


A solution to a Dec-POMDP is a set of policies $\rho_i$, one for each agent. It maps action-observation sequences of this agent to actions in $A_i$.
As in POMDPs, these policies can be specified using a policy graph for each agent. The policy graph for agent $i$ associates nodes with actions in $A_i$
and edges with observations in $\Omega_i$. 

\subsection{Public and Private Actions and Variables}

Public variables are state variables that several agents manipulate directly, while private variables are manipulated by a single agent only. 
The concept of \emph{private} and \emph{public} (or \emph{local} and \emph{global}) variables was introduced by
Brafman and Domshlak~\cite{} in the context of their work on factored planning. This concept has been used extensively
in work on privacy-preserving multi-agent planning (e.g., ~\cite{}) and, more recently in work on solving qualitative variants of Dec-POMDPs~\cite{}. As we are building on ideas in this latter work, we now explain how we extend them to the
case of factored Dec-POMDPs.

We associate with each action $a$ the set of variables it can affect or it is affected by, which we refer to as  {\em subjects}$(a)$. 
The effect could take place through the transitions, rewards, or observations associated with $a$.

More specifically, let $a\in A_i$ be an action of agent $i$. We identify $a_i$ with the joint action $(\mbox{no-op},\ldots, a_i,\ldots,\mbox{no-op})$.
We say that variable $X_i$ is a {\em subject} of $a\in A_j$ if there is some state $s$ for which there is a positive probability that the value 
$X_i$ changes following $a$, or if there are two states $s_1,s_2$ that differ only in the value of $X_i$ such that $R(s_1,a,s')\neq R(s_2,a,s')$ or $T(s_1,a,s')\neq T(s_2,a,s')$ for some state $s'$, or alternatively $O(s_1,a,o)\neq O(s_2,a,o)$ for some observation $o$.
Similarly, $\omega_i$ is a subject of $a\in A_j$ if there exists a state $s$ such that there is a positive probability
of observing $\omega_i$ when $a$ is executed and $s$ is reached.

As explained above, we expect that most actions do not interact. In that case, it is straightforward to get the post-action distribution for their combination from the specification of the single-agent actions they contain. But, as we work in multi-agent environment, some actions may affect some variables only when applied jointly with other actions, or may have a different effect on these variables when applied jointly.
We refer to these as $\emph{collaborative}$ actions. 

We say that $X_i$ is the subject of a collaborative action $a$ that consists of single agent actions
$a_{i_1},\ldots,a_{i_k}$ (and {\em no-ops}) if $X_i$ is the subject of $a$, as defined above, and is not the subject of each of the actions $a_{i_j}$.%
\footnote{A complete treatment of the subtleties of this issue is beyond the scope of this paper. The above definition will be sufficient for our purpose.}

We say that a variable $X_j$ is \emph{relevant} to agent $i$, if  $X_j\in subjects(a)$ for some $a\in A_i$.

We can now define the concept of {\em private} and {\em public} variables.
$X_i$ is {\em private} to agent $j$ if $X_i$, if it is not relevant to any agent $k\neq j$.
Otherwise, if there are other agents whose $X_i$ is relevant to, we say that $X_i$ is {\em public}. 

An action $a\in A_j$ is public if at least one of its subjects $X_i$ is public. A collaborative action is always public.

\section{FDMAP - Factorized Distributed Multi Agent Planning}

We now describe our approach for producing policy graphs for agents acting in a Dec-POMDP. Our approach first generates a team solution, and then generates single agent policies where each agent attempts to fulfill its part in the team solution. Finally, these policies are synchronized. 
\guy{The high level description here may be too detailed and technical.}\eliran{tried to simplify a bit}

From the input factored Dec-POMDP problem $P$, we first generate the team POMDP $P_{team}$. $P_{team}$ is identical to $P$, ignoring the underlying multi-agent structure. That is, the actions are the joint actions and the observations are the joint observation, viewed as applied 
and observed by a single agent. Equivalently, this can be viewed as a Dec-POMDP in which all observations are communicated accurately and instantaneously 
to all agents.

We solve $P_{team}$ using an off-the-shelf POMDP solver, SARSOP \cite{} \eliran{citation}, and output the team policy. We then use the team policy to produce traces, which are simulations of the team policy over the team problem. With the traces at hand, we start projecting the team problem with respect to each agent.

First, we extract from the traces a set of contexted actions for each of the agents, meaning we identify in which context the agent has executed its public actions in the team plan. Then, we associate a reward to each such contexted action.


% Some solvers provide a policy graph directly, but SARSOP provides a policy in the form of a set of $\alpha$-vectors.\eliran{why is it relevant? SARSOP also provides a way to convert the alpha vectors into a graph anyway. I explained later why we still use the traces} Instead, we use SARSOP to generate execution traces from the policy. From the traces we can learn each agent's behavior in the team solution.
% We use the traces to extract contexted actions for the next step in the algorithm. That is, we identify in which context each agent executes a public action in the plan. In addition, we associate a reward for each such contexted action.
% We use traces and not the team policy graph itself, as policy graphs are difficult to work with. They are based on the state distribution of the initial belief state, and most of the time represent highly obfuscated contexts, manifested by belief states with large support sets.
\guy{I do not understand the last sentence - are we trying to explain why policy graphs are not good for us? I am not sure that the argument holds - if we had a graph we could have used it to get the info that we need.}\eliran{We can get a graph, but getting proper contexts in the form of action-state would be inaccurate. We start from an initial belief and then can't know exactly which action was applied in which state. We should should later on extract this from the policy graph but it's more difficult, since we'll be calculating all possible traces. Should I use that argument? that we could use the policy graph to extract all possible traces but it's more difficult and a bit of an overkill?}\guy{This requires a discussion. Ronen, can we meet sometime this week to discuss all open issues?}\eliran{Open issue - In general, do we really need to motivate the fact that we use traces over policy graphs? What we would do with the policy graph is extracting the exact same things - starting with the initial belief state, we would extract from each vertex pairs containing its action and each state that exist in the belief state, then progress to the neighboring vertices. The traces are simply a mediated version of it}
\ronen{This is something we should discuss, I agree. But I think it is fine to use it as is.
Please compile a list of points for discussion/clarification.}

Using these contexted actions and their rewards, together with the factored Dec-POMDP, we  generate
one single-agent problem for each agent. The dynamics of each single-agent problem is similar to that of the Dec-POMDP, except that some variables are projected away. We design the reward function to be such that agents are rewarded for behaving in a manner that is similar to how they behave in the team solution.

Finally, we process the single-agent policies and align them to try and ensure that actions are properly synchronized when they are executed in a decentralized manner. The high-level pseudo-code is described below. We described the first step (generating $P_{team}$) above. In the rest of this section we explain these steps in more detail.

\begin{algorithm}
\caption{GenerateAgentPolicies \guy{This pseudo code is pretty useless. It just lists the phases. I don't think that it adds anything.}\eliran{drop it?}\guy{Or provide the implementation of each step}\eliran{except for centralize, the steps are all the other pseudo codes}}
\begin{algorithmic}[tbph]
\State Input: $P$, $\alpha$, $p_{team}$
\State $P_{team} \gets \Call{Centralize}{P}$
\State {\em Traces} $\gets \Call{ProduceTraces}{P_{team}, \alpha, p_{team}}$
\State {\em RawSAPolicies}$ \gets \Call{ProjectAndSolve}{P_{team},{\mathit Traces}}$
\State {\em SAPolicies} $\gets \Call{ProcessAndAlign}{\mathit{RawSAPolicies}}$
\State {return {\em SAPolicies}}
\end{algorithmic}
\end{algorithm}

\subsection{Producing the Traces}

Having generated the team problem, $P_{team}$, we 
must specify two hyper-parameters: a confidence parameter $\alpha$ and a precision parameter $\epsilon_{team}$. We generate a $\epsilon_{team}$-optimal solution
to $P_{team}$ using an off-the-shelf POMDP solver. \eliran{$\epsilon_{team}$ should be small enough so that the resulting policy will satisfy all \textbf{our} desired goals. Can we assume that the team problem is easy enough so that we don't have to do some smart picking but only pick the precision to be very small or set a predefined timeout?}
Then we generate sufficiently many traces so that with probability of at least $\alpha$, for every initial state $s$, there is a trace that starts at $s$. An $\epsilon$-optimal solution is a solution whose value differs by at most $\epsilon$ from the value of an optimal solution.

\eliran{Open issue - capturing every initial state is both very heuristic and quite weak -- need to change that - let's discuss it}
For that, it is sufficient to select the number of traces $n_t$ to be such that $\sum_{s\in supp(b_0)}(1-Pr(b_0=s))^{n_t} \leq 1-\alpha$.
Let $E$ denote the event in which we cover all possible initial states, $s \in supp(b_0)$, in $n_t$ traces. Then we want to pick $n_t$ such that $Pr(E)\geq \alpha$, using the union bound we get: $Pr(E)\geq 1-\sum_{s\in supp(b_0)}(1-Pr(b_0=s))^{n_t}$.

\begin{algorithm}
\caption{ProduceTraces \guy{this is also useless. Either remove or provide the actual implementation (loops)}\eliran{not sure what can be expanded here, thought it would be useful for understanding where each parameter go}}
\begin{algorithmic}[tbph]
\State Input: $P_{team}$, $\alpha$, $\epsilon_{team}$
\State {\em{TeamPolicy} $\gets \Call{POMDPSolver}{P_{team}, \epsilon_{team}}$}
\State {$n_t \gets \Call{NumTracesRequired}{P, \alpha}$}
\State {\em{Traces} $\gets \Call{Simulate}{\mathit{TeamPolicy}, n_t}$}
\State {return \em{Traces}}
\end{algorithmic}
\end{algorithm}

\subsection{Extracting Contexted Actions}
When constructing a POMDP for each agent, we attempt for the optimal policy in that POMDP to lead the agent to behave similarly to how it behaves in the team plan. That is, the agent should execute the same public actions that it executes in the team plan, in the same context that each action was executed in the team plan. We must define appropriate conditions for rewarding the agent when it executes an action from the team plan, which we
call the \emph{projected context}. 

Yet, similar behavior doesn't necessarily mean identical one, and is affected by the way we project the contexts.
The projected context can be associated with a specific state. However, requiring that an action would be executed only in the same state as in the team plan is too restrictive. Various aspects of the state may be too specific or irrelevant, and can be generalized to other states.

We start with the states in the trace in which an action is executed by an agent. We refer to a pair of state in which
an action is executed in the trace and the action itself,
as a \emph{contexted action}. A contexted action is said to be \emph{public} contexted action, if its action is public.

For agent $i$, we denote with \cact{i}
the set of all the public contexted actions in the traces, that contain an action of our agent.
%, and their corresponding rewards with $Rewards_i$.
\cact{i} contains \emph{public} actions only since the private objectives of the agent are left untouched when transitioning to the single-agent problem.
Next, we project the context of each contexted action to
the set of variables that are subjects of the action \emph{and} relevant to agent $i$. \eliran{I narrowed it to relevant subjects only, that's at least what we've done so far. Without it we may be able to drop the verification thing as Guy suggested, but we haven't checked it yet. Let me know if it's ok with you}
\ronen{Isn't it the case that it is enough to use subjects? If something is a subject, then it is relevant, isn't it?
The verification issue is something we need to discuss more deeply. Probably after you try it with the right context.}
By that we obtain a set of projected contexted-action, denoted by $\pcact{i}$, consisting of
a projected-context and an action. We will associate
the rewards with the elements of this latter set. We will refer to projected contexted-actions as PCAs from now on. Notice that when projecting different contexted actions, we might result with identical PCAs.
\eliran{running example}
% As an example, we can think of a simple Box-Pushing 1 by 2 grid, where there's an agent and a box in both tiles. The left tile is marked by \emph{L} and the right tile by \emph{R}. \emph{Agent1} and \emph{Box1} are placed on the left, while \emph{Agent2} and \emph{Box2} on the right. The state is composed of 4 variables: the location of each box -- $(X_1, X_2)$ -- and the location of each agent -- $(X_3, X_4)$. Each agent can push only its respective box. The goal of each agent is to push the box to his fellow agent, but the push action has a positive failure probability. 
In the team policy traces, we would find the following \emph{public contexted} actions:
\begin{itemize}
    \item \emph{(L, R, L, R),Agent-1-Push-Right}
    \item \emph{(L, R, L, R),Agent-2-Push-Left}
    \item \emph{(L, R, L, L),Agent-1-Push-Right}
    \item \emph{(L, R, R, R),Agent-2-Push-Left}
\end{itemize}

Each action appears in two different contexts, as throughout the traces there may be scenarios in which one of the push actions fail while the other does not.

When extracting the PCAs for \emph{Agent1}, we would encounter two possible candidates:
\begin{itemize}
    \item \emph{(L, R, L, R),Agent-1-Push-Right}
    \item \emph{(L, R, L, L),Agent-1-Push-Right}
\end{itemize}

We want to determine the variables onto which we project the contexts with respect to \emph{Agent1} and its action \emph{Agent-1-Push-Right}. The only subject of \emph{Agent-1-Push-Right} is $X_1$, the location of \emph{Box1}, and the additional variable that is owned by \emph{Agent1} is $X_3$ which is the agent's location. Therefore, the contexts are projected onto the variable $X_1, X_3$. 

When projecting both candidates, we get the same projected-context, \emph{(L, *, L, *)}, hence obtaining a single PCA.
Thus, in the single-agent problem, \emph{Agent1} will be rewarded for pushing \emph{Box1} to the left when both itself and the box are in the left tile, regardless of the locations of \emph{Box2} and \emph{Agent2}.

We will now describe how the single-agent problems are constructed, given the PCAs and their rewards.
How the reward values are computed will be described afterwards. We will use $Rewards_i$ to refer to the set
of reward values associated with $\pcact{i}$.


\subsection{Single Agent Projection}

Given the PCAs and their rewards, we construct single-agent problems that reward each agent for following its role in the team plan, as well as its own private objectives. The single agent POMDP of agent $i$ contains some the actions of $i$, and some actions of other agents. The actions of other agents are used to ``simulate'' some of the behaviors of the other agents -- behaviors that eventually enable the agent to carry out its own actions.

The single-agent POMDP $P_i$ for agent $i$ is obtained by modifying $P_{team}$
as follows:
\begin{enumerate}
\item Remove from the problem any \emph{public} action that does not appear in the traces, regardless of the agent applying it. As public actions alter facts in the world that are relevant to more than one agent, we completely forbid the application of a public action that was not part of the team solution.
\item Remove any sensing actions of other agents.
In the real world, the agent does not have any access to other agents' observations and must plan based on its local view only.
\item The \emph{private} actions of other agents are transformed into deterministic actions that succeed with probability 1.
As private actions change only private variables, and an agent cannot sense other agent's private variables, this projection allows the agent to progress the world towards states in which it must act.
If we do not assume that actions have a success probability, we can omit this alternation completely and "pay" by having larger, more complicated single-agent policies.

\item Add penalties for the application of all remaining \emph{public} actions in any context that did not appear in the team plan. We want to discourage an agent from applying public actions out of the context of the team plan. The penalty is chosen to be $-1\cdot\max_{r\in Rewards_i}\{r\} \cdot|\pcact{i}|$, as an upper bound on the sum of rewards that can be achieved from applying the contexted action. This ensures that no undesirable action is worth applying, even in exchange for applying all the contexted actions. There is no penalty for other agents' PCAs
(i.e., actions in $\bigcup_{j\neq i} \pcact{j}$). We want
to allow the agent to simulate other agents' contexted public actions in order to plan the execution of its own
actions at appropriate times.
\item Add the rewards for PCAs (explained later).
This reward will override the above penalty, but only under the specified context of each action. 
\item Remove rewards related to public variables the agent
can achieve -- the goal of the single-agent POMDP is to imitate the team policy, not compute an alternative solution. More specifically, given a public variable $X_i$, two state $s, s'$ that differ only on $X_i$, and an action $a\in A_i$ such that $R(s, a, s'')\neq R(s', a, s'')$, we set both rewards to 0.
\end{enumerate}

After applying these changes, we obtain the single-agent problem $P_i$. Before solving it, we need to specify a precision hyperparameter that determines how close to optimal the policy should be, similarly to the one that we specified for the team problem, but now picked separately for each single-agent problem.
For the single-agent problem $P_i$, we pick the precision $\epsilon_i$ to guarantee that if the policy is $\epsilon_i$-optimal, then no PCA will be ignored by the policy. % ADD EXPLANATION%
We denote by $R_{min}^i =  \min_{r\in Rewards_i}$t
i.e., $\epsilon_i=\min_{r\in Rewards_i}\{r\}\cdot\gamma^{l_m}$ \eliran{need to change}


Finally, we solve each single-agent problem $P_i$ using our off-the-shelf POMDP solver, SARSOP, resulting in a $\epsilon_i$-optimal policy for each agent. The generated policies will likely
contain non-sensing actions of other agents, which require
some post-processing, explained later on.

\begin{algorithm}
\caption{ProjectAndSolve \eliran{add pseudo code for ProjectPCAs?}}
\begin{algorithmic}[tbph]
\State Input: $P_{team}$, $Traces$
\For {all agent $i$}
\State $\pcact{i},\mathit{Rewards_i} \gets \Call{ExtractPCAs}{P_{team}, Traces, i}$
\State {$\mathit{SAProblem_i}$ $\gets \Call{Project}{P_{team}, \pcact{i}, Rewards_i}$}
\State {$\epsilon_i \gets \Call{GetSAPrecision}{\mathit{MaxTraceLength}, \gamma}$}
\State {$\mathit{RawSAPolicy_i} \gets \Call{POMDPSolver}{\mathit{SAProblem_i}, \epsilon_i}$}
\EndFor
\State {return $\mathit{RawSAPolicy_1}, \mathit{RawSAPolicy_2},..., \mathit{RawSAPolicy_n}$}
\end{algorithmic}
\end{algorithm}


\subsection{Rewards in the Projection}

After we compute the PCAs for the agent, we need to specify their respective rewards. The rewards should encourage the agent to perform the team plan actions in their corresponding contexts, in the same order they appeared in the team solution. 
To do so, we exploit the discount factor, which makes it beneficial for the planner to apply higher rewarding actions earlier. Hence, we need to order the PCAs, and assign higher rewards to actions that appear earlier in the order.

Arbitrary increasing rewards may not be sufficient. First, the planner might need to insert costly actions that precede a PCA for achieving some needed precondition. This may lead to a scenario where it is more beneficial to apply the PCAs in a different order or even not to apply a PCA at all, as the costs might be higher than the reward of the PCAs. 
Furthermore, when we consider non-deterministic actions, the planner might find it beneficial to apply a PCA without verifying, to some extent, that the PCA had succeeded, hence possibly violating the order.

Therefore, we need to design the rewards, and especially the differences between the rewards, so that the desired behavior is manifested through them. The planner should find it beneficial to execute the actions in the right order. That is, striving for achieving the effects of the actions in the correct order should increase the expected discounted reward of the single-agent problem.

To compute the rewards, we need to first determine an order over the PCAs we extracted for the agent.
We go over the traces and construct a \emph{directed acyclic graph} of PCAs, forcing a DAG by not adding edges that form cycles.
We start with an empty graph, and go over all traces. Let $l$ be a trace. For each event $e_j$ in the trace, we compute the PCA of its contexed action with respect to agent $i$. We denote this PCA with $pca_j$.
Then, we add a vertex that represents $pca_j$, assuming it doesn't already exist. We add an edge between the vertex corresponding to $pca_{j-1}$ and $pca_j$.
This implies that the current action cannot appear before the one preceding it.
Once we went over all traces and constructed the graph, we compute a topological order
for it, giving us a sequence of PCAs $(\pi_1, a_1),..., (\pi_q, a_q)$, where the first instance corresponds to the action that appeared earliest.

\eliran{Not Implemented - Replace sequence with sets representing the levels of the DAG}
Now, we begin by associating a reward for the last PCA in the sequence, namely the \emph{base} reward, and then iteratively calculate the reward for each preceding one.
The base reward is chosen so that applying the PCA will be beneficial in any possible scenario. We need to avoid a scenario where many costly actions are required to reach a state satisfying the context of the action, so that their total cost surpasses the reward, making it non-beneficial.
Next, we describe the formulas that are used to compute to rewards. In these calculations, we assume that each action has a success probability. Following that, we describe how we can we calculate the rewards without assuming these success probabilities exist.

Let $MaxCost$ be the maximal negative reward that can be achieved in the problem. Let $MinSP$ be the minimal success probability of actions that yield negative rewards. Let $MaxTraceLength$ be the maximal trace length we produced, and let $sp_i$ denote the success probability of action $a_i$. We set $\epsilon > 0$ to some arbitrary positive scalar. We compute $r_q$, the base reward:
\begin{equation}
\label{eqn:rq}
   r_q = \frac{\sum_{i=1}^{MaxTraceLength-1}(\frac{MaxCost}{MinSP} \cdot \gamma^{i-1})}{\gamma^{MaxTraceLength}\cdot sp_q} + \epsilon
\end{equation}%
The numerator is an upper bound for the expected discounted cost we would pay before applying the action. The denominator amplifies that cost to be beneficial when scheduled as the last action in the policy.
\eliran{Harder case - change max trace length to max gap between public actions} 

Equation~\ref{eqn:rq} ensures that the planner will always find the application of $a_q$ in the context $\pi_q$ beneficial, and thus will insert it to the policy.

After calculating the base reward, which is set as the reward of $(\pi_q, a_q)$, we can calculate the rewards associated with the rest of the PCAs in the sequence. To ensure that the order is maintained, using the same intuition as with the base reward, we compute the reward for $(\pi_i, a_i)$ so that the planner will always prefer to apply $a_i$ in $\pi_i$ before applying $a_{i+1}$ in $\pi_{i+1}$. 
An interesting cases arises when the failure probability of $a_i$ is small. Then, after applying the PCA once, it may be better for the planner to assume that the PCA succeeded and apply $a_{i+1}$, without any form of verification, causing the order to break.
\guy{I do not understand why we enforce "verification" - if it is beneficial to observe, the planner should do so, and if not, then not. Such "vitrification" should appear in the team plan to begin with, assuming that they are needed.}
\eliran{Even if the verification appears in the team plan, the artificial rewards we add to the public actions would cause the planner to postpone this verification to a later stage and violate the order, as it would be more beneficial for him to apply the public actions as soon as possible. I hope that later on we would find other ways to design the reward function so that the team plan behavior could be more naturally preserved.}\guy{I disagree. Rewards are given for executing an action at a state. If the planner is unsure of the state, it will observe, if need be.}\eliran{Open Issue - I understand your meaning. The problem is that in the code, we still didn't implement the projection so that it would be only with respect to the variables relevant to the agent, but with respect to the action's subjects. This point is very crucial, though I think that both options (projecting onto subjects and onto relevant variables) would yield good results for now. Let's discuss this as well}
We set $r_i=\gamma \cdot \frac{r_{i+1}}{1-sp_i} \cdot sp_{i+1} + \epsilon$ \eliran{explain further?}
and by that we ensure that regardless of whether $a_i$ fails or not, it would still be beneficial to verify that and apply $a_i$ again if necessary, before applying $a_{i+1}$. We compute $r_i$ until we reach $r_1$, and by that associating a reward to each of the agent's PCAs.

Throughout the calculations, we can associate a success probability to each PCA, given the action success probability. If we do not assume that actions have success probabilities, we can instead incorporate into each PCA, in addition to its projected context and action, the effect it had on the state variables, and then use the probability of that specific effect instead.

\subsection{Policy Adjustment and Alignment}

We run the planner on each of the single agent projections that we generate, constructing a set of single agent policies for the projection. We now adapt the policies to apply to the joint problem. First, the projection of agent $i$ contains actions of other agents that must be removed.

We now need to align the policies in order to synchronize different agent actions to occur in the same order as in the team plan. We need to ensure that an action $a_1$ of one agent that generates a precondition for an action $a_2$ of another agent would be executed before $a_2$. We also need to ensure that collaborative actions are executed at the same time by all participating agents. 

Given a policy graph of agent $i$, we consider the actions of any other agent as \emph{foreign} actions with respect to that policy graph. The policy adjustment process for agent $i$ can be divided into three steps:
\begin{enumerate}
    \item Remove all the \emph{private} actions of agents other than $i$ in the agent $i$'s policy graph.
    \item Insert \emph{no-op} actions into the graph to synchronize it, as much as possible, with other agents' policy graphs. This is the alignment procedure.
    \item Convert all the \emph{public} actions of  agents other than $i$ to no-ops.
    \item Handle potential \emph{collaborative} actions issue that might occur, in which slight unsynchronization leads the collaborating agents to enter an infinite loop.
\end{enumerate}
We now describe the steps in detail.

Starting with the first step, we remove all the foreign private action from the policy graph, and proceed to the second step, which is the alignment procedure.

Each single-agent policy contains only a superficial plan for the other agents, if at all, and therefore its actions need to be aligned with respect to the true policies of other agents. 
\guy{It is not because of the policy graph, but because of the non-deterministic structure}\eliran{If we had a partial policy that is specified as a tree (like you had in the QDECPOMDP I suppose), we could essentially perform a perfect alignment with respect to that specific policy (yet not with respect to the problem itself). The cycles prevent us from performing a perfect alignment even to the policy itself. I understand that with respect to the problem itself, perfect alignment is not possible due to the non-deterministic structure. Do you want to elaborate on both aspects here?}\guy{I disagree - if this is the problem - convert the graph to a tree up to a specified horizon, and align.}\eliran{ok I see, changed that}
Since we are dealing with non-deterministic actions, it is impossible to perform an alignment in which all action are perfectly coordinated.
Instead, we expect the alignment to reduce the required postponement of actions, but currently do not guarantee a minimal waiting time.

We alter the graphs by inserting \emph{no-op} actions, so that corresponding public action vertices will appear in the same depth in all graphs, thus preventing the agents from applying actions prematurely.
To define what corresponding vertices are, we use the notion of an \emph{identifier} of a public action vertex. The identifier of a public action vertex in the policy graph is the
sequence of public actions that appear along the \emph{simple} path from the root to the vertex.
\eliran{Harder case - several identifiers}

The sequence of public actions represents the team-relevant actions that took place prior to the current action, and serves as a summarization of the sequence of events. As the identifier is not agent-specific, it allows us to look for it in other policy graphs and determine which vertices represent similar
sequences of events, namely corresponding vertices.

We now describe the algorithm in detail.
The alignment is an iterative process. We are given with policy graphs $G_1,...,G_n$ which are the output of the last iteration, and initially set to be the raw policy graphs. In each iteration, we align every graph with respect to all other graphs, and stop once convergence is reached. The process is applied iteratively as some alignment might need to propagate through iterations. Algorithm~\ref{} provides the high-level pseudo-code of a single iteration.
\eliran{replace with running example}
For example, consider the case of three ordered agents, where each agent relies on the actions of his preceding agents. In the first alignment iteration, Agent 2 will delay its actions according to Agent 1's no-op requirements. Agent 3 would need to wait for the second alignment iteration before observing the proper time in which Agent 2's actions should be applied, and only then could delay its own actions accordingly.

Recall that in a policy graph, each vertex represents an action and each edge represents an observation. From a vertex that represents a non-sensing action, there is a single outgoing edge that represents the \emph{null-obs} observation. From a vertex representing a sensing action there is one outgoing edge for each possible observation.

We now describe the alignment of a single policy graph. For each public action $a$ in the policy graph $G_i$ of agent $i$, we identify this action in the graphs of other agents.
Then, we postpone $a$ by inserting no-op actions, so that $a$ occurs at the same time in all policy graphs, given the maximal path to $a$ in all policy graphs.

We traverse $G_i$ using breadth-first search. For a vertex $v\in G_i$ representing a public action of agent $i$, we extract the $identifier$ of $v$. Having the identifier of $v$, we traverse all other graphs $G_j$, and in each we search for a vertex $v'$ that matches the identifier. $v'$ matches the identifier if there's a simple path from the root of $G_j$ to it that contains the identifier.\eliran{Harder case - exhaustive match, more precise}
If we found such $v'$ that matches $v$, we set the {\em no-op} amount of $G_j$ that is required for $v$, denoted by $m_j$, to be the length of the \emph{longest simple path} from the root to $v'$. Else we set $m_j$ to 0.
Once we calculated $m_j$ for all $j\neq i$, we need to determine the final amount of {\em no-ops} added to $v$ in $G_i$. First, we consider $max_{j\neq i}\{m_j\}$. However, the maximal requirement does not take into account {\em no-ops} that were already added to preceding vertices in $G_i$, or the depth of $v$ itself, and must be
corrected by subtracting from it a \emph{compensation term} which is the sum of the \emph{longest simple path} from the root to $v$ and the \emph{minimal} {\em no-op} amount added to any of the predecessors of $v$. Overall, we set the {\em no-op} amount of $v$ to be $max_{j\neq i}\{m_j\} - \mathit{CompensationTerm}$.

Having computed the  {\em no-op} amount for each of the public action vertices in $G_i$, we insert them to it by appending the required number of consecutive {\em no-op} vertices \emph{prior} to the public action vertex, thus postponing it.
As we mentioned, in each iteration of the algorithm we apply this procedure to all policy graphs, one by one, and halt upon convergence.

\eliran{Not Implemented - Running with concurrent BFSs}

After the alignment procedure has converged, we proceed to the the third step. Unlike the case of \emph{private} actions that were previously eliminated, \emph{public} actions of other agents were left in the graphs to guide the alignment process. Following the alignment, we convert them into \emph{no-ops}, and by that make the agent wait while other agents are performing their public actions.

Finally, in the fourth step, we handle the problem of a potential ``livelock''
between \emph{collaborative} actions. Consider a scenario where two agents need to perform a non-deterministic collaborative action whose effect can be directly sensed. The action is costly so they must apply it the minimal necessary number of times. To do so, following every application, they perform a \emph{sensing} action that senses the effect of that collaborative action.
Given non-deterministic actions -- causing the alignment to be imperfect -- there might be scenarios in which the agents become unsynchronized. Then, they might be applying the collaborative and sensing actions in an alternating manner, where one agent performs the collaborative action while the other performs the sensing action, causing them to enter a livelock. To handle that, given a collaborative action with $n$ collaborating agents, we modify the graph so that every collaborative action that is part of a cycle is repeated by every agent for $n$ times instead of just once. This way, a livelock can never occur.

\begin{algorithm}
\caption{Alignment Iteration}
\begin{algorithmic}[tbph]
\State Input: PolicyGraphs $G_1, ..., G_M$
\For{$G_i,  i\in\{1, ..., M\}$}
	\State {$\mathit{NoopsReqs} \gets \mathit{VertexToIntMapping}$}
      \State {$\mathit{CurrBFS} \gets \Call{BFS}{G_i}$}
      \While {$\mathit{CurrBFS.queue}$ is not empty}
	\State {$v \gets \mathit{CurrBFS.queue.pop}$}
	\State {$a \gets v.action$}
	\If {$a$ is public action}
	\State {$\mathit{identifier} \gets \Call{GetIdentifier}{v}$}
	\State {$\mathit{MaxNoop} \gets 0$}
	\For {$G_j,  j\in\{1, ..., M\}\setminus\{i\}$}
	\State {$\mathit{CurrNoop} \gets \Call{NoopReq}{G_j, \mathit{identifier}}$}
	\State {$\mathit{MaxNoop} \gets max(\mathit{MaxNoop}, \mathit{CurrNoop})$}
	\EndFor
	\State {$\mathit{NoopsReqs}[v] \gets \mathit{MaxNoop} - \mathit{CompensationTerm}$}
	\EndIf
	\EndWhile
	\State {$G_i' \gets \Call{AddNoops}{G_i, \mathit{NoopsReqs}}$}
\EndFor
\State {return $G_1', ..., G_M'$}
\end{algorithmic}
\end{algorithm}

\section{Empirical Study}
We provide experimental results while focusing on large planning horizons on large scale problems.
The experiments were conducted on a variation of Cooperative Box Pushing, which we specify in the next subsection.
We compare our algorithm with two Dec-POMDP solvers, GMAA-ICE \cite{} and JESP \cite{}, using MADP-tools. \cite{}.
We evaluate FDMAP on a Windows machine with 4 cores of and 8GB of memory.
We evaluate both GMMA-ICE and JESP on a Linux machine with 4 cores and 8GB of memory.


\subsection{\cbp}
We present a variation of the well known Cooperative Box Pushing domain, in which the need shifts from a good local policy for each agent that relies mostly on short responses to observations, to a decentralized policy that relies on good coordination between agents and requires larger planning horizons to be accomplished. A grid tile can contain any number of agents and boxes, and each agent has both move and push actions in each of the four possible directions, as well as a sensing action for a box and a no-op action.
An agent (or agents), can push a box only when positioned in the same grid with it.
All action except for the push actions are deterministic.
Light boxes can be pushed by a single agent, while heavy boxes can only be pushed by the collaborative push of two agents.
The goal of the agents is to move the boxes to the target tile, located at the upper left corner of the grid.
Initially, each box can appear in either the target tile (making it invaluable to handle) or the lower right tile, with equal probability.
Each action has a cost (except for no-op), and a reward is given for pushing a box to the target tile. The costs are 5 for move, 1 for sense, 30 for push, 20 for collaborative push (per agent), and the reward is 500. In addition, there's a penalty of 500 for pushing a box \emph{out} of the target tile to avoid abuse.
The rewards were chosen so that it would be more beneficial to sense rather than blindly push.
A domain instance of $m$ tiles with $n$ agents $l$ light boxes and $h$ heavy boxes, has $m^{n+l+h}$ states, $(5\cdot(k+1)+4h\cdot(n-1))^n$ actions and $3^n$ observations. 

\subsection{Domain Configurations}
We demonstrate results on six configurations of \cbp. Each box can equally start at either the top-left or bottom-lower corner, so we only specify the initial location of each agent:
\begin{enumerate}
    \item 2 by 2 grid, 1 light box and 2 agents located at tiles (1,2) and (2,2)
    \item 2 by 2 grid, 2 light boxes and 2 agents located at tiles (1,2) and (2,2)
    \item 2 by 2 grid, 3 light boxes and 2 agents located at tiles (1,2) and (2,2)
    \item 2 by 3 grid, 2 light boxes and 3 agents located at tiles (1,2), (1,3) and (2,3)
    \item 2 by 3 grid, 3 light boxes and 3 agents located at tiles (1,2), (1,3) and (2,3)
    \item 3 by 3 grid, 1 light box and 2 heavy boxes, 2 agents located at tiles (1,3) and (3,1). In this configuration we also increase the reward and penalty for pushing a box in and out of the target tile to 1000, so that it will remain beneficial for the agents to push all boxes.
\end{enumerate}

\begin{center}
    \begin{tabular}{||c|c|c|c|c||}
         \hline
         \multicolumn{5}{||c||}{\cbp \ Configurations} \\
         \hline
         Index & Name & $|S|$ & $|A|$ & $|\Omega| $ \\ 
         \hline
         1 & BP-22201 & 64 & 100 & 9 \\
         \hline
         2 & BP-22202 & 256 & 225 & 9 \\
         \hline
         3 & BP-22203 & 1024 & 400 & 9 \\
         \hline
         4 & BP-23302 & 7776 & 3375 & 27 \\ 
         \hline
         5 & BP-23303 & 46656 & 8000 & 27 \\
         \hline
         6 & BP-33221 & 59049 & 324 & 9 \\
         \hline
    \end{tabular}
\end{center}

\subsection{Comparison Setting}
We compared FDMAP with GMAA-ICE and DP-JESP. For GMAA-ICE and DP-JESP, the horizon stands for the planning horizon. Since FDMAP calculates a policy for an infinite horizon, the horizon parameter specifies the number of steps the policy was evaluated under.
The value metric for GMAA-ICE and DP-JESP specifies the policy value. In FDMAP we measured the average discounted accumulated reward of 1000 simulations, where in each simulation the policy was run for the number of steps specified in the horizon column. The last horizon prefixed with Max is the maximal horizon measured for reaching the goal state in all simulations.

GMAA-ICE and DP-JESP were given with 4000 seconds limit to solve each $\langle\textit{configuration, horizon}\rangle$ pair, FDMAP was given with the same amount to solve the problem once and output an infinite horizon solution.
The times shown for GMAA-ICE do not include the problem loading. FDMAP times do not include writing SARSOP policies and graphs to disk, as they are highly dependent on hardware quality and can effectively remain in memory throughout the whole process.
% \eliran{explain that the process is done in separated steps and should be unified?}\ronen{Not essential now. EVentually, you should have code that is usable by others.}

In both GMAA-ICE and DP-JESP, configurations BP-23302, BP-23303 and BP-33221 could not be parsed in the time given. Therefore we provide comparisons only for the first three configurations, BP22201 and BP22202. For the latter three we provide the maximal and average number of steps it took FDMAP to reach a goal state - excluding instances in which the initial state is a goal state.

In DP-JESP, X marks a general timeout. In GMAA-ICE we mark two different timeout options - FF refers to failure of finding a full policy for the required horizon, where FH refers to an earlier stage timeout when computing the heuristic function.

\begin{center}
    \begin{tabular}{||c|c|c|c|c|c|c||}
         \hline
         \multicolumn{7}{||c||}{BP-22201} \\
         \hline
         Horizon & \multicolumn{2}{|c|}{DP-JESP} & \multicolumn{2}{|c|}{GMAA-ICE} & \multicolumn{2}{|c||}{FDMAP}\\ 
         \hline
         & Time & Value & Time & Value & Time & Value \\
         \hline
         3 & 14.11 & 0 & 2.08 & 0 & 1.94 & -22.14 \\
         \hline
         4 & 1279.06 & 80 & 368.54 & 76.64 & " & -22.54 \\
         \hline
         5 & X & - & 366.58 & 108.74 & " & -30.94 \\ 
         \hline
         6 & X & - & FF & - & " & 108.96 \\
         \hline
         7 & X & - & FF & - & " & 141.245 \\
         \hline
         Max=15 & X & - & FF & - & " & 192.546 \\
         \hline
    \end{tabular}
\end{center}

\begin{center}
    \begin{tabular}{||c|c|c|c|c|c|c||}
         \hline
         \multicolumn{7}{||c||}{BP-22202} \\
         \hline
         Horizon & \multicolumn{2}{|c|}{DP-JESP} & \multicolumn{2}{|c|}{GMAA-ICE} & \multicolumn{2}{|c||}{FDMAP}\\ 
         \hline
         & Time & Value & Time & Value & Time & Value \\
         \hline
         3 & 262.28 & 0 & 50.16 & 0 & 4.37 & -25.36 \\
         \hline
         4 & X & - & FF & - & " & -33.97 \\
         \hline
         5 & X & - & FF & - & " & -41.87 \\ 
         \hline
         6 & X & - & FF & - & " & 104.83 \\
         \hline
         7 & X & - & FF & - & " & 200.21 \\
         \hline
         Max=24 & X & - & FF & - & " & 357.23 \\
         \hline
    \end{tabular}
\end{center}

\begin{center}
    \begin{tabular}{||c|c|c|c|c|c|c||}
         \hline
         \multicolumn{7}{||c||}{BP-22203} \\
         \hline
         Horizon & \multicolumn{2}{|c|}{DP-JESP} & \multicolumn{2}{|c|}{GMAA-ICE} & \multicolumn{2}{|c||}{FDMAP}\\ 
         \hline
         & Time & Value & Time & Value & Time & Value \\
         \hline
         3 & 3146.58 & 0 & 1856.81 & 0 & 27.08 & -27.51 \\
         \hline
         4 & X & - & FF & - & " & -29.13 \\
         \hline
         5 & X & - & FF & - & " & -38.04 \\ 
         \hline
         6 & X & - & FH & - & " & 96.29 \\
         \hline
         7 & X & - & FH & - & " & 121.68 \\
         \hline
         Max=33 & X &  & FH & - & " & 541.35 \\
         \hline
    \end{tabular}
\end{center}

\begin{center}
    \begin{tabular}{||c|c|c|c|c||}
         \hline
         \multicolumn{5}{||c||}{FDMAP}\\ 
         \hline
         Problem & MaxSteps & AvgSteps & Time & Value \\
         \hline
         BP-23302 & 36 & 16 & 106.4 & 250.79 \\
         \hline
         BP-23303 & 65 & 25 & 1832.55 & 351.19 \\
         \hline
         BP-33221 & 126 & 41 & 3150.02 & 244.91 \\
         \hline
    \end{tabular}
\end{center}
\section{Related Work}
QDec-POMDPs~\cite{} \eliran{citation} are qualitative version of Dec-POMDPS that tackle a conceptually simpler, more structured, model. In QDec-POMDPs 
non-determinism replaces stochastic uncertainty. The model is factored (i.e., described at the level of state variables rather than states), and actions 
are described using preconditions and non-determinstic effects. Although QDec-POMDPs are also NEXT-Time hard, recent work in the area that leverages heuristic-search planners, has been able to scale up to much larger domains (e.g., box pushing on a grid of size 24,  12 agents and 12 boxes, implying a state space of $24^{24}$)
albeit, under the assumptions that actions are deterministic.

Dec-POMDP algorithms.

\section{Conclusion}

\section{Future Research}
\eliran{its more of a sketch for now so we don't forget what we've talked about}
There are two directions in which the solver can be improved -  scalability and solution quality.
The use of online planners instead of offline ones can greatly improve the scale of solvable problems. The changes in terms of algorithm architecture are minor, as we merely need to be able to produce the single agent policy graphs using an online solver. \ronen{In fact, it seems we could use an RL algorithm here to generate a policy for each agent, as we can simulate as many traces as we wish.
In fact, we could use the RL algorithm to solve the team POMDP. This would generate the needed traces as well. We would learn incrementally
both the team solution and the single-agent solution. Using this idea, we could probably scale up to very large problems. In fact, we can use this
approach to do MA RL. If we can do the projections. What we need is a simulator that let's us control all the agents at once.}
In terms of solution quality, we would want to use more principled methods of reward shaping, that come from the worlds of reinforcement learning, in order to define the reward heuristic. We would want to achieve both the properties we already achieved using our current heuristic, and to still be able to optimize the expected discounted reward of the problem.
Also, in order to fully achieve that, the alignment algorithm will need to include some form of confidence aspect, where we no longer ignore cycles but rather look at non-simple paths and try to increase the certainty about the world's state. 

\bibliography{Bibliography-File}
\bibliographystyle{aaai}
\end{document}