\def\year{2018}\relax
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai18}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage[ruled,noend,linesnumbered]{algorithm2e}
\DontPrintSemicolon




\newtheorem{theorem}{Theorem}
%\newtheorem\star {genthm}{Reminder of Theorem~\ref{thr:general}}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition}
\newtheorem{definition}{Definition}


\newtheorem{example}{Example}

\newcommand{\mm}{1,\ldots,m}
\newcommand{\mmset}{\{1,\ldots,m\}}
\newcommand{\wsum}{W_{\mathrm{sum}}}
\newcommand{\CLP}{\mathrm{CLP}}
\newcommand{\UCM}{\mathrm{UCM}}
\newcommand{\score}{\mathrm{score}}
\newcommand{\scoretype}{\mathrm{ScoreIndex}}
\newcommand{\coeff}{\mathrm{coeff}}
\newcommand{\DFT}{\mathrm{DFT}}
\newcommand{\poly}{\mathrm{poly}}
\newcommand{\degg}{\mathrm{deg}}
\newcommand{\SP}{\mathrm{span}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\FG}{{\FF[G]}}
\newcommand{\GF}{\mathrm{GF}}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand* {\rev}{\textsc{reverse}{}}
\newcommand{\revv}{\mathrm{rev}}
\newcommand{\barG}{\bar{G}}
\newcommand{\barV}{\bar{V}}
\newcommand{\barE}{\bar{E}}
\newcommand{\barW}{\bar{w}}
\newcommand{\barN}{\bar{n}}
\newcommand{\barC}{\bar{c}}
\newcommand{\barCC}{\bar{C}}
\newcommand{\term}{\mathrm{term}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\weff}{w_{\mathrm{eff}}}
\newcommand{\LPS}{\textsc{LP-solve}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\RR}{\mathcal{R}}
\newcommand{\LL}{\mathcal{L}}

\newcommand\vecc{\mathbf}
\newcommand\vecgreek{\bm}
\newcommand{\veca}{\vecgreek{\alpha}}
\newcommand{\vecb}{\vecgreek{\beta}}
\newcommand{\vecs}{\vecgreek{\sigma}}
\newcommand{\Ra}{\mathcal{R}_{\veca}}
\newcommand{\NP}{\mathsf{NP}} 
\newcommand{\LPCM}{\mathsf{LP}_{\mathrm{CM}}}
\newcommand{\LPB}{\mathsf{LP}_{\mathrm{B}}}
\newcommand{\EE}{\mathbb{E}} 


\pdfinfo{
/Title (Approximating Bribery in Scoring Rules)
/Author (Orgad Keller, Avinatan Hassidim, Noam Hazon)}
\setcounter{secnumdepth}{0}

\begin{document}
\title{Approximating Bribery in Scoring Rules}
\author{Orgad Keller\\
Department of Computer Science\\
Bar-Ilan University\\ 
Israel\\
\texttt{orgad.keller@gmail.com}\\
\And Avinatan Hassidim\\
Department of Computer Science\\
Bar-Ilan University\\
Israel\\
\texttt{avinatan@cs.biu.ac.il}\\ 
\And Noam Hazon\\
Department of Computer Science\\
Ariel University\\
Israel\\
\texttt{noamh@ariel.ac.il}}

% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\maketitle
\begin{abstract}
The classic bribery problem is to find a minimal subset of voters who need to change their vote to make some preferred candidate win. We find an approximate solution for this problem for a broad family of scoring rules (which includes Borda and $t$-approval), in the following sense: if there is a strategy which requires bribing $k$ voters, we efficiently find a strategy which requires bribing at most $k+\widetilde{O}(\sqrt{k})$ voters.

Our algorithm is based on a randomized reduction from bribery to coalitional manipulation (UCM). To solve the UCM problem, we apply the Birkhoff-von Neumann (BvN) decomposition to a fractional manipulation matrix. This allows us to  limit the size of the possible ballot search space reducing it from exponential to polynomial, while still obtaining good approximation guarantees. 	Finding the optimal solution in the truncated search space yields a new algorithm for UCM, which is of independent interest.
\end{abstract}

\section{Introduction}	
%How many voters must one bribe in order to make his favorite candidate win an election? 
We study the popular preferential model for elections, where each of the agents (also:  \emph{voters})  \emph{ranks} the candidates. Then, some function---known as a \emph{voting rule} or a protocol---is applied on the voter rankings in order to decide on the winner(s). Ideally, we would like the voters to be truthful: that is, that the rankings submitted by each voter would correspond to his real preference over the alternatives. When the voters might have the incentive to do otherwise, we refer to the voting protocol as \emph{manipulable}. Unfortunately, a celebrated result in social choice theory achieved independently by Gibbard and Satterthwaite~\cite{gibbard1973manipulation,satterthwaite1975strategy} shows that when the number of candidates is at least $3$, any reasonable voting rule  is manipulable. 

Manipulation can take several different forms. In \emph{unweighted  coalitional manipulation} (UCM), a set of voters (hereafter the \emph{manipulators}) sharing a preference for a candidate $p$  try to coordinate their voting so that $p$ will win the election. This is assuming that all other voters are truthful and that their preferences are known. 
%An equivalent definition is that a central interested party is willing to convince other people to join the election and vote according to its orders.
%An equivalent scenario, is that given an upcoming election, where all participating voters are known to be truthful, a centralized party of interest is willing to convince additional people that were not planning on voting to join the election and vote using a strategy supplied by the party of interest. This formulation models the event of canvassing, especially when the targeted audience are people that were not planning on exercising their right to vote. 
%In \emph{constructive control by adding voters} (CCAV), the interested party can add voters to en election in order to make  $p$ win, however, as opposed to UCM, the set of extra voters (and specifically, their strategies) to choose from is specified as part of the input. In \emph{constructive control by deleting voters} (CCDV), the interested party can delete participating voters  in order to make  $p$ win.
In \emph{bribery}, an interested party is willing to pay some of the voters to change their vote into a given ballot, such that $p$ will prevail. 
%Thus, it is a combination of CCDV and UCM, as---under some of the cost models---bribing a set of voters is equivalent to deleting them and then replacing them with the same number of manipulators. 
In both cases, the goal is to make $p$ win using minimal resources, for example, the number of manipulators or bribed voters, respectively. 
%In addition, it is worth mentioning there are destructive variants in which the goal is not to promote a preferred candidate $p$, but to prevent the currently leading candidate from winning, and there are also weighted variants
% (WCM, WCCAV, WCCDV, weighted-bribery, respectively) 
%in which different voters have different weights to their ballot.

The importance of the aforementioned manipulation forms ranges beyond their immediate definition. First, various manipulation forms model various aspects of \emph{campaign management}:  which electorate or demographic should a candidate target during his campaign, and where should his or her campaign manager direct the campaign funds. In recent years, the significance of targeting specific electorates in election campaigns had skyrocketed,
see~\cite{hillary} for one example out of many. In this context, bribery models the act of targeting prospective voters and convincing them to change their vote, and UCM models the act of urging people not planning on voting to exercise their right to vote (when it is done by an interested third party supporting one candidate).  


Second, they can also be seen as means by which we measure \emph{how well} did a candidate do in the election, or equivalently how far away was her from winning it. They achieve so by calculating the effort needed to promote her enough in order to win~\cite{DBLP:conf/atal/FaliszewskiST17}. In some way, such measures are more robust compared to intrinsic measures like the candidate score in the election. 

In response to the Gibbard-Satterthwaite theorem, extensive research on the computational aspects of social choice focused on providing some hope against manipulation
%in the  form of  proving that for many voting rules, while manipulative strategies exist, 
by showing that in many scenarios, finding a manipulative strategy is computationally hard. In such cases research naturally shifts its focus to  devising  approximations and heuristics.

In this paper, we shall focus on one of the most important families of voting rules, known as \emph{positional scoring rules}, or scoring rules for short. For a given \emph{score vector} $\veca= (\alpha_{0},\ldots,\alpha_m)$, where $\alpha_{0}\leq \cdots \leq \alpha_m$ and $m+1$ is the number of candidates, a scoring rule $\Ra$ is a voting rule where each voter awards $\alpha_m,\ldots,\alpha_{0}$ points to the candidates he ranked in places $1,2,\ldots,m+1$, respectively. 
The winners are the candidates with maximum aggregate score. 
%In the weighted setting, each manipulator/voter $v$ has an associated integer weight $w_{v}$, with the effect that a score $\alpha_{j}$ awarded by him to a candidate is multiplied by $w_v$. 
Popular cases of scoring rules are the Plurality, Veto, and Borda voting rules.

A string of results researched both the hardness and approximability of UCM for various voting rules, and in particular scoring rules. However, is seems that the equivalent landscape for bribery is lacking: while several results had shown that Borda-Bribery is $\NP$-hard, and that the same holds even for the relatively simple $t$-approval-bribery~\cite{DBLP:journals/jair/FaliszewskiHH09,DBLP:conf/aaai/BrelsfordFHSS08,lin2012solving,DBLP:conf/aaai/BredereckFNT16,DBLP:journals/iandc/BredereckCFNN16}, only little work was done on approximating bribery~\cite{DBLP:conf/atal/Faliszewski08,DBLP:conf/wine/ElkindF10,DBLP:journals/iandc/BredereckCFNN16}. We aim at filling this gap, by providing  results which target the most classic bribery model, where the goal is to minimize the number of bribed voters. Specifically we show that for many scoring rules:
\begin{quotation}
	\emph{If there exists a strategy making a preferred candidate $p$ win by bribing $k$ voters, then we can \emph{efficiently} find a strategy making her win while bribing additional $\widetilde{O}(\sqrt{k})$ voters.\footnote{As common in the literature, the $\widetilde{O}$ notation discards poly-logarithmic factors.}}
\end{quotation}

Our additive approximation can be seen as a $(1+o(1))$-multiplicative approximation. It should be noted that the non-trivial feat was to provide a guarantee tighter than any constant-factor approximation. To provide an intuition as to why a constant-factor approximation is relatively easy, let us focus on one relatively simple rule, namely $t$-approval. 
%To make the exposition even easier, assume an even $m$ and $t = m/2$. 
If $k$ is the optimal number of manipulators needed, this means that the gap between any candidate and $p$ was at most $2k$, as bribing a voter can decrease the gap between some candidate and $p$ by at most $2$. Now imagine the following strategy: iteratively, pick a voter that did not vote for $p$ (if no such voter exists, then $p$ is already winning), and bribe him to transfer one point from any candidate he currently supports, to $p$. This decreases the aforementioned gap by at least $1$, and therefore at most $2k$ bribed voters are required by this procedure. Thus, we have just described a $2$-multiplicative approximation to $t$-approval-bribery.
 


We provide a non-trivial approximation to $\Ra$-Bribery for large families of scoring rules (encompassing  well-known ones, like $t$-approval variants and Borda). Our methods are based on  relaxed linear programs that are transformed to a valid solution (i.e., bribery strategy) using a seminal result from the interplay of combinatorics and geometry, namely the \emph{Birkhoff-von Neumann (BvN) decomposition}~\cite{birkhoff1946tres,von1953certain,konig2001theorie}, and specifically the constructive proof to its related theorem. We use them as a tool to reduce the size of the valid strategy search space from exponential to polynomial. It thus provides an important insight on the underlying combinatorial properties of manipulation under scoring rules, and is interesting in its own right.

En route to providing our approximation for $\Ra$-Bribery, we also supply new approximation results for  $\Ra$-UCM. Our results target both approximation objectives appearing in the literature, namely minimizing the number of required manipulators (see~\cite{DBLP:journals/ai/ZuckermanPR09,DBLP:conf/sigecom/XiaCP10}), and minimizing the score margin between the highest non-preferred candidate and $p$ (see~\cite{DBLP:conf/aaai/BrelsfordFHSS08}).



%This decomposition, along with its related theorem and constructive proof, enables us to transform the search space of valid voting strategies from exponential to polynomial.



%Two main objectives for approximation appear in the literature. The first focuses on minimizing the \emph{margin} between the highest non-preferred candidate and $p$, or some additive function thereof\footnote{For example, the score of the highest non-preferred candidate, or the difference in margin with and without the manipulation} which also attains its minimum when the margin is minimized (and thus are equivalent from the optimization perspective). It was first used in some older results; for example Brelsford et al.~\cite{DBLP:conf/aaai/BrelsfordFHSS08} provide approximation for $\Ra$-WCM for the limited case the number of candidates is constant. Then recently, Keller et al.~\cite{DBLP:conf/atal/KellerHH17} have removed the requirement  on the number of candidates,  providing a randomized algorithm providing $O(W \max_i \abs{\alpha_{i+\beta} - \alpha_i})$ additive approximation to the margin, where $\beta= \widetilde{O}(\sqrt{m})$ and $W$ is the sum of manipulator weights. Notice that in this example and others, $W$ translates to $k$, number of manipulators/deleted voters/bribed voters in the unweighted case. As an  example, for Borda-UCM, their approximation translates to $\widetilde{O}(k\sqrt{m})$.


%This paper continues both lines of research simultaneously. For $\Ra$-UCM given any $\veca$, we provide an $\widetilde{O}(\alpha_m \sqrt{k})$ additive approximation to the minimum margin. To achieve that, we use a seminal result from the interplay of combinatorics and geometry, the Birkhoff-von Neumann decomposition~\cite{birkhoff1946tres,von1953certain,konig2001theorie}. We then provide an $\widetilde{O}(\sqrt{k})$ approximation to $\Ra$-bribery for broad classes of scoring rules we call \emph{constant scoring rules} and \emph{non-concentrated scoring rules}. In constant scoring rules, as the name suggests, $\alpha_{m}$ is constant. In  non-concentrated scoring rules, the average score $\bar{\alpha}=\sum_{i=0}^m \alpha_{i}$ is at most $(1-\epsilon)\alpha_m$, for some constant $\epsilon>0$.
%
%We note that every scoring rule we deem as \emph{reasonable} is contained in one of the above classes, and that our solution for $\Ra$-Bribery  uses our solution for $\Ra$-UCM as a subprocedure. 
%all $\veca$ where either $\alpha_{m}$ is constant or that that the average score $\bar{\alpha}=\sum_{j=0}^{m}\alpha_i / m$ is at most $(1-\epsilon)\alpha_m$ for some constant  

\subsection{Our Results and Contributions}
We focus on two families of scoring rules, namely \emph{constant} and \emph{non-concentrated} scoring rules, defined as follows. A scoring rule $\Ra$ is called \emph{constant} if $\alpha_m = O(1)$. A scoring rule is called \emph{non-concentrated} if $\bar{\alpha} \leq (1-\epsilon)\alpha_m$, for some constant $\epsilon > 0$,
where $\bar{\alpha} = 1/(m+1) \sum_{j=0}^{m} \alpha_j$ is the average of the values in $\veca$.
Our results can be summarized by the following theorems. For $\Ra$-Bribery:
\begin{theorem}\label{thr:brib}
	In $\Ra$-Bribery under constant or non-concentrated $\Ra$,  let $k$ be the minimum number of voters needed to be bribed in order to make $p$ win. Then	there exists a  polynomial-time randomized algorithm using at most $k+\widetilde{O}(\sqrt{k})$-voters, with exponentially-small failure probability.
\end{theorem}
This theorem immediately yields the following:
\begin{corollary}
	In bribery under Borda, $t$-approval, plurality, or veto, let $k$ be the minimum number of voters to be bribed in order to make $p$ win. Then	there exists a  polynomial-time randomized algorithm using at most $k+\widetilde{O}(\sqrt{k})$-voters, with exponentially-small failure probability.
\end{corollary}

The above theorem shall use the constructive proof of the following  algorithm for $\Ra$-UCM as a subprocedure:
\begin{theorem}\label{thr:UCM-margin}
	For $\Ra$-UCM under any score vector $\veca$, let $k$ be the number of given manipulators. There exists a  polynomial-time randomized algorithm  yielding an $\widetilde{O}(\alpha_m \sqrt{k})$-additive-approximation to the margin-minimization objective, with exponentially-small failure probability.
\end{theorem}



Moving to the objective of minimizing manipulators, the above theorem also enables the following result which---for the specific case of non-concentrated scoring rules---improves the $m-2$ additive approximation given in~\cite{DBLP:conf/sigecom/XiaCP10} when $k = o(m)$:
\begin{theorem}\label{thr:UCM-number}
	For $\Ra$-UCM under non-concentrated $\Ra$,  let $k$ be the minimum number of manipulators required to make $p$ win. Then there exists a  polynomial-time randomized algorithm using at most $k+\widetilde{O}(\sqrt{k})$ manipulators, with exponentially-small failure probability.
\end{theorem}

Aside for the above results, we provide tighter analysis for Borda-UCM, based on a result in~\cite{DBLP:journals/ai/ZuckermanPR09}:
\begin{theorem}\label{thr:zuck}
	The algorithm in~\cite{DBLP:journals/ai/ZuckermanPR09} for Borda-UCM, yields an $(m-1)/2$-additive-approximation to the objective of margin-minimization.
\end{theorem}
For brevity, we defer the proof of Theorem~\ref{thr:zuck} to the full version of this paper. It should be noted that while we prove that their algorithm provides tighter analysis for Borda, an equivalent property for \emph{all}  scoring rules is not known to hold. 

%\begin{theorem}
%	There exists a  polynomial-time exact algorithm for $t$-Approval-UCM which finds the  strategy optimizing  the objective of margin-minimization.
%\end{theorem}

\subsection{Related Work}
\subsubsection{Bribery.}
In the standard bribery model (every voter has a unit price), $t$-approval-bribery is $\NP$-hard~\cite{lin2012solving} except for some small values of $t$ for which $t$-approval-bribery and $t$-veto-bribery become easy~\cite{DBLP:journals/jair/FaliszewskiHH09}. Borda-bribery is $\NP$-hard as well~\cite{DBLP:conf/aaai/BrelsfordFHSS08}. 
%Discussing other models like swap- and shift-bribery~\cite{DBLP:conf/sagt/ElkindFS09} is beyond the scope of this section.

Little work was done on approximating bribery. Of interest is an FPTAS for plurality-weighted-\$bribery~\cite{DBLP:conf/atal/Faliszewski08} and other work
targeting other models like  shift-bribery~\cite{DBLP:conf/sagt/ElkindFS09,DBLP:conf/wine/ElkindF10,DBLP:journals/iandc/BredereckCFNN16}. See survey in~\cite{DBLP:reference/choice/FaliszewskiR16} for more details. 



\subsubsection{Coalitional Manipulation.} When $m$ is bounded, $\Ra$-UCM is always easy~\cite{DBLP:journals/jacm/ConitzerSL07}. When it is not, it was shown that $t$-approval (and thus plurality and veto) are still easy~\cite{DBLP:journals/ai/ZuckermanPR09,lin2012solving}, and then recently, that every scoring rule where $\veca$ comprises of a constant number of coefficients is as well~\cite{DBLP:conf/ecai/HemaspaandraS16}.
Other cases are not necessarily easy; in particular Borda is $\NP$-hard~\cite{DBLP:conf/ijcai/BetzlerNW11,DBLP:journals/ai/DaviesKNWX14}. 

Its weighted counterpart $\Ra$-WCM (where every voter has an associated weight by which his ballot is multiplied) is always hard for $m\geq 3$, besides the single exception of the plurality scoring rule~\cite{DBLP:journals/jacm/ConitzerSL07,DBLP:journals/jcss/HemaspaandraH07,DBLP:journals/jair/ProcacciaR07}. 
%Notice that these results predate the aforementioned proof that Borda-UCM is hard.

As for  approximations, two main objectives for approximation appear in the literature. The first focuses on minimizing the \emph{margin} between the highest non-preferred candidate and $p$, or some additive function thereof\footnote{For example, the score of the highest non-preferred candidate, or the difference in margin with and without the manipulation.} which also attains its minimum when the margin is minimized (and thus is equivalent from the optimization standpoint). It was used in some older results, but also garnered recent interest: Brelsford et al.~\cite{DBLP:conf/aaai/BrelsfordFHSS08} provided approximation for $\Ra$-WCM for the limited case the number of candidates is constant.
%: provide an FPTAS to the maximum score of a non-preferred candidate for $\Ra$-WCM, and then in some recent ones:, 
Keller et al.~\cite{DBLP:conf/atal/KellerHH17} have removed the requirement  on the number of candidates,  and showed a randomized algorithm providing $O(k \max_i \abs{\alpha_{i+\beta} - \alpha_i})$ additive approximation to the margin, where $\beta= \widetilde{O}(\sqrt{m})$ and $k$ is the number of manipulators.
%\footnote{In this example and others, $k$ translates to $W$, the sum of weights of number of manipulators in the weighted case. As an  example, for Borda-WCM, their approximation translates to $\widetilde{O}(W\sqrt{m})$.}

The second approximation objective revolves around minimizing the number of voters involved in the manipulation. For UCM it is the number of added manipulators;
%and CCAV, it is the number of added manipulation. For CCDV and 
for bribery, it is the number of bribed voters. Zuckerman et al.~\cite{DBLP:journals/ai/ZuckermanPR09} show that for Borda-UCM, their greedy algorithm, called \rev{} elsewhere~\cite{DBLP:journals/ai/DaviesKNWX14}, provides an additive $+1$ approximation for this objective. 
%The equivalent algorithm for Borda-WCM might have to use extra manipulators besides the $k$ that are provided as input. In that case, the extra manipulator sum of weights needs to be at least the maximum weight of an ``original'' given manipulator. 
Xia et al.~\cite{DBLP:conf/sigecom/XiaCP10} provide an $m-2$ additive approximation for $\Ra$-UCM. 
%In the case of $\Ra$-WCM, each of the extra manipulators should have weight at least half the maximum weight of an ``original'' given manipulator.
%For Veto, their $\rev{}$ algorithm finds the exact solution. Lin~\cite{lin2012solving} mentions that it provides an exact solution to any $t$-approval instance\footnote{However, he attributes this to their paper. We could not find the proof there, besides the Veto case. A more general proof should be very similar.}. 


%Brelsford et al.~\cite{DBLP:conf/aaai/BrelsfordFHSS08} provide an FPTAS to the maximum score of a non-preferred candidate for $\Ra$-WCM. Zuckerman et al.~\cite{DBLP:journals/ai/ZuckermanPR09} provide  an additive $+1$ approximation for the objective of finding the minimum number of manipulators needed for Borda-UCM. The equivalent algorithm for Borda-WCM might have to use extra manipulators besides the $k$ that are provided as input. In that case, the extra manipulator sum of weights needs to be at least the maximum weight of an ``original'' given manipulator. Xia et al.~\cite{DBLP:conf/sigecom/XiaCP10} provide a $m-2$ additive approximation for $\Ra-UCM$. In the case of $\Ra$-WCM, each of the extra manipulators should have weight at least half the maximum weight of an ``original'' given manipulator.

%Two main objectives for approximation appear in the literature. The first focuses on minimizing the \emph{margin} between the highest non-preferred candidate and $p$, or some additive function thereof\footnote{For example, the score of the highest non-preferred candidate, or the difference in margin with and without the manipulation} which also attains its minimum when the margin is minimized (and thus are equivalent from the optimization perspective). It was first used in some older results; for example Brelsford et al.~\cite{DBLP:conf/aaai/BrelsfordFHSS08} provide approximation for $\Ra$-WCM for the limited case the number of candidates is constant. Then recently, Keller et al.~\cite{DBLP:conf/atal/KellerHH17} have removed the requirement  on the number of candidates,  providing a randomized algorithm providing $O(W \max_i \abs{\alpha_{i+\beta} - \alpha_i})$ additive approximation to the margin, where $\beta= \widetilde{O}(\sqrt{m})$ and $W$ is the sum of manipulator weights. Notice that in this example and others, $W$ translates to $k$, number of manipulators/deleted voters/bribed voters in the unweighted case. As an  example, for Borda-UCM, their approximation translates to $\widetilde{O}(k\sqrt{m})$.

\subsection{Preliminaries}
%\paragraph{Candidate Set.} With a slight change of notation, let $C=\{c_0,c_1,\ldots,c_m\}$ be a candidate set consisting of  the preferred candidate $p=c_0$ and the other $m$ candidates $c_1,\ldots,c_m$. Note that we changed the notation so that the overall number of candidates is $m+1$; this will help streamline the writing.

An election $E=(C,V)$ is defined by a candidate set $C=\{p,c_1,\ldots,c_m\}$ and a set of $n$ voters $V$ where each voter submits a ranking of the candidates according to its preference. Then, some \emph{decision rule} $\mathcal{R}$ is applied in order to decide on the winner(s); formally $\mathcal{R}(E) \subseteq C$ is the set of winners of the elections. In the specific case of a \emph{positional scoring rule} $\Ra$, the rule is described by a vector  $\veca = (\alpha_0,\alpha_1,\ldots,\alpha_m)$ for which $\alpha_0\leq \alpha_1 \leq \cdots \leq\alpha_m$, and $\alpha_m$ is polynomial in $m$, used as follows: each voter awards $\alpha_i$ to the candidate ranked $(m+1-i)$-th. Finally, the winning candidate is the one with the highest aggregated score. In the specific case of Borda scoring rule, we have that $\veca=(0,1,\ldots,m-1,m)$. In $t$-approval, $\veca=(\vecc{0}^{m+1-t};\vecc{1}^t)$ where $\vecc{0}^{t'}$ (resp.\ $\vecc{1}^{t'}$) is $0$ (resp.\ $1$) concatenated $t'$ times. plurality (resp.\ veto) is the specific case of $1$-approval (resp.\ $m$-approval).

We assume the non-unique-winner/co-winner model where $p$ is considered a winner even if she is not the only winner. Extending our work for the unique-winner model is rather straightforward, but omitted for brevity. 

\subsection{Problem Definitions}
\begin{description}
	\item[Bribery.] Given an election $E$ under a rule $\mathcal{R}$ and a preferred candidate $p$, the goal is to bribe the minimum amount of voters, such that $p$ will win, where bribing a voter is the act of replacing its ballot by a ranking to our choosing. The output is thus the identity of the bribed voters along with their new ballots.
	
	\item[Minimum-manipulator-UCM.] Given an election $E$ under a rule $\mathcal{R}$ and a preferred candidate $p$. The goal is to add the least amount of additional voters (the manipulators), and to determine their strategies, such that $p$ will win.
	
	\item[Minimum-margin-UCM.] Given an election $E$ under a scoring rule $\Ra$, a preferred candidate $p$, and the number of allowed manipulators $k$, the goal is to determine the manipulator strategies, such that the margin $\max_{c \in C \setminus \{p\}}s(c) - s(p)$ is minimized, where $s(c')$ is $c'$'s final score. Notice that as the number of manipulators is limited, $p$ will not necessarily win. 
\end{description}
	

As we focus on scoring rules $\Ra$, we will define the \emph{scoring profile} $\vecgreek{\sigma}$ such that $\sigma(c)$ is the initial score of $c$. Notice that for UCM, having $\vecs$ in the input makes $V$ redundant. Also notice that for minimum-margin-$\Ra$-UCM, minimizing the margin boils down to to minimizing $\max_{c \in C \setminus \{p\}}s(c) $, as $s(p)$ is determined in advance (every manipulator will award her the maximum score possible and thus $s(p)=\sigma(p)+k\alpha_{m}$). Therefore we can effectively discard $p$ when solving the problem, and use  $\veca' = (\alpha_0,\alpha_1,\ldots,\alpha_{m-1})$ instead of $\veca$ (i.e., $\alpha_m$ is removed).

%\paragraph{Types of Scoring Rules.}
%	Let $\veca$ be a scoring vector. Then a scoring rule $\Ra$ is called \emph{constant} if $\alpha_m = O(1)$. A scoring rule is called \emph{non-concentrated} if $\bar{\alpha} \leq (1-\epsilon)\alpha_m$, for some constant $\epsilon > 0$,
%	where $\bar{\alpha} = 1/m \sum_{j=0}^{m} \alpha_j$ is the average of the values in $\veca$.

For ease of notation, let $\NN=nm$ denote the natural size of the input and let $[m]=\{1,\ldots,m\}$ and $[m]_0=\{0,\ldots,m-1\}$. For two parameters $a,b$, we denote the continuous set $[a-b,a+b]$ as $[a \pm b]$.



\paragraph{Probabilities.} 
Throughout the lemmas in this paper, a constant $\lambda > 1$ will be used, where its value will be  determined in the main theorems. Many of the lemmas contain mathematical expressions that are said to hold with probability at least $1-c\NN^{-\lambda+d}$ for some constants $c,d$. That is, their \emph{failure} probability is arbitrarily-chosen polynomially-small (in $m$), based on our selection of $\lambda$, where `failure' refers to the event that the discussed expression does not hold, and more generally, to the event that algorithm does not provide the desired approximation guarantee. We would sometimes write ``with failure probability  at most $c\NN^{-\lambda+d}$'' when presenting such expressions, and sometimes informally use the term  `with high probability' when the exact probability is clear from context.

As the aforementioned $\lambda$ is constant and always appears in both the runtime and approximation factors in polynomial form, it would not have an effect on the asymptotic behavior of both. 

%Throughout the paper, when we use the term `with high probability', we mean an arbitrarily-chosen polynomially-small failure probability, i.e., success probability of the form $1-\NN^{-\lambda}$ where $\lambda \geq 1$ is a constant that can be chosen without affecting the asymptotic running time. `Failure' refers to the event that the algorithm does not provide the desired approximation guarantee.


We use the following corollary of the well-known Chernoff bounds, focusing on their behavior for arbitrarily-chosen polynomially-small error-probabilities: 
%\begin{lemma}\label{cor:1}
%	Let $X_1,\ldots,X_n$ be independent random variables where $X_i \in [0,U]$ for all $i$, $\lambda$ be some constant, and $m$ be some large enough value. Let $X=\sum_{i=1}^n X_i$. Then
%	\begin{equation*}
%	\Pr[X \notin [\EE[X] \pm  R(\lambda, U,n)] ]  \leq \frac{1}{\NN^\lambda}\ .
%	\end{equation*}
%	where $R(\lambda, U,n)=U \sqrt{\lambda n \ln \NN} =\widetilde{O}(U\sqrt{n})$
%\end{lemma}	

\begin{lemma}\label{cor:2}
	Let $X_1,\ldots,X_n$ be independent random variables where $X_i \in [0,U]$ for all $i$, $\lambda$ be some constant, and $\NN$ be some large enough value. Let $X=\sum_{i=1}^n X_i$. Then
	\begin{equation*}
	\Pr[X \notin [\EE[X] \pm  R_1(\lambda, U,\EE[X]) ] ]  
	\leq \NN^{-\lambda}
	\end{equation*}
	where $R_1(\lambda, U,\EE[X]) \leq 6\lambda \max\{\sqrt{U \EE[X]}, U\}\ln \NN = 
	 \widetilde{O}(\sqrt{U \EE[X] } + U) $ is the ``approximation error'' we allow w.r.t.\ the expected value $\EE[X]$.
\end{lemma}	
For brevity, we defer the proof to the full version of this paper.


%\section{Main Linear Programs used} 
%Our methods will make use of two linear programs (LPs). 
%Before detailing our algorithm for $\mathcal{R}$-bribery, we will require some tools from the coalitional manipulation domain. 

\section{Algorithm For UCM}
We begin by presenting an approximation for minimum-margin UCM, which is outlined as Algorithm~\ref{alg:approx-UCM}. It will be used as a subprocedure in our Bribery algorithm.
Consider the relaxed \emph{linear program} (LP) for minimizing the margin for $\Ra$-UCM, described as follows.  

Given an instance of UCM, as described  by a score profile $\vecs=(\sigma(c_i))_{i \in [m]}$ and the number of manipulators $k$, 
we start by defining the variables $x_{i,j}$ for $(i,j) \in [m]\times[m]_0$, and the variable $\theta$, with the intent that $x_{i,j}$ will equal the number of times candidate $c_i$ received a score of type $\alpha_j$, and that $\theta$ will serve as the upper-bound on each candidate's final aggregate score. Notice that since the voters are unweighted (i.e., they are identical; all have the same voting power) we do not care which manipulators awarded her those scores; this is true due to~\cite[Theorem~7]{DBLP:journals/ai/DaviesKNWX14} where they show that every score assignment such that each candidate has received $k$ scores, and each \emph{score-type} $\alpha_j$  is repeated exactly $k$ times, can be modified in polynomial time
into a valid strategy without affecting each candidate's final score.

%\begin{lemma}[{See~\cite[Theorem~7]{DBLP:journals/ai/DaviesKNWX14}}]~\label{lem:davies}
%	Every score assignment such that each candidate have received $k$ scores, and each \emph{score-type} $\alpha_j$  is repeated exactly $k$ times, can be modified in polynomial time\footnote{The polynomial runtime follows by the constructive proof in their theorem, and is also mentioned in the paragraph in their paper following the theorem.} into a valid strategy without affecting each candidate's final score.
%\end{lemma} 
%since Davies et al.~\cite[Theorem~7]{DBLP:journals/ai/DaviesKNWX14} show that every score assignment such that each candidate have received $T$ scores, and each \emph{score-type} $\alpha_j$  is repeated exactly $T$ times, this assignment can be modified in polynomial time into a valid strategy without affecting each candidate's final score.
The LP is defined as:
%Assume we have already fixed a bound $k$ on the final score of each candidate that we wish not to surpass. Then a natural formulation is:
\begin{equation}
\min_{\vecc{x}} \theta
\end{equation} 
subject to:
\begin{align}
&\sum_{i=1}^m x_{i,j}=k &\forall j\in[m]_0\label{eq:orig:1}\ ,\\
&\sum_{j=0}^{m-1}x_{i,j}=k &\forall i\in[m]\label{eq:orig:2}\ ,\\
&\sigma(c_i)+ \sum_{j=0}^{m-1}\alpha_j x_{i,j}\leq \theta &\forall i\in[m]\label{eq:orig:3}\ ,\\
&x_{i,j} \in [0,k] & \forall i\in[m],j\in[m]_0\label{eq:to_rel}\ ,
\end{align}
%&x_{i,j} \in \{0,\ldots,k\} & \forall i\in[m],j\in[m]_0\label{eq:to_rel}\ ,
where (\ref{eq:orig:1}) guarantees that every score was awarded $k$ times, (\ref{eq:orig:2}) guarantees that every candidate was given $k$ scores, and (\ref{eq:orig:3}) guarantees that  every candidate gets at most $\theta$ points. The constraint $x_{i,j} \in [0,k]$ is a relaxation of the constraint $x_{i,j} \in \{0,\ldots,k\}$, however, the latter would have  caused the LP to become an \emph{integer program} (IP), and solving such is $\NP$-hard.  We denote the relaxed LP as $\LPCM(\veca, \vecs, k)$.

%We present an algorithm based on the Birkhoff-von Neumann (BvN) decomposition of doubly Stochastic matrices, as follows.

Assume we solve the above LP, and let $\vecc{x^\star}$ be the resulting solution w.r.t.\ the objective value $\theta^\star$. As an optimum of the LP, $\vecc{x^\star}$ denotes some allocation of scores such that the margin in minimized, however, as this allocation is fractional, it does not translate into a valid strategy. 
%At this point, many randomized rounding type of algorithm try to round the values in $\vecc{x^\star}$ randomly such that w.h.p.\ a good enough integral solution is obtained. Usually the rounding is done in such a way such that  an initial generated integral solution $\vecc{\tilde{x}}$ is generated, for which it holds that $\EE[f(\vecc{\tilde{x}})] = f(\vecc{x^\star})$, for some function $f$ which is usually related to either the constraints or the objective.

Many algorithms solve such scenarios using some form of \emph{randomized rounding} over the fractional variables. This seems quite problematic as the variables are highly interdependent, where their dependency is given by the LP constraints. These constraints  should still hold even after the rounding. 

To work around this, notice that
each possible ballot corresponds to some permutation $\pi$ (with the meaning that $c_i$ receives a score of $\alpha_{\pi(i)}$), and therefore randomized rounding should be done on the ballot level. However, to do so we have to define some distribution over the ballots (permutations), and there are $m!$ of them. This is where the BvN decomposition comes to our rescue. 

Observe the matrix $\vecc{Y}=[y_{i,j}]$ where $ y_{i,j}=x^\star_{i,j}/k$ and notice that it is \emph{doubly-stochastic}, that is $\sum_i Y_{i,j} = \sum_j Y_{i,j} = 1$.  Roughly speaking, the Birkhoff-von Neumann theorem states that each doubly-stochastic matrix is a point in the Birkhoff polytope, whose vertices are all the $m!$ permutation matrices.  In other words, every doubly-stochastic matrix can be obtained by a convex combination (a weighted sum with coefficients in $[0,1]$ which sum to $1$) of all permutation matrices. However, the surprising fact---as shown by various constructive proofs to the theorem---is that such a convex sum can be found in which the number of nonzero coefficients is at most $m^2$. 

Let us now state a suitable variant of the Birkhoff-von Neumann theorem:
\begin{theorem}[BvN Theorem]\label{thr:bvn}
	Let $\vecc{Y}$ be a doubly-stochastic matrix. Then we can decompose $\vecc{Y}$ to a convex combination of at most $m^2$ permutation matrices, that is, we decompose $\vecc{Y} = \lambda_1 \vecc{P}_{\pi_1}+ \cdots + \lambda_q \vecc{P}_{\pi_q}$ where each $\pi_t$ is a permutation with  $\vecc{P}_{\pi_t}$ being its corresponding permutation matrix, each $\lambda_t \in [0,1]$ and $\sum_{t} \lambda_t = 1$, and $q \leq m^2$. This decomposition can be found in polynomial time. 
\end{theorem}
For the sake of completeness, we supply a constructive proof of this theorem in the full version of this paper. 


The remarkable thing about this form of the BvN decomposition, is that it implies that when choosing ballots for each of the manipulators, we only have to consider at most $m^2$ ballots---a polynomial number---out of the the $m!$ possible ballots (that is, permutations of order $m$).

Let $\Pi = \{\pi_1, \ldots, \pi_q\}$ (resp.\ $\lambda_1,\ldots,\lambda_q$) be the set of permutations (resp.\ weights) used in the above decomposition, and let $\hat{p} \colon \Pi \to [0,1]$ be a distribution over $\Pi$ the such that $
\hat{p}(\pi_t) = \lambda_t
$.

We proceed as follows. For each manipulator $\ell$, we randomly draw a random permutation $\pi_{\ell} \sim \hat{p}$, and assign it to $\ell$ as his ballot, that is $\ell$ awards $c_i$ a score of $\alpha_{\pi_{\ell}(i)}$. 

\begin{algorithm}[t]
	\caption{$\Ra$-UCM Algorithm.}
	\label{alg:approx-UCM}
	Solve $\LPCM(\veca,\vecs,k)$, and let $\vecc{x^\star }$ be the resulting solution.\;
	Apply the BvN decomposition on $\vecc{Y}=\vecc{x^\star }/k$, and let $\Pi = \{\pi_1, \ldots, \pi_q\} $ be the resulting permutations with respective weights $\lambda_1,\ldots,\lambda_q$.\tcc*{The divisiion in $\vecc{x^\star }/k$ is element-wise}
	Define a distribution $\hat{p} \colon \Pi \to [0,1]$ over $\Pi$ such that $\hat{p}(\pi_t) = \lambda_t$.\; 
	For each manipulator $\ell$, randomly draw a random permutation $\pi_{\ell} \sim \hat{p}$, and assign it to $\ell$ as his ballot.\tcc*{meaning $\ell$ awards $c_i$ a score of $\alpha_{\pi(i)}$}
\end{algorithm}

Let $R_2(\lambda, \alpha_{m-1},k)= 6\lambda\alpha_{m-1} \sqrt{(k+1)} \ln \NN $. The following lemma shows that this bounds the additional points received by a candidate by the rounding process:
\begin{lemma}\label{thr:UCM}
	With failure probability at most $\NN^{-\lambda+1}$, the above algorithm adds at most $R_2(\lambda, \alpha_{m-1},k) $ points to the score each candidate had received by the LP.
\end{lemma}
\begin{proof}
%	Let $\tilde{x}_{i,j,\ell}$ be the indicator variable indicating whether or not $\ell$ awarded $\alpha_{j}$ to $c_i$, and specifically, 
Let $\tilde{x}_{i,j}
%=\sum_{\ell=1}^{k}\tilde{x}_{i,j,\ell}
$ 
be the number of $\alpha_j$ scores awarded to $c_i$ by the manipulators. Now focus on some $i$, and let $Q$ be all points awarded to $c_i$ by the manipulators, according to our algorithm. On one hand, $Q = \sum_{j=0}^{m-1} \alpha_j \tilde{x}_{i,j}$. On the other hand, $Q=\sum_{\ell=1}^{k} \alpha_{\pi_{\ell}(i)}$, and thus
%	Then $Q=\sum_{\ell=1}^{k} z_{i,\ell}$ where $z_{i,\ell}  =\sum_{j=0}^{m-1} \alpha_j \tilde{x}_{i,j,\ell} \in [0, \alpha_{m-1}]$ is the number of points awarded to $c_i$ by $\ell$. Notice that:

%	\begin{align*}
%	Q &= \sum_{j=0}^{m-1} \alpha_j \sum_{\ell=1}^{k}  \tilde{x}_{i,j,\ell}\\
%	&=  \sum_{\ell=1}^{k} \left(\sum_{j=0}^{m-1} \alpha_j \tilde{x}_{i,j,\ell}\right)\\
%	&=  \sum_{\ell=1}^{k} z_{i,\ell}
%	\end{align*}
%	where $z_{i,\ell}  =\sum_{j=0}^{m-1} \alpha_j \tilde{x}_{i,j,\ell} \in [0, \alpha_m]$ is the points awarded to $c_i$ by $\ell$. 
%	Then:
%	\begin{align*}
%	\EE[Q] &= \sum_{\ell=1}^{k} \EE[z_{i,\ell}]\\
%	&=k \EE[\alpha_{\pi(i)} ]\\
%	&=k \sum_{\pi \in \Pi} p(\pi)\alpha_{\pi(i)} \\
%	&=  k \sum_{j=0}^{m-1} \alpha_{j}\sum_{\substack{{\pi \in \Pi}\\\pi(i)=j }} p(\pi)\\
%	&=  k \sum_{j=0}^{m-1} \alpha_{j}\sum_{\substack{{\pi \in \Pi}\\\pi(i)=j }} y_{i,j}\\
%	&=  \sum_{j=0}^{m-1} \alpha_{j} x^\star_{i,j}\\
%	\end{align*}
\begin{equation*}
\EE[Q] = \sum_{\ell=1}^{k} \EE[\alpha_{\pi_{\ell}(i)}]\\
%=k \EE[\alpha_{\pi_{\ell}(i)} ]\\
=k \sum_{\pi \in \Pi} \hat{p}(\pi)\alpha_{\pi(i)}\ ,
\end{equation*}
where the last equality follows by the $\pi_{\ell}$'s being i.i.d. By further splitting the summation in the r.h.s.:
\begin{equation*}
\EE[Q] =  k \sum_{j=0}^{m-1} \alpha_{j}\sum_{\substack{{\pi \in \Pi}\\\pi(i)=j }} \hat{p}(\pi)\\
=  k \sum_j \alpha_{j} y_{i,j}\\
=  \sum_{j=0}^{m-1} \alpha_{j} x^\star_{i,j}\\
\end{equation*}
	In addition, notice that 
	\begin{equation}\label{eq:tttt}
		\sum_{j=0}^{m-1} \alpha_{j} x^\star_{i,j} \leq \alpha_{m-1}\sum_{j=0}^{m-1}x^\star_{i,j}= \alpha_{m-1}k\ .
	\end{equation}
	Applying Corollary~\ref{cor:2} to $Q=\sum_{\ell=1}^{k}\alpha_{\pi_{\ell}(i)}$ and recalling that $\alpha_{\pi_{\ell}(i)} \in [0,\alpha_{m-1}]$, we get that with   failure probability at most $\NN^{-\lambda}$:
	\begin{align*}
\left\lvert Q - \EE[Q]\right\rvert &\leq R_1(\lambda,\alpha_{m-1},\EE[Q])\\
&\leq 6\lambda \max\{\alpha_{m-1}\sqrt{k}, \alpha_{m-1}\}\ln \NN\\
&= 6\lambda \alpha_{m-1} \sqrt{k} \ln \NN\\
&\leq R_2(\lambda, \alpha_{m-1},k)\ .
\end{align*}
where the first inequality follows by an application of Corollary~\ref{cor:2}, the second by Eq.~(\ref{eq:tttt}), and the first equality by naturally assuming that $k \geq 1$ (otherwise there are no manipulators and the problem instance is degenerate).


In words, when we created valid ballots for each of the voters, the score of each candidate increased by at most $R_2(\lambda, \alpha_{m-1},k)$ with probability failure probability at most $\NN^{-\lambda}$.  By applying the union-bound over all $m\leq \NN$ candidates, it can be made to hold for all candidates simultaneously  with failure probability at most $\NN^{-\lambda+1}$. 
\end{proof}
%\begin{corollary}
%	The above algorithm is an additive $\widetilde{O}(\alpha_m \sqrt{k})$ algorithm to UCM when the objective is minimum margin, with arbitrarily-chosen exponentially-small failure probability.
%\end{corollary}
Let $\tilde{\theta}=\max_{i\in[m]}\sigma(c_i)+\sum_{j=0}^{m-1} \alpha_j \tilde{x}_{i,j}$ be the objective value obtained by our algorithm. We are now ready to complete the proof of Theorem~\ref{thr:UCM-margin}:
\begin{proof}[Proof of Theorem~\ref{thr:UCM-margin}]
	This is true by the fact that the algorithm adds  at most $R_2(\lambda, \alpha_{m-1},k)$ points to each candidate, and thus $\tilde{\theta} \leq \theta^\star + R_2(\lambda, \alpha_{m-1},k) \leq \bar{\theta} + R_2(\lambda, \alpha_{m-1},k)$, where $\bar{\theta}$ is the UCM (integral) optimum. The last inequality holds since the LP is a relaxation of the original IP. The overall running time is polynomial, as it is comprised of solving an LP~\cite{DBLP:journals/combinatorica/Karmarkar84}, followed by the polynomial-time BvN decomposition. 
	The above algorithm has a polynomially-small failure probability $\NN^{-\lambda+1}$. By choosing e.g.\ $\lambda=2$ and running it a linear number of times, and choosing the run yielding minimal $\tilde{\theta}$, the failure probability becomes exponentially-small, while the runtime stays polynomial.
\end{proof}
%Theorem~\ref{thr:UCM-margin} follows directly from the above argument.
The proof of Theorem~\ref{thr:UCM-number} continues from here, by showing that for non-concentrated scoring rules, when the margin
is at most $R_2(\lambda, \alpha_{m-1},k)$, then with high probability $O(R_2(\lambda, \alpha_{m-1},k) / \alpha_{m-1}) = \widetilde{O}(\sqrt{k})$ additional manipulators are needed in order to close the gap. We defer the proof to the full version of the paper, and note that it is similar in nature to the proof of Lemma~\ref{lem:how-to-bribe}.

\section{Algorithm For Bribery}
We can now move to describe the actual bribery LP, denoted $\LPB(\veca,\vecs)$. Let $y_v$  be an indicator variable for  each voter $v$ indicating whether he should be bribed. As bribing voters boils down to their deletion, followed by re-adding them with a new ballot, we also need to describe how to allocate the points of the new ballots. Therefore we define---as in the UCM case---the variables $x_{i,j}$ with the intent that $x_{i,j}$ will equal the number scores of type $j$ awarded to $c_i$.  The  relaxed LP is defined as follows: 
\begin{equation*}
\min_{\vecc{x}, \vecc{y}, \theta,k} k
\end{equation*} 
subject to:
\begin{align}
&\sum_{v \in V}y_v = k &\label{eq:sumxb}\\
&\sigma(c_i) - \sum_{v \in V}\alpha_{j(v,c_i)} y_{v} + \sum_{j=0}^{m-1} \alpha_j x_{i,j}\leq \theta &\forall i\in[m]\label{eq:mainb}\ ,
\end{align}
\begin{align}
&\sigma(p) - \sum_{v \in V}\alpha_{j(v,p)} y_{v} + \alpha_m k \geq \theta &, \label{eq:p-const}\\
&\sum_{i=1}^m x_{i,j}=k &\forall j\in[m]_0\label{eq:orig:11}\ ,\\
&\sum_{j=0}^{m-1}x_{i,j}=k &\forall i\in[m]\label{eq:orig:22}\ ,\\
&y_{v} \in [0,1] & \forall v\ ,\\
&x_{i,j} \in [0,k] & \forall i,j\ ,
\end{align}
where $\alpha_{j(v,c)}$ is the score currently given by a voter $v$ to a candidate $c$. Constraints~(\ref{eq:orig:11},\ref{eq:orig:22}) are identical to their corresponding constraints in the coalitional manipulation LP. 
%The other constraints are presented in a way that will ease the exposition: for example,~
Constraint~(\ref{eq:sumxb}) makes sure that the number of bribed voters will be $k$, the variable we seek to minimize. 
%This is although we could have just minimized on $\sum_{v \in V}y_v$ and do without $k$. 
(\ref{eq:mainb},\ref{eq:p-const}) together make sure that $p$ has a final score greater than or equal to any other candidate. 
%We could have combined them into a single constraints, however using two constraints along with the use of $\theta$ here will make the presentation clearer. 
Notice that the use of $\alpha_m k$ in~(\ref{eq:p-const})  stems from the fact that the score awarded to $p$ by the $k$ bribed voters is known; each will give her the maximum score available. 


Our algorithm is outlined as Algorithm~\ref{alg:approx-U}, and is comprised of four stages. In the first stage, we shall solve the Bribery LP, as previously defined. In the second, we will choose an initial set of voters to bribe using a simple form of randomized rounding. 
%These voters would not be the only voters we shall bribe, as we shall later see; however, 
This choice of voters will reduce the problem to an instance of the UCM problem, in which the number of manipulators is known (as we have already determined them). This is the point---the third stage---where we shall use our UCM algorithm, to determine their strategy. Our main claim here is that by bribing not too many voters, and assigning them ballots, we have reduced this bribery instance to another bribery instance, in which the margin is small---at most $\widetilde{O}(\alpha_{m}\sqrt{k})$. Then, at the fourth stage, we will show that this margin is relatively easy to close, by using at most $\widetilde{O}(\sqrt{k})$ additional bribed voters.
%Set $\tilde{y}_v \gets 1$ with probability $y_v$, else $\tilde{y}_v \gets 0$
\begin{algorithm}[t]
	\caption{$\Ra$-bribery Algorithm.}
	\label{alg:approx-U}
	Solve the Bribery LP as described, and let $(\vecc{x^\star };\vecc{y^\star },\theta^\star )$ and $k^\star$ be the resulting solution\;
	\ForEach{v}{$\tilde{y}_v \gets 
		\begin{cases}
		1&\text{with probability }y^\star _v\\
		0&\text{otherwise}
		\end{cases}$
	}
	Set $f \gets \sum_{v \in V}\tilde{y}_v - \sum_{v \in V} y^\star _v$ and let $\tilde{k}=\sum_{v \in V} \tilde{y}_v$\;
	Define  $\hat{\sigma}(c_i)= \sigma(c_i) - \sum_{v \in V}j(v,c_i) \tilde{y}_{v}$ for every $i \in [m]$\;
	Apply our UCM algorithm on the input $((\alpha_{0},\ldots,\alpha_{m-1}),\vecgreek{\hat{\sigma}}, \tilde{k})$ \;
	\While{$\hat{\sigma}(p) <\max_{c \in C'} \hat{\sigma}(c)$ }{\tcc* {while $p$ loses}   Pick at most $\epsilon^{-1}N$ voters and bribe them according to Lemma~\ref{lem:how-to-bribe2}/Lemma~\ref{lem:how-to-bribe}}
\end{algorithm}


\subsubsection{Stage 1: Solving the Bribery LP.}
We solve $\LPB$, obtaining the solution $(\vecc{x^\star },\vecc{y^\star },\theta^\star )$ w.r.t.\ the optimal objective $k^\star $. While $(\vecc{x^\star },\vecc{y^\star },\theta^\star )$ is a fractional solution (since it solves a relaxed LP) it will enable us to obtain an \emph{integral} solution without too much compromise on the increase in the number of bribed voters.
% $(\vecc{\tilde{x}},\vecc{\tilde{y}},\tilde{\theta})$ w.r.t.\ an optimum value $\tilde{k}$, without too much compromise on the increase $\tilde{k} - k^\star $. 

\subsubsection{Stage 2: Rounding $\vecc{y^\star }$.}
We round the vector $\vecc{y^\star }$ without touching $\vecc{x^\star }$ for now. This is done as follows:
\begin{equation*}
\tilde{y}_v = \begin{cases}
1 & \text{with probability } y^\star _v\ ;\\
0 & \text{otherwise.}
\end{cases}
\end{equation*}
Now let $\tilde{k}=\sum_{v \in V}\tilde{y}_v$.
\begin{lemma}\label{eq:factor-b}
	 Recall that  $R_2(\lambda, \alpha_{m},k)= 6\lambda\alpha_{m} (k+1)^{1/2} \ln \NN $. Then:
	\begin{itemize}
		\item With probability at least $1-\NN^{-\lambda}$,
		\begin{equation*}
		\tilde{k} \in [k^\star  \pm R_2(\lambda, 1,k^\star)]\ ,
		\end{equation*}
		\item For every $c \in C\setminus\{p\}$, with probability at least $1-\NN^{-\lambda}$, 
		\begin{equation*}
		\sigma(c) - \sum_{v \in V}\alpha_{j(v,c)} \tilde{y}_{v} + \sum_{j=0}^{m-1}\alpha_j x^{\star }_{i,j} 
		\leq \theta^\star  +  R_2(\lambda, \alpha_{m},k^\star)\ ,
		\end{equation*}
		\item With probability at least $1-\NN^{-\lambda}$, 
		\begin{equation*}
		\sigma(p) - \sum_{v \in V}\alpha_{j(v,p)} \tilde{y}_{v} + \alpha_m k^\star \geq \theta^\star -  R_2(\lambda, \alpha_{m},k^\star)\ .
		\end{equation*}
	\end{itemize}
	
\end{lemma}
\begin{proof}[Proof sketch] Full proof is deferred to the full version of this paper. As a sketch,
	the first part follows by Corollary~\ref{cor:2}. The second and third by the LP definition, and Corollary~\ref{cor:2} with Eqs.~(\ref{eq:mainb}) and~(\ref{eq:p-const}), respectively.
\end{proof}


\subsubsection{Stage 3: Running UCM.} 
Let $f=\tilde{k}-k^\star$, $\veca'=(\alpha_{0},\ldots,\alpha_{m-1})$, and let  $\hat{\sigma}(c_i)=\sigma(c_i) - \sum_{v \in V}\alpha_{j(v,c_i)} \tilde{y}_{v}$. In words, $\hat{\sigma}(c_i)$ is $\sigma(c_i)$ when it is adjusted for the loss of the voters who were deleted as described by the vector $\tilde{\vecc{y}}$. Notice that $\hat{\vecgreek{\sigma}}$ is only defined for the non preferred candidates. We have now reduced the problem to the UCM problem: $\hat{\vecgreek{\sigma}}$ is the new score profile,  $\tilde{k}$ is the number of manipulators we have at our disposal (one for each of the deleted voters), and $\veca'$ is $\veca$ without the score $\alpha_m$ which is always awarded to $p$ by the manipulators, and thus is irrelevant to the input. We call our UCM algorithm on $(\veca', \hat{\vecgreek{\sigma}}, \tilde{k})$. We do not rely here on its approximation guarantee provided by Theorem~\ref{thr:UCM-margin}, but on the stronger claim, hinted by Lemma~\ref{thr:UCM}, that the $R_2(\lambda, \alpha_{m-1},k)$ factor is an additive term not just w.r.t.\ the optimum, but also w.r.t.\ the fractional solution of $\LPCM(\veca', \vecgreek{\hat{\sigma}}, \tilde{k})$. 

In this spirit, let $\LL$ be a shorthand to $\LPCM(\veca', \vecgreek{\hat{\sigma}}, \tilde{k})$, and let $\theta_{\LL}$ be the optimal objective value of $\LL$. We provide the two following lemmas, where the first will compare  $\theta_{\LL}$ to the value $\theta^\star$. The second will then directly compare the result of our UCM algorithm to $\theta_{\LL}$.
\begin{lemma}\label{lem:double_star}
	With failure probability at most $2\NN^{-\lambda+1}$, it holds that $\theta_{\LL}\leq \theta^\star  +  2R_2(\lambda, \alpha_{m},k^\star)$.
\end{lemma}
\begin{proof}
	We will manually define a (not necessarily optimal) solution to $\LL$ where the objective is at most $\theta^\star  + 2R_2(\lambda, \alpha_{m},k^\star)$. As the LP solution cannot be worse, the lemma will follow.
	
	Let $x^{\star \star }_{i,j} = (\tilde{k}/k^\star )x^{\star }_{i,j}$ for all $i,j$. Notice that now Equations~(\ref{eq:orig:1},\ref{eq:orig:2}) hold w.r.t.\ $\vecc{x^{\star \star }}$ and $\tilde{k}$, as required. When we also  plug $\vecc{x^{\star \star }}$ into~(\ref{eq:mainb}) instead of $\vecc{x^{\star}}$, the l.h.s.\ of~(\ref{eq:mainb}) increases by at most  
	\begin{align*}
		\sum_{j=0}^{m-1} \alpha_j x^{\star \star }_{i,j}-\sum_{j=0}^{m-1} \alpha_j  x^\star _{i,j} &= (\tilde{k}-k^\star )/k^\star   \sum_{j=0}^{m-1} \alpha_j  x^\star _{i,j}\\
%		&=f/k^\star \sum_{j=0}^{m-1} \alpha_j x^\star _{i,j}\\
		&\leq f\alpha_m /k^\star  \sum_{j=0}^{m-1} x^\star _{i,j}\\
		&=  \alpha_m f
	\end{align*}
	We conclude that for all $i$ with failure probability at most $2\NN^{-\lambda}$, it holds that:
	\begin{multline*}
	\hat{\sigma}(c_i)  + \sum_{j=0}^{m-1} \alpha_j x^{\star \star }_{i,j}\\
	= \sigma(c_i) - \sum_{v \in V}\alpha_{j(v,c_i)} \tilde{y}_{v} + \sum_{j=0}^{m-1}\alpha_j x^{\star \star }_{i,j} \\ 
	\leq \sigma(c_i) - \sum_{v \in V}\alpha_{j(v,c_i)} \tilde{y}_{v} + \sum_{j=0}^{m-1}\alpha_j x^{\star }_{i,j} + \alpha_m f\\
	\leq \theta^\star  +  R_2(\lambda, \alpha_{m-1},k^\star)+\alpha_m f \\
	=\theta^\star  +  R_2(\lambda, \alpha_{m-1},k^\star)+\alpha_m R_2(\lambda, 1,k^\star)\\
	\leq \theta^\star  +  2R_2(\lambda, \alpha_{m},k^\star)\ ,
	\end{multline*}
	where the first equality is by definition, the second follows from the above argument, and the third and fourth follow by Lemma~\ref{eq:factor-b} (where each also contributes $\NN^{-\lambda}$ to the failure probability). Notice that we have just showed here that Equation~(\ref{eq:orig:3}) holds w.r.t.\ $\vecc{x^{\star \star }}$ and $\tilde{k}$, as required.
	
	Since we want the above to hold for all $i$ simultaneously, the failure probability becomes at most $2\NN^{-\lambda+1}$ by the union bound.
	We have just defined a valid solution with objective value at most $\theta^\star  +   2R_2(\lambda, \alpha_{m},k^\star)$ with high probability; as $\theta_{\LL}$ cannot be worse, we are done.
\end{proof}


Let $\tilde{\theta}$ be the maximum candidate score as a result of our UCM algorithm on the input $(\veca', \hat{\vecgreek{\sigma}}, \tilde{k})$. Define: 
\begin{multline*}
	R_3=2R_2(\lambda, \alpha_{m},k^\star ) \\
	+R_2(\lambda, \alpha_{m-1},k^\star + R_2(\lambda, 1,k^\star))\ .
\end{multline*}
We claim the following:
\begin{lemma}\label{lem:tt}
	With probability at least $1-4\NN^{-\lambda + 1}$, it holds that $\tilde{\theta} \leq \theta^\star  + 	R_3$.
\end{lemma}

\begin{proof}
	By combining Lemma~\ref{thr:UCM} w.r.t.\ $\tilde{\theta}$ and $\theta_{\LL}$ and Lemma~\ref{lem:double_star}, we obtain that, with failure probability at most $4\NN^{-\lambda + 1}$:
	\begin{align*}
	\tilde{\theta} &\leq \theta_{\LL} + R_2(\lambda, \alpha_{m-1},\tilde{k})\\
	&\leq \theta_{\LL} + R_2(\lambda, \alpha_{m-1},k^\star + R_2(\lambda, 1,k^\star))\\
	%	&\leq  \theta_{\LL}  + 2R_2(\lambda, \alpha_{m},k^\star )\\
	&\leq  \theta^\star  + 2R_2(\lambda, \alpha_{m},k^\star )\\
	&\qquad +R_2(\lambda, \alpha_{m-1},k^\star + R_2(\lambda, 1,k^\star))\\
	&= \theta^\star  + 	R_3\ ,
	\end{align*}
	where the first inequality is by Lemma~\ref{thr:UCM}, the second by Lemma~\ref{eq:factor-b}, 
%	the third by the fact that $R_2(\lambda, \alpha_{m-1},k^\star + R_2(\lambda, 1,k^\star)) \leq 2R_2(\lambda, \alpha_{m},k^\star )$, and 
and the third by
	Lemma~\ref{lem:double_star}. The aforementioned failure probability is obtained by a union-bound over the failure probability of each of the lemmas used. 
\end{proof}

\subsubsection{Stage 4: Bribing More Voters.} Let us return to $\LPB(\veca,\vecs)$, and let $\vecc{\tilde{x}}$ be the allocation of the scores to the candidates according to our UCM algorithm, w.r.t. the optimum value $\tilde{\theta}$.
Does plugging $(\vecc{\tilde{x}};\vecc{\tilde{y}};\tilde{\theta}, \tilde{k})$ into their respective places in the LP constitute a valid solution? The answer is unfortunately no; while we showed that all other constraints hold, Eq.~(\ref{eq:p-const}) does not necessarily hold. 
$\sigma(p) - \sum_{v \in V}j(v,p) \tilde{y}_{v} + \alpha_m \tilde{k}$ might be less then $\tilde{\theta}$. 
However, we can prove that the margin needed for Eq.~(\ref{eq:p-const}) to hold is not too large; let $R_4 = R_2(\lambda, \alpha_{m},k^\star)
 +  \alpha_{m}R_2(\lambda, 1,k^\star) + 	R_3$. Then:
\begin{lemma}\label{lem:margin}
	With probability at least $1-6 \NN^{-\lambda + 1}$, it holds that $\sigma(p) - \sum_{v \in V}\alpha_{j(v,p)} \tilde{y}_{v} + \alpha_m\tilde{k} \geq \tilde{\theta}-R_4$.
\end{lemma}
\begin{proof}
	Let $Q=\sigma(p) - \sum_{v \in V}\alpha_{j(v,p)} \tilde{y}_{v} + \alpha_m\tilde{k}$. Then assuming none of the previous lemmas fail:
	\begin{align*}
	Q&\geq \sigma(p) - \sum_{v \in V}\alpha_{j(v,p)} \tilde{y}_{v}+ \alpha_m k^\star- \alpha_{m} R_2(\lambda, 1,k^\star)\\
	&\geq \theta^\star  - (R_2(\lambda, \alpha_{m},k^\star) +  \alpha_{m}R_2(\lambda, 1,k^\star))\\
	&\geq \tilde{\theta}  - (R_2(\lambda, \alpha_{m},k^\star)\\
	&\qquad +  \alpha_{m}R_2(\lambda, 1,k^\star) + 	R_3)\\
	&=\tilde{\theta}  - R_4\ ,
	\end{align*}
	where the first inequality follows by  the first part of Lemma~\ref{eq:factor-b}, the second follows by the third part of Lemma~\ref{eq:factor-b}, the third by Lemma~\ref{lem:tt}. The failure probability is bounded by $6 \NN^{-\lambda + 1}$, by a union-bound on the failure probabilities of the lemmas used. 
\end{proof}
Summing up, Lemma~\ref{lem:margin} shows that currently $p$ might be still losing by a margin of  at most $R_4=\widetilde{O}(\alpha_m\sqrt{k^\star})$.


The next two lemmas will show the relation between this margin, and the number of manipulators needed in order to close it and make $p$ win. The proofs will be constructive and will supply a polynomial-time algorithm. Let $C'=C \setminus \{p\}$ and let $N=\ln ^{1+\delta} \NN$ for some constant $\delta>0$. Let $s'(c)$ be the current score of $c$ at some point in time and let $g = \max_{c \in C' }s'(c)-s'(p)$ be the margin at this point in time.

\begin{lemma}\label{lem:how-to-bribe2}
	Let $\Ra$ be a constant voting rule. By bribing at most $N$ voters we can change the margin to be $g' \leq \max\{0, g - \epsilon\alpha_m N\}$ for some constant $\epsilon>0$. That is, we can either make $p$ win, or at least reduce the margin by which she loses by $\epsilon \alpha_m N$.
\end{lemma}
\begin{proof}
	By repeating the following procedure at most $N$ times: pick a voter who gave $p$ at most $\alpha_m-1 $ points (if no such voter exists, $p$ is already winning and we can stop repeating the procedure), and change his ballot such that $p$ and the candidate ranked first are swapped. Notice that by this the gap between any $c$ and $p$ has decreased by at least $1$.
	
	If the above procedure stopped short of $N$ iterations, then $p$ is winning and we are done. Otherwise the gap is decreased by at least $N$. Setting $\epsilon=1/\alpha_m$ we are done.
\end{proof}


%\begin{lemma}\label{lem:how-to-bribe2}
%	Let $\Ra$ be a constant voting rule, and let $g = \max_{c \in C' }\sigma_t(c)-\sigma_t(p)$ be the margin at some point in time $t$. There exists a constant $\epsilon > 0$ such that by bribing at most $d \leq 2N$ voters we can obtain a change the margin to be $g' \leq \max\{0, g - \epsilon \alpha_m N\}$. That is, we can either make $p$ win, or at least reduce the margin by which she loses by $\epsilon \alpha_m N$.
%\end{lemma}
%\begin{proof}
%	Define $\epsilon = 1/(2\alpha_{m})$. We prove by splitting to two easy cases:
%	\begin{itemize}
%		\item Let $A$ be the set of un-bribed voters who awarded $p$ at most $\alpha_m -1 < (1-\epsilon)\alpha_m$ points, and notice the this means that all other voters gave $p$ a maximum score which is either $\alpha_m$ or some other score equal to $\alpha_m$, if such exists (e.g., all ``one'' scores in $t$-approval). If $\abs{A} \leq 2N$, simply bribe all voters in $A$ and let them move $p$ to the top position. $p$ is now ranked top by all voters and received the maximum score obtainable and by the co-winner assumption she won.
%		
%		\item Otherwise, let $\abs{A} > 2N$ and we are done: we can just bribe $N$ arbitrary voters from $A$ and let them move $p$ to the top position, thus increasing $g$ by $\epsilon \alpha_m N$ points.	 
%	\end{itemize} 
%\end{proof}

Non-concentrated rules have a much more involved proof:
\begin{lemma}\label{lem:how-to-bribe}
	Let $\veca$ be such that $\bar{\alpha} \leq (1-6\epsilon) \alpha_m$ for some constant $0 < \epsilon < 1/6$.
	 Then by bribing at most $\epsilon^{-1}N$ voters we can---with failure probability at most $2\NN^{-\lambda+1}$---change the margin to be $g' \leq \max\{0, g - \epsilon \alpha_m N\}$. That is, we can either make $p$ win, or at least reduce the margin by which she loses by $\epsilon \alpha_m N$.
\end{lemma}
\begin{proof}	We split to cases; we will first discuss two easy ones: 
	\begin{itemize}
		\item Let $A$ be the set of un-bribed voters who awarded $p$ at most $\alpha_m -1$ points. If $\abs{A} \leq \epsilon^{-1}N$, simply bribe all voters in $A$ and let them move $p$ to the top position. $p$ is now ranked top by all voters and received the maximum score obtainable and by the co-winner assumption she wins.
		
		\item Otherwise, let $B \subseteq A$ be the set un-bribed voters which gave $p$ at most $(1-\epsilon) \alpha_m$ points. If $\abs{B} \geq N$ we are done: we can just bribe $N$ of them and let them move $p$ to the top position, thus decreasing $g$ by $\epsilon \alpha_m N$ points.	 
	\end{itemize} 
	If the two above cases do not hold, then it holds that $\abs{A} > \epsilon^{-1}N$, but $\abs{B} < N$. In words, there are at most $\abs{B} < N$ voters who gave $p$ at most $(1-\epsilon) \alpha_m$ points, and $\abs{A \setminus B} > 5N$ voters who gave $p$ more than $(1-\epsilon) \alpha_m$ points. For the time being, we bribe the voter-set $B$ using the method described in the latter item; we will shortly bribe another $N$ voters as well.
	%	, and those  $N$ additional voters will be enough for the lemma to hold. Therefore we can  ignore the set $A$ in the analysis.
	
	Assume we have bribed $B$; then by now all voters have given $p$ more than $(1-\epsilon) \alpha_m$ points. In other words, $p$'s current score is at least $(1-\epsilon) \alpha_m n$.
	Now randomly pick $N$  voters from $A\setminus B$. Let them all put $p$ in the top position, and rank all other candidates randomly, that is, the ranking of all other candidates will be determined by a random permutation. Now let $c$ be some candidate and define $r$ such that $s'(c)=r \alpha_m n$. In words, $c$ has received $r\alpha_m$ points from each voter \emph{on average}. Now assume for a moment we first \emph{delete} the $N$ voters we bribe, and only then \emph{re-add} the voters with their new ballots.
	
	When we delete $N$ voters, $c$ loses $r \alpha_m N$ points in expectation. Formally, let $X_c$ be the number of points $c$ had actually lost. Then $\EE[X_c] = r\alpha_m N $. We want to make sure that $c$ will lose \emph{approximately} $r\alpha_m N$ points.
	However---as it is many times the case---we are afraid that $X_c$ will diverge too much from $\EE[X_c]$. To analyze that, note that we can treat $X_c$ as a sum of independent random variables $X_{v,c}$, where 
	\begin{equation*}
	X_{v,c} = 
	\begin{cases}
	\alpha_{j(v,c)} & \text{if $v$ is chosen to be bribed;} \\
	0 & \text{otherwise.} 
	\end{cases}
	\end{equation*}	
	By Corollary~\ref{cor:2}, we get that $X_c \in [r\alpha_mN \pm R_1(\lambda, \alpha_{m}, r\alpha_m N) ] \subseteq [r\alpha_mN \pm R_1(\lambda, \alpha_{m}, \alpha_m N)]$ with failure probability at most $\NN^{-\lambda}$. 
	
	
	When we re-add the bribed voters according to our scheme, $c$ receives a score in $[\bar{\alpha} N \pm R_1(\lambda, \alpha_{m}, \alpha_m N)]$ points with failure probability at most $\NN^{-\lambda}$---again, by a similar application of Corollary~\ref{cor:2}. 
	
%	In particular, notice that since it holds that $\ln \NN = o(N)$, then $R_1(\lambda, \alpha_{m}, \alpha_m N)=o(\alpha_m N)$. Therefore, 
Summing up, after the entire bribery process, $c$ had lost at least $(r\alpha_m - \bar{\alpha})N-2R_1(\lambda, \alpha_{m}, \alpha_m N)$ points with failure probability at most $2\NN^{-\lambda}$. Using the union-bound, the same can be made to hold for all $m$ candidates simultaneously with failure probability at most $2\NN^{-\lambda+1}$.
	
	
	
	We can now split to cases; candidates with $r\geq 1-4\epsilon$ lost at least $\epsilon \alpha_m N$ points, assuming that  $2R_1(\lambda, \alpha_{m}, \alpha_m N) \leq\epsilon \alpha_m N$ (as it is asymptotically; otherwise the entire input is constant-sized). 
	Candidates with $r<  1-4\epsilon$ might have gained points in the process, however the number of points gained in the process is bounded by the number of points awarded in the voter re-addition stage. Since the number of these awarded points is at most  $\bar{\alpha}N + R_1(\lambda, \alpha_{m}, \alpha_m N)$, each such candidate $c$ now have score of at most $s''(c) \leq (1-4\epsilon)\alpha_mn + \bar{\alpha} N + R_1(\lambda, \alpha_{m}, \alpha_m N)$. However,  since $N \leq \epsilon n$ (follows by the fact that $n \geq \abs{A}> \epsilon^{-1}N$), and $R_1(\lambda, \alpha_{m}, \alpha_m N) \leq\epsilon \alpha_m N < \epsilon \alpha_m n$, 
	\begin{align*}
	s''(c) &\leq (1-4\epsilon)\alpha_m n + \bar{\alpha} N+ R_1(\lambda, \alpha_{m}, \alpha_m N)\\
	&\leq (1-4\epsilon)\alpha_m n + \bar{\alpha} \epsilon n+ \epsilon \alpha_m n\\
	&< (1-4\epsilon)\alpha_m n + \alpha_m \epsilon n + \epsilon \alpha_m n\\
	&= (1-2\epsilon)\alpha_m n  \leq s'(p) - \epsilon\alpha_m n \ .
	\end{align*}
	
	We conclude that after this process, every candidate either lost $\epsilon \alpha_m N$ points, or gained points, but in that case never surpassed $s'(p) - \epsilon\alpha_m n \leq s''(p) - \epsilon\alpha_m n$. The amount of voters we have bribed is $\abs{B} + N \leq 2N < \epsilon^{-1}N$. The lemma thus follows.	
\end{proof}
With Lemmas~\ref{lem:how-to-bribe2} and~\ref{lem:how-to-bribe}, we have just shown that for many types of $\veca$, the ratio between a margin to the number of bribed voters needed in order to close the margin is $O(\alpha_m)$.
This leads to the following:
\begin{lemma}\label{lem:f'}
	Assuming that Lemma~\ref{lem:margin} did not fail, then besides the $\tilde{k}= k^\star +f$ voters we have already bribed, with failure probability at most $\lceil R_4/(\epsilon \alpha_m \ln^{1+\delta}\NN)\rceil \cdot 2\NN^{-\lambda+1}$, it  holds that
	at most $f'=\epsilon^{-2} R_4 / \alpha_m + \epsilon^{-1}\ln^{1+\delta}\NN $ additional voters are needed to be bribed in order for $p$ to win, for some constant $\epsilon > 0$.
\end{lemma}
\begin{proof}
	By repeatedly applying the algorithm in the constructive proof of either  Lemma~\ref{lem:how-to-bribe2} or Lemma~\ref{lem:how-to-bribe}, until $p$ wins. For constant scoring rules the analysis is straightforward. For non-concentrated scoring rules, since every batch of $\epsilon^{-1}N=\epsilon^{-1}\ln^{1+\delta}\NN$ bribed voters decrease the margin by at least $\epsilon \alpha_m N$ points, at most $f'=\lceil R_4/(\epsilon \alpha_m N)\rceil \cdot  \epsilon^{-1}N$ bribed voters are needed.
	
	As for the failure probability, we can be conservative and require that each of the $\lceil R_4/(\epsilon \alpha_m N)\rceil $ iterations will succeed; using the union-bound, the probability any of the iterations will fail is at most $\lceil R_4/(\epsilon \alpha_m N)\rceil \cdot 2\NN^{-\lambda+1}$.
\end{proof}
%\begin{corollary}
%	For any scoring rule $\Ra$ such that either $\alpha_m = O(1)$ or that $\bar{\alpha} \leq (1-\epsilon)\alpha_m$ for some constant $\epsilon > 0$, given an instance of $\Ra$-bribery, let $\bar{k}$ be the number of voters to be bribed by an optimal strategy. Then our algorithm can make $p$ win  w.h.p.\ by bribing at most $\bar{k} + \widetilde{O}(\sqrt{\bar{k}})$ voters.
%\end{corollary}
We are now ready to complete the proof for Theorem~\ref{thr:brib}. 
\begin{proof}[Proof of Theorem~\ref{thr:brib}]
	Let $\bar{k}$ be the number of voters bribed by an optimal strategy, and notice that $k^\star \leq \bar{k}$, since the LP is a relaxation of the original problem.
	Following  the above discussion, we had bribed overall $k^\star +f+f' \leq \bar{k} +f+f'$ voters. For the sake of brevity, and since our concern is order of magnitude analysis, we will only loosely bound both the approximation factor $f+f'$ and the failure probability. 
	
	Since $R_4$ can be loosely bounded by $41\lambda^2 \alpha_m  (\bar{k}+1)^{1/2}\ln^2 \NN $, then 
	$f+f'$ is bounded by $43\lambda^2 \epsilon^{-2} (\bar{k}+1)^{1/2} \ln^2 \NN = \widetilde{O}(\sqrt{\bar{k}})$. As for the failure probability, we require both Lemmas~\ref{lem:margin} and~\ref{lem:f'} to succeed; the probability any of them would fail is at most $6\NN^{-\lambda+1}+\lceil R_4/(\epsilon \alpha_m N)\rceil \cdot 2\NN^{-\lambda+1} \leq (48\lambda^2 \epsilon^{-1} \ln \NN) \cdot (\bar{k}+1)^{1/2}  2\NN^{-\lambda+1}=\widetilde{O}(\bar{k})/\NN^{\lambda-1}$. Setting $\lambda=3$ will thus provide at most $1/\Omega(\NN)$ failure probability, since $\bar{k} \leq n$.    
	
%	The above algorithm has a polynomially-small failure probability. 
	By running the algorithm a linear number of times, and choosing the run yielding minimal number of bribed voters, the failure probability becomes exponentially-small, while the runtime stays polynomial.
\end{proof}


\section{Conclusions}
Bribery can be seen as a two stage process: voter elimination, followed by the addition of voters with new ballots. The former can be seen as a set cover variant, while the latter is exactly a coalitional manipulation instance. However these problems should not be solved independently, and deciding which voters to eliminate must be tightly integrated with the decision on their new strategy. We showed that this can be achieved by an LP: its fractional solution, determines both stages at once. While we cannot retain this property when requiring an integral solution, the LP still enables us (a) to take all information into account when deciding who to eliminate, and (b) to create a UCM instance which does not add much to the objective.

\section{Acknowledgments}
	This work was supported by the Israel Science Foundation, under Grant No. 1488/14 and Grant No. 1394/16.


 
	
\bibliographystyle{aaai}
\bibliography{voting_short_form}


	
\end{document}