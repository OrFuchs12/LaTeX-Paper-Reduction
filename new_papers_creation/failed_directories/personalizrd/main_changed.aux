\relax 
\bibstyle{aaai24}
\citation{shepitsen2008personalized,lee2012prea,yao2020rlper}
\citation{wu2023personalized}
\citation{ayer2012or}
\citation{capponi2022personalized}
\citation{breton2020randomized}
\citation{jdrf_blog_2022}
\newlabel{sec:introduction}{{1}{1}{}{}{}}
\citation{macqueen1967classification,lloyd1982least}
\citation{dempster1977maximum}
\citation{barrett2008learning}
\citation{todorov2012mujoco}
\citation{den2020reinforcement}
\citation{hassouni2018personalization,zhu2018group,grua2018exploring,el2019end,el2022ph}
\citation{martin2004agentx,goindani2020cluster}
\citation{tabatabaei2018narrowing,baucum2022adapting}
\citation{nadiger2019federated}
\citation{perkins2002lyapunov,hans2008safe,moldovan2012safe,junges2016safety}
\citation{finn2017model}
\citation{hayes2022practical}
\newlabel{sec:introduction_related}{{1.1}{2}{}{}{}}
\citation{littman1994markov}
\citation{diana2021algorithms}
\citation{mandal2022socially}
\citation{kar2023feature}
\citation{schulman2017proximal}
\citation{schulman2015high}
\newlabel{sec:problem}{{2}{3}{}{}{}}
\newlabel{sec:problem_markov}{{2.1}{3}{}{}{}}
\newlabel{sec:problem_represented}{{2.2}{3}{}{}{}}
\newlabel{eq:objective_joint}{{2}{3}{}{}{}}
\newlabel{sec:problem_proximal}{{2.3}{3}{}{}{}}
\newlabel{eq:loss_ppo_critic}{{3}{3}{}{}{}}
\citation{schulman2015trust}
\citation{shengyi2022the37implementation}
\citation{macqueen1967classification,lloyd1982least}
\citation{dempster1977maximum}
\newlabel{eq:loss_ppo_actor_short}{{4}{4}{}{}{}}
\newlabel{sec:our}{{3}{4}{}{}{}}
\newlabel{sec:our_factorized}{{3.1}{4}{}{}{}}
\newlabel{eq:objective_assignment}{{5}{4}{}{}{}}
\newlabel{eq:optimal_assignment}{{6}{4}{}{}{}}
\newlabel{eq:objective_policy}{{7}{4}{}{}{}}
\newlabel{sec:our_representatives}{{3.2}{4}{}{}{}}
\newlabel{eq:loss_our_critic}{{8}{4}{}{}{}}
\newlabel{eq:loss_our_actor}{{9}{4}{}{}{}}
\newlabel{sec:our_hard}{{3.3}{4}{}{}{}}
\newlabel{eq:hard_Q_table}{{10}{4}{}{}{}}
\citation{barrett2008learning,Alegre+2022bnaic}
\citation{todorov2012mujoco,tassa2018deepmind,tunyasuvunakool2020}
\citation{finn2017model}
\newlabel{eq:hard_E_step}{{11}{5}{}{}{}}
\newlabel{theorem}{{1}{5}{}{}{}}
\newlabel{sec:our_soft}{{3.4}{5}{}{}{}}
\newlabel{eq:soft_loss}{{12}{5}{}{}{}}
\newlabel{sec:experiments}{{4}{5}{}{}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:results_resource_k=1}{{1a}{6}{An optimal path, k=1}{}{}}
\newlabel{sub@fig:results_resource_k=1}{{a}{6}{An optimal path, k=1}{}{}}
\newlabel{fig:results_resource_k=2}{{1b}{6}{Paths of EM, k=2}{}{}}
\newlabel{sub@fig:results_resource_k=2}{{b}{6}{Paths of EM, k=2}{}{}}
\newlabel{fig:results_resource_k=3}{{1c}{6}{Paths of EM, k=3}{}{}}
\newlabel{sub@fig:results_resource_k=3}{{c}{6}{Paths of EM, k=3}{}{}}
\newlabel{fig:results_resource_k=5}{{1d}{6}{Paths of EM, k=5}{}{}}
\newlabel{sub@fig:results_resource_k=5}{{d}{6}{Paths of EM, k=5}{}{}}
\newlabel{fig:results_resource}{{1}{6}{Paths that representatives learn in Resource Gathering after being trained with our EM algorithm for different $k$ and $n=25$ ($0$-th random seed). The representatives divide the map such that 1) each tile is visited by some policy and 2) policies jointly minimize the average episode length.}{}{}}
\newlabel{sec:experiments_resource}{{4.1}{6}{}{}{}}
\newlabel{fig:results_resource_sw}{{2}{6}{Performance of ours and baseline algorithms in Resource Gathering for different $k$ and $n=25$. The black dashed line represents the optimum for $k=25$. For each $k$, all algorithms are trained for 1 million transitions per policy. For $k=1$, all algorithms reduce to solving an MDP with a single policy. Confidence intervals represent standard errors.}{}{}}
\newlabel{sec:experiments_mujoco}{{4.2}{6}{}{}{}}
\newlabel{fig:results_mujoco}{{3}{7}{Performance of ours and baseline algorithms in MuJoCo environments. For each $k$, all algorithms are trained for 2 million transitions per policy. The number of agents is $n=1000$ for $k=50$ and $n=100$ for smaller $k$. For $k=1$, all algorithms reduce to solving an MDP with a single policy. Confidence intervals represent standard errors.}{}{}}
\newlabel{fig:results_histograms}{{4}{7}{Histograms of agent assignments learned by ours and baseline algorithms for $n=100$, $k=5$ in HalfCheetah ($0$-th random seed). Each color denotes one of five representatives and bars of this color denote the target velocities of agents assigned to this representative. The expected behavior is a division of the agents' velocities into five intervals of similar sizes, one for each representative. Histograms for other environments are reported in the Appendix.}{}{}}
\newlabel{sec:conclusion}{{5}{7}{}{}{}}
\bibdata{main}
\citation{paszke2019pytorch}
\citation{shengyi2022the37implementation}
\citation{schulman2017proximal}
\citation{schulman2015high}
\citation{kingma2014adam}
\newlabel{lemma:app}{{1}{12}{}{}{}}
\newlabel{theorem:app}{{2}{12}{}{}{}}
\newlabel{alg:algorithm}{{1}{12}{EM-like meta-algorithm}{}{}}
\newlabel{eq:loss_ppo_actor}{{15}{12}{}{}{}}
\newlabel{fig:results_histograms_app}{{5}{14}{Histograms of agent assignments learned by ours and baseline algorithms for $n=100$, $k=5$ in Ant, Hopper, and Walker2d ($0$-th random seed). Each color denotes one of five representatives and bars of this color denote the target velocities of agents assigned to this representative. The expected behavior is a division of the agents' velocities into five intervals of similar sizes, one for each representative.}{}{}}
\gdef \@abspage@last{14}
