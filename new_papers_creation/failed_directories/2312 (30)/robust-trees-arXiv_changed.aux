\relax 
\bibstyle{aaai24}
\citation{song2022learning}
\citation{brodley1996identifying}
\citation{tanno2019learning,lukasik2020does}
\citation{manwani2013noise,yang2019robust}
\citation{grinsztajn2022tree,kaggle2021state}
\citation{ghosh2017robust,zhang2018generalized}
\citation{yang2019robust,wilton2022positive}
\citation{frenay2013classification}
\citation{song2022learning}
\citation{natarajan2013learning,patrini2017making}
\citation{manwani2013noise,ghosh2015making,ghosh2017robust}
\citation{zhang2018generalized}
\citation{kim2019nlnl}
\citation{wang2019symmetric}
\citation{Lyu2020Curriculum}
\citation{ma2020normalized}
\newlabel{sec:related}{{2}{1}{}{}{}}
\newlabel{sec:related@cref}{{[section][2][]2}{[1][1][]1}}
\citation{breiman1984classification,breiman2001random,geurts2006extremely}
\citation{breiman1984classification}
\citation{mantas2014credal}
\citation{ghosh2017robustness}
\citation{zhou2019improving}
\citation{yang2019robust}
\citation{manwani2013noise}
\citation{ghosh2015making,ghosh2017robust}
\citation{ghosh2017robust}
\citation{zhang2018generalized}
\citation{zhang2018generalized}
\newlabel{sec:background}{{3}{2}{}{}{}}
\newlabel{sec:background@cref}{{[section][3][]3}{[1][2][]2}}
\citation{yang2019robust,wilton2022positive}
\citation{breiman1984classification,painsky2018universality,yang2019robust,wilton2022positive}
\citation{ghosh2017robust}
\citation{zhang2018generalized}
\newlabel{sec:theory}{{4}{3}{}{}{}}
\newlabel{sec:theory@cref}{{[section][4][]4}{[1][3][]3}}
\newlabel{thmt@@lie@data}{{\def \theequation {\@arabic {\c@equation }}\setcounter {equation}{1}}{3}{}{}{}}
\newlabel{thmt@@lie@data@cref}{{[section][4][]4}{[1][3][]3}}
\newlabel{thmt@@lie}{{1}{3}{}{}{}}
\newlabel{thmt@@lie@cref}{{[theorem][1][]1}{[1][3][]3}}
\newlabel{thm:loss_impurity_equivalence}{{1}{3}{}{}{}}
\newlabel{thm:loss_impurity_equivalence@cref}{{[theorem][1][]1}{[1][3][]3}}
\newlabel{result a}{{1}{3}{}{}{}}
\newlabel{result a@cref}{{[theorem][1][]1}{[1][3][]3}}
\newlabel{defn:conservative}{{1}{3}{}{}{}}
\newlabel{defn:conservative@cref}{{[defn][1][]1}{[1][3][]3}}
\newlabel{thmt@@universal@data}{{\def \theequation {\@arabic {\c@equation }}\setcounter {equation}{1}}{3}{}{}{}}
\newlabel{thmt@@universal@data@cref}{{[section][4][]4}{[1][3][]3}}
\newlabel{thmt@@universal}{{2}{3}{}{}{}}
\newlabel{thmt@@universal@cref}{{[theorem][2][]2}{[1][3][]3}}
\newlabel{thm:universal}{{2}{3}{}{}{}}
\newlabel{thm:universal@cref}{{[theorem][2][]2}{[1][3][]3}}
\newlabel{eq:misclassification_impurity}{{2}{3}{}{}{}}
\newlabel{eq:misclassification_impurity@cref}{{[equation][2][]2}{[1][3][]3}}
\newlabel{thmt@@rl@data}{{\def \theequation {\@arabic {\c@equation }}\setcounter {equation}{2}}{3}{}{}{}}
\newlabel{thmt@@rl@data@cref}{{[section][4][]4}{[1][3][]3}}
\newlabel{thmt@@rl}{{2.1}{3}{}{}{}}
\newlabel{thmt@@rl@cref}{{[corollary][1][]2.1}{[1][3][]3}}
\newlabel{cor:robust_losses}{{2.1}{3}{}{}{}}
\newlabel{cor:robust_losses@cref}{{[corollary][1][]2.1}{[1][3][]3}}
\newlabel{thmt@@ohp@data}{{\def \theequation {\@arabic {\c@equation }}\setcounter {equation}{2}}{3}{}{}{}}
\newlabel{thmt@@ohp@data@cref}{{[section][4][]4}{[1][3][]3}}
\newlabel{thmt@@ohp}{{3}{3}{}{}{}}
\newlabel{thmt@@ohp@cref}{{[theorem][3][]3}{[1][3][]3}}
\newlabel{cor:one-hot-predictions}{{3}{3}{}{}{}}
\newlabel{cor:one-hot-predictions@cref}{{[theorem][3][]3}{[1][3][]3}}
\citation{breiman1984classification}
\citation{breiman1984classification}
\newlabel{thmt@@hoeffding@data}{{\def \theequation {\@arabic {\c@equation }}\setcounter {equation}{2}}{4}{}{}{}}
\newlabel{thmt@@hoeffding@data@cref}{{[section][4][]4}{[1][4][]4}}
\newlabel{thmt@@hoeffding}{{4}{4}{}{}{}}
\newlabel{thmt@@hoeffding@cref}{{[theorem][4][]4}{[1][4][]4}}
\newlabel{thm:hoeffding}{{4}{4}{}{}{}}
\newlabel{thm:hoeffding@cref}{{[theorem][4][]4}{[1][4][]4}}
\newlabel{thmt@@es@data}{{\def \theequation {\@arabic {\c@equation }}\setcounter {equation}{2}}{4}{}{}{}}
\newlabel{thmt@@es@data@cref}{{[section][4][]4}{[1][4][]4}}
\newlabel{thmt@@es}{{5}{4}{}{}{}}
\newlabel{thmt@@es@cref}{{[theorem][5][]5}{[1][4][]4}}
\newlabel{thm:early_stopping}{{5}{4}{}{}{}}
\newlabel{thm:early_stopping@cref}{{[theorem][5][]5}{[1][4][]4}}
\newlabel{sec:new-framework}{{5}{4}{}{}{}}
\newlabel{sec:new-framework@cref}{{[section][5][]5}{[1][4][]4}}
\newlabel{thmt@@distloss@data}{{\def \theequation {\@arabic {\c@equation }}\setcounter {equation}{2}}{4}{}{}{}}
\newlabel{thmt@@distloss@data@cref}{{[section][5][]5}{[1][4][]4}}
\newlabel{thmt@@distloss}{{6}{4}{}{}{}}
\newlabel{thmt@@distloss@cref}{{[lemma][6][]6}{[1][4][]4}}
\newlabel{lem:distloss}{{6}{4}{}{}{}}
\newlabel{lem:distloss@cref}{{[lemma][6][]6}{[1][4][]4}}
\newlabel{thmt@@neloss@data}{{\def \theequation {\@arabic {\c@equation }}\setcounter {equation}{2}}{4}{}{}{}}
\newlabel{thmt@@neloss@data@cref}{{[section][5][]5}{[1][4][]4}}
\newlabel{thmt@@neloss}{{7}{4}{}{}{}}
\newlabel{thmt@@neloss@cref}{{[lemma][7][]7}{[1][4][]4}}
\newlabel{lem:neloss}{{7}{4}{}{}{}}
\newlabel{lem:neloss@cref}{{[lemma][7][]7}{[1][4][]4}}
\newlabel{thmt@@negexp@data}{{\def \theequation {\@arabic {\c@equation }}\setcounter {equation}{2}}{4}{}{}{}}
\newlabel{thmt@@negexp@data@cref}{{[section][5][]5}{[1][4][]4}}
\newlabel{thmt@@negexp}{{8}{4}{}{}{}}
\newlabel{thmt@@negexp@cref}{{[theorem][8][]8}{[1][4][]4}}
\newlabel{thm:negexp}{{8}{4}{}{}{}}
\newlabel{thm:negexp@cref}{{[theorem][8][]8}{[1][4][]4}}
\citation{ghosh2017robustness}
\citation{zhang2018generalized}
\citation{breiman1984classification}
\citation{mantas2014credal}
\citation{zhang2018generalized}
\citation{yang2019robust}
\citation{ghosh2017robustness}
\citation{misc_covertype_31}
\citation{lang1995newsweeder}
\citation{misc_mushroom_73}
\citation{lecun1998gradient}
\citation{krizhevsky2009learning}
\citation{moustafa2015unsw}
\citation{kiryo2017positive,wilton2022positive}
\citation{pennington2014glove}
\citation{collobert2001parallel}
\citation{chang2011libsvm}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:neg-exp}{{1}{5}{Left: NE loss as a function of the margin $y\setbox \z@ \hbox {\mathsurround \z@ $\textstyle y$}\mathaccent "0362{y}$. Right: Cross entropy (also GCE impurity with $q=0$), GCE impurities for $q=0.2, 0.5, 0.8$, the misclassification impurity (also GCE impurity with $q=1$ and NE impurity with $\lambda =1$), and NE impurities for $\lambda = 0.2, 0.5, 0.8$. The impurities have been scaled for better comparison. }{}{}}
\newlabel{fig:neg-exp@cref}{{[figure][1][]1}{[1][4][]5}}
\newlabel{tab:datasets}{{1}{5}{Benchmark datasets.}{}{}}
\newlabel{tab:datasets@cref}{{[table][1][]1}{[1][5][]5}}
\newlabel{sec:experiments}{{6}{5}{}{}{}}
\newlabel{sec:experiments@cref}{{[section][6][]6}{[1][5][]5}}
\citation{pedregosa2011scikit}
\citation{zhang2018generalized}
\citation{mantas2014credal}
\newlabel{fig:dt-bin}{{2}{6}{ Mean test accuracy with 2x sd bands for DT on binary classification problems using different splitting criteria. Training labels corrupted using uniform noise $\eta \in \{0.0, 0.1,0.2,0.3,0.4\}$ and class conditional noise CC1 $(0.1,0.3)$ and CC2 $(0.2,0.4)$. }{}{}}
\newlabel{fig:dt-bin@cref}{{[figure][2][]2}{[1][5][]6}}
\newlabel{fig:dt-mc}{{3}{6}{ Mean test accuracy with 2x sd bands for DT on multiclass classification problems using different splitting criteria. Training labels corrupted using uniform noise $\eta \in \{0.0,0.1,0.2,0.3,0.4\}$ and class conditional (CC) noise. }{}{}}
\newlabel{fig:dt-mc@cref}{{[figure][3][]3}{[1][5][]6}}
\newlabel{fig:rf-bin}{{4}{6}{ Mean test accuracy with 2x sd bands for RF on binary classification problems using different splitting criteria. Training labels corrupted using uniform noise $\eta \in \{0.0,0.1,0.2,0.3,0.4\}$ and class conditional noise CC1 $(0.1,0.3)$ and CC2 $(0.2,0.4)$. }{}{}}
\newlabel{fig:rf-bin@cref}{{[figure][4][]4}{[1][5][]6}}
\newlabel{fig:rf-mc}{{5}{6}{ Mean test accuracy with 2x sd bands for RF on multiclass classification problems using different splitting criteria. Training labels corrupted using uniform noise $\eta \in \{0.0,0.1,0.2,0.3,0.4\}$ and class conditional (CC) noise. }{}{}}
\newlabel{fig:rf-mc@cref}{{[figure][5][]5}{[1][5][]6}}
\bibdata{aaai24}
\bibdata{refs}
\newlabel{fig:threshold_tuning}{{6}{7}{Performance of DT using NE loss with different values of $\lambda $ on MNIST digits, binary and multiclass classification, 20\% of noisily labeled training data used for validation. Results reported as mean $\pm $ 2x sd over 5 replications.}{}{}}
\newlabel{fig:threshold_tuning@cref}{{[figure][6][]6}{[1][7][]7}}
\citation{zhang2018generalized}
\newlabel{app:equivalence_thm}{{A}{8}{}{}{}}
\newlabel{app:equivalence_thm@cref}{{[appendix][1][2147483647]A}{[1][8][]8}}
\newlabel{app:proof_universal}{{B}{9}{}{}{}}
\newlabel{app:proof_universal@cref}{{[appendix][2][2147483647]B}{[1][9][]9}}
\newlabel{thmt@@earthmoving@data}{{\def \theequation {\@arabic {\c@equation }}\setcounter {equation}{2}}{9}{}{}{}}
\newlabel{thmt@@earthmoving@data@cref}{{[appendix][2][2147483647]B}{[1][9][]9}}
\newlabel{thmt@@earthmoving}{{9}{9}{}{}{}}
\newlabel{thmt@@earthmoving@cref}{{[lemma][9][]9}{[1][9][]9}}
\newlabel{lemma:earth-moving}{{9}{9}{}{}{}}
\newlabel{lemma:earth-moving@cref}{{[lemma][9][]9}{[1][9][]9}}
\newlabel{eq:earthmoving}{{3}{9}{}{}{}}
\newlabel{eq:earthmoving@cref}{{[equation][3][2147483647]3}{[1][9][]9}}
\newlabel{eq:lb}{{4}{9}{}{}{}}
\newlabel{eq:lb@cref}{{[equation][4][2147483647]4}{[1][9][]9}}
\newlabel{app:cor_robust_losses}{{C}{10}{}{}{}}
\newlabel{app:cor_robust_losses@cref}{{[appendix][3][2147483647]C}{[1][10][]10}}
\newlabel{app:one-hot-predictions}{{D}{10}{}{}{}}
\newlabel{app:one-hot-predictions@cref}{{[appendix][4][2147483647]D}{[1][10][]10}}
\newlabel{app:hoeffding}{{E}{10}{}{}{}}
\newlabel{app:hoeffding@cref}{{[appendix][5][2147483647]E}{[1][10][]10}}
\newlabel{eq:booles}{{5}{10}{}{}{}}
\newlabel{eq:booles@cref}{{[equation][5][2147483647]5}{[1][10][]10}}
\citation{breiman1984classification}
\newlabel{eq:lower}{{7}{11}{}{}{}}
\newlabel{eq:lower@cref}{{[equation][7][2147483647]7}{[1][11][]11}}
\newlabel{app:early_stopping}{{F}{11}{}{}{}}
\newlabel{app:early_stopping@cref}{{[appendix][6][2147483647]F}{[1][11][]11}}
\newlabel{thmt@@maxnormequality@data}{{\def \theequation {\@arabic {\c@equation }}\setcounter {equation}{7}}{11}{}{}{}}
\newlabel{thmt@@maxnormequality@data@cref}{{[appendix][6][2147483647]F}{[1][11][]11}}
\newlabel{thmt@@maxnormequality}{{10}{11}{}{}{}}
\newlabel{thmt@@maxnormequality@cref}{{[lemma][10][]10}{[1][11][]11}}
\newlabel{lemma:max-norm-equality}{{10}{11}{}{}{}}
\newlabel{lemma:max-norm-equality@cref}{{[lemma][10][]10}{[1][11][]11}}
\newlabel{eq:compare_maxs}{{8}{11}{}{}{}}
\newlabel{eq:compare_maxs@cref}{{[equation][8][2147483647]8}{[1][11][]11}}
\newlabel{eq:mi_es}{{9}{11}{}{}{}}
\newlabel{eq:mi_es@cref}{{[equation][9][2147483647]9}{[1][11][]11}}
\newlabel{eq:ycdf}{{10}{12}{}{}{}}
\newlabel{eq:ycdf@cref}{{[equation][10][2147483647]10}{[1][12][]12}}
\newlabel{app:NE-loss}{{I}{12}{}{}{}}
\newlabel{app:NE-loss@cref}{{[appendix][9][2147483647]I}{[1][12][]12}}
\newlabel{app:class-conditional-noise}{{J}{12}{}{}{}}
\newlabel{app:class-conditional-noise@cref}{{[appendix][10][2147483647]J}{[1][12][]12}}
\newlabel{app:cifar10-pretrained}{{K}{12}{}{}{}}
\newlabel{app:cifar10-pretrained@cref}{{[appendix][11][2147483647]K}{[1][12][]12}}
\newlabel{fig:pre_trained}{{7}{13}{Mean test accuracy with 2x standard deviation bands for decision tree and random forest with different splitting criteria. Training labels corrupted using uniform noise $\eta \in \{0.0,0.1,0.2,0.3,0.4\}$ and class conditional noise CC1, CC2 for binary classification and CC for multiclass classification. }{}{}}
\newlabel{fig:pre_trained@cref}{{[figure][7][2147483647]7}{[1][12][]13}}
\newlabel{alg:ccn}{{1}{13}{Class Conditional Noise}{}{}}
\newlabel{alg:ccn@cref}{{[algorithm][1][2147483647]1}{[1][12][]13}}
\newlabel{app:results}{{L}{13}{}{}{}}
\newlabel{app:results@cref}{{[appendix][12][2147483647]L}{[1][12][]13}}
\newlabel{app:alternate_early_stopping}{{M}{13}{}{}{}}
\newlabel{app:alternate_early_stopping@cref}{{[appendix][13][2147483647]M}{[1][13][]13}}
\newlabel{fig:alternate_early_stopping}{{8}{13}{ Left: NE loss with $\lambda =0$ ranks the split producing a pure child node above the one that does not. Right: NE loss with $\lambda =1$ gives opposite rankings. }{}{}}
\newlabel{fig:alternate_early_stopping@cref}{{[figure][8][2147483647]8}{[1][13][]13}}
\newlabel{tab:dt-bin}{{2}{14}{Mean test accuracy (\%) $\pm $ 2 standard deviations for DT on binary classification problems. Methods giving the largest mean accuracy and those with overlapping 97.5\% confidence interval are highlighted for each dataset and noise setting.}{}{}}
\newlabel{tab:dt-bin@cref}{{[table][2][2147483647]2}{[1][13][]14}}
\newlabel{tab:dt-mc}{{3}{15}{Mean test accuracy (\%) $\pm $ 2 standard deviations for DT on multiclass classification problems. Methods giving the largest mean accuracy and those with overlapping 97.5\% confidence interval are highlighted for each dataset and noise setting.}{}{}}
\newlabel{tab:dt-mc@cref}{{[table][3][2147483647]3}{[1][13][]15}}
\newlabel{tab:rf-bin}{{4}{16}{Mean test accuracy (\%) $\pm $ 2 standard deviations for RF on binary classification problems. Methods giving the largest mean accuracy and those with overlapping 97.5\% confidence interval are highlighted for each dataset and noise setting.}{}{}}
\newlabel{tab:rf-bin@cref}{{[table][4][2147483647]4}{[1][13][]16}}
\newlabel{tab:rf-mc}{{5}{17}{Mean test accuracy (\%) $\pm $ 2 standard deviations for RF on multiclass classification problems. Methods giving the largest mean accuracy and those with overlapping 97.5\% confidence interval are highlighted for each dataset and noise setting.}{}{}}
\newlabel{tab:rf-mc@cref}{{[table][5][2147483647]5}{[1][13][]17}}
\gdef \@abspage@last{17}
