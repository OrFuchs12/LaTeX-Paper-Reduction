@inproceedings{lukasik2020does,
	author = {Lukasik, Michal and Bhojanapalli, Srinadh and Menon, Aditya and Kumar, Sanjiv},
	booktitle = {International Conference on Machine Learning},
	organization = {PMLR},
	pages = {6448--6458},
	title = {{Does label smoothing mitigate label noise?}},
	year = {2020},
}

@inproceedings{tanno2019learning,
	author = {Tanno, Ryutaro and Saeedi, Ardavan and Sankaranarayanan, Swami and Alexander, Daniel C and Silberman, Nathan},
	booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages = {11244--11253},
	title = {{Learning from noisy labels by regularized estimation of annotator confusion}},
	year = {2019},
}


@inproceedings{ghosh2017robustness,
  title={On the robustness of decision tree learning under label noise},
  author={Ghosh, Aritra and Manwani, Naresh and Sastry, PS},
  booktitle={Advances in Knowledge Discovery and Data Mining: 21st Pacific-Asia Conference, PAKDD 2017, Jeju, South Korea, May 23-26, 2017, Proceedings, Part I 21},
  pages={685--697},
  year={2017},
  organization={Springer}
}



@article{ghosh2017robust, 
  title={Robust Loss Functions under Label Noise for Deep Neural Networks}, 
  volume={31}, 
  url={https://ojs.aaai.org/index.php/AAAI/article/view/10894}, 
  DOI={10.1609/aaai.v31i1.10894}, 
  abstractNote={ &lt;p&gt; In many applications of classifier learning, training data suffers from label noise. Deep networks are learned using huge training data where the problem of noisy labels is particularly relevant. The current techniques proposed for learning deep networks under label noise focus on modifying the network architecture and on algorithms for estimating true labels from noisy labels. An alternate approach would be to look for loss functions that are inherently noise-tolerant. For binary classification there exist theoretical results on loss functions that are robust to label noise. In this paper, we provide some sufficient conditions on a loss function so that risk minimization under that loss function would be inherently tolerant to label noise for multiclass classification problems. These results generalize the existing results on noise-tolerant loss functions for binary classification. We study some of the widely used loss functions in deep networks and show that the loss function based on mean absolute value of error is inherently robust to label noise. Thus standard back propagation is enough to learn the true classifier even under label noise. Through experiments, we illustrate the robustness of risk minimization with such loss functions for learning neural networks. &lt;/p&gt; }, 
  number={1}, 
  journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
  author={Ghosh, Aritra and Kumar, Himanshu and Sastry, P. S.}, 
  year={2017}, 
  month={Feb.} 
}

@article{ghosh2015making,
  title={Making risk minimization tolerant to label noise},
  author={Ghosh, Aritra and Manwani, Naresh and Sastry, PS},
  journal={Neurocomputing},
  volume={160},
  pages={93--107},
  year={2015},
  publisher={Elsevier}
}

@article{zhang2018generalized,
  title={Generalized cross entropy loss for training deep neural networks with noisy labels},
  author={Zhang, Zhilu and Sabuncu, Mert},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{yang2019robust,
  title={On the robust splitting criterion of random forest},
  author={Yang, Bin-Bin and Gao, Wei and Li, Ming},
  booktitle={2019 IEEE International Conference on Data Mining (ICDM)},
  pages={1420--1425},
  year={2019},
  organization={IEEE}
}

@article{wilton2022positive,
  title={Positive-Unlabeled Learning using Random Forests via Recursive Greedy Risk Minimization},
  author={Wilton, Jonathan and Koay, Abigail and Ko, Ryan and Xu, Miao and Ye, Nan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24060--24071},
  year={2022}
}

@article{mantas2014credal,
  title={Credal-C4. 5: Decision tree based on imprecise probabilities to classify noisy data},
  author={Mantas, Carlos J and Abellan, Joaquin},
  journal={Expert Systems with Applications},
  volume={41},
  number={10},
  pages={4625--4637},
  year={2014},
  publisher={Elsevier}
}

@inproceedings{zhou2019improving,
  title={Improving robustness of random forest under label noise},
  author={Zhou, Xu and Ding, Pak Lun Kevin and Li, Baoxin},
  booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={950--958},
  year={2019},
  organization={IEEE}
}

@article{frenay2013classification,
  title={Classification in the presence of label noise: a survey},
  author={Fr{\'e}nay, Beno{\^\i}t and Verleysen, Michel},
  journal={IEEE transactions on neural networks and learning systems},
  volume={25},
  number={5},
  pages={845--869},
  year={2013},
  publisher={IEEE}
}

@article{johnson2022survey,
  title={A survey on classifying big data with label noise},
  author={Johnson, Justin M and Khoshgoftaar, Taghi M},
  journal={ACM Journal of Data and Information Quality},
  volume={14},
  number={4},
  pages={1--43},
  year={2022},
  publisher={ACM New York, NY}
}

@article{song2022learning,
  title={Learning from noisy labels with deep neural networks: A survey},
  author={Song, Hwanjun and Kim, Minseok and Park, Dongmin and Shin, Yooju and Lee, Jae-Gil},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@inproceedings{painsky2018universality,
  title={On the universality of the logistic loss function},
  author={Painsky, Amichai and Wornell, Gregory},
  booktitle={2018 IEEE International Symposium on Information Theory (ISIT)},
  pages={936--940},
  year={2018},
  organization={IEEE}
}



@book{breiman1984classification,
series = {Wadsworth statistics/probability series},
publisher = {Wadsworth International Group},
booktitle = {Classification and regression trees},
isbn = {0534980538},
year = {1984},
title = {Classification and regression trees},
language = {eng},
address = {Belmont, Calif.},
author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
keywords = {Discriminant analysis; Regression analysis; Trees (Graph Theory)},
lccn = {83-319708},
}



@article{manwani2013noise,
  title={Noise tolerance under risk minimization},
  author={Manwani, Naresh and Sastry, PS},
  journal={IEEE transactions on cybernetics},
  volume={43},
  number={3},
  pages={1146--1151},
  year={2013},
  publisher={IEEE}
}

@inproceedings{ma2020normalized,
  title={Normalized loss functions for deep learning with noisy labels},
  author={Ma, Xingjun and Huang, Hanxun and Wang, Yisen and Romano, Simone and Erfani, Sarah and Bailey, James},
  booktitle={International conference on machine learning},
  pages={6543--6553},
  year={2020},
  organization={PMLR}
}

@inproceedings{wang2019symmetric,
  title={Symmetric cross entropy for robust learning with noisy labels},
  author={Wang, Yisen and Ma, Xingjun and Chen, Zaiyi and Luo, Yuan and Yi, Jinfeng and Bailey, James},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={322--330},
  year={2019}
}

@inproceedings{
    Lyu2020Curriculum,
    title={Curriculum Loss: Robust Learning and Generalization  against Label Corruption},
    author={Yueming Lyu and Ivor W. Tsang},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=rkgt0REKwS}
}

@inproceedings{kim2019nlnl,
  title={Nlnl: Negative learning for noisy labels},
  author={Kim, Youngdong and Yim, Junho and Yun, Juseung and Kim, Junmo},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={101--110},
  year={2019}
}

@inproceedings{menon2015learning,
  title={Learning from corrupted binary labels via class-probability estimation},
  author={Menon, Aditya and Van Rooyen, Brendan and Ong, Cheng Soon and Williamson, Bob},
  booktitle={International conference on machine learning},
  pages={125--134},
  year={2015},
  organization={PMLR}
}

@article{natarajan2013learning,
  title={Learning with noisy labels},
  author={Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep K and Tewari, Ambuj},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@phdthesis{van2015machine,
  title={Machine learning via transitions},
  author={Van Rooyen, Brendan},
  year={2015},
  school = {College of Engineering and Computer Science, The Australian National University},
}

@inproceedings{patrini2017making,
  title={Making deep neural networks robust to label noise: A loss correction approach},
  author={Patrini, Giorgio and Rozza, Alessandro and Krishna Menon, Aditya and Nock, Richard and Qu, Lizhen},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1944--1952},
  year={2017}
}

@article{sukhbaatar2014training,
  title={Training convolutional networks with noisy labels},
  author={Sukhbaatar, Sainbayar and Bruna, Joan and Paluri, Manohar and Bourdev, Lubomir and Fergus, Rob},
  journal={arXiv preprint arXiv:1406.2080},
  year={2014}
}

@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent},
  journal={the Journal of machine Learning research},
  volume={12},
  pages={2825--2830},
  year={2011},
  publisher={JMLR. org}
}

@article{hoeffding1994probability,
  title={Probability inequalities for sums of bounded random variables},
  author={Hoeffding, Wassily},
  journal={The collected works of Wassily Hoeffding},
  pages={409--426},
  year={1994},
  publisher={Springer}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{geurts2006extremely,
  title={Extremely randomized trees},
  author={Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
  journal={Machine learning},
  volume={63},
  pages={3--42},
  year={2006},
  publisher={Springer}
}

@inproceedings{moustafa2015unsw,
  title={UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set)},
  author={Moustafa, Nour and Slay, Jill},
  booktitle={2015 military communications and information systems conference (MilCIS)},
  pages={1--6},
  year={2015},
  organization={IEEE}
}

@misc{krizhevsky2009learning,
  author = {Krizhevsky, Alex},
  title = {Learning multiple layers of features from tiny images},
  howpublished = {MSc thesis, University of Toronto},
  year = {2009}
}


@misc{misc_covertype_31,
  author       = {Blackard,Jock},
  title        = {{Covertype}},
  year         = {1998},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C50K5N}
}

@article{collobert2001parallel,
  title={A parallel mixture of SVMs for very large scale problems},
  author={Collobert, Ronan and Bengio, Samy and Bengio, Yoshua},
  journal={Advances in Neural Information Processing Systems},
  volume={14},
  year={2001}
}

@incollection{lang1995newsweeder,
  title={Newsweeder: Learning to filter netnews},
  author={Lang, Ken},
  booktitle={Machine learning proceedings 1995},
  pages={331--339},
  year={1995},
  publisher={Elsevier}
}

@misc{misc_mushroom_73,
  title        = {{Mushroom}},
  year         = {1987},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5959T}
}

@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}

@article{kiryo2017positive,
  title={Positive-unlabeled learning with non-negative risk estimator},
  author={Kiryo, Ryuichi and Niu, Gang and Du Plessis, Marthinus C and Sugiyama, Masashi},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{chang2011libsvm,
  title={LIBSVM: a library for support vector machines},
  author={Chang, Chih-Chung and Lin, Chih-Jen},
  journal={ACM transactions on intelligent systems and technology (TIST)},
  volume={2},
  number={3},
  pages={1--27},
  year={2011},
  publisher={Acm New York, NY, USA}
}

@article{wu2008top,
  title={Top 10 algorithms in data mining},
  author={Wu, Xindong and Kumar, Vipin and Ross Quinlan, J and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J and Ng, Angus and Liu, Bing and Yu, Philip S and others},
  journal={Knowledge and information systems},
  volume={14},
  pages={1--37},
  year={2008},
  publisher={Springer}
}

@inproceedings{snow2008cheap,
  title={Cheap and fast--but is it good? evaluating non-expert annotations for natural language tasks},
  author={Snow, Rion and O’connor, Brendan and Jurafsky, Dan and Ng, Andrew Y},
  booktitle={Proceedings of the 2008 conference on empirical methods in natural language processing},
  pages={254--263},
  year={2008}
}

@inproceedings{brodley1996identifying,
  title={Identifying and eliminating mislabeled training instances},
  author={Brodley, Carla E and Friedl, Mark A and others},
  booktitle={Proceedings of the National Conference on Artificial Intelligence},
  pages={799--805},
  year={1996}
}

@article{grinsztajn2022tree,
  title={Why do tree-based models still outperform deep learning on typical tabular data?},
  author={Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={507--520},
  year={2022}
}

@misc{kaggle2021state,
  title={State of Machine Learning and Data Science 2021},
  author={Kaggle},
	year = {2021},
}
