\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}

\bibitem[{mis(1987)}]{misc_mushroom_73}
 1987.
\newblock {Mushroom}.
\newblock UCI Machine Learning Repository.
\newblock {DOI}: https://doi.org/10.24432/C5959T.

\bibitem[{Blackard(1998)}]{misc_covertype_31}
Blackard, J. 1998.
\newblock {Covertype}.
\newblock UCI Machine Learning Repository.
\newblock {DOI}: https://doi.org/10.24432/C50K5N.

\bibitem[{Breiman(2001)}]{breiman2001random}
Breiman, L. 2001.
\newblock Random forests.
\newblock \emph{Machine learning}, 45: 5--32.

\bibitem[{Breiman et~al.(1984)Breiman, Friedman, Olshen, and Stone}]{breiman1984classification}
Breiman, L.; Friedman, J.~H.; Olshen, R.~A.; and Stone, C.~J. 1984.
\newblock \emph{Classification and regression trees}.
\newblock Wadsworth statistics/probability series. Belmont, Calif.: Wadsworth International Group.
\newblock ISBN 0534980538.

\bibitem[{Brodley, Friedl et~al.(1996)}]{brodley1996identifying}
Brodley, C.~E.; Friedl, M.~A.; et~al. 1996.
\newblock Identifying and eliminating mislabeled training instances.
\newblock In \emph{Proceedings of the National Conference on Artificial Intelligence}, 799--805.

\bibitem[{Chang and Lin(2011)}]{chang2011libsvm}
Chang, C.-C.; and Lin, C.-J. 2011.
\newblock LIBSVM: a library for support vector machines.
\newblock \emph{ACM transactions on intelligent systems and technology (TIST)}, 2(3): 1--27.

\bibitem[{Collobert, Bengio, and Bengio(2001)}]{collobert2001parallel}
Collobert, R.; Bengio, S.; and Bengio, Y. 2001.
\newblock A parallel mixture of SVMs for very large scale problems.
\newblock \emph{Advances in Neural Information Processing Systems}, 14.

\bibitem[{Fr{\'e}nay and Verleysen(2013)}]{frenay2013classification}
Fr{\'e}nay, B.; and Verleysen, M. 2013.
\newblock Classification in the presence of label noise: a survey.
\newblock \emph{IEEE transactions on neural networks and learning systems}, 25(5): 845--869.

\bibitem[{Geurts, Ernst, and Wehenkel(2006)}]{geurts2006extremely}
Geurts, P.; Ernst, D.; and Wehenkel, L. 2006.
\newblock Extremely randomized trees.
\newblock \emph{Machine learning}, 63: 3--42.

\bibitem[{Ghosh, Kumar, and Sastry(2017)}]{ghosh2017robust}
Ghosh, A.; Kumar, H.; and Sastry, P.~S. 2017.
\newblock Robust Loss Functions under Label Noise for Deep Neural Networks.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 31(1).

\bibitem[{Ghosh, Manwani, and Sastry(2015)}]{ghosh2015making}
Ghosh, A.; Manwani, N.; and Sastry, P. 2015.
\newblock Making risk minimization tolerant to label noise.
\newblock \emph{Neurocomputing}, 160: 93--107.

\bibitem[{Ghosh, Manwani, and Sastry(2017)}]{ghosh2017robustness}
Ghosh, A.; Manwani, N.; and Sastry, P. 2017.
\newblock On the robustness of decision tree learning under label noise.
\newblock In \emph{Advances in Knowledge Discovery and Data Mining: 21st Pacific-Asia Conference, PAKDD 2017, Jeju, South Korea, May 23-26, 2017, Proceedings, Part I 21}, 685--697. Springer.

\bibitem[{Grinsztajn, Oyallon, and Varoquaux(2022)}]{grinsztajn2022tree}
Grinsztajn, L.; Oyallon, E.; and Varoquaux, G. 2022.
\newblock Why do tree-based models still outperform deep learning on typical tabular data?
\newblock \emph{Advances in Neural Information Processing Systems}, 35: 507--520.

\bibitem[{Kaggle(2021)}]{kaggle2021state}
Kaggle. 2021.
\newblock State of Machine Learning and Data Science 2021.

\bibitem[{Kim et~al.(2019)Kim, Yim, Yun, and Kim}]{kim2019nlnl}
Kim, Y.; Yim, J.; Yun, J.; and Kim, J. 2019.
\newblock Nlnl: Negative learning for noisy labels.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, 101--110.

\bibitem[{Kiryo et~al.(2017)Kiryo, Niu, Du~Plessis, and Sugiyama}]{kiryo2017positive}
Kiryo, R.; Niu, G.; Du~Plessis, M.~C.; and Sugiyama, M. 2017.
\newblock Positive-unlabeled learning with non-negative risk estimator.
\newblock \emph{Advances in neural information processing systems}, 30.

\bibitem[{Krizhevsky(2009)}]{krizhevsky2009learning}
Krizhevsky, A. 2009.
\newblock Learning multiple layers of features from tiny images.
\newblock MSc thesis, University of Toronto.

\bibitem[{Lang(1995)}]{lang1995newsweeder}
Lang, K. 1995.
\newblock Newsweeder: Learning to filter netnews.
\newblock In \emph{Machine learning proceedings 1995}, 331--339. Elsevier.

\bibitem[{LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner}]{lecun1998gradient}
LeCun, Y.; Bottou, L.; Bengio, Y.; and Haffner, P. 1998.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86(11): 2278--2324.

\bibitem[{Lukasik et~al.(2020)Lukasik, Bhojanapalli, Menon, and Kumar}]{lukasik2020does}
Lukasik, M.; Bhojanapalli, S.; Menon, A.; and Kumar, S. 2020.
\newblock {Does label smoothing mitigate label noise?}
\newblock In \emph{International Conference on Machine Learning}, 6448--6458. PMLR.

\bibitem[{Lyu and Tsang(2020)}]{Lyu2020Curriculum}
Lyu, Y.; and Tsang, I.~W. 2020.
\newblock Curriculum Loss: Robust Learning and Generalization against Label Corruption.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Ma et~al.(2020)Ma, Huang, Wang, Romano, Erfani, and Bailey}]{ma2020normalized}
Ma, X.; Huang, H.; Wang, Y.; Romano, S.; Erfani, S.; and Bailey, J. 2020.
\newblock Normalized loss functions for deep learning with noisy labels.
\newblock In \emph{International conference on machine learning}, 6543--6553. PMLR.

\bibitem[{Mantas and Abellan(2014)}]{mantas2014credal}
Mantas, C.~J.; and Abellan, J. 2014.
\newblock Credal-C4. 5: Decision tree based on imprecise probabilities to classify noisy data.
\newblock \emph{Expert Systems with Applications}, 41(10): 4625--4637.

\bibitem[{Manwani and Sastry(2013)}]{manwani2013noise}
Manwani, N.; and Sastry, P. 2013.
\newblock Noise tolerance under risk minimization.
\newblock \emph{IEEE transactions on cybernetics}, 43(3): 1146--1151.

\bibitem[{Moustafa and Slay(2015)}]{moustafa2015unsw}
Moustafa, N.; and Slay, J. 2015.
\newblock UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set).
\newblock In \emph{2015 military communications and information systems conference (MilCIS)}, 1--6. IEEE.

\bibitem[{Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and Tewari}]{natarajan2013learning}
Natarajan, N.; Dhillon, I.~S.; Ravikumar, P.~K.; and Tewari, A. 2013.
\newblock Learning with noisy labels.
\newblock \emph{Advances in neural information processing systems}, 26.

\bibitem[{Painsky and Wornell(2018)}]{painsky2018universality}
Painsky, A.; and Wornell, G. 2018.
\newblock On the universality of the logistic loss function.
\newblock In \emph{2018 IEEE International Symposium on Information Theory (ISIT)}, 936--940. IEEE.

\bibitem[{Patrini et~al.(2017)Patrini, Rozza, Krishna~Menon, Nock, and Qu}]{patrini2017making}
Patrini, G.; Rozza, A.; Krishna~Menon, A.; Nock, R.; and Qu, L. 2017.
\newblock Making deep neural networks robust to label noise: A loss correction approach.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, 1944--1952.

\bibitem[{Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, and Dubourg}]{pedregosa2011scikit}
Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.; Weiss, R.; and Dubourg, V. 2011.
\newblock Scikit-learn: Machine learning in Python.
\newblock \emph{the Journal of machine Learning research}, 12: 2825--2830.

\bibitem[{Pennington, Socher, and Manning(2014)}]{pennington2014glove}
Pennington, J.; Socher, R.; and Manning, C.~D. 2014.
\newblock GloVe: Global Vectors for Word Representation.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)}, 1532--1543.

\bibitem[{Song et~al.(2022)Song, Kim, Park, Shin, and Lee}]{song2022learning}
Song, H.; Kim, M.; Park, D.; Shin, Y.; and Lee, J.-G. 2022.
\newblock Learning from noisy labels with deep neural networks: A survey.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}.

\bibitem[{Tanno et~al.(2019)Tanno, Saeedi, Sankaranarayanan, Alexander, and Silberman}]{tanno2019learning}
Tanno, R.; Saeedi, A.; Sankaranarayanan, S.; Alexander, D.~C.; and Silberman, N. 2019.
\newblock {Learning from noisy labels by regularized estimation of annotator confusion}.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 11244--11253.

\bibitem[{Wang et~al.(2019)Wang, Ma, Chen, Luo, Yi, and Bailey}]{wang2019symmetric}
Wang, Y.; Ma, X.; Chen, Z.; Luo, Y.; Yi, J.; and Bailey, J. 2019.
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, 322--330.

\bibitem[{Wilton et~al.(2022)Wilton, Koay, Ko, Xu, and Ye}]{wilton2022positive}
Wilton, J.; Koay, A.; Ko, R.; Xu, M.; and Ye, N. 2022.
\newblock Positive-Unlabeled Learning using Random Forests via Recursive Greedy Risk Minimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 35: 24060--24071.

\bibitem[{Yang, Gao, and Li(2019)}]{yang2019robust}
Yang, B.-B.; Gao, W.; and Li, M. 2019.
\newblock On the robust splitting criterion of random forest.
\newblock In \emph{2019 IEEE International Conference on Data Mining (ICDM)}, 1420--1425. IEEE.

\bibitem[{Zhang and Sabuncu(2018)}]{zhang2018generalized}
Zhang, Z.; and Sabuncu, M. 2018.
\newblock Generalized cross entropy loss for training deep neural networks with noisy labels.
\newblock \emph{Advances in neural information processing systems}, 31.

\bibitem[{Zhou, Ding, and Li(2019)}]{zhou2019improving}
Zhou, X.; Ding, P. L.~K.; and Li, B. 2019.
\newblock Improving robustness of random forest under label noise.
\newblock In \emph{2019 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 950--958. IEEE.

\end{thebibliography}
