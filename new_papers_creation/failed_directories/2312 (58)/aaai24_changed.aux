\relax 
\bibstyle{aaai24}
\citation{newman2006finding}
\citation{wang2010fast}
\citation{felzenszwalb2004efficient}
\citation{moradi2015effective}
\citation{newman2006modularity,newman2003mixing}
\citation{wang2017mgae}
\citation{kipf2016semi,hamilton2017inductive,velickovic2017graph}
\citation{zhang2018link,ying2018hierarchical,ying2018graph}
\citation{wang2017mgae}
\citation{park2019symmetric}
\citation{liu2023simple,kulatilleke2022scgc,xia2021self}
\citation{yue2022survey}
\citation{yang2016modularity}
\citation{wu2020deep}
\citation{mandaglio2018consensus}
\citation{choong2018learning,bhatia2018dfuzzy}
\citation{choong2018learning}
\citation{bhatia2018dfuzzy}
\citation{sun2021graph}
\citation{muller2023graph}
\citation{yang2012defining}
\citation{shi2000normalized}
\citation{fortunato2016community}
\citation{newman2006modularity}
\citation{newman2006modularity}
\citation{brandes2006maximizing}
\citation{newman2006finding,blondel2008fast}
\citation{newman2006modularity}
\citation{yang2012defining}
\citation{scarselli2008graph,kipf2016semi,hamilton2017inductive,velickovic2017graph}
\citation{zhou2020graph}
\newlabel{sec:prob}{{}{2}{}{}{}}
\newlabel{eq:q_matrix}{{2}{2}{}{}{}}
\citation{kipf2016semi}
\citation{klambauer2017self}
\newlabel{subsec::node_embedding}{{}{3}{}{}{}}
\newlabel{subsec::similarity_computation}{{}{3}{}{}{}}
\newlabel{subsec::objective_function}{{}{3}{}{}{}}
\citation{blondel2008fast}
\citation{frey2007clustering}
\citation{zhang1996birch}
\citation{hartigan1979algorithm}
\citation{sen2008collective}
\citation{shchur2018pitfalls}
\citation{shchur2018pitfalls,shchur2019overlapping}
\newlabel{subsec::final_clustering}{{}{4}{}{}{}}
\newlabel{subsec::complexity}{{}{4}{}{}{}}
\newlabel{sec::experiments}{{}{4}{}{}{}}
\citation{muller2023graph}
\citation{peixoto2014efficient}
\citation{perozzi2014deepwalk}
\citation{velivckovic2018deep}
\citation{zhang2019attributed}
\citation{wang2019attributed}
\citation{bo2020structural}
\citation{shchur2019overlapping}
\citation{ying2018graph}
\citation{bianchi2020spectral}
\citation{bianchi2020spectral}
\citation{newman2006modularity}
\citation{yang2012defining}
\citation{muller2023graph}
\citation{velickovic2017graph}
\citation{xu2018powerful}
\citation{hamilton2017inductive}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab::dataset}{{1}{5}{Statistics of the datasets. $|V|$, $|E|$, $|X|$, and $|Y|$ denote the number of nodes, edges, features, and node labels.}{}{}}
\newlabel{fig:lambda_effect_on_modularity_nmi}{{1}{5}{Effect of auxiliary information on different metrics and datasets with varying $\lambda $. As expected, $Q$ tends to decrease as $\lambda $ increases since $Q$ is dependent only on the graph structure whereas $\lambda $ adds weight to the label-based loss. The NMI increases as $\lambda $ increases since NMI is directly related to labels.}{}{}}
\newlabel{tab:graph_based_eval}{{2}{6}{Performance across datasets evaluated using graph conductance $C$ and graph modularity $Q$, with three $\lambda $ settings (0, 0.2, 0.8) from our method. The best and second-best methods are highlighted in \textbf  {bold} and \underline  {underlined} for each dataset-metric pair. We fixed the optimal $\lambda $ value per dataset in our method (i.e., 0.0 or 0.2) during comparison, and our method demonstrates the best or comparable performance to the baselines. Unavailable results and non-convergence are labeled as N/A and N/C.}{}{}}
\newlabel{tab:label_based_eval}{{3}{6}{Performance across datasets evaluated using NMI and F1-score metrics, with three $\lambda $ settings (0, 0.2, 0.8) from our method. The best and second-best methods are highlighted in \textbf  {bold} and \underline  {underlined} for each dataset-metric pair. We fixed the optimal $\lambda $ value per dataset in our method (i.e., 0.8) during comparison, and our method demonstrates the best or comparable performance across most datasets. Unavailable results and non-convergence are labeled as N/A and N/C.}{}{}}
\newlabel{tab:base_model_effect}{{4}{6}{Performance of our method with various GNN base models assessed across different metrics for all datasets, along with the calculation of average performance across datasets. Notably, the performance remains consistent across different variants of GNN models, demonstrating the absence of a clear winner. This shows the robustness of our model in adapting to diverse GNN architectures. For each dataset, we emphasize performance variations: a performance increase of more than 5 is highlighted in \textbf  {bold}, while a decrease is marked with an \underline  {underline}, both in comparison to GCN. Similarly, in the average row, we identify the best-performing GNN base model in \textbf  {bold} and the worst-performing with an \underline  {underline}, considering all metrics across datasets.}{}{}}
\newlabel{tab:additional_regularization}{{5}{7}{Performance of our method with varying $\alpha $ regularization parameter. Additional regularization has a negligible effect while avoiding potential trivial clustering.}{}{}}
\newlabel{fig:lambda_effect_on_number_of_communities}{{2}{7}{The number of communities varying $\lambda $ hyperparameter for different datasets. We observe that the number of communities found is similar across different $\lambda $ values except in some cases.}{}{}}
\bibdata{aaai24}
\gdef \@abspage@last{8}
