\begin{thebibliography}{10}

\bibitem{bibikar2022federated}
{\sc Bibikar, S., Vikalo, H., Wang, Z., and Chen, X.}
\newblock Federated dynamic sparse training: Computing less, communicating less, yet learning better.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence\/} (2022), vol.~36, pp.~6080--6088.

\bibitem{cao2022synergistic}
{\sc Cao, Y.-H., Sun, P., Huang, Y., Wu, J., and Zhou, S.}
\newblock Synergistic self-supervised and quantization learning.
\newblock In {\em European Conference on Computer Vision\/} (2022), Springer, pp.~587--604.

\bibitem{chen2020simple}
{\sc Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.}
\newblock A simple framework for contrastive learning of visual representations.
\newblock In {\em International conference on machine learning\/} (2020), PMLR, pp.~1597--1607.

\bibitem{chen2021exploring}
{\sc Chen, X., and He, K.}
\newblock Exploring simple siamese representation learning.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\/} (2021), pp.~15750--15758.

\bibitem{chen2021communication}
{\sc Chen, Y., Hashemi, A., and Vikalo, H.}
\newblock Communication-efficient variance-reduced decentralized stochastic optimization over time-varying directed graphs.
\newblock {\em IEEE Transactions on Automatic Control 67}, 12 (2021), 6583--6594.

\bibitem{chen2021decentralized}
{\sc Chen, Y., Hashemi, A., and Vikalo, H.}
\newblock Decentralized optimization on time-varying directed graphs under communication constraints.
\newblock In {\em ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\/} (2021), IEEE, pp.~3670--3674.

\bibitem{chen2019communication}
{\sc Chen, Y., Sun, X., and Jin, Y.}
\newblock Communication-efficient federated deep learning with layerwise asynchronous model update and temporally weighted aggregation.
\newblock {\em IEEE transactions on neural networks and learning systems 31}, 10 (2019), 4229--4238.

\bibitem{davis2019stochastic}
{\sc Davis, D., and Drusvyatskiy, D.}
\newblock Stochastic model-based minimization of weakly convex functions.
\newblock {\em SIAM Journal on Optimization 29}, 1 (2019), 207--239.

\bibitem{diao2020heterofl}
{\sc Diao, E., Ding, J., and Tarokh, V.}
\newblock Heterofl: Computation and communication efficient federated learning for heterogeneous clients.
\newblock {\em arXiv preprint arXiv:2010.01264\/} (2020).

\bibitem{ghosh2020efficient}
{\sc Ghosh, A., Chung, J., Yin, D., and Ramchandran, K.}
\newblock An efficient framework for clustered federated learning.
\newblock {\em Advances in Neural Information Processing Systems 33\/} (2020), 19586--19597.

\bibitem{he2021ssfl}
{\sc He, C., Yang, Z., Mushtaq, E., Lee, S., Soltanolkotabi, M., and Avestimehr, S.}
\newblock Ssfl: Tackling label deficiency in federated learning via personalized self-supervision.
\newblock {\em arXiv preprint arXiv:2110.02470\/} (2021).

\bibitem{he2016deep}
{\sc He, K., Zhang, X., Ren, S., and Sun, J.}
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition\/} (2016), pp.~770--778.

\bibitem{hubara2017quantized}
{\sc Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y.}
\newblock Quantized neural networks: Training neural networks with low precision weights and activations.
\newblock {\em The Journal of Machine Learning Research 18}, 1 (2017), 6869--6898.

\bibitem{isik2022information}
{\sc Isik, B., Weissman, T., and No, A.}
\newblock An information-theoretic justification for model pruning.
\newblock In {\em International Conference on Artificial Intelligence and Statistics\/} (2022), PMLR, pp.~3821--3846.

\bibitem{jiang2020federated}
{\sc Jiang, D., Shan, C., and Zhang, Z.}
\newblock Federated learning algorithm based on knowledge distillation.
\newblock In {\em 2020 International Conference on Artificial Intelligence and Computer Engineering (ICAICE)\/} (2020), IEEE, pp.~163--167.

\bibitem{jiang2019improving}
{\sc Jiang, Y., Kone{\v{c}}n{\`y}, J., Rush, K., and Kannan, S.}
\newblock Improving federated learning personalization via model agnostic meta learning.
\newblock {\em arXiv preprint arXiv:1909.12488\/} (2019).

\bibitem{jin2017escape}
{\sc Jin, C., Ge, R., Netrapalli, P., Kakade, S.~M., and Jordan, M.~I.}
\newblock How to escape saddle points efficiently.
\newblock In {\em International conference on machine learning\/} (2017), PMLR, pp.~1724--1732.

\bibitem{kairouz2021advances}
{\sc Kairouz, P., McMahan, H.~B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.~N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et~al.}
\newblock Advances and open problems in federated learning.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning 14}, 1--2 (2021), 1--210.

\bibitem{karimireddy2020scaffold}
{\sc Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh, A.~T.}
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In {\em International conference on machine learning\/} (2020), PMLR, pp.~5132--5143.

\bibitem{krizhevsky2009learning}
{\sc Krizhevsky, A., Hinton, G., et~al.}
\newblock Learning multiple layers of features from tiny images.

\bibitem{li2022federated}
{\sc Li, Q., Diao, Y., Chen, Q., and He, B.}
\newblock Federated learning on non-iid data silos: An experimental study.
\newblock In {\em 2022 IEEE 38th International Conference on Data Engineering (ICDE)\/} (2022), IEEE, pp.~965--978.

\bibitem{li2020federated}
{\sc Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.}
\newblock Federated optimization in heterogeneous networks.
\newblock {\em Proceedings of Machine learning and systems 2\/} (2020), 429--450.

\bibitem{li2019convergence}
{\sc Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z.}
\newblock On the convergence of fedavg on non-iid data.
\newblock {\em arXiv preprint arXiv:1907.02189\/} (2019).

\bibitem{lin2020ensemble}
{\sc Lin, T., Kong, L., Stich, S.~U., and Jaggi, M.}
\newblock Ensemble distillation for robust model fusion in federated learning.
\newblock {\em Advances in Neural Information Processing Systems 33\/} (2020), 2351--2363.

\bibitem{liu2021self}
{\sc Liu, H., HaoChen, J.~Z., Gaidon, A., and Ma, T.}
\newblock Self-supervised learning is more robust to dataset imbalance.
\newblock {\em arXiv preprint arXiv:2110.05025\/} (2021).

\bibitem{makhija2022federated}
{\sc Makhija, D., Ho, N., and Ghosh, J.}
\newblock Federated self-supervised learning for heterogeneous clients.
\newblock {\em arXiv preprint arXiv:2205.12493\/} (2022).

\bibitem{mcmahan2017communication}
{\sc McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.}
\newblock Communication-efficient learning of deep networks from decentralized data.
\newblock In {\em Artificial Intelligence and Statistics\/} (2017), pp.~1273--1282.

\bibitem{mertikopoulos2020almost}
{\sc Mertikopoulos, P., Hallak, N., Kavis, A., and Cevher, V.}
\newblock On the almost sure convergence of stochastic gradient descent in non-convex problems.
\newblock {\em Advances in Neural Information Processing Systems 33\/} (2020), 1117--1128.

\bibitem{rastegari2016xnor}
{\sc Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A.}
\newblock Xnor-net: Imagenet classification using binary convolutional neural networks.
\newblock In {\em European conference on computer vision\/} (2016), Springer, pp.~525--542.

\bibitem{reisizadeh2020fedpaq}
{\sc Reisizadeh, A., Mokhtari, A., Hassani, H., Jadbabaie, A., and Pedarsani, R.}
\newblock Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization.
\newblock In {\em International Conference on Artificial Intelligence and Statistics\/} (2020), PMLR, pp.~2021--2031.

\bibitem{servetnyk2020unsupervised}
{\sc Servetnyk, M., Fung, C.~C., and Han, Z.}
\newblock Unsupervised federated learning for unbalanced data.
\newblock In {\em GLOBECOM 2020-2020 IEEE Global Communications Conference\/} (2020), IEEE, pp.~1--6.

\bibitem{sun2011scalar}
{\sc Sun, J.~Z., and Goyal, V.~K.}
\newblock Scalar quantization for relative error.
\newblock In {\em 2011 Data Compression Conference\/} (2011), IEEE, pp.~293--302.

\bibitem{vershynin2018high}
{\sc Vershynin, R.}
\newblock {\em High-dimensional probability: An introduction with applications in data science}, vol.~47.
\newblock Cambridge university press, 2018.

\bibitem{wang2022does}
{\sc Wang, L., Zhang, K., Li, Y., Tian, Y., and Tedrake, R.}
\newblock Does learning from decentralized non-iid unlabeled data benefit from self supervision?
\newblock In {\em The Eleventh International Conference on Learning Representations\/} (2022).

\bibitem{wen2017terngrad}
{\sc Wen, W., Xu, C., Yan, F., Wu, C., Wang, Y., Chen, Y., and Li, H.}
\newblock Terngrad: Ternary gradients to reduce communication in distributed deep learning.
\newblock {\em Advances in neural information processing systems 30\/} (2017).

\bibitem{yoon2022bitwidth}
{\sc Yoon, J., Park, G., Jeong, W., and Hwang, S.~J.}
\newblock Bitwidth heterogeneous federated learning with progressive weight dequantization.
\newblock In {\em International Conference on Machine Learning\/} (2022), PMLR, pp.~25552--25565.

\bibitem{zhang2017zipml}
{\sc Zhang, H., Li, J., Kara, K., Alistarh, D., Liu, J., and Zhang, C.}
\newblock Zipml: Training linear models with end-to-end low precision, and a little bit of deep learning.
\newblock In {\em International Conference on Machine Learning\/} (2017), PMLR, pp.~4035--4043.

\bibitem{zhang2020fixed}
{\sc Zhang, X., Liu, S., Zhang, R., Liu, C., Huang, D., Zhou, S., Guo, J., Guo, Q., Du, Z., Zhi, T., et~al.}
\newblock Fixed-point back-propagation training.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\/} (2020), pp.~2330--2338.

\bibitem{zhao2018federated}
{\sc Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., and Chandra, V.}
\newblock Federated learning with non-iid data.
\newblock {\em arXiv preprint arXiv:1806.00582\/} (2018).

\bibitem{zhou2016dorefa}
{\sc Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y.}
\newblock Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients.
\newblock {\em arXiv preprint arXiv:1606.06160\/} (2016).

\bibitem{zhu2020towards}
{\sc Zhu, F., Gong, R., Yu, F., Liu, X., Wang, Y., Li, Z., Yang, X., and Yan, J.}
\newblock Towards unified int8 training for convolutional neural network.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\/} (2020), pp.~1969--1979.

\bibitem{zhuang2022divergence}
{\sc Zhuang, W., Wen, Y., and Zhang, S.}
\newblock Divergence-aware federated self-supervised learning.
\newblock {\em arXiv preprint arXiv:2204.04385\/} (2022).

\end{thebibliography}
