\documentclass[a4paper,11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}  
\usepackage{url}
\pdfoutput=1


\input{defpacks}
\title{Fed-QSSL: A Framework for Personalized Federated Learning under Bitwidth and Data Heterogeneity}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Yiyue Chen\textsuperscript{\rm 1},
    Haris Vikalo\textsuperscript{\rm 1},
    Chianing Wang\textsuperscript{\rm 2}
    \thanks{This work was supported by Toyota Motor North America.}
}
% \date{The University of Texas at Austin, }
\date{
    %Afiliations
    \textsuperscript{\rm 1}The University of Texas at Austin\\
    \textsuperscript{\rm 2}Toyota InfoTech Lab USA\\
    yiyuechen@utexas.edu,  hvikalo@utexas.edu,
    johnny.wang@toyota.com
}
% \author{Yiuye Chen, Abolfazl Hashemi, Haris Vikalo}
\begin{document}
	\maketitle
		\vskip 0.3in
	
\begin{abstract}
	Motivated by high resource costs of centralized machine learning schemes as well as data privacy concerns, federated learning (FL) emerged as an efficient alternative that relies on aggregating locally trained models rather than collecting clients' potentially private data. In practice, available resources and data distributions vary from one client to another, creating an inherent system heterogeneity that leads to deterioration of the performance of conventional FL algorithms. In this work, we present a federated quantization-based self-supervised learning scheme (Fed-QSSL) designed to address heterogeneity in FL systems. At clients' side, to tackle data heterogeneity we leverage distributed self-supervised learning while utilizing low-bit quantization to satisfy constraints imposed by local infrastructure and limited communication resources. At server's side, Fed-QSSL deploys de-quantization, weighted aggregation and re-quantization, ultimately creating models personalized to both data distribution as well as specific infrastructure of each client's device. We validated the proposed algorithm on real world datasets, demonstrating its efficacy, and theoretically analyzed impact of low-bit training on the convergence and robustness of the learned models.
\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\input{Introduction}

\section{Preliminaries}
\input{Preliminaries}

\section{Algorithm}
\input{Algorithm}

\section{Theoretical Analysis}
\input{Theoretical}

\section{Experimental Results}
\input{Simulations}

\section{Conclusion and Future Work}
We introduced the federated quantized self-supervised learning (Fed-QSSL) algorithm, an effective framework for FL in bitwidth and data heterogeneity settings. We analytically studied the impact of low-bit training on the convergence and robustness of FL, and experimentally demonstrated that Fed-QSSL achieves more robust and personalized performance than benchmarking algorithms. Future work may include an extension to large-scale settings where it is meaningful to cluster clients with similar data distributions, and train per-cluster models. 
For such systems, it is of interest to develop schemes that aim to optimally manage utilization of the available resources.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section*{A. Appendix}
	This section is organized as follows. Section~A.1 provides notation and sets up the stage for theoretical analysis. Section A.2-4 present proofs of Lemma~1, Theorem~1 and Theorem~2, respectively. Section~A.5 contains details of the local training under low bitwidth constraints.
	
	\subsection*{A.1. Preliminaries}
	\subsubsection*{Notation}
Let $n$ denote the number of clients in a FL system. Client $k$ owns dataset $D_k$ containing $|D_k|$ samples drawn independently from distribution $\mathcal{D}_k$. Data sample $x \in \R^d$ is a $d$-dimensional vector. Parameters of the learned SSL model are denoted by $\w \in \R^{m \times d}$. We use $[n]$ to denote the set $\{1, 2, \cdots, n \}$. Let $\mathcal{L}$ denote the global objective function, while $\mathcal{L}_k$ denotes local objective at client $k$. The inner product of two vectors, $\x$ and $\z$, is denoted by $\langle \x, \z \rangle$. Finally, let $\|\cdot \|$ be the $\ell_2$-norm of a vector or Frobenius norm of a matrix, and let $\| \cdot \|_2$ denote the $\ell_2$-norm of a matrix.

\subsubsection*{Background and settings}

%Before the proofs, we first review the setting we use for the analysis.

We consider a federated learning system with $n$ clients participating in model learning via self-supervised learning (SSL). The local objective function used by client $k$ is
\begin{align*}
    \mathcal{L}_{SSL, k}(\w) = -\mathbb{E}[(\w(x_{k, i}) +\xi_{k, i})^T(\w(x_{k, i}) +\xi'_{k, i})] + \frac{1}{2} \|\w^T \w \|^2,
\end{align*}
where $\xi_{k, i}$ and $\xi'_{k, i} $ denote random noise added to the samples. Then the global objective function is defined as $\mathcal{L}_{SSL} = \sum_{k=1}^n \frac{|D_k|}{|D|}\mathcal{L}_{SSL, k}(\w)$. The goal of the FL system is to find $\w$ that minimizes the global objective. The optimization problem can be written equivalently as $\min \mathcal{L}(\w) = \|\bar{X} - \w^T \w \|^2$, where $\bar{X} = \sum_k \frac{|D_k|}{|D|}X_k$ and $X_k = \mathbb{E}_{x \sim D_k}(xx^T) = \frac{1}{|D_k|}\sum_{i=1}^{|D_k|} x_{k, i}x_{k, i}^T$ denotes the empirical data covariance matrix at client $k$. Minimization of the local objective can equivalently be stated as $\min \mathcal{L}_{k}(\w) = \|X_k - \w^T \w\|^2$. 
% Consider the low-bitwidth training scenario, where the local training is conducted in low precision values, the global objective becomes $\mathcal{L}_{QSSL}(\w) = \|\bar{X} - (\w^{(Q)})^T \w^{(Q)} \|^2$.

Recall that the focus of our analysis is on the impact of low-bit operations deployed in local training on the convergence and robustness of Fed-QSSL. For analytical tractability, we simplify operations at the server by replacing sophisticated de-quantizer by a layer-wise codebook mapping low-bitwidth weights to floating-point values according to $\w'^{(Q)}_{k, t} = \w^{(Q)}_{k, t}$. The subsequent aggregation follows FedAvg and computes $\w_{t, G} = \sum_{k=1}^n \frac{|D_k|}{|D|} \w'^{(Q)}_{k, t}$, while re-quantization uses the $\tanh$-based quantizer. For simplicity, we assume that the clients quantize weights/activations using the same bitwidth, i.e., $s_1 = s_2 = \cdots = s_n = R$; moreover, local gradient quantization is performed using the same number of bits (i.e., $s_k$). 


Quantization centers used for layer-wise scalar quantization of the model parameters and gradients are found via companding quantization. Specifically, full-precision input $\x$ is transformed by a nonlinear function $c$, e.g., $\tanh$ function, and then uniformly quantized. The output $Q(\x)$ is generated by taking the value of the inverse function, $c^{-1}$, of the quantized value \cite{sun2011scalar}. Uniform quantization is  a special case of companding quantization, obtained by setting $c(\x) = \x$. In Fed-QSSL, local quantizers can also be viewed as special cases of the companding quantizers. We consider fixed rate quantization, where each codeword (quantization center) used for quantization has the same length. When the quantization rate is $R$, the codebook has $K = 2^R$ codewords \cite{sun2011scalar}. 

We further assume that the quantization noise has zero mean, which can be achieved by adopting stochastic quantization: Given a sequence of quantization centers $Q_1 \leq \cdots \leq Q_{2^R}$, for $x \in [Q_j, Q_{j+1}]$ we have $Q(x) = Q_j$ with probability $\frac{Q_{j+1} - x}{Q_{j+1} - Q_j}$ and $Q(j) = Q_{j+1}$ with probability $\frac{x - Q_{j}}{Q_{j+1} - Q_j}$. 


    \subsection*{A.2. Proof of Lemma~1}
    We first review notation featured in the lemma. 
Let $\epsilon_{\w}$ and $\epsilon_g$ denote the error induced by quantizing parameters and gradients, respectively. Let $\epsilon_r$ be the quantization error induced by the server when re-quantizing an aggregated model. 

When training at full precision using gradient descent, assuming $E$ epochs of local training per communication round, the global model update is formed as
\begin{align*}
    \w_{t+E, G} = \sum_{k=1}^n \frac{|D_k|}{|D|}(\w_{t, G} - \sum_{s = 0}^{E-1} \alpha_{t+s} \frac{\partial \mathcal{L}_{SSL, k}}{\partial \w_{t+s, k}}).
\end{align*}

When performing low-bit quantized training, quantized weights of the model trained by client $k$ are found as 
\begin{align*}
    \w_{k, t+1}^{(Q)} = \w_{k, t}^{(Q)} - \alpha_t (\g_{k, t} + \epsilon_{g_k, t} )+ \epsilon_{\w_k, t},
\end{align*}
where $\w_{k, t}^{(Q)}$ denotes the low-bitwidth model parameters at time $t$, $\epsilon_{g_k, t} $ is the noise added to the gradient in the quantization step, and $\epsilon_{\w_k, t}$ denotes the noise added to the model parameters after gradient update as the parameters retain low bitwidth representation. After $E$ local training epochs, the server collects the models and aggregates them according to 
\begin{eqnarray*}
    \w_{t+E, G} & = &\sum_{k=1}^n \frac{|D_k|}{|D|} \w_{t+E, k}^{(Q)} \\ & = & \sum_{k=1}^n \frac{|D_k|}{|D|} \w_{k, t}^{{Q}}
    - \sum_{s=0}^{E-1} [\alpha_{t+s}(\g_{k, t+s} + \epsilon_{g_k, t+s} ) - \epsilon_{\w_k, t+s}].
\end{eqnarray*}
Since the aggregated model is not necessarily in low bit, an additional re-quantization step is required to form its quantized version $\w_{t+E, G}^{(Q)} = \w_{t+E, G} + \epsilon_{r, t+E}$, which is then sent to clients.

We proceed to Lemma~1, an intermediate result characterizing the variance of quantization errors, $\mathbb{E}_t[\|\epsilon_{g_k, t} \|^2]$ and $\mathbb{E}_t[ \|\epsilon_{\w_k, t}\|^2] $. The lemma shows that given low-bitwidth model parameters and small gradient perturbation, the variance of $\epsilon_w$ is proportional to the norm of the gradient perturbation. In other words, when a small perturbation is added to low-bitwidth model parameters, the noise induced by quantizing the perturbed parameters has small variance.

For the result in Lemma~1, we need to make the following assumptions.

\newtheorem*{assumption1}{Assumption 1}
\begin{assumption1}
    All quantization operators in the low-bit training are unbiased.
\end{assumption1}
\newtheorem*{assumption2}{Assumption 2}
\begin{assumption2}
    Expected gradient estimate is unbiased and bounded, 
    
    $\mathbb{E}_t \|\g_{k, t} \|^2 \leq G^2$.
\end{assumption2}

\newtheorem*{lemma1}{Lemma 1}
\begin{lemma1}
Suppose Assumptions \ref{Assumption-1}-\ref{Assumption-2} hold, and that client $k$ computes update of the quantized model parameters $\w_{k, t+1}^{(Q)}$ at bitwidth
$s_k = R$. Then the gradient quantization error $\epsilon_{g_k, t}$ satisfies $\mathbb{E}_t[\|\epsilon_{g_k, t} \|^2] = \O(G^2/2^{2R})$, and the local re-quantization error $\epsilon_{\w_k, t}$ satisfies $\mathbb{E}_t[ \|\epsilon_{\w_k, t}\|^2] = \O(\alpha_t G^2/2^{2R})$.
\end{lemma1}

\begin{proof}
%Suppose that the gradient estimate is bounded by some constant, $G > 0$, i.e., $\|\frac{\partial \L_{QSSL}}{\partial \w_{t, k}} \|^2 \leq G$ for any $k, t$. 
Recall that the quantization in Algorithm~1 is conducted layer-wise. Thus, it suffices to analyze quantization errors for the parameters in the first layer -- the results readily extend to the parameters in the remaining layers. Given a local update
\begin{align*}
    \w_{k, t+1}^{(Q)} = \w_{k, t}^{(Q)} - \alpha_t (\g_{k, t} + \epsilon_{g_k, t} )+ \epsilon_{\w_k, t},
\end{align*}
the gradient update can be viewed as an $\O(\alpha_t \|\g_{k, t}\|)$ perturbation of the corresponding original codebook value. Since $\mathbb{E}_t\|\g_{k, t} \|^2 \leq G^2$ and we use quantile quantization for gradient compression, the gradient quantization error $\epsilon_{g_k, t}$ has range within $\O(\|\g_{k, t}\|)$. 
%At the demoninator, $2^{R}$ comes from an upper bound from uniform quantization error, which is proportional to $1/2^{R}$.
Therefore, variance of the gradient quantization error $\mathbb{E}_t[\|\epsilon_{g_k, t}\|^2]$ is $\O(G^2)$. 

To analyze weight quantization, let $\w_{\min}$ and $\w_{\max}$ be the minimal and maximal value in $\w_{k, t}^{(Q)}$, respectively, and let $\w'_{\min}$ and $\w'_{\max}$ denote the minimal and maximal value in $\w_{k, t}^{(Q)} - \alpha_t (\g_{k, t} + \epsilon_{g_k, t} )$, respectively. It then holds that $| \w_{\min} - \w'_{\min} | $ is $\O(\alpha_t \|\g_{k, t}\|)$ and $| \w_{\max} - \w'_{\max} | $ is $\O(\alpha_t \|\g_{k, t}\|)$. By leveraging fixed rate companding quantization with $\tanh$ compander and the fact that the derivative of $\tanh$ decreases from $1$ to $0$ as the absolute value of the variable increases, it can be shown that the $k$-th codeword in the codebook for $\w^{(Q)}_{k, t+1}$ is at most $\O(\alpha_t \|\g_{k, t}\|)$ away from the $k$-th codeword in the codebook for $\w^{(Q)}_{k, t}$, which implies that by using stochastic quantization operator, the variance of $\epsilon_{\w_k, t}$, $\mathbb{E}_t[\|\epsilon_{\w_k, t}\|^2 ]$, is $\O(\alpha_t G^2)$.

%Next, we investigate the relationship between the variance of the quantization error and the quantization rate $R$. On the one hand, \cite{sun2011scalar} has shown that for scalar quantization, the optimal compander for the fixed rate quantization has MSE error is proportional to $\frac{1}{2^{2R}}$. On the other hand, neural network weights do not necessarily follow uniform distribution; \textcolor{blue}{the uniform quantization MSE error can be thought of as the worst case and an upper bound. We can therefore consider the optimal companding quantization MSE error as the lower bound and both bound is proportional to $\frac{1}{2^{2R}}$. 
%When the codebook has $2^R$ codewords, the variance of $\epsilon_{\w_k, t}$, $\mathbb{E}_t[\|\epsilon^2_{\w_k, t} \|^2]$, is also proportional to $\frac{1}{2^{2R}}$ and of order $\O(\alpha_t^2 G^2/2^{2R})$ and similarly, $\mathbb{E}_t[\|\epsilon_{g_k, t}\|^2] $ is of order $\O( G^2/2^{2R}) $.}

In order to derive the relationship between the variance of the quantization error and the quantization rate in Fed-QSSL, we consider two types of quantizations: uniform quantization and companding quantization with optimal compander. Since neural network weights are typically not uniformly distributed but rather follow centered distributions such as Gaussian or Laplacian \cite{isik2022information}, the uniform quantization MSE, which is $\O(1/2^{2R})$ with respect to $R$, can be used as an upper bound on the quantization error. Regarding the lower bound, \cite{sun2011scalar} has shown that the lowest MSE for companding quantization is achieved by the optimal compander with the corresponding MSE $\O(1/2^{2R})$ with respect to $R$. Therefore, given the quantization rate $R$, the error of the weight quantizer in Fed-QSSL is $\O(1/2^{2R})$. As for the gradient quantizer which relies on the quantile quantization method, one can invoke the quantization MSE of uniform quantization as an upper bound and the MSE of the optimal companding quantization as a lower bound. By combining the two, it follows that the gradient quantization error is $\O(1/2^{2R})$.
Combining the orders with respect to $G$ and $R$, we can complete the proof.
\end{proof}


The corollary below follows directly from the lemma.

\newtheorem*{corollary1}{Corollary 1}
\begin{corollary1}
Instate the assumptions of Lemma~1, and let all clients use the same learning rate $\alpha_{t-E+1} = \cdots = \alpha_t$ within the same communication round. Then there exists $G_q = \O(G^2/2^{2R})$ such that $\mathbb{E}_t[\|\epsilon_{\w_k, t} \|^2 ]\leq \alpha_t G_q^2 $ and $\mathbb{E}_t[\|\epsilon_{r, t} \|^2 ]\leq \alpha_t G_q^2 $.
\end{corollary1}
\begin{proof}
From Lemma~1, when locally performing single-step gradient update, the variance of $\epsilon_{\w_k, t}$ is $\O(\alpha_t G^2/2^{2R})$. As for the global update, since all clients run $E$ local training epochs, each with constant learning rate, and the update is formed by summing $E$ gradient update, the variance of the re-quantization error $\epsilon_{r, t}$ is $\O(\alpha_t G^2 E) $. Therefore, there exists $G_q =  \O(G^2/2^{2R})$ such that $\mathbb{E}_t[\|\epsilon_{\w_k, t} \|^2 ]\leq \alpha_t G_q^2 $ and $\mathbb{E}_t[\|\epsilon_{r, t} \|^2 ]\leq \alpha_t G_q^2 $.
\end{proof}

    \subsection*{A.3. Proof of Theorem~1}
    We first review several key definitions and then re-state the theorem.
Recall that the global objective $\mathcal{L}$ is non-convex and non-smooth but satisfies weakly-convex assumption. We thus proceed with convergence analysis by relying on techniques encountered in dealing with weakly convex objectives \cite{davis2019stochastic}.

Recall that a function $f: \R^d \to \R $ is $\rho$-weakly convex if $f(x) + \frac{\rho}{2}\|x \|^2$ is convex; this definition is readily extended to matrix variable $\w \in \R^{m \times d}$. In general, any function of the form $f(x) = g(h(x))$, where $g(\cdot)$ is convex and Lipschitz and $h(\cdot)$ is a smooth map with Lipschitz Jacobian, is weakly-convex \cite{davis2019stochastic}.
Since the global objective function is $ \mathcal{L} = \|\bar{X} - \w^T \w \|^2$,
it is straightforward to see that the global SSL objective satisfies weak convexity with $\rho \geq 4\|\bar{X} \|$. For a weakly-convex function $\mathcal{L}$, the key construction is the Moreau envelope,
\begin{align*}
    \phi_{\lambda}(x) := \min_y \{\mathcal{L}(y) + \frac{1}{2\lambda}\|y-x \|^2 \},
\end{align*}
with the corresponding proximal map defined as
\begin{align*}    \mathrm{prox}_{\phi_{\lambda}(x) } : = \mathrm{argmin}_y  \{\mathcal{L}(y) + \frac{1}{2\lambda}\|y-x \|^2 \}.
\end{align*}

The convergence can be quantified and tracked via $\|\nabla \phi_{\lambda}(x) \|$; in particular, small norm of the gradient of $\phi_{\lambda}(x) $ implies that $x$ is close to a point $\hat{x} := \mathrm{prox}_{\phi_{\lambda}}(x)$ which is stationary for $\phi$. 

\newtheorem*{theorem1}{Theorem 1}
\begin{theorem1}
Suppose all assumptions in Lemma~1 and Corollary~1 hold.
For all $\bar{\rho} > \rho$, after $T$ communication rounds of Fed-QSSL
\begin{align*}
    & \frac{1}{\sum_{t=0}^T \alpha_{tE}} \sum_{t=0}^T \alpha_{tE} \mathbb{E} [ \|\nabla \phi_{\frac{1}{\bar{\rho}}} (\w^{(Q)}_{tE, G}) \|^2 ] \\ & \leq \frac{E \bar{\rho}}{\bar{\rho} - \rho} \frac{ \phi_{\frac{1}{\bar{\rho}}}(\w^{(Q)}_{0, G}) - \min \phi +  \bar{\rho}(G^2\sum_{t=0}^T \alpha^2_{tE}+3G_q^2\sum_{t=0}^T \alpha_{tE})}{\bar{\rho}\sum_{t=0}^T \alpha_{tE} },
\end{align*}
where $E$ denotes the number of local training epochs per communication round (for a total of $tE$ training epochs after $t$ communication rounds) and $\w^{(Q)}_{0, G}$ is the quantized parameter initialization. 
\end{theorem1}


\begin{proof}
To analyze the FedAvg-based aggregation with full device participation and $E$ local epochs per communication round, we introduce the following notations. Let $\w^{(Q)}_{k, t}$ denote the parameters of model deployed by client $k$ at time $t$. When $t+ 1 \in \mathcal{I}_E = \{E, 2E, \cdots \}$, the server collects local models and performs aggregation. Let us consider two sequences,
\begin{align*}
    \v_{k, t+1} & = \w^{(Q)}_{k, t} - \alpha_t \g^{(Q)}_k(\w_{k, t}) + \epsilon_{\w_k, t} \\
    \w^{(Q)}_{k, t+1} & = \begin{cases}
    \v_{k, t+1} & t+1 \notin \mathcal{I}_E \\
    \sum_{k=1}^n p_k \v_{k, t+1} + \epsilon_{r, t+1} & t+1 \in \mathcal{I}_E
    \end{cases}
\end{align*}
where the auxiliary variable $\v_{t+1, k}$ is an intermediate result of one-step quantized (stochastic) gradient descent update for $\w^{(Q)}_{k, t}$ \footnote{Please note that this notation is consistent with the global update $\w^{(Q)}_{k, t} $ in Algorithm~1 when $t \in \mathcal{I}_E$. For all other time steps, global aggregation is not accessible and has no bearing to real applications.} and $p_k = \frac{|D_k|}{|D|}$. As stated before, local quantized gradient estimate $\g^{(Q)}_k(\w_{k, t})$ is assumed to be an unbiased estimate of the true gradient. For the global model analysis we examine two virtual sequences, $\{\v_{t, G} \}$ and $ \{ \w^{(Q)}_{t, G} \}$, with $\v_{t, G} = \sum_{k=1}^n p_k \v_{k, t}$ and $\w^{(Q)}_{t, G} = \sum_{k=1}^n p_k\w^{(Q)}_{k, t}$. In the full participation scenario, it always holds that $\v_{t, G} = \w^{(Q)}_{t, G} $ when $t \notin \mathcal{I}_E$ and 
$\v_{t, G} + \epsilon_{r, t} = \w^{(Q)}_{t, G}$ otherwise. 

Define $v_t = \mathbb{E}_t[\g(\w^{(Q)}_{t, G})]$ where $\g(\w^{(Q)}_{t, G}) = \sum_{k=1}^n p_k (\g^{(Q)}_{k, t} - \epsilon_{\w_k, t}/\alpha_t )$ and $\hat{\x} := \mathrm{prox}_{\phi_{\frac{1}{\bar{\rho}}}}(\x)$ for any constant $\bar{\rho} > \rho$. 
Following Corollary~1, there exists $G, G_q >0 $ such that
$\mathbb{E}_t [\| \g(\w^{(Q)}_{t, G})\|^2] \leq G^2 + 2G_q^2$ and $\mathbb{E}_t\|\epsilon_{r, t} \|^2 \leq \alpha_t G_q^2$.
%where $G$ is the constant for stochastic gradient estimate and $G_q$ is the constant for quantization error, 
When $t \notin \mathcal{I}_E$, it holds that
\begin{align*}
    & \mathbb{E}_t [\phi_{\frac{1}{\bar{\rho}}}(\w^{(Q)}_{t+1, G})]\\
    & \leq \mathbb{E}_t [\mathcal{L}(\hat{\w}^{(Q)}_{t, G}) + \frac{\bar{\rho}}{2}\|\hat{\w }_{t, G} - \w^{(Q)}_{t+1, G} \|^2  ] \\
    & = \mathcal{L}(\hat{\w }^{(Q)}_{t, G}) + \mathbb{E}_t[ \frac{\bar{\rho}}{2}\|\hat{\w}^{(Q)}_{t, G} - \v_{t+1, G} \|^2 ] \\
    & = \mathcal{L}(\hat{\w }^{(Q)}_{t, G}) + \frac{\bar{\rho}}{2}\mathbb{E}_t [\| (\w^{(Q)}_{t, G} - \hat{\w }^{(Q)}_{t, G}) -  \alpha_{t} \g(\w^{(Q)}_{t, G}) \|^2 ] \\
    & \leq \mathcal{L}(\hat{\w}^{(Q)}_{t, G}) + \frac{\bar{\rho}}{2} \| \w^{(Q)}_{t, G} - \hat{\w }^{(Q)}_{t, G}\|^2 + \bar{\rho} \alpha_t \mathbb{E}_t\langle \hat{\w }^{(Q)}_{t, G} - \w^{(Q)}_{t, G}, \g(\w^{(Q)}_{t, G}) \rangle + \bar{\rho}(\alpha_t^2G^2+2\alpha_t G_q^2) \\
    & \leq \phi_{\frac{1}{\bar{\rho}}}(\w^{(Q)}_{t, G}) + \bar{\rho} \alpha_t \langle \hat{\w}^{(Q)}_{t, G} - \w^{(Q)}_{t, G}, v_t \rangle + \bar{\rho}(\alpha_t^2G^2+2\alpha_t G_q^2) \\
    & \leq  \phi_{\frac{1}{\bar{\rho}}}(\w^{(Q)}_{t, G}) + \bar{\rho}\alpha_t (\mathcal{L}(\hat{\w}^{(Q)}_{t, G} ) - \mathcal{L}(\w^{(Q)}_{t, G}) + \frac{\rho}{2} \|\w^{(Q)}_{t, G} - \hat{\w }^{(Q)}_{t, G} \|^2 ) + \bar{\rho}(\alpha_t^2G^2+2\alpha_t G_q^2),
\end{align*}
where the second inequality follows from the Young's inequality on the gradient variance, while the last inequality follows from the property of $\rho$-weakly convex functions. 

When $t \in \mathcal{I}_E$, we recall that $\v_{t, G} + \epsilon_{r, t} = \w_{t, G} $ and $\mathbb{E}_t[\epsilon_{r, t}] = 0$, leading to
\begin{align*}
    & \mathbb{E}_t [\phi_{\frac{1}{\bar{\rho}}}(\w^{(Q)}_{t+1, G})]\\
    & \leq \mathbb{E}_t [\mathcal{L}(\hat{\w}^{(Q)}_{t, G}) + \frac{\bar{\rho}}{2}\|\hat{\w }^{(Q)}_{t, G} - \w^{(Q)}_{t+1, G} \|^2  ] \\
    & = \mathcal{L}(\hat{\w }^{(Q)}_{t, G}) + \mathbb{E}_t[ \frac{\bar{\rho}}{2}\|\hat{\w}^{(Q)}_{t, G} - \v_{t+1, G} - \epsilon_{r, t} \|^2 ] \\
    & = \mathcal{L}(\hat{\w }^{(Q)}_{t, G}) + \frac{\bar{\rho}}{2}\mathbb{E}_t [\| (\w^{(Q)}_{t, G} - \hat{\w }^{(Q)}_{t, G}) -  \alpha_{t} \g(\w^{(Q)}_{t, G}) - \epsilon_{r} \|^2 ] \\
    & \leq \mathcal{L}(\hat{\w}^{(Q)}_{t, G}) + \frac{\bar{\rho}}{2} \| \w^{(Q)}_{t, G} - \hat{\w }^{(Q)}_{t, G}\|^2 + \bar{\rho} \alpha_t \mathbb{E}_t\langle \hat{\w }^{(Q)}_{t, G} - \w^{(Q)}_{t, G}, \g(\w^{(Q)}_{t, G}) \rangle + \bar{\rho}(\alpha_t^2G^2+3\alpha_t G_q^2) \\
    & \leq  \phi_{\frac{1}{\bar{\rho}}}(\w^{(Q)}_{t, G}) + \bar{\rho}\alpha_t (\mathcal{L}(\hat{\w}^{(Q)}_{t, G} ) - \mathcal{L}(\w^{(Q)}_{t, G}) + \frac{\rho}{2} \|\w^{(Q)}_{t, G} - \hat{\w }^{(Q)}_{t, G} \|^2 ) + \bar{\rho}(\alpha_t^2G^2+3\alpha_t G_q^2).
\end{align*}

Next, by using the fact that $\mathcal{L}(x) + \frac{\bar{\rho}}{2} \|x - x_t \|^2$ is $(\bar{\rho} - \rho)$-strongly convex we obtain 
\begin{align*}
    \mathcal{L}(\hat{\w}^{(Q)}_{t, G} ) - \mathcal{L}(\w^{(Q)}_{t, G}) + \frac{\rho}{2} \|\w^{(Q)}_{t, G} - \hat{\w }^{(Q)}_{t, G} \|^2 \leq \frac{\rho - \bar{\rho}}{\bar{\rho}^2} \|\nabla \phi_{\frac{1}{\bar{\rho}}} (\w^{(Q)}_{t, G}) \|^2.
\end{align*}
This further implies that
\begin{align*}
    \mathbb{E}_t [\phi_{\frac{1}{\bar{\rho}}}(\w^{(Q)}_{t+1, G})] & \leq  \phi_{\frac{1}{\bar{\rho}}}(\w^{(Q)}_{t, G}) - \alpha_t \frac{\bar{\rho} - \rho}{\bar{\rho}} \|\nabla \phi_{\frac{1}{\bar{\rho}}} (\w^{(Q)}_{t, G}) \|^2 + \bar{\rho}(\alpha_t^2G^2+3\alpha_t G_q^2).
\end{align*}
By telescoping and rearranging, we obtain
\begin{align*}
    & \frac{1}{\sum_{t=0}^T \alpha_{t}} \sum_{t=0}^T \alpha_{t} \mathbb{E} [ \|\nabla \phi_{\frac{1}{\bar{\rho}}} (\w^{(Q)}_{t, G}) \|^2 ] \\
    & \leq \frac{ \bar{\rho}}{\bar{\rho} - \rho} \frac{ \phi_{\frac{1}{\bar{\rho}}}(\w^{(Q)}_{0, G}) - \min \phi +  \bar{\rho}(G^2\sum_{t=0}^T \alpha^2_{t}+3G_q^2\sum_{t=0}^T \alpha_{t})}{\bar{\rho}\sum_{t=0}^T \alpha_{t}}.
\end{align*}

Recall that $\w_{t, G}$ is accessible only when $t \in \mathcal{I}_E$; hence, we consider only these time steps.
If we assume that at each communication round the client uses the same learning rate throughout $E$ local training epochs, we finally obtain that 
\begin{align*}
    & \frac{1}{\sum_{t=0}^T \alpha_{tE}} \sum_{t=0}^T \alpha_{tE} \mathbb{E} [ \|\nabla \phi_{\frac{1}{\bar{\rho}}} (\w^{(Q)}_{tE, G}) \|^2 ] \\
    & \leq \frac{E \bar{\rho}}{\bar{\rho} - \rho} \frac{ \phi_{\frac{1}{\bar{\rho}}}(\w^{(Q)}_{0, G}) - \min \phi +  \bar{\rho}(G^2\sum_{t=0}^T \alpha^2_{tE}+3G_q^2\sum_{t=0}^T \alpha_{tE})}{\bar{\rho}\sum_{t=0}^T \alpha_{tE} }.
\end{align*}
\end{proof}

While this result demonstrates convergence to a neighborhood of a stationary point, the specific SSL objective, $\mathcal{L}(\w) = \|\bar{X} - \w^T \w \|^2$, has property that all local minima are global minima \cite{jin2017escape}. The convergence to local minima via gradient descent using  bounded-variance gradient estimates has been studied in \cite{mertikopoulos2020almost} under more restrictive conditions including smoothness. It has been shown that for $\Gamma \geq \lambda_1(\bar{X})$, function $\mathcal{L}(\w)$ is $16\Gamma$-smooth within the region $\{\w | \|\w \|_2^2 \leq \Gamma \}$. Thus a suitable choice of $\Gamma$ and step size $\alpha_t$ guarantee that $\w_t$ remains within the region as long as $\w_{t-1}$ is within the region \cite{jin2017escape}. Therefore, Fed-QSSL approaches the neighborhood of the local minima of $\mathcal{L}(\cdot)$ and, ultimately, that of the global minima. 

    \subsection*{A.4. Proof of Theorem~2}
    {\textbf{Heterogeneous data construction. }}
The construction of heterogeneous data follows \cite{wang2022does}. Suppose there are $n = \Theta(d^{1/20})$ data sets/sources, one for each client, consisting of points that belong to $2n$ classes. For the data set at client $k$, the majority of points belong to classes $2k-1$ and $2k$, while a few remaining points come from other classes. We use $e_1, \dots, e_d$ to denote the standard unit basis of $\R^d$. A data sample from class $2k-1$ is generated as $x^{(2k-1)} = e_k - \sum_{i=1, i\neq k}^n q^{(2k-1, i)}\tau e_i + \mu \xi^{(2k-1)} $, where $q^{(2k-1, i)} \in \{0, 1 \} $ is uniformly sampled, $\xi^{(2k-1)} \sim \mathcal{N}(0, I)$, $\tau = d^{1/5}$ and $\mu = d^{-1/5}$. Likewise, a data sample from class $2k$ is generated as $x^{(2k)} = -e_k - \sum_{i=1, i\neq k}^n q^{(2k, i)}\tau e_i + \mu \xi^{(2k-1)} $. The numbers of data points in classes $2k-1$ and $2k$ are the same and of order $poly(d)$. As for the other classes, a data sample from class $2i-1$ is generated as $x^{(2i-1)} = e_i + \mu \xi^{(2i-1)}$ when $i \neq k$ and there are no data samples in class $2i$. The numbers of data points in class $2k-1$ and class $2k$ are the same and of order $\O(d^{\beta})$ for some $\beta \in (0, 1)$ such that $\O(nd^{\beta}) \leq \O(d^{1/5})$, implying that the total number of data samples in infrequent classes is much smaller than the number of data samples in the frequent classes (i.e., in classes $2k-1$ and $2k$). For simplicity, we assume that all data sets are of the same cardinality, i.e., $\frac{|D_k|}{|D|} = \frac{1}{n}$ for all $k \in [n]$.

The following theorem quantifies the robustness property.

\newtheorem*{theorem2}{Theorem 2}
\begin{theorem2}
Suppose assumptions of Theorem~1 hold and $n = \Theta(d^{1/20})$. In a $2n$-way classification task, when in local training the update is $\epsilon$ away from the optimal solution $\w_k^*$, the representation vector learned by client $k$ with high probability satisfies $ \frac{d^{2/5} - \O(d^{-2/5}) + 2e_j^T (\w_k^*)^T \epsilon e_j + (e_j^T \epsilon)^2}{d^{2/5} + \O(d^{-2/5}) + \|2(\w_k^*)^T \epsilon + \epsilon^T \epsilon  \|} \leq r_{i}^k \leq 1$ for $i \in [n]\backslash \{k \}$. As for the global objective, when the update is $\epsilon_1$ away from the optimal solution $\w^*$, the learned representation vector $\bar{r}$ with high probability satisfies 

$\frac{d^{2/5} - \Theta(d^{7/20}) + \O(d^{-1/20}) - \O(d^{2/5}) + 2e_j^T (\w^*)^T \epsilon_1 e_j + (e_j^T \epsilon_1)^2 }{d^{2/5} - \Theta(d^{7/20}) + \O(d^{-1/20}) + \|2(\w^*)^T \epsilon_1 + \epsilon_1^T \epsilon_1  \|} \leq \bar{r}_i \leq 1$ for all $i \in [n]$. 
\end{theorem2}

{\textbf{Technical challenges.}}
While the robustness of $\w^*$ was analyzed in \cite{wang2022does}, it is unclear how to extended it to quantifying robustness when the update obtained by the proposed algorithm approaches the optimal solution. To facilitate such analysis, we initiate it at the step the obtained update is $\epsilon_1$ distance away from the optimal $\w^*$; this maps the problem to that of analyzing perturbed representation vectors and data covariance matrix. We conduct such analysis by borrowing ideas from matrix perturbation theory. 

\begin{proof}
For the local data set/source at client $k$, 
\[
X_k = \mathbb{E}_{x \sim D_k}(xx^T) = \frac{1}{|D_k|}\sum_{i=1}^{|D_k|} x_{k, i}x_{k, i}^T
\]
is the empirical covariance matrix. Its expectation is
\begin{small}
\begin{equation}
\begin{aligned}
    \mathbb{E}[X_k] & = \mathrm{diag}(\tau^2 + O(d^{-2/5}), \cdots, 1+O(d^{-2/5}), \cdots, \tau^2 + O(d^{-2/5}),  O(d^{-2/5}) \\
    & \quad \cdots,  O(d^{-2/5}) ) \\
    & = \mathrm{diag}( d^{2/5}+O(d^{-2/5}), \cdots, 1+O(d^{-2/5}), \cdots, d^{2/5}+O(d^{-2/5}), O(d^{-2/5}), \\ 
    & \quad \cdots, O(d^{-2/5})  ), \label{eq:r1}
\end{aligned}
\end{equation}
\end{small}
where the first $n$ diagonal entries in the first equality are $ \tau^2+ O(d^{-2/5})$, except the $k$-th entry which is $ 1+ O(d^{-2/5})$; the remaining $d-n$ diagonal entries are $ O(d^{-2/5})$.

Using matrix concentration bounds \cite{vershynin2018high} and Weyl's inequality, we have that with high probability 
\begin{align}\label{eq:r2}
    |\lambda_{k, i} - \lambda_i(\mathbb{E}X_k)| \leq \|X_k - \mathbb{E}X_k \|_2 \leq O(d^{-2/5}),
\end{align}
where $\lambda_{k, i}$ represents the $i$-th largest eigenvalue of $X_k$.

Since $|D_k|$ is of the order $poly(d)$, one can leverage Lemma~E.1 in \cite{liu2021self} to obtain that $\sum_{i=1}^{|D_k|} |\langle \xi_{k, i}, e_k \rangle| /|D_k| \leq O(d^{1/10})$. For $\mu = d^{-1/5}$,
\begin{align*}
    e_j^T X_k e_j  = \mathbb{E}[(e_j^Tx)^2] \geq [\mathbb{E}(e_j^T x)]^2 \geq (\frac{1}{3}\tau - \mu \sum_{i=1}^{|D_k|} \frac{1}{|D_k|}|e_j^T \xi_{k, i}| ) = \Omega(\tau^2) = \Omega(d^{2/5})
\end{align*}
with probability at least $1-\frac{1}{2}e^{-d^{1/10}}$. The second inequality follows from the fact that for at least $1/3$ data points it holds that either $q^{2k-1, j}$ or $q^{2k, j}$ is $1$. From \eqref{eq:r1} and \eqref{eq:r2}, one can further obtain that
\begin{equation}
\begin{aligned}
    e_j^T X_k e_j & = e_j^T \mathbb{E}[X_k] e_j + e_j^T (X_k - \mathbb{E}(X_k)) e_j  \\
    & \geq d^{2/5} + \O(d^{-2/5}) - \|X_k - \mathbb{E}(X_k) \| \geq d^{2/5} - \O(d^{-2/5}), \label{eq:r3}
\end{aligned}
\end{equation}
and
\begin{equation}
    \begin{aligned}
        \lambda_{k, 1} \leq \lambda_1(\mathbb{E}(X_k)) + \O(d^{-2/5}) = d^{2/5} + \O(d^{-2/5}).
    \end{aligned}
\end{equation}

Recall that the local optimization problem can equivalently be written as the minimization of a local objective function $\mathcal{L}_k = \|X_k - \w^T \w \|^2$. By Eckart-Young-Mirsky theorem, minimizing $\mathcal{L}_k$ leads to solution $\w_k^* \in \R^{m \times d}$ whose rows are eigenvectors of the first $m$ eigenvalues of $X_k$. Let $\{v_{k, 1}, \cdots, v_{k, d} \}$ be the set of $d$ orthonormal eigenvectors of $X_k$, $X_k = \sum_{i=1}^d \lambda_{k, i} v_{k, i}v_{k, i}^T$, and $(\w^*_k)^T(\w^*_k) = \sum_{i=1}^m \lambda_{k, i} v_{k, i}v_{k, i}^T $. We readily obtain that 
\begin{align}\label{eq:r4}
    \lambda_{k, 1} \sum_{i=1}^d (e_j^T v_{k, i})^2 \geq e_j^T X_k e_j = \sum_{i=1}^d \lambda_{k, i} (e_j^T v_{k, i})^2
\end{align}
with probability of at least $1-\frac{1}{2}e^{-d^{1/10}}$. Suppose the update is within $\epsilon$ distance from the optimal solution, i.e., $\w_{k, t} = \w_k^* + \epsilon$. Let $\w_{k, t}^T \w_{k, t} = \sum_{i=1}^m \lambda'_{k, i} (v'_{k, i})^T v'_{k, i}$. For $i > m$, we define $v'_{k, i} = v_{k, i}$ and $\lambda'_{k, i} = \lambda_{k, i}$. Invoking result from matrix perturbation theory,
\begin{align}\label{eq:matrix}
    \max_{1\leq i \leq d} |\lambda_t(A+E) - \lambda_t(A)| \leq \|E \|_2 \leq \|E \|,
\end{align}
which implies that $ |\lambda_{k, i} - \lambda'_{k, i} |$ is upper bounded by $\|2(\w_k^*)^T \epsilon + \epsilon^T \epsilon\|$ for any $i \in [n]$. From \eqref{eq:r3}-\eqref{eq:r4} and $\w_{k, t} = \w_k^* + \epsilon $, it follows that 
\begin{align*}
    \sum_{i=1}^d (e_j^T v'_{k, i})^2 & \geq \frac{e_j^T X_k e_j + 2e_j^T (\w_k^*)^T \epsilon e_j + (e_j^T \epsilon)^2 }{\lambda'_{k, 1}} \\
    & \geq \frac{e_j^T X_k e_j + 2e_j^T (\w_k^*)^T \epsilon e_j + (e_j^T \epsilon)^2}{\lambda_{k, 1} + \|2(\w_k^*)^T \epsilon + \epsilon^T \epsilon \|} \\
    & \geq \frac{d^{2/5} - \O(d^{-2/5}) + 2e_j^T (\w_k^*)^T \epsilon e_j + (e_j^T \epsilon)^2}{d^{2/5} + \O(d^{-2/5}) + \|2(\w_k^*)^T \epsilon + \epsilon^T \epsilon \|}.
\end{align*}
Let $r^k = [r_1^k, \cdots, r_d^k ]^T $ be the representation vector learned at client $k$. Then $\forall i \in [n]/\{k \}$,
\begin{align*}
    \frac{d^{2/5} - \O(d^{-2/5}) + 2e_j^T (\w_k^*)^T \epsilon e_j + (e_j^T \epsilon)^2}{d^{2/5} + \O(d^{-2/5}) + \|2(\w_k^*)^T \epsilon + \epsilon^T \epsilon  \|} \leq r_{i}^k.
\end{align*}

% [Here we may assume that $\omega$ is in the order of $\O(d^{2})$ for the eigenvalue since the dimension of $\omega$ is $m \times d$.]

Next we move to the global objective, $ \mathcal{L} = \|\bar{X} - \w^T \w \|^2$, where $\bar{X} = \mathbb{E}_{x \sim D}(xx^T) = \sum_k \frac{|D_k|}{|D|}X_k = \frac{1}{|D|}\sum_{i=1}^{|D|}x_i x_i^T$ is the empirical global data covariance matrix. Following the linearity of expectation and the fact that 
\begin{align*}
    \frac{(n-1)d^{2/5}+1}{n} & = (1 - \Theta(d^{-1/20}))d^{2/5} + \O(d^{-1/20}) \\
    & = d^{2/5} - \Theta(d^{7/20}) + \O(d^{-1/20}),
\end{align*}
we obtain the expectation of the covariance matrix as
\begin{align*}
    \mathbb{E}[{\bar{X}}] & = \mathrm{diag}( d^{2/5} - \Theta(d^{7/20}) + \O(d^{-1/20}), \cdots, d^{2/5} - \Theta(d^{7/20}) + \O(d^{-1/20}), \\
    & \quad \O(d^{-2/5}), \cdots, \O(d^{-2/5}) )
\end{align*}
where the first $n$ entries are of the same order while the remaining $d-n$ entries are $\O(d^{-2/5})$. Let $\{v_{1}, \cdots, v_{d} \}$ be the set of $d$ orthonormal eigenvectors of $\bar{X} = \sum_{i=1}^d \lambda_{i} v_{i}v_{i}^T$, and $(\w^*)^T(\w^*) = \sum_{i=1}^m \lambda_{i} v_{i}v_{i}^T $. 

Follow similar techniques to analyze local objectives leads to
\begin{align*}
    e_j^T \bar{X} e_j & \geq d^{2/5} - \Theta(d^{7/20}) + \O(d^{-1/20}) - \O(d^{2/5}) \\
    \lambda_1(\bar{X}) & \leq \lambda_1(\mathbb{E}(\bar{X})) + \O(d^{2/5}) = d^{2/5} - \Theta(d^{7/20}) + \O(d^{-1/20}).
\end{align*}

Next, we consider perturbation $\w_{t} = \w^* + \epsilon_1 $ of the global optimal solution $\w^*$. Let $\w_{t}^T \w_{t} = \sum_{i=1}^m \lambda'_{i} (\bar{v}'_{i})^T \bar{v}'_{i}$. For $i > m$, denote $\bar{v}'_{i} = \bar{v}_{i}$ and $\lambda'_{i} = \lambda_{i}$. Then
\begin{align*}
    \sum_{i=1}^d (e_j^T \bar{v'}_{i})^2 & \geq \frac{e_j^T \bar{X} e_j + 2e_j^T (\w^*)^T \epsilon_1 e_j + (e_j^T \epsilon_1)^2 }{\lambda'_{1}} \\
    & \geq \frac{e_j^T \bar{X} e_j + 2e_j^T (\w^*)^T \epsilon_1 e_j + (e_j^T \epsilon_1)^2 }{\lambda_{1} + \|2(\w^*)^T \epsilon_1 + \epsilon_1^T \epsilon_1 \|} \\
    & \geq \frac{d^{2/5} - \Theta(d^{7/20}) + \O(d^{-1/20}) - \O(d^{2/5}) + 2e_j^T (\w^*)^T \epsilon_1 e_j + (e_j^T \epsilon_1)^2 }{d^{2/5} - \Theta(d^{7/20}) + \O(d^{-1/20}) + \|2(\w^*)^T \epsilon_1 + \epsilon_1^T \epsilon_1  \|}
\end{align*}
where the second inequality is due to 
the matrix perturbation result \eqref{eq:matrix} applied to the global data covariance matrix.

We then let $\bar{r} = [\bar{r}_1, \cdots, \bar{r}_d ]^T$ be the representation vector learned from the global objective and $\forall i \in [n]$,
\begin{align*}
    \frac{d^{2/5} - \Theta(d^{7/20}) + \O(d^{-1/20}) - \O(d^{2/5}) + 2e_j^T (\w^*)^T \epsilon_1 e_j + (e_j^T \epsilon_1)^2 }{d^{2/5} - \Theta(d^{7/20}) + \O(d^{-1/20}) + \|2(\w^*)^T \epsilon_1 + \epsilon_1^T \epsilon_1  \|} \leq \bar{r}_i 
\end{align*}

\end{proof}

\begin{algorithm}[tb]
\renewcommand{\thealgorithm}{2}
\caption{Fed-QSSL local training: client $k$ trains on $L$-layer network with $s_k$-bit weights and activations using $s_k+2$-bit gradients.}
\label{alg:algorithm3}
\textbf{Require}: a minibatch of inputs $a_0$ , previous weights $\w^{(Q)}_{k, t}$ in $s_k$-bit, learning rate $\alpha_t$, activation function $h$ \\
\begin{algorithmic}[1] %[1] enables line numbers
\STATE $\{$ Forward pass $\}$
\FOR{$l = 1$ to $L$}
\STATE $\Tilde{a}_l \leftarrow $ forward($a_{l-1}, (\w^{(Q)}_{k, t})_{l}$)
\STATE $a_{l} \leftarrow h(\Tilde{a}_l )$
\STATE Compute $a^{(Q)}_{l}$ through $\tanh$-based quantizer
\ENDFOR
\STATE $\{$ Back propagation $\}$
\STATE Compute the gradient $g_{a_L}$ at the last layer and then compress to $g^{(Q)}_{a_L}$
\FOR{$l = L$ to $1$}
\STATE %Backpropagation on the input to get $g_{a_{l-1}}$ using $g^{(Q)}_{a_l}, (\w^{(Q)}_{k, t})_{l}$
$g_{a_{l-1}} \leftarrow  $ inputs backpropagation$(g^{(Q)}_{a_l}, (\w^{(Q)}_{k, t})_{l} ) $
\STATE % Backpropagation on the weights to get $(g_{\w_{k, t}})_{l}$ using $g^{(Q)}_{a_l}, a^{(Q)}_{l-1} $
$(g_{\w_{k, t}})_{l} \leftarrow $ weights backpropagation$(g^{(Q)}_{a_l}, a^{(Q)}_{l-1}) $
\STATE Compute $g^{(Q)}_{a_{l-1}} $ via quantile quantization
\STATE Compute $(g^{(Q)}_{k, t})_{l}$ via quantile quantization
\ENDFOR
\STATE $\{$ Weight update $\}$
\STATE $ \w^{(Q)}_{k, t+1} = Q(\w^{(Q)}_{k, t} - \alpha_t \g^{(Q)}_{k, t}) $
\end{algorithmic}
\end{algorithm}

    \subsection*{A.5. Local Training Implementation Details}
    Details of local quantized training (steps 5-6 in Fed-QSSL in the main manuscript) are specified in Algorithm \ref{alg:algorithm3}. In particular, in the forward pass we go from the first layer to the last one, obtain quantized activations along the way; in the backward propagation, we go from the last layer back to the first layer, computing and quantizing gradients in the process.
	
	

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	\bibliography{bib}
	\bibliographystyle{acm}
    
\end{document}