
In this section, we present analytical results that provide insights into Fed-QSSL, with a focus on the impact of low-bitwidth training on the convergence and robustness. For the sake of tractability, we consider the SSL formulation utilizing local objective defined as $\mathcal{L}_{SSL,k}(\w) = -\mathbb{E}[(\w(x_{k, i}) +\xi_{k, i})^T(\w(x_{k, i}) +\xi'_{k, i})] + \frac{1}{2} \|\w^T \w \|^2 $, where $\xi_{k, i}$ and $\xi'_{k, i}$ denote random noise added to the data sample, while the global objective is defined as $\mathcal{L}_{SSL} = \sum_{k=1}^n \frac{|D_k|}{|D|}\mathcal{L}_{SSL, k}(\w)$.
This objective is obtained from the InfoNCE loss $\mathcal{L}_{CL, k}$ by replacing normalization via negative signals by an alternative regularization term \cite{wang2022does}. Optimizing $\mathcal{L}_{SSL}$ is equivalent to minimizing $\mathcal{L}(\w) = \|\bar{X} - \w^T \w \|^2$ where $\bar{X} = \sum_k \frac{|D_k|}{|D|}X_k$ and $X_k = \mathbb{E}_{x \sim D_k}(xx^T) = \frac{1}{|D_k|}\sum_{i=1}^{|D_k|} x_{k, i}x_{k, i}^T$, the empirical covariance matrix of client $k$'s data. Since the aim of this analysis is to assess the impact of low-bitwidth operations at client devices on the convergence and robustness of federated learning, we simplify operations at the server by replacing the sophisticated de-quantizer in Algorithm \ref{alg:algorithm2} by a layer-wise codebook mapping low-bitwidth weights to floating-point values according to $\w'^{(Q)}_{k, t} = \w^{(Q)}_{k, t}$. The subsequent aggregation follows FedAvg and computes $\w_{t, G} = \sum_{k=1}^n \frac{|D_k|}{|D|} \w'^{(Q)}_{k, t}$, while re-quantization uses a $\tanh$ based quantizer.

For simplicity, we assume that clients quantize weights/activations using the same bitwidth, i.e., $s_1 = s_2 = \cdots = s_n = R$; moreover, local gradient quantization is performed using the same number of bits. When bitwidth varies across client devices, the devices with higher bitwidth typically have smaller quantization error. Our forthcoming analysis can be viewed as providing error bounds for a system in which clients deploy bitwidth no less than $R$. Local datasets are generated in a distributed and $2n$-way manner, i.e., local dataset at client $k$ is generated such that the labels of data samples are skewed to classes $2k-1$ and $2k$, with a few samples coming from other classes. This distribution is close to the pathologically non-iid case with two dominant classes. More details about heterogeneous data generation are provided in Appendix.

We consider three types of quantization errors: $\epsilon_{\w}$ denotes the model parameter quantization error, $\epsilon_g$ denotes the gradient quantization error, and $\epsilon_r$ denotes the quantization error induced by the server when re-quantizing the aggregated model. Below we use subscripts $k$ and $t$ to indicate the client and the epoch, respectively. Quantization can be viewed as adding noise to the full precision values, i.e., $\g^{(Q)}_{k, t} = \g_{k, t} + \epsilon_{g_k, t} $. In quantized training at client $k$, the update can be expressed as
\begin{align*}
    \w_{k, t+1}^{(Q)} = \w_{k, t}^{(Q)} - \alpha_t (\g_{k, t} + \epsilon_{g_k, t} )+ \epsilon_{\w_k, t}
\end{align*}
where $\w_{k, t}^{(Q)}$ denotes the low-bitwidth model parameters at time $t$, $\epsilon_{g_k, t}$ is the noise added to the gradient in the quantization step, and $\epsilon_{\w_k, t}$ denotes the noise added to the model parameters after gradient update as the parameters retain low-bitwidth representation. After $E$ local training epochs, the server collects the models and aggregates them according to 
\begin{align*}
    & \w_{t+E, G} = \sum_{k=1}^n \frac{|D_k|}{|D|} \w_{t+E, k}^{(Q)} = \sum_{k=1}^n \frac{|D_k|}{|D|} \w_{k, t}^{{Q}} \\
    & \quad - \sum_{s=0}^{E-1} [\alpha_{t+s}(\g_{k, t+s} + \epsilon_{g_k, t+s} ) - \epsilon_{\w_k, t+s}].
\end{align*}
Since the aggregated model is not necessarily in low bitwidth, an additional re-quantization step is required to form its quantized version $\w_{t+E, G}^{(Q)} = \w_{t+E, G} + \epsilon_{r, t+E}$, which is then sent to the clients.

Quantization centers used for layer-wise scalar quantization of the model parameters and gradients are found via companding quantization. Specifically, full-precision input $\x$ is transformed by a nonlinear function $c$, e.g., $\tanh$ function, and then uniformly quantized. The output $Q(\x)$ is generated by taking the value of the inverse function, $c^{-1}$, of the quantized value \cite{sun2011scalar}. Uniform quantization is  a special case of companding quantization obtained by setting $c(\x) = \x$. In Fed-QSSL, local quantizers can also be viewed as special cases of the companding quantizers.

To proceed with the analysis, we make the following assumption on the quantizers.
\begin{assumption}\label{Assumption-1}
    All quantization operators in the low-bit training are unbiased.
\end{assumption}
This assumption is commonly encountered in prior work (see, e.g., \cite{reisizadeh2020fedpaq}); an example is the stochastic quantizer, i.e., given a sequence of quantization centers $Q_1 \leq \cdots \leq Q_{2^R}$, for $x \in [Q_j, Q_{j+1}]$ we have $Q(x) = Q_j$ with probability $\frac{Q_{j+1} - x}{Q_{j+1} - Q_j}$ and $Q(j) = Q_{j+1}$ with probability $\frac{x - Q_{j}}{Q_{j+1} - Q_j}$. This assumption implies that quantization errors $\epsilon_{\w}$, $\epsilon_{g}$, and $\epsilon_r$ are zero-mean.

We further make the following assumption on gradient estimates, frequently encountered in literature.
\begin{assumption}\label{Assumption-2}
Expected gradient estimate is unbiased and bounded, $\mathbb{E}_t \|\g_{k, t} \|^2 \leq G^2$.
\end{assumption}

The following lemma provides a bound on the quantization errors (for proof please see Appendix).
\begin{lemma}\label{lemma1}
Suppose Assumptions \ref{Assumption-1}-\ref{Assumption-2} hold, and that client $k$ computes update of the quantized model parameters $\w_{k, t+1}^{(Q)}$ at bitwidth
$s_k = R$. Then the gradient quantization error $\epsilon_{g_k, t}$ satisfies $\mathbb{E}_t[\|\epsilon_{g_k, t} \|^2] = \O(G^2/2^{2R})$, and the local re-quantization error $\epsilon_{\w_k, t}$ satisfies $\mathbb{E}_t[ \|\epsilon_{\w_k, t}\|^2] = \O(\alpha_t G^2/2^{2R})$.
\end{lemma}

This lemma indicates that by viewing the gradient update $\alpha_t (\g_{k, t} + \epsilon_{g_k, t} )$ as a perturbation of $\w^{(Q)}_{k, t}$, the variance of the weight quantization error $\epsilon_{\w_k, t}$ in the local update is of the order $\O(\alpha_tG^2)$. It further implies that the variance of the re-quantization error $\epsilon_{r, t}$ in the global update is of the order $\O(\alpha_t G^2 E)$ when $\alpha_{t-E+1} = \cdots = \alpha_t$. 

The next corollary readily follows from Lemma~1.

\begin{corollary}\label{corollary1}
Instate the assumptions of Lemma \ref{lemma1}, and let all clients use the same learning rate $\alpha_{t-E+1} = \cdots = \alpha_t$. Then there exists $G_q = \O(G^2/2^{2R})$ such that $\mathbb{E}_t[\|\epsilon_{\w_k, t} \|^2 ]\leq \alpha_t G_q^2 $ and $\mathbb{E}_t[\|\epsilon_{r, t} \|^2 ]\leq \alpha_t G_q^2 $.
\end{corollary}

Next, we consider the convergence of the quantized SSL training. Note that objective $\mathcal{L} = \|\bar{X} - \w^T \w \|^2$ is generally a non-convex and non-smooth function. In our analysis, we consider a class of functions that satisfies the $\rho$-weak convexity. 

\begin{definition}
A function $f: \R^d \to \R $ is $\rho$-weakly convex if $f(x) + \frac{\rho}{2}\|x \|^2$ is convex.
\end{definition}
Generally, any function of the form $f(x) = g(h(x))$, where $g(\cdot)$ is convex and Lipschitz and $h(\cdot)$ is a smooth map with Lipschitz Jacobian, is weakly-convex.
Clearly, the federated SSL objective satisfies $\rho$-weak convexity with $\rho \geq 4\|\bar{X} \|$. To analyze weakly-convex objective function $\mathcal{L}$, we introduce the Moreau envelope
\begin{align*}
    \phi_{\lambda}(x) := \min_y \{\mathcal{L}(y) + \frac{1}{2\lambda}\|y-x \|^2 \}
\end{align*}
and define the corresponding proximal map as
\begin{align*}
    \mathrm{prox}_{\phi_{\lambda}(x) } : = \mathrm{argmin}_y  \{\mathcal{L}(y) + \frac{1}{2\lambda}\|y-x \|^2 \}.
\end{align*}

We use $\|\nabla \phi_{\lambda}(x) \|$ as the convergence indicator. Intuitively, small norm of the gradient of $\phi_{\lambda}(x) $ implies that $x$ is close to a point $\hat{x}$ that is stationary for $\phi$. For Fed-QSSL, the following result holds.

\begin{theorem}\label{thm1}
Suppose all assumptions of Lemma \ref{lemma1} and Corollary \ref{corollary1} hold.
%$\mathbb{E}_t [\| \g(\w_{t, G})\|^2] \leq G^2 + G_q^2$ where $G$ is for stochastic gradient estimate and $G_q$ is for quantization error. 
For all $\bar{\rho} > \rho$, after $T$ communication rounds of Fed-QSSL
\begin{align*}
    & \frac{1}{\sum_{t=0}^T \alpha_{tE}} \sum_{t=0}^T \alpha_{tE} \mathbb{E} [ \|\nabla \phi_{\frac{1}{\bar{\rho}}} (\w^{(Q)}_{tE, G}) \|^2 ] \leq  \frac{E \bar{\rho}}{\bar{\rho} - \rho} \times \\& \frac{ \phi_{\frac{1}{\bar{\rho}}}(\w^{(Q)}_{0, G}) - \min \phi +  \bar{\rho}(G^2\sum_{t=0}^T \alpha^2_{tE}+3G_q^2\sum_{t=0}^T \alpha_{tE}) }{\bar{\rho}\sum_{t=0}^T \alpha_{tE} },
\end{align*}
where $E$ denotes the number of local training epochs per communication round (for a total of $tE$ training epochs after $t$ communication rounds) and $\w^{(Q)}_{0, G}$ is the quantized parameter initialization. 
\end{theorem}
Theorem~1 implies convergence of the algorithm to a nearly stationary state of the objective function. When $\alpha_t$ decreases as $\O(1/\sqrt{t})$, the upper bound vanishes as $t \to \infty$. The optimal solution $\w^*$ of the global objective satisfies $\| \nabla \phi_{\frac{1}{\bar{\rho}}} (\w^*) \|^2  = 0$; the vanishing upper bound implies proximity to a neighborhood of the optimal solution.{\footnote{While using gradient descent estimates with bounded variance typically leads to converge to local minima \cite{mertikopoulos2020almost}, a property of the SSL objective is that all local minima are global minima \cite{jin2017escape}.}} As the quantization rate $R$ approaches full precision rate, $G_q$ vanishes and the upper bound collapses to the value obtained by vanilla stochastic gradient descent analysis.

It is further of interest to analyze robustness of the learned representations. To this end, we first formally define the representation vector.
\begin{definition}\cite{wang2022does}
    Let $\mathcal{S} \subset \R^d$ be the subspace spanned by the rows of the learned feature matrix $\w \in \R^{m \times d}$ for the embedding function $f_{\w}(x) = \w x$. The representability of $\mathcal{S}$ is defined as the vector $r = [r_1, \cdots, r_d]^T$ such that $r_i = \|\Pi_{\mathcal{S}}(e_i) \|^2$ for $i \in [d]$, where $\Pi_{\mathcal{S}}(e_i) $ denotes the projection of standard basis $e_i$ onto $\mathcal{S}$ and thus $r_i = \sum_{j=1}^s\langle e_i, v_j \rangle^2$, $s = \dim(\mathcal{S})$ and $\{v_j \}$ is the set of orthonormal bases of $\mathcal{S}$.
\end{definition}

Vector $r$ introduced in this definition quantifies representability by comparing unit bases of different feature spaces. A good feature space should be such that the entries of $r$ are large, indicating its standard unit bases can well represent $e_1, \dots, e_d$, the unit bases in $\R^d$. 

\begin{theorem}\label{thm2}
Suppose assumptions of Theorem \ref{thm1} hold and $n = \Theta(d^{1/20})$. In a $2n$-way classification task, when in local training the update is $\epsilon$ away from the optimal solution $\w_k^*$, the representation vector learned by client $k$ with high probability satisfies $ \frac{d^{2/5} - \O(d^{-2/5}) + 2e_j^T (\w_k^*)^T \epsilon e_j + (e_j^T \epsilon)^2}{d^{2/5} + \O(d^{-2/5}) + \|2(\w_k^*)^T \epsilon + \epsilon^T \epsilon  \|} \leq r_{i}^k \leq 1$ for $i \in [n]\backslash \{k \}$. As for the global objective, when the update is $\epsilon_1$ away from the optimal solution $\w^*$, the learned representation vector $\bar{r}$ with high probability satisfies $\frac{d^{2/5} - \Theta(d^{7/20}) + \O(d^{-1/20}) - \O(d^{2/5}) + 2e_j^T (\w^*)^T \epsilon_1 e_j + (e_j^T \epsilon_1)^2 }{d^{2/5} - \Theta(d^{7/20}) + \O(d^{-1/20}) + \|2(\w^*)^T \epsilon_1 + \epsilon_1^T \epsilon_1  \|} \leq \bar{r}_i \leq 1$ for all $i \in [n]$. 
\end{theorem}

Theorem \ref{thm2} implies that as $\epsilon_1$ vanishes, the entries in representation vectors have strictly positive lower bound and thus do not vanish. This theorem states that the representation space learned from the SSL objectives is such that for the $2n$ basis directions that generate the data, any two clients have similar representability.
The theorem further implies that the learned representation vectors are not biased towards local data distributions and are capable of performing well on uniformly distributed data, indicating robustness of the scheme. 
%While this derived robustness result relies on the construction of heterogeneous data distributions, the utilization of unlabeled small buffer $D_g$ in de-quantization and re-quantization at server side further faciliates the robustness performance in Algorithm \ref{alg:algorithm2}. 
For the proof of Theorem~2, please see Appendix.



%-------------Introducing algorithm with deq+req