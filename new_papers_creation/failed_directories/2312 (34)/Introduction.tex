Federated learning (FL) \cite{mcmahan2017communication, kairouz2021advances} recently emerged in response to the challenges in data privacy, storage cost and computation commonly faced by the conventional (centralized) learning systems. In centralized learning, a server collects and stores data to be used for model training, raising concerns regarding potential leakage of sensitive information and inefficient utilization of the storage and computation resources. Lately, advancements in software and hardware technologies have equipped smart edge devices with computational power that enables local data processing, allowing implementation of distributed frameworks such as FL in a variety of real-world applications \cite{chen2019communication}. In FL systems, participating devices collaboratively learn a model while preserving privacy by training on local data that remains private. A server aggregates local updates to obtain a global model which is then distributed to the clients; in turn, the clients continue local training using the latest global model as a starting point. In addition to the conventional supervised learning, federated learning is suitable for meta-learning and unsupervised learning problems \cite{jiang2019improving, servetnyk2020unsupervised}. 

Heterogeneity in local data distribution, as well as imbalance in the computation and memory capabilities of the participating devices, present major challenges to federated learning \cite{zhao2018federated, yoon2022bitwidth}. Practical scenarios where both sources of heterogeneity occur include healthcare disorder prediction for patients with different profiles and monitoring devices, transportation systems over different vehicles and terrain, and indoor/outdoor air quality detection, to name just a few. Several recent FL techniques aim to 
alleviate the impact of data, model or device heterogeneity \cite{jiang2020federated, lin2020ensemble, diao2020heterofl, wang2022does}. However, variations in bitwidth available to the clients that participate in training have been much
less studied \cite{yoon2022bitwidth}. To our knowledge, there exists no prior work in literature investigating FL in 
setting where both the local data and device bitwidth are heterogeneous -- the focus of our work.

%While prior work has not explored solution to the problem on both bitwidth and data heterogeneity, it is natural in practical applications that clients need to train local models in both heterogeneity. We therefore explore the FL problem combining data and bitwidth heterogeneity and address the challenges from both heterogeneity. 

On one hand, clients' devices collect and/or generate data at different times and locations, leading to discrepancy in data amounts and distribution. When distributed devices train locally, each learned model optimizes an objective specified in regard to a local dataset. If the data across participating devices is non-IID, the devices end up optimizing distinct objectives which generally adversely affects 
training convergence and, ultimately, negatively impacts performance of the resulting global model trained by classical FL algorithms such as FedAvg \cite{mcmahan2017communication, li2019convergence}. This is exacerbated when a client's dataset contains only a small subset of classes or few data points, leading to an insufficiently expressive local model. In turn, using such models in the aggregation step at the server leads to a global model that lacks robustness and in general may significantly underperform its centrally trained counterparts \cite{zhao2018federated}.

On the other hand, to satisfy the constraints on local compute and memory footprint as well as on the communication bandwidth between clients and the central server, each client needs to train a local model in low bitwidth operations and store the updated model in low bitwidth \cite{yoon2022bitwidth}. When the clients' devices have different bitwidth capabilities, a number of novel challenges arise including: (1) Quantized training conducted at lower precision levels does not necessarily lead to expressive local model; (2) the server needs to aggregate local models that are represented by different number of bits; and (3) following the aggregation of local updates, the server should communicate to clients the new global model re-quantized at levels matching the capabilities of different devices. Prior works that deploy model compression in distributed settings focus on reducing the communication cost \cite{reisizadeh2020fedpaq, chen2021communication, chen2021decentralized}; those methods utilize full precision weights in the training process rather than trying to train under weight precision constraint. To address the problem of aggregating models trained at different precision, \cite{yoon2022bitwidth} proposed a progressive weight de-quantizer that transforms low precision models to their full precision versions before aggregating them. The de-quantizer there requires the server to train DenseNet for de-quantizing low precision models; the method assumes the local data distribution is the same across different clients and thus does not apply to the heterogeneous data settings studied in this paper.
%; however, the re-quantized models are not personalized to clients' data distribution. 
Note that all of these methods are restricted to supervised learning.

The contributions of this paper are summarized as follows:

\begin{enumerate}

\item For the first time, a novel FL problem characterized by heterogeneous local data distributions and varied capabilities of clients' devices is studied.
This is the first work to consider the non-trivial task of aggregating models which due to varied bitwidth and heterogeneous data have incompatible parameters and different expressiveness.
While prior work addresses either source of heterogeneity in isolation of the other, to our knowledge the combination of the two has not been investigated previously. Indeed, new challenges set forth by the combination of the heterogeneity sources render the prior techniques ineffective.

\item We present a new FL framework, Fed-QSSL, that enables learning personalized and robust low-bitwidth models in settings characterized by diverse infrastructure and data distributions. Fed-QSSL combines low-bitwidth quantization training with self-supervised 
learning at the client side, and deploys de-quantization, weighted aggregation and re-quantization at the 
server side.

\item We theoretically analyze the impact of low-bit training on the convergence and robustness of learned
models. In particular, we present a bound on the variance of the quantization errors in local and global training and investigate the associated convergence speed. The analysis demonstrates that low-bit training with 
Fed-QSSL on heterogeneous data allows learning meaningful representations. 

\item Finally, the efficacy of the proposed algorithm is tested in a variety of experiments.

\end{enumerate}

\subsection{Related work}

{\bf Bitwidth heterogeneity in FL.} To address local infrastructure constraints, \cite{diao2020heterofl} rely
on models with simple architectures and low-bit parameter representation. However, existing prior work
typically assumes that clients train models of same precision, e.g., the same bitwidth, which may often
be violated in practice since the participating devices generally have different computational power and/or memory 
footprint. The varying infrastructure capabilities of the clients' devices imply that the models they are able to 
deploy differ in size, i.e., devices with more computational power and memory allow models of higher 
precision (e.g., 16-bit and 32-bit) while less capable devices may only handle low-precision models (e.g.,
those storing parameters in 2-bit or 4-bit precision). To satisfy local infrastructure constraints, not only should 
a model be represented and stored at reduced precision levels but the training process should also rely on 
operations at such levels (i.e., perform quantized training in low-bitwidth). Recently, \cite{yoon2022bitwidth} 
studied the bitwidth heterogeneity problem in FL and proposed a progressive de-quantization of the received
local models prior to their aggregation into a global model.

{\bf Quantized neural network training.} 
When the compute/storage resources are limited, on-device training of deep neural networks at full precision is rendered infeasible. To this end, recent works have explored model size reduction and the training that leads to low bitwidth weights/activations. These include binarized neural networks (BNN) and XNOR-Net which binarize the weights and activations of convolutional layers to reduce the computation complexity in the forward pass \cite{hubara2017quantized, rastegari2016xnor}.
Specifically, in the forward pass the computationally expensive convolutional operations are implemented via bitwise operation kernels that evaluate the dot product of binary vectors $\x$ and $\mathbf{y}$
%For instance, when the vectors are $1$-bit, the $1$-bit dot product kernel is 
\begin{align*}
    \x \cdot \mathbf{y} = \mathrm{bitcount}(\mathrm{and} (\x, \mathbf{y})), x_i, y_i \in \{0, 1 \} \ \forall i,
\end{align*}
where $\mathrm{bitcount}(\cdot)$ counts the number of bits in its argument. Such a kernel can be generalized to accommodate operations on $M$-bit fixed-point integer sequence $x$ and $K$-bit fixed-point integer sequence $y$, incurring computation complexity of $\O(MK)$ (i.e., the complexity is proportional to the vector bitwidths). Related work \cite{zhou2016dorefa} presents DoReFa-Net which reduces the backpropagation computation cost by quantizing gradients in the backward pass, and studies the effect of the weight, activation and gradient bitwidth choice on the model performance. Subsequent works include techniques that deploy training without floating-point operations, gradient clipping, and training under mixed precision \cite{wen2017terngrad, zhang2017zipml, zhu2020towards, zhang2020fixed}. While all of the aforementioned methods investigate quantization in supervised learning settings, recently there has been interest in quantizing models for self-supervised learning. In \cite{cao2022synergistic}, the authors propose self-supervised and quantization learning (SSQL), which contrasts features learned from the quantized and full precision models in order to provide reasonable accuracy of quantized models and boost the accuracy of the full precision model. While this work can balance the accuracy and bitwidth in resource constrained settings, it still requires full precision operations and gradient storage during training. 

%In the distributed learning setting, the representation learning nature in SSL has motivated a few recent work to look into the efficacy of self supervised learning in learning robust representations from heterogeneous data sources \cite{wang2022does}.

{\bf Non-IID data and distributed self-supervised learning.} There have been multiple efforts to ameliorate the impact of data heterogeneity on the performance of distributed learning systems \cite{karimireddy2020scaffold, ghosh2020efficient, li2022federated}. Recently, self-supervised learning (SSL) has been shown effective in distributed settings where the data is large-scale and imbalanced \cite{wang2022does}. In contrast to supervised learning, which requires large amount of labeled data for model training, self-supervised learning defines a pre-train task that enables extracting expressive representations; those representations can then be utilized in various downstream tasks in computer vision and natural language processing \cite{chen2020simple,chen2021exploring}. While the majority of self-supervised learning methods focus on the centralized setting where the data is collected for central training, recent works have attempted to bridge the self-supervised and federated learning \cite{he2021ssfl, zhuang2022divergence, makhija2022federated}.  