Let us consider a federated learning system with $n$ clients, where the clients rely on privately owned data to collaboratively learn a global model. Rather than communicating the data, the clients send to a coordinating central server the models trained locally. In turn, the server forms a global model by aggregating the received local models, and re-distributes the global model back to the clients for further local training. Let $D_k = \{x_{k, i} \}_{i=1}^{|D_k|}$ denote the data owned by client $k$, where $x_{k, i} \sim \mathcal{D}_k$ is a $d$-dimensional data point in $D_k$ and $|D_k|$ denotes the number of local data points. The distribution $\mathcal{D}_k$ of data varies from one client to another, leading to system-wide data heterogeneity. The union $D = \bigcup_{k=1}^n D_k$ is assumed to be a dataset of uniform distribution, i.e., the data classes in $D$ are balanced. 

In self-supervised learning problems, an embedding function $f_{\w}(\cdot)$, parameterized by $\w$, is learned as a feature extractor for extracting expressive representations from data. In particular, we will denote the representation vector for point $x$ by $f_{\w}(x)$; such a representation vector is then used for downstream tasks such as image classification or object detection. Contrastive learning is a popular methodology for identifying the embedding function $f_{\w}(\cdot)$ \cite{chen2020simple}. The aim of contrastive learning is to find an embedding function such that the distance between the learned representations of data point $x$ and its positive signal $x^+$ (generated from $x$) is small, while the distance between the representations of $x$ and negative samples $x^{-}$ (extracted from the training batch) is large.\footnote{Given a data sample $x$, its positive signal may be a variant obtained by adding noise, changing color, or clipping; negative signals include data samples from different classes, or simply other data samples in the same training batch.}
A commonly used objective in contrastive learning is the InfoNCE loss; specifically, the local objective for client $k$ is
\[
\mathcal{L}_{CL, k}(\w) = \frac{1}{|D_k|}\sum_{i=1}^{|D_k|} -\log ( \frac{\mathbb{D}_{k, i}^+}{ \mathbb{D}_{k, i}^+ + \sum_j \mathbb{D}_{k, i, j}^-}), 
\]
where
$\mathbb{D}_{k, i}^+ = \exp(-\mathbb{D}(f_{\w}(x_{k, i}), f_{\w}(x_{k, i}^+))/\tau$, $\mathbb{D}_{k, i, j}^- = \exp(-\mathbb{D}(f_{\w}(x_{k, i}), f_{\w}(x_{k, j}^-))/\tau$, and
$\tau >0$ is a temperature parameter controlling the sharpness of the exponential term. The function $\mathbb{D}(\cdot)$ is typically defined as the cosine distance, i.e., $\mathbb{D}(z_1, z_2) = -\frac{z_1 \cdot z_2}{\|z_1 \| \|z_2 \|}$. Note that instead of the negative sample term, an alternative SSL strategy adds to the objective a feature prediction function $g(\cdot)$ that helps avoid potential collapse i.e. low accuracy of the trained classifier layer \cite{chen2021exploring}; in that case, the objective becomes 
\[
\mathcal{L}_{siam, k}(\w) =  \frac{1}{|D_k|}\sum_{i=1}^{|D_k|} \mathbb{D}(g(f_{\w}(x_{k, i})), f_{\w}(x_{k, i}^+)).
\]

In federated learning, each client has a local data source $D_k$. The clients collaboratively search for $\w$ that minimizes the global objective function
\begin{align}
    \mathcal{L}(\w) = \sum_{k=1}^n \frac{|D_k|}{|D|}\mathcal{L}_k(\w).
\end{align}
To solve this distributed optimization without data sharing, numerous federated learning schemes have been proposed including the FedAvg algorithm \cite{mcmahan2017communication}. At each iteration of FedAvg, central server samples a subset of clients which then run $E$ local update epochs. The server collects updated models from the selected clients and aggregates them to form a global model, which is distributed to all clients for further local training/inference. The number of local training epochs, $E$, controls the computation-communication tradeoff, i.e., larger $E$ leads to less frequent communication. 

We further assume that participating clients have devices with varied compute, memory and communication capabilities, and therefore train and deploy models at different bitwidth precision. Let $s_k$ denote the number of bits used to represent parameters $\w_k$ of the model deployed by client $k$. When training, client $k$ utilizes bit convolutional kernels and conducts operations on $s_k$-bit objects rather than performing full precision (i.e., $32$-bit floating-point) operations. When aggregating the collected local models into global model $\w_G$, the server takes into consideration that the models are trained at different precision levels. The server itself is assumed to be capable of performing operations at full precision. Finally, the global model is compressed to various low bitwidth representations and sent to the corresponding clients, i.e., client $k$ receives updated model in $s_k$ bit representation.

In our proposed framework, client $k$ performs low-bit training to learn parameters $\w_k$ of its local model; note that the low-bit training and quantization are deployed in all steps, including the forward pass, backpropagation and model update. In each round of training, model parameters in $s_k$-bit representation are fed into the forward pass via bit convolution kernels, and the gradient computed from the backward propagation is quantized and stored. After updating the model by using gradient descent, the model parameters are quantized to $s_k$ bits and stored. 
% For the forward and backward pass, \cite{zhou2016dorefa} proposed a straightforward method to quantize weights, activations in most layers and gradients in the neural network training, which can be extended to a more general setting where weights/activations of all layers are quantized to satisfy more demanding constraints. 
