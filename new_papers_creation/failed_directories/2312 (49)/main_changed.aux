\relax 
\bibstyle{aaai24}
\citation{mcmahan2017communication,li2018federated,karimireddy2019scaffold}
\citation{zhu2019deep}
\citation{li2019fedmd,gong2022preserving}
\citation{mcmahan2017communication,li2018federated,karimireddy2019scaffold}
\citation{zhu2019deep}
\citation{li2019fedmd,gong2022preserving}
\citation{mcmahan2017communication,li2018federated,karimireddy2019scaffold}
\citation{wang2020federated,li2019fair,hsu2020federated}
\citation{chang2019cronus}
\citation{zhu2019deep,geiping2020inverting}
\newlabel{sec:intro}{{}{1}{}{}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig1}{{1}{1}{ (a) Parameter-based methods recursively exchange model parameters between each local and server-side \citep  {mcmahan2017communication,li2018federated, karimireddy2019scaffold}, which is highly vulnerable to a security attack \citep  {zhu2019deep}. (b) Distillation-based methods utilize auxiliary task-dependent real data to conduct co-distillation between each local and the central server \citep  {li2019fedmd, gong2022preserving}. (c) Our FL method conducts one-way distillation from locals to the server with generated data, eliminating the prerequisite of additional data required by typical distillation, and the security weaknesses of white-box attacks caused by recursive parameter exchange. }{}{}}
\citation{li2019fedmd,lin2020ensemble,gong2022preserving}
\citation{zhu2021data,zhang2022fedzkt,zhang2022fine}
\citation{torkzadehmahani2019dp,chen2020gs}
\citation{zhu2021data,zhang2022fedzkt,zhang2022fine}
\citation{hinton2015distilling}
\citation{asif2019ensemble,xiang2020learning}
\citation{wu2019distilled}
\citation{fang2019data,chen2019data}
\citation{yin2020dreaming}
\citation{nayak2019zero}
\citation{mcmahan2017communication,hsu2019measuring,li2018federated}
\citation{jeong2018communication}
\citation{li2019fedmd,chang2019cronus,li2021practical}
\citation{lin2020ensemble,gong2022preserving}
\citation{zhu2021data,zhang2022fedzkt,zhang2022fine}
\citation{gong2022federated,li2021practical}
\newlabel{sec:related}{{}{2}{}{}{}}
\citation{goodfellow2020generative,radford2015unsupervised}
\newlabel{fig2}{{2}{3}{\textbf  {The overall pipeline of the proposed FedIOD.} We conduct distillation in input and output spaces to transfer knowledge from the locally trained task model $T_k$ and the auxiliary discriminator $D_k$ to the central task model $S$. \textbf  {Input distillation} optimizes central generator $G$ to generate transferred input on which local models (1) achieve consensus on its semantic clarity, (2) and simultaneously produce diverse predictions. The latter is to exploit each local's unique expertise under the heterogeneous FL setting. \textbf  {Output distillation} leverages per-input importance for output ensemble knowledge transfer. }{}{}}
\newlabel{eq:softmax}{{1}{3}{}{}{}}
\newlabel{eq:ganloss}{{2}{3}{}{}{}}
\citation{hinton2015distilling}
\citation{hinton2015distilling}
\newlabel{eq:confloss}{{3}{4}{}{}{}}
\newlabel{eq:jsd}{{4}{4}{}{}{}}
\newlabel{eq:uniqueloss}{{5}{4}{}{}{}}
\newlabel{eq:kl}{{6}{4}{}{}{}}
\newlabel{eq:bias}{{7}{4}{}{}{}}
\newlabel{alg}{{1}{4}{FedIOD}{}{}}
\newlabel{eq:aggweight}{{8}{4}{}{}{}}
\citation{mcmahan2017communication}
\citation{li2018federated}
\citation{hsu2019measuring}
\citation{zhu2021data}
\citation{lin2020ensemble}
\citation{li2019fedmd}
\citation{gong2022preserving}
\citation{lin2020ensemble,zhu2021data}
\citation{lin2020ensemble,zhu2021data}
\citation{hsu2019measuring}
\citation{lin2020ensemble}
\citation{menze2014multimodal,bakas2018identifying}
\citation{chang2020synthetic}
\citation{chang2020synthetic}
\citation{chang2020synthetic}
\citation{johnson2016perceptual}
\citation{chang2020synthetic}
\citation{chang2020synthetic}
\citation{chang2020synthetic}
\citation{kumar2017dataset}
\citation{vu2019methods}
\citation{naylor2018segmentation}
\citation{chang2020synthetic}
\citation{johnson2016perceptual}
\citation{chen2016dcan}
\citation{vu2019methods}
\citation{chang2020synthetic}
\citation{chang2020synthetic}
\newlabel{eq:agglogits}{{9}{5}{}{}{}}
\newlabel{eq:mimicloss}{{10}{5}{}{}{}}
\newlabel{eq:overallloss}{{11}{5}{}{}{}}
\newlabel{eq:aji}{{12}{5}{}{}{}}
\citation{chang2020synthetic,mcmahan2017communication}
\citation{torkzadehmahani2019dp}
\citation{chen2020gs}
\citation{papernot2018scalable}
\citation{gong2022preserving}
\newlabel{tab:cifarcompare}{{1}{6}{Accuracy (\%) comparisons on the CIFAR-10 and CIFAR-100 datasets with ResNet-8 and $K$=20. ``Standalone'' indicates the performance of local models trained with individual private data. Several popular FL methods are compared with parameter-based and distillation-based FL prior arts, among which \citep  {lin2020ensemble, zhu2021data} employ both parameter exchange and model output distillation.}{}{}}
\newlabel{tab:brats}{{2}{6}{Comparisons on the BraTS2018 dataset with $K$=10 under the same net with FedAvg and AsynDGAN. ``Centralized'': centralized training when all local data are collected together. }{}{}}
\newlabel{tab:nucleicrossorgan}{{3}{6}{Comparisons on the TCGA dataset with four cross-organ local nodes. All methods use the same segmentation net provided by \citep  {chang2020synthetic} for a fair comparison. }{}{}}
\newlabel{fig3}{{3}{6}{Qualitative comparisons on cross-site cross-organ nuclei segmentation tasks. The three rows visualize instance contours on test images from Cell17, TCGA, and TNBC.}{}{}}
\citation{zhu2019deep}
\citation{mcmahan2017communication}
\newlabel{tab:nucleicrosssite}{{4}{7}{Comparisons of cross-site cross-organ nuclei segmentation tasks with Cell17, TCGA, TNBC as distributed local data. For a fair comparison, all methods use the same U-Net architecture as the segmentation model and the same post-processing to convert the semantic prediction into instance segmentation results. }{}{}}
\newlabel{tab:privacy-utility}{{5}{7}{Compare FedIOD and FedKD in terms of accuracy (\%) on CIFAR10 ($K$=20, $\alpha $=1) under same privacy cost. }{}{}}
\newlabel{fig:fid}{{4}{7}{ Comparison of FID scores between FedIOD and FedAvg on (a) 9 randomly selected local clients; and (b) average score under CIFAR10 ($K$=20, $\alpha $=1) FL setting. A larger FID indicates a stronger privacy guarantee. }{}{}}
\bibdata{aaai24}
\citation{fang2021mosaicking}
\citation{lin2020ensemble}
\citation{szegedy2017inception}
\citation{deng2009imagenet}
\citation{lin2020ensemble,hsu2020federated}
\citation{hinton2015distilling}
\citation{lin2020ensemble,hsu2020federated}
\citation{hinton2015distilling}
\citation{lin2020ensemble,hsu2020federated}
\citation{lin2020ensemble,hsu2020federated}
\citation{chang2020synthetic}
\citation{mcmahan2017communication}
\citation{chang2020synthetic}
\citation{mcmahan2017communication}
\citation{menze2014multimodal,bakas2018identifying}
\citation{chang2020synthetic}
\citation{chang2020synthetic}
\citation{chang2020synthetic}
\citation{he2016deep}
\citation{isola2017image}
\citation{ronneberger2015u}
\citation{chang2020synthetic}
\citation{chang2020synthetic}
\citation{chang2020synthetic}
\citation{johnson2016perceptual}
\citation{mcmahan2017communication,chang2020synthetic}
\newlabel{tab:cifarablation}{{6}{11}{Ablation study on CIFAR-10 with ResNet-8, $K$=20 and $\alpha $=0.1. For the training of $L_\text  {gan}$, we compare our weighting scheme (Eq. 2) with the typical average ensemble. For the ensemble scheme of $L_\text  {mimic}$, we compare our per-sample, per-class importance weighting (Eq. 8) with $*$ which represents the weighting scheme commonly used in other FL methods \citep  {lin2020ensemble, hsu2020federated}. To compare $\tau $, we only list the result with a typical value $\tau $=1\nobreakspace  {}\citep  {hinton2015distilling}.}{}{}}
\newlabel{tab:cifars1}{{7}{11}{Ablation study on the fidelity of the generated data, with $K$=20 and $\alpha $=0.1 on CIFAR-10. For the ensemble scheme of $L_\text  {mimic}$, we compare our per-sample per-class importance weighting (Eq. 8) with the weighting scheme commonly used in other FL methods (represented with $*$) \citep  {lin2020ensemble, hsu2020federated}. We compare both typical inception score and adapted inception score evaluated by each locally trained model $T_k$ (taking average of all local models).}{}{}}
\newlabel{eq:hausdorff}{{13}{11}{}{}{}}
\citation{kumar2017dataset}
\citation{vu2019methods}
\citation{naylor2018segmentation}
\citation{vu2019methods}
\citation{kumar2017dataset}
\citation{naylor2018segmentation}
\citation{vu2019methods}
\citation{kumar2017dataset}
\citation{naylor2018segmentation}
\citation{chang2020synthetic}
\citation{chang2020synthetic}
\citation{chang2020synthetic}
\citation{he2016deep}
\citation{isola2017image}
\citation{ronneberger2015u}
\citation{chang2020synthetic}
\citation{chang2020synthetic}
\citation{johnson2016perceptual}
\citation{chang2020synthetic}
\newlabel{fig:s1}{{5}{12}{Visualization of testing results on BraTS2018 dataset with $K$=10. We compare ours with AsynDGAN\nobreakspace  {}\citep  {chang2020synthetic} and FedAvg\nobreakspace  {}\citep  {mcmahan2017communication}. We highlight the contours extracted from each method's segmentation prediction as well as the ground-truth. The zoomed part is shown at the left-bottom of each image and demonstrates that our method achieves much closer prediction to the ground-truth. }{}{}}
\newlabel{fig:s2}{{6}{12}{Visualization of synthetic data on cross-organ TCGA dataset with $K$=4. We compare ours with AsynDGAN\nobreakspace  {}\citep  {chang2020synthetic}. We zoom the instance region at the left-bottom of each image where our method succeeds to generate corresponding nuclei instances while the counterpart fails. }{}{}}
\citation{Guo_2021_CVPR,gong2022federated}
\citation{zbontar2018fastMRI}
\citation{hdtd-5j88-20}
\citation{ronneberger2015u}
\citation{Guo_2021_CVPR,gong2022federated}
\citation{gong2022federated}
\citation{zbontar2018fastMRI}
\citation{hdtd-5j88-20}
\citation{Guo_2021_CVPR}
\citation{gong2022federated}
\citation{mcmahan2017communication,Guo_2021_CVPR}
\citation{torkzadehmahani2019dp}
\citation{chen2020gs}
\citation{papernot2018scalable}
\citation{gong2022preserving}
\newlabel{tab:nucleis2}{{8}{13}{The performance of locally trained models under the cross-site cross-organ nuclei segmentation setting with Cell17\nobreakspace  {}\citep  {vu2019methods}, TCGA\nobreakspace  {}\citep  {kumar2017dataset}, TNBC\nobreakspace  {}\citep  {naylor2018segmentation} as distributed local data. }{}{}}
\newlabel{tab:mriin}{{9}{13}{Results on cross-domain MRI image reconstruction with fastMRI, BraTS2020, and IXI as locally held data (abbreviated as F, B, I respectively). We compare SSIM and PSNR with parameter-based methods, FedAvg and FL-MRCM, as well as distillation-based prior art FedAD. }{}{}}
\newlabel{fig:s3}{{7}{14}{Visualization of MRI image reconstruction with IXI, BraTS2020 and fastMRI as locally held data. We compare ours with two other FL methods: FedAvg and FL-MRCM. Each FL method trains with T1/T2- weighted IXI, BraTS2020, fastMRI as local data and tests on T1 IXI, T2 IXI, T1 BraTS2020, T2 BraTS2020, T1 fastMRI, T2 fastMRI test set respectively. The second column of each sub-figure is the error map (absolute difference) between the reconstructed images and the ground truth (GT). }{}{}}
\citation{chang2019cronus,zhu2019deep,geiping2020inverting}
\citation{zhu2019deep}
\citation{mcmahan2017communication}
\citation{mcmahan2017communication}
\citation{mcmahan2017communication}
\newlabel{fig:bandwidth}{{8}{15}{ Comparisons of communication cost for (a) CIFAR10 ($K$=20, $\alpha $=0.1) classification; and (b) BraTS2018 segmentation to reach certain performance. }{}{}}
\gdef \@abspage@last{15}
