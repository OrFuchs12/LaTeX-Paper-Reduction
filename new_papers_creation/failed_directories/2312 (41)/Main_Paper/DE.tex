\section{Dual Extrapolation (DE)}
Our novel POT rounding algorithm permits the development of Dual Extrapolation (DE) for POT. From our analysis, DE is a first-order and parallelizable algorithm that can approximate POT distance up to $\varepsilon$ accuracy with $\mathcal{\widetilde O}(1/\varepsilon)$ parallel depth and $\mathcal{\widetilde O}(n^2/\varepsilon)$ total work. 
\subsection{Setup}
For each feasible $\vx$, we have $\norm{\vx}_1 = \norm{\vX}_1 + \norm{\vp}_1 + \norm{\vq}_1 = \norm{\vr}_1 + \norm{\vc}_1 - s$. We can normalize $\vx = \vx / (\norm{\vr}_1 + \norm{\vc}_1 -s), \: \vb = \vb / (\norm{\vr}_1 + \norm{\vc}_1 -s)$. These imply $\vx \in \Delta_{n^2+2n}$. The POT problem formulation \eqref{main_problem} is now updated as
\begin{equation} \label{POT_areaconvex}
    \min_{\vx \in \Delta_{n^2+2n}} \:  \inner{\vd}{\vx} ~~ \text{s.t.} ~~ \vA\vx = \vb,
\end{equation}
We then consider the $\ell_1$ penalization for the problem (\ref{POT_areaconvex}) and show that it has equal optimal value and $\varepsilon$-approximate minimizer to those of the POT formulation (\ref{POT_areaconvex}) (more details in $\ell_1$ Penalization subsection in Appendix). Through a primal-dual point of view, the $\ell_1$ penalized objective \eqref{l1_penalized} can be rewritten as
\begin{equation} \label{bilinear_objective}
    \min_{\vx \in \mathcal{X}} \max_{\vy \in \mathcal{Y}} \: F(\vx,\vy) \defeq \vd^\top \vx + 23 \norm{\vd}_\infty \left( \vy^\top \vA \vx - \vy^\top \vb \right), 
\end{equation}
with $\mathcal{X} = \Delta_{n^2+2n}, \mathcal{Y} = [-1,1]^{2n+1}$. Note that the term $23$ comes from the guarantees of  $\textsc{Round-POT}$ (Theorem \ref{prop:rounding}, Lemma \ref{lem:l1_regression}). Let $\mathcal{Z} = \mathcal{X} \times \mathcal{Y}$ such that $\vx \in \mathcal{X}$ and $\vy \in \mathcal{Y}$. For a bilinear objective $F(\vx,\vy)$ that is convex in $\vx$ and concave in $\vy$, it is natural to define the gradient operator $g(\vx,\vy) = (\nabla_{\vx} F(\vx,\vy),- \nabla_{\vy} F(\vx,\vy))$. Specifically for the objective \eqref{bilinear_objective}, we have $g(\vx,\vy) = (\vd + 23 \norm{\vd}_\infty \vA^\top \vy, -23 \norm{\vd}_\infty (\vA \vx -\vb)).$  This minimax objective can be solved with the dual extrapolation framework \citep{nesterov_2006}, which requires strongly convex regularizers. This setup can be relaxed with the notion of area-convexity \citep[Definition 1.2]{Sherman-2017-Area}, in the following definition.
\begin{definition}
    \label{defn:area_convex}
    (Area Convexity) A regularizer $r$ is $\kappa$-area-convex w.r.t an operator g if for any $x_1, x_2, x_3$ in its domain,
    \begin{align*}
        &\kappa \sum_{i}^3 r(x_i) - 3\kappa r \left(\frac{\sum_{i}^3 x_i}{3}\right) \geq \inner{g(x_2)- g(x_1)}{x_2-x_3}.
    \end{align*}
\end{definition}
The regularizer chosen for this framework is the Sherman regularizer, introduced in \citep{Sherman-2017-Area}
\begin{equation} \label{regularizer}
    r(\vx,\vy) = 2 \norm{\vd}_\infty \left( 10 \inner{\vx}{\log{\vx}} + \vx^\top \vA^\top (\vy^2) \right),
\end{equation}
in which $\vy^2$ is entry-wise. While this regularizer has a similar form to that in \citep{Jambulapati-2019-Direct}, the POT formulation leads to a different structure of $\vA$. For instance, $\norm{\vA}_1 = 3$ instead of $2$. This following lemma shows that chosen $r$ is 9-area-convex (its proof is in the Proof of Lemma \ref{9_area_convex} section in the Appendix).
\begin{lemma} \label{9_area_convex}
    $r$ is 9-area-convex with respect to the gradient operator $g$, i.e., $\kappa = 9$.
\end{lemma}

\subsection{Algorithmic Development}

\begin{algorithm}[t]
    \caption{Dual Extrapolation for POT}
    \label{alg:Dual_Extrapolation_POT}
    \begin{algorithmic}[1]
    \REQUIRE linearized cost $\vd$; linear operator $\vA$; constraints $\vb$; area-convexity coefficient $\kappa$; initial states \(\vs^0_{\vx} = \zeros_{n^2+2n}, \vs^0_{\vy} = \zeros_{2n+1}\); iterations M (Theorem \ref{Complexity_for_AM})
    \STATE \(\nabla_\vx r(\bar{\vz}) = 20 \norm{
    \vd}_\infty (1 - \log(n^2+2n)) \ones_{n^2 +2n}\)
    \STATE \(\nabla_\vy r(\bar\vz) = \zeros_{2n+1}\)
    \FOR{$t=0,1,2,\ldots,T-1$}
        \STATE \(\vv = \vs^t_{\vx} - \nabla_\vx r(\bar{\vz})\)
         \STATE \(\vu =  \vs^t_{\vy} - \nabla_\vy r(\bar{\vz})\)
        \STATE \( (\vz_\vx^t, \vz_\vy^t) = \mathtt{AM}(M, \vv, \vu)\) 
        \STATE \(\vv = \vv + (\vd + 23 \norm{\vd}_\infty \vA^\top \vz^t_{\vy}) / \kappa\) 
        \STATE \(\vu = \vu - 23 \norm{\vd}_\infty ( \boldsymbol A \vz_{\vx}^t - \vb) / \kappa\) 
        \STATE \( (\vw_{\vx}^t, \vw_{\vy}^t) = \mathtt{AM}(M, \vv, \vu)\)
        \STATE \(\vs^{t+1}_\vx = \vs^t_\vx + (\vd + 23 \norm{\vd}_\infty \vA^\top \vw_{\vy}^t) / (2 \kappa) \)
         \STATE \(\vs^{t+1}_{\vy} = \vs^{t}_{\vy} - 23 \norm{\vd}_\infty( \boldsymbol A \vw_{\vx}^t -\vb) / (2 \kappa) \)
    \ENDFOR
    \ENSURE 
        $ \bar{\vw}_{\vx} = \sum_{t=0}^{T-1} \vw_{\vx}^{t} / T$, $\bar{\vw}_{\vy} = \sum_{t=0}^{T-1} \vw_{\vy}^{t} / T$.
    \end{algorithmic}
\end{algorithm}

The main motivation is the DE Algorithm \ref{alg:General_Dual_Extrapolation} in Appendix, proposed by \citep{nesterov_2006}. This general DE framework essentially has two proximal steps per iteration, while maintaining a state $\vs$ in the dual space.
% (more details are included in General Dual Extrapolation subsection in Appendix). 
We follow \citep{Jambulapati-2019-Direct} and update $\vs$ with $1/2 \kappa$ rather than $1/ \kappa$ \citep{nesterov_2006}. The proximal steps (steps 2 and 3 in Algorithm \ref{alg:General_Dual_Extrapolation}) needs to minimize: 
\begin{equation} \label{General_AM}
    P(\vx, \vy) \defeq \inner{\vv}{\vx} + \inner{\vu}{\vy} + r(\vx,\vy).
\end{equation}
This can be solved efficiently with an Alternating Minimization ($\mathtt{AM}$) approach \citep{Jambulapati-2019-Direct}. Details for $\mathtt{AM}$ are included in the Appendix.  
Combining both algorithms, we have the DE for POT Algorithm \ref{alg:Dual_Extrapolation_POT}, where each of the two proximal steps is solved by the $\mathtt{AM}$ subroutine.
% Algorithm \ref{alg:General_Dual_Extrapolation} is solved by Algorithm \ref{alg:alternating_minimization}. 

\subsection{Computational Complexities}
Firstly, we bound the regularizer $r$ to satisfy the convergence guarantees \citep{Jambulapati-2019-Direct} in Proof of Lemma \ref{T} subsection in Appendix. In that same subsection, we also derive the required number of iterations $T$ in DE with respect to $\Theta$, the range of the regularizer. Next, we have this essential lemma that bounds the number of iterations to evaluate a proximal step.
\begin{theorem} \label{Complexity_for_AM}
    (Complexity of $\mathtt{AM}$) For $T = \lceil 36 \Theta/\varepsilon \rceil$ iterations of DE, $\mathtt{AM}$ Algorithm \ref{alg:alternating_minimization} obtains additive error $\varepsilon / 2$ in 
    \begin{equation*}
        M = 24 \log \left( \left( 840 \norm{\vd}_\infty / \varepsilon^2 + 6 / \varepsilon \right) \Theta + 1336 \norm{\vd}_\infty / 9 \right)
    \end{equation*}
    iterations. This is done in wall-clock time $\mathcal{O}(n^2 \log \eta)$ with $\eta = \log n \norm{\vd}_\infty / \varepsilon$.
\end{theorem}
The proof of this theorem can be found in Proof of Theorem \ref{Complexity_for_AM} subsection in Appendix. The main proof idea is to  bound the number of iterations required to solve the proximal steps. This explicit bound for the number of $\mathtt{AM}$ iterations is novel as in DE for OT \cite{Jambulapati-2019-Direct}, the authors runs a while loop and do not analyze its final number of iterations. We can now calculate the computational complexity of the DE algorithm. The proof is in Proof of Theorem \ref{theorem:DE_complexity} subsection in Appendix.
\begin{theorem} (Complexity of DE)
    \label{theorem:DE_complexity}
    In $\mathcal{\widetilde O}(n^2 \norm{\vC}_{max} / \varepsilon)$ wall-clock time, the DE Algorithm \ref{alg:Dual_Extrapolation_POT} returns $(\bar{\vw}_{\vx}, \bar{\vw}_{\vy}) \in \mathcal{Z}$ so that the duality gap \eqref{duality_gap} is less than $\varepsilon$.
\end{theorem}

% \begin{remark}
%     This result improves the current best rate for approximating POT using Sinkhorn \citep{nhatho-mmpot}. Our contributions include the explicit bounds for the number of iterations $M$ for Alternating Minimization (Theorem \ref{Complexity_for_AM}) and Dual Extrapolation (Theorem \ref{theorem:DE_complexity}).
% \end{remark}