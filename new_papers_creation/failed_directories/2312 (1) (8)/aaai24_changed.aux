\relax 
\bibstyle{aaai24}
\citation{h8}
\citation{2018cell}
\citation{2016animal}
\citation{2016cars}
\citation{2022BMNet,2021FAMNet}
\citation{2022RepRPN}
\citation{2023zsc}
\citation{nag2022zero}
\citation{2021clip}
\citation{2022maskclip,li2023clipsurgery}
\citation{2021FAMNet}
\citation{2017drone}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:intro}{{1}{1}{ Comparison between two-stage pipeline and one-stage pipeline\nobreakspace  {}(ours). The two-stage pipeline requires training the exemplar discoverer\nobreakspace  {}(orange) before the counter\nobreakspace  {}(blue), along with the need for an extra training dataset to optimize the discoverer. In contrast, our one-stage pipeline is designed to be simpler and does not necessitate any additional data or training stage. }{}{}}
\citation{h0,h1,h2,h3}
\citation{2016cars,2017drone}
\citation{2016animal}
\citation{2018cell}
\citation{d0,2017drone,d1}
\citation{r0,r1,r2,r3}
\citation{2019GMN}
\citation{2019GMN}
\citation{2021CFOCNet}
\citation{2022BMNet}
\citation{2021FAMNet}
\citation{gong2022class}
\citation{2022RepRPN}
\citation{2023zsc}
\citation{wang2022learning,gu2021ppt,2020lp,2022vpt}
\citation{li2021prefix,2022vpt}
\citation{2022maskclip,li2023clipsurgery}
\citation{li2022languagedriven,rao2022denseclip,luddecke2022image}
\newlabel{Related:VPT}{{2.2}{2}{}{}{}}
\newlabel{Method:Formulation}{{3.1}{2}{}{}{}}
\newlabel{Method:preliminaries}{{3.2}{2}{}{}{}}
\citation{2022maskclip,li2023clipsurgery}
\citation{2022vpt}
\newlabel{fig:VLCounter_overview}{{2}{3}{ Overview of VLBase and VLCounter: each without and with colored components. The end-to-end baseline, VLBase, employs CLIP encoders to extract both image and text embeddings. Then, the decoder processes the image-text similarity map along with visual embeddings to count the number of specified objects. With three colored modules, VLCounter leverages the generalization capability of VLBase to be tailored for object counting. }{}{}}
\newlabel{Method:VLBase}{{4.1}{3}{}{}{}}
\newlabel{eq3}{{3}{3}{}{}{}}
\newlabel{eq3countingloss}{{4}{3}{}{}{}}
\newlabel{fig:SPT}{{3}{3}{ Illustration for Semantic-conditioned Prompt Tuning\nobreakspace  {}(SPT). In addition to learnable visual prompts\nobreakspace  {}(orange) in the image encoder, text features\nobreakspace  {}(yellow) are integrated to specify the desired semantics. }{}{}}
\newlabel{Method:SPT}{{4.2}{3}{}{}{}}
\citation{2022vpt}
\citation{2019GMN}
\citation{2021FAMNet}
\citation{2022BMNet}
\citation{2022BMNet}
\citation{2022RepRPN}
\citation{2023zsc}
\citation{hoffmann2022ranking,moon2023query}
\newlabel{Method:LAT}{{4.3}{4}{}{}{}}
\newlabel{Method:SaSC}{{4.4}{4}{}{}{}}
\newlabel{fig:SaSC}{{4}{4}{ The flow of Semantic-aware Skip Connection\nobreakspace  {}(SaSC) and architecture of feature projection block. Intermediate visual features are projected and filtered with an object-aware counting map $\hat  {S}$ to produce object-relevant encoder features. Consequently, these are integrated into its counterpart in the decoder. }{}{}}
\newlabel{Method:Loss}{{4.5}{4}{}{}{}}
\citation{2017adamw}
\citation{2021FAMNet}
\citation{2017drone}
\newlabel{tab:main}{{1}{5}{ Quantitative comparison to state-of-the-art approaches on the FSC147 dataset. $\alpha $ in the rightmost column indicates an additional cost necessary for the exemplar discovery process in the context of the two-stage pipeline. }{}{}}
\newlabel{tab:CARPK}{{2}{5}{Cross-dataset validation performance on the CARPK and PUCPR+ dataset.}{}{}}
\newlabel{eq7contrastiveloss}{{9}{5}{}{}{}}
\newlabel{tab:ablation}{{3}{5}{ Ablation study on each component of VLCounter. }{}{}}
\citation{2021FAMNet,2022BMNet}
\citation{li2023clipsurgery}
\citation{2021clip}
\newlabel{tab:ablation_component}{{4}{6}{Analysis of semantic-conditioning techniques in SPT and SaSC.}{}{}}
\newlabel{tab:ablation_plural}{{5}{6}{Analysis of pluralized context to prompt the class names.}{}{}}
\newlabel{fig:qualitative_results}{{5}{7}{ Qualitative comparison of VLBase and VLCounter on the FSC-147\nobreakspace  {}(Top 4 rows) and CARPK\nobreakspace  {}(Bottom 2 rows). Class names and counting values are shown at the right top of the query image\nobreakspace  {}($I$) and the predicted density map, respectively. }{}{}}
\bibdata{aaai24}
\newlabel{tab:depth}{{6}{17}{Effect of the depth of learnable tokens}{}{}}
\newlabel{fig:num}{{6}{17}{ Effect of the number of learnable tokens. }{}{}}
\newlabel{fig:LAT}{{7}{17}{ (a) displays a distribution diagram of the values in $W$, while (b) illustrates the distribution of values in $B$, both of which are employed for the affine transformation of LAT. }{}{}}
\citation{li2023clipsurgery}
\citation{jiang2023clip}
\newlabel{fig:sasc}{{8}{18}{ Effect of the combinations of encoder layers. }{}{}}
\newlabel{tab:clipcount_fsc}{{7}{18}{Comparision with CLIP-Count on FSC147 dataset}{}{}}
\newlabel{tab:clipcount}{{8}{19}{Comparision with CLIP-Count in the number of learnable parameters, MACs, and performance on diverse datasets}{}{}}
\newlabel{fig:qualitative_results_suppl}{{9}{19}{ Qualitative comparison of VLBase and VLCounter on the FSC-147. Class names and counting values are shown at the right top of the query image\nobreakspace  {}($I$) and the predicted density map, respectively. }{}{}}
\gdef \@abspage@last{19}
