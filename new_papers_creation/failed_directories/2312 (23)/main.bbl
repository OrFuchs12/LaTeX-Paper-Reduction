\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Behnke and Heafield(2020)}]{behnke-heafield-2020-losing}
Behnke, M.; and Heafield, K. 2020.
\newblock Losing Heads in the Lottery: Pruning Transformer Attention in Neural
  Machine Translation.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, 2664--2674. Online: Association for
  Computational Linguistics.

\bibitem[{Behnke and
  Heafield(2021{\natexlab{a}})}]{behnke-heafield-2021-pruning}
Behnke, M.; and Heafield, K. 2021{\natexlab{a}}.
\newblock Pruning Neural Machine Translation for Speed Using Group Lasso.
\newblock In \emph{Proceedings of the Sixth Conference on Machine Translation},
  1074--1086. Online: Association for Computational Linguistics.

\bibitem[{Behnke and Heafield(2021{\natexlab{b}})}]{behnke2021pruning}
Behnke, M.; and Heafield, K. 2021{\natexlab{b}}.
\newblock Pruning neural machine translation for speed using group lasso.
\newblock In \emph{Proceedings of the sixth conference on machine translation},
  1074--1086.

\bibitem[{Bian et~al.(2021)Bian, Huang, Cai, Yuan, and
  Church}]{bian-etal-2021-attention}
Bian, Y.; Huang, J.; Cai, X.; Yuan, J.; and Church, K. 2021.
\newblock On Attention Redundancy: A Comprehensive Study.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, 930--945. Online: Association for Computational Linguistics.

\bibitem[{Black et~al.(2021)Black, Leo, Wang, Leahy, and Biderman}]{gpt-neo}
Black, S.; Leo, G.; Wang, P.; Leahy, C.; and Biderman, S. 2021.
\newblock {GPT-Neo: Large Scale Autoregressive Language Modeling with
  Mesh-Tensorflow}.
\newblock {If you use this software, please cite it using these metadata.}

\bibitem[{Blodgett et~al.(2021)Blodgett, Lopez, Olteanu, Sim, and
  Wallach}]{blodgett2021stereotyping}
Blodgett, S.~L.; Lopez, G.; Olteanu, A.; Sim, R.; and Wallach, H. 2021.
\newblock Stereotyping Norwegian salmon: An inventory of pitfalls in fairness
  benchmark datasets.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, 1004--1015.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.~D.; Dhariwal, P.;
  Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:
  1877--1901.

\bibitem[{Caliskan, Bryson, and Narayanan(2017)}]{caliskan2017semantics}
Caliskan, A.; Bryson, J.~J.; and Narayanan, A. 2017.
\newblock Semantics Derived Automatically from Language Corpora Contain
  Human-Like Biases.
\newblock \emph{Science}, 356(6334): 183--186.

\bibitem[{Cao et~al.(2022)Cao, Pruksachatkun, Chang, Gupta, Kumar, Dhamala, and
  Galstyan}]{cao-etal-2022-intrinsic}
Cao, Y.~T.; Pruksachatkun, Y.; Chang, K.-W.; Gupta, R.; Kumar, V.; Dhamala, J.;
  and Galstyan, A. 2022.
\newblock On the Intrinsic and Extrinsic Fairness Evaluation Metrics for
  Contextualized Language Representations.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, 561--570. Dublin,
  Ireland: Association for Computational Linguistics.

\bibitem[{Cohen et~al.(2022)Cohen, Roberts, Molina, Butryna, Jin, Kulshreshtha,
  Hutchinson, Zevenbergen, Aguera-Arcas, Chang et~al.}]{cohen2022lamda}
Cohen, A.~D.; Roberts, A.; Molina, A.; Butryna, A.; Jin, A.; Kulshreshtha, A.;
  Hutchinson, B.; Zevenbergen, B.; Aguera-Arcas, B.~H.; Chang, C.-c.; et~al.
  2022.
\newblock LaMDA: Language models for dialog applications.

\bibitem[{Delobelle et~al.(2022)Delobelle, Tokpo, Calders, and
  Berendt}]{delobelle-etal-2022-measuring}
Delobelle, P.; Tokpo, E.; Calders, T.; and Berendt, B. 2022.
\newblock Measuring Fairness with Biased Rulers: A Comparative Study on Bias
  Metrics for Pre-trained Language Models.
\newblock In \emph{Proceedings of the 2022 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, 1693--1706. Seattle, United States: Association for
  Computational Linguistics.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{devlin2018bert}
Devlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.
\newblock {BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding}.
\newblock In \emph{NAACL}, 4171--4186.

\bibitem[{Dhamala et~al.(2021)Dhamala, Sun, Kumar, Krishna, Pruksachatkun,
  Chang, and Gupta}]{dhamala2021bold}
Dhamala, J.; Sun, T.; Kumar, V.; Krishna, S.; Pruksachatkun, Y.; Chang, K.-W.;
  and Gupta, R. 2021.
\newblock Bold: Dataset and metrics for measuring biases in open-ended language
  generation.
\newblock In \emph{Proceedings of the 2021 ACM conference on fairness,
  accountability, and transparency}, 862--872.

\bibitem[{Dixon et~al.(2018)Dixon, Li, Sorensen, Thain, and
  Vasserman}]{dixon2018measuring}
Dixon, L.; Li, J.; Sorensen, J.; Thain, N.; and Vasserman, L. 2018.
\newblock Measuring and mitigating unintended bias in text classification.
\newblock In \emph{Conference on AI, Ethics, and Society}.

\bibitem[{Fan, Grave, and Joulin(2020)}]{Fan2020Reducing}
Fan, A.; Grave, E.; and Joulin, A. 2020.
\newblock Reducing Transformer Depth on Demand with Structured Dropout.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Fan et~al.(2021)Fan, Li, Zhang, Ao, Wu, Meng, and
  Sun}]{fan-etal-2021-layer}
Fan, C.; Li, J.; Zhang, T.; Ao, X.; Wu, F.; Meng, Y.; and Sun, X. 2021.
\newblock Layer-wise Model Pruning based on Mutual Information.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, 3079--3090. Online and Punta Cana, Dominican
  Republic: Association for Computational Linguistics.

\bibitem[{Frankle and Carbin(2019)}]{frankle2018the}
Frankle, J.; and Carbin, M. 2019.
\newblock The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural
  Networks.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Guo and Caliskan(2021)}]{guo2021detecting}
Guo, W.; and Caliskan, A. 2021.
\newblock Detecting emergent intersectional biases: Contextualized word
  embeddings contain a distribution of human-like biases.
\newblock In \emph{Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics,
  and Society}, 122--133.

\bibitem[{Han, Mao, and Dally(2015)}]{han2015deep}
Han, S.; Mao, H.; and Dally, W.~J. 2015.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}.

\bibitem[{Han et~al.(2015)Han, Pool, Tran, and Dally}]{han2015learning}
Han, S.; Pool, J.; Tran, J.; and Dally, W. 2015.
\newblock Learning both weights and connections for efficient neural network.
\newblock \emph{Advances in neural information processing systems}, 28.

\bibitem[{He and Choi(2021)}]{he-choi-2021-stem}
He, H.; and Choi, J.~D. 2021.
\newblock The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with
  Transformer Encoders.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, 5555--5577. Online and Punta Cana, Dominican
  Republic: Association for Computational Linguistics.

\bibitem[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training}
Hoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.; Cai, T.; Rutherford,
  E.; Casas, D. d.~L.; Hendricks, L.~A.; Welbl, J.; Clark, A.; et~al. 2022.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}.

\bibitem[{Kurita et~al.(2019)Kurita, Vyas, Pareek, Black, and
  Tsvetkov}]{kurita2019measuring}
Kurita, K.; Vyas, N.; Pareek, A.; Black, A.~W.; and Tsvetkov, Y. 2019.
\newblock Measuring Bias in Contextualized Word Representations.
\newblock In \emph{Proceedings of the First Workshop on Gender Bias in Natural
  Language Processing}, 166--172.

\bibitem[{Levy, Lazar, and Stanovsky(2021)}]{levy2021collecting}
Levy, S.; Lazar, K.; and Stanovsky, G. 2021.
\newblock Collecting a Large-Scale Gender Bias Dataset for Coreference
  Resolution and Machine Translation.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2021}, 2470--2480.

\bibitem[{Li et~al.(2020{\natexlab{a}})Li, Feng, Meng, Han, Wu, and
  Li}]{li2019unified}
Li, X.; Feng, J.; Meng, Y.; Han, Q.; Wu, F.; and Li, J. 2020{\natexlab{a}}.
\newblock A Unified {MRC} Framework for Named Entity Recognition.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, 5849--5859. Online: Association for
  Computational Linguistics.

\bibitem[{Li et~al.(2020{\natexlab{b}})Li, Sun, Meng, Liang, Wu, and
  Li}]{li2019dice}
Li, X.; Sun, X.; Meng, Y.; Liang, J.; Wu, F.; and Li, J. 2020{\natexlab{b}}.
\newblock Dice Loss for Data-imbalanced {NLP} Tasks.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, 465--476. Online: Association for
  Computational Linguistics.

\bibitem[{Lieber et~al.(2021)Lieber, Sharir, Lenz, and
  Shoham}]{lieber2021jurassic}
Lieber, O.; Sharir, O.; Lenz, B.; and Shoham, Y. 2021.
\newblock Jumainrassic-1: Technical details and evaluation.

\bibitem[{Liu et~al.(2022)Liu, Liu, Radev, and Neubig}]{liu2022brio}
Liu, Y.; Liu, P.; Radev, D.; and Neubig, G. 2022.
\newblock {BRIO}: Bringing Order to Abstractive Summarization.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, 2890--2903. Dublin,
  Ireland: Association for Computational Linguistics.

\bibitem[{May et~al.(2019)May, Wang, Bordia, Bowman, and
  Rudinger}]{may2019measuring}
May, C.; Wang, A.; Bordia, S.; Bowman, S.~R.; and Rudinger, R. 2019.
\newblock On Measuring Social Biases in Sentence Encoders.
\newblock In \emph{Conference of the North {A}merican Chapter of the
  Association for Computational Linguistics}.

\bibitem[{Meade, Poole-Dayan, and Reddy(2022)}]{meade2021empirical}
Meade, N.; Poole-Dayan, E.; and Reddy, S. 2022.
\newblock An Empirical Survey of the Effectiveness of Debiasing Techniques for
  Pre-trained Language Models.
\newblock In \emph{{Annual} {Meeting} of the {Association} for {Computational}
  {Linguistics}}.

\bibitem[{Merity et~al.(2017)Merity, Xiong, Bradbury, and
  Socher}]{meritypointer}
Merity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2017.
\newblock Pointer Sentinel Mixture Models.
\newblock In \emph{ICLR}.

\bibitem[{Michel, Levy, and Neubig(2019)}]{NEURIPS2019_2c601ad9}
Michel, P.; Levy, O.; and Neubig, G. 2019.
\newblock Are Sixteen Heads Really Better than One?
\newblock In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d\textquotesingle
  Alch\'{e}-Buc, F.; Fox, E.; and Garnett, R., eds., \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc.

\bibitem[{Nadeem, Bethke, and Reddy(2021)}]{nadeem2021stereoset}
Nadeem, M.; Bethke, A.; and Reddy, S. 2021.
\newblock StereoSet: Measuring stereotypical bias in pretrained language
  models.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, 5356--5371.

\bibitem[{Nangia et~al.(2020)Nangia, Vania, Bhalerao, and
  Bowman}]{nangia2020crows}
Nangia, N.; Vania, C.; Bhalerao, R.; and Bowman, S. 2020.
\newblock CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in
  Masked Language Models.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, 1953--1967.

\bibitem[{Narang et~al.(2017)Narang, Diamos, Sengupta, and
  Elsen}]{narang2017exploring}
Narang, S.; Diamos, G.; Sengupta, S.; and Elsen, E. 2017.
\newblock Exploring Sparsity in Recurrent Neural Networks.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Prasanna, Rogers, and Rumshisky(2020)}]{prasanna-etal-2020-bert}
Prasanna, S.; Rogers, A.; and Rumshisky, A. 2020.
\newblock {W}hen {BERT} {P}lays the {L}ottery, {A}ll {T}ickets {A}re {W}inning.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, 3208--3229. Online: Association for
  Computational Linguistics.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever}]{radford2019language}
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019.
\newblock {Language Models are Unsupervised Multitask Learners}.
\newblock \emph{OpenAI Blog}, 1(8): 9.

\bibitem[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young et~al.}]{rae2021scaling}
Rae, J.~W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoffmann, J.; Song, F.;
  Aslanides, J.; Henderson, S.; Ring, R.; Young, S.; et~al. 2021.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}.

\bibitem[{Rajpurkar, Jia, and Liang(2018)}]{rajpurkar2018know}
Rajpurkar, P.; Jia, R.; and Liang, P. 2018.
\newblock Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, 784--789. Melbourne,
  Australia: Association for Computational Linguistics.

\bibitem[{Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang}]{rajpurkar2016squad}
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.
\newblock {SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}.

\bibitem[{Rotman, Feder, and Reichart(2021)}]{rotman2021model}
Rotman, G.; Feder, A.; and Reichart, R. 2021.
\newblock Model compression for domain adaptation through causal effect
  estimation.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9: 1355--1373.

\bibitem[{Rudinger et~al.(2018)Rudinger, Naradowsky, Leonard, and
  Van~Durme}]{rudinger2018gender}
Rudinger, R.; Naradowsky, J.; Leonard, B.; and Van~Durme, B. 2018.
\newblock Gender Bias in Coreference Resolution.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 2 (Short Papers)}, 8--14.

\bibitem[{Sajjad et~al.(2023)Sajjad, Dalvi, Durrani, and
  Nakov}]{sajjad2023effect}
Sajjad, H.; Dalvi, F.; Durrani, N.; and Nakov, P. 2023.
\newblock On the effect of dropping layers of pre-trained transformer models.
\newblock \emph{Computer Speech \& Language}, 77: 101429.

\bibitem[{Smith et~al.(2022{\natexlab{a}})Smith, Hall, Kambadur, Presani, and
  Williams}]{smith2022m}
Smith, E.~M.; Hall, M.; Kambadur, M.; Presani, E.; and Williams, A.
  2022{\natexlab{a}}.
\newblock “I’m sorry to hear that”: Finding New Biases in Language Models
  with a Holistic Descriptor Dataset.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, 9180--9211.

\bibitem[{Smith et~al.(2022{\natexlab{b}})Smith, Patwary, Norick, LeGresley,
  Rajbhandari, Casper, Liu, Prabhumoye, Zerveas, Korthikanti
  et~al.}]{smith2022using}
Smith, S.; Patwary, M.; Norick, B.; LeGresley, P.; Rajbhandari, S.; Casper, J.;
  Liu, Z.; Prabhumoye, S.; Zerveas, G.; Korthikanti, V.; et~al.
  2022{\natexlab{b}}.
\newblock Using deepspeed and megatron to train megatron-turing nlg 530b, a
  large-scale generative language model.
\newblock \emph{arXiv preprint arXiv:2201.11990}.

\bibitem[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi,
  Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama}
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.;
  Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et~al. 2023.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}.

\bibitem[{Voita et~al.(2019)Voita, Talbot, Moiseev, Sennrich, and
  Titov}]{voita-etal-2019-analyzing}
Voita, E.; Talbot, D.; Moiseev, F.; Sennrich, R.; and Titov, I. 2019.
\newblock Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy
  Lifting, the Rest Can Be Pruned.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, 5797--5808. Florence, Italy: Association for
  Computational Linguistics.

\bibitem[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman}]{wang2018glue}
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. 2018.
\newblock {GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural
  Language Understanding.
\newblock In \emph{{EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting
  Neural Networks for {NLP}}.

\bibitem[{Wang and Komatsuzaki(2021)}]{gpt-j}
Wang, B.; and Komatsuzaki, A. 2021.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}.

\bibitem[{Wang et~al.(2022)Wang, Variengien, Conmy, Shlegeris, and
  Steinhardt}]{wang2022interpretability}
Wang, K.~R.; Variengien, A.; Conmy, A.; Shlegeris, B.; and Steinhardt, J. 2022.
\newblock Interpretability in the Wild: a Circuit for Indirect Object
  Identification in GPT-2 small.
\newblock In \emph{NeurIPS ML Safety Workshop}.

\bibitem[{Webster et~al.(2020)Webster, Wang, Tenney, Beutel, Pitler, Pavlick,
  Chen, Chi, and Petrov}]{webster2020measuring}
Webster, K.; Wang, X.; Tenney, I.; Beutel, A.; Pitler, E.; Pavlick, E.; Chen,
  J.; Chi, E.; and Petrov, S. 2020.
\newblock Measuring and reducing gendered correlations in pre-trained models.
\newblock \emph{arXiv preprint arXiv:2010.06032}.

\bibitem[{Yu, Bohnet, and Poesio(2020)}]{yu-etal-2020-named}
Yu, J.; Bohnet, B.; and Poesio, M. 2020.
\newblock Named Entity Recognition as Dependency Parsing.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, 6470--6476. Online: Association for
  Computational Linguistics.

\bibitem[{Zayed et~al.(2023{\natexlab{a}})Zayed, Mordido, Shabanian, and
  Chandar}]{zayed2023should}
Zayed, A.; Mordido, G.; Shabanian, S.; and Chandar, S. 2023{\natexlab{a}}.
\newblock Should We Attend More or Less? Modulating Attention for Fairness.
\newblock \emph{arXiv preprint arXiv:2305.13088}.

\bibitem[{Zayed et~al.(2023{\natexlab{b}})Zayed, Parthasarathi, Mordido,
  Palangi, Shabanian, and Chandar}]{zayed2022deep}
Zayed, A.; Parthasarathi, P.; Mordido, G.; Palangi, H.; Shabanian, S.; and
  Chandar, S. 2023{\natexlab{b}}.
\newblock Deep Learning on a Healthy Data Diet: Finding Important Examples for
  Fairness.
\newblock In \emph{AAAI Conference on Artificial Intelligence}.

\bibitem[{Zhang et~al.(2021)Zhang, Huang, Feng, and
  Cao}]{zhang-etal-2021-enlivening}
Zhang, T.; Huang, H.; Feng, C.; and Cao, L. 2021.
\newblock Enlivening Redundant Heads in Multi-head Self-attention for Machine
  Translation.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, 3238--3248. Online and Punta Cana, Dominican
  Republic: Association for Computational Linguistics.

\bibitem[{Zhang, Zhou, and Li(2020)}]{ijcai2020p560}
Zhang, Y.; Zhou, H.; and Li, Z. 2020.
\newblock Fast and Accurate Neural CRF Constituency Parsing.
\newblock In Bessiere, C., ed., \emph{Proceedings of the Twenty-Ninth
  International Joint Conference on Artificial Intelligence, {IJCAI-20}},
  4046--4053. International Joint Conferences on Artificial Intelligence
  Organization.
\newblock Main track.

\bibitem[{Zhao et~al.(2018)Zhao, Wang, Yatskar, Ordonez, and
  Chang}]{zhao-etal-2018-gender}
Zhao, J.; Wang, T.; Yatskar, M.; Ordonez, V.; and Chang, K.-W. 2018.
\newblock Gender Bias in Coreference Resolution: Evaluation and Debiasing
  Methods.
\newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 2 (Short Papers)}, 15--20. New Orleans, Louisiana:
  Association for Computational Linguistics.

\bibitem[{Zhu and Gupta(2018)}]{h.2018to}
Zhu, M.~H.; and Gupta, S. 2018.
\newblock To Prune, or Not to Prune: Exploring the Efficacy of Pruning for
  Model Compression.

\end{thebibliography}
