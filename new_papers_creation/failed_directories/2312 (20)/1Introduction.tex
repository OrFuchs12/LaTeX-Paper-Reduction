
\section{Introduction}
Joint decision-making in multi-agent reinforcement learning (MARL) often requires dealing with 
an exponential expansion of the joint state-action space as the number of agents increases. To this end, recent MARL algorithms~\cite{VDN,NEURIPS2022_c40d1e40,QMIX,chen2023minimizing,chen2022multi} often leverage 
value-function factorization techniques -- which optimize joint action-value functions through the maximization of factorized, per-agent utilities -- to improve learning and sampling efficiency.  

Current value factorization methods mostly adhere to the Individual-Global-Max (IGM) \cite{QMIX} principle, which asserts the coherence between joint and individual greedy action selections. It requires value function factorization to be strictly monotonic, i.e., if local agents maximize their local utilities, 
the joint value function can attain the global maximum. While such monotonic factorization facilitates efficient maximization during training and simplifies the decentralization of the learned policies, it is restricted to {\bf monotonic value function representations} and may lead to poor performance in situations that exhibit non-monotonic characteristics~\cite{WQMIX}. 
In response to the limited representation expressiveness due to monotonic factorization, recent works have considered a number of mitigation strategies, e.g., WQMIX \cite{WQMIX} that formulates a weighted projection toward monotonic functions, QTRAN \cite{QTRAN} that utilizes an additional advantage value estimator, QPLEX \cite{QPLEX} that leverages a dueling structure, and ResQ \cite{ResQ} that introduces an additional residual function network. However, these methods tend to compensate for the representation gap caused by monotonic factorization, subject to the IGM principle.
QTRAN exhibits low sample efficiency and QPLEX may obtain suboptimal results, while WQMIX  tends to possess high approximation errors in non-optimal actions,  and it is difficult for ResQ to find the optimal actions for a scenario requiring highly-coordinated agent exploration~\cite{ResQ}. Going beyond monotonicity and IGM while ensuring decentralized action selection is still an open question\cite{mei2023remix}.


The pivotal insight in this paper is that monotonic value-function factorization may not be necessary for enabling local action selections in MARL. Indeed, we show that monotonic factorization under IGM may lead to suboptimal policies that only recover an arbitrarily small fraction of optimal global actions that are achievable through an ideal policy with full observability.
Instead, we propose a novel approach of concave value-function factorization for high representation expressiveness, called ConcaveQ. It employs a deep neural network representation of concave mixing networks for value-function decomposition. The concave mixing network estimates the joint action-value output $Q_{tot}$ through a concave function $f_{concave}$ of the input local utilities $Q_i$ conditioned on agent $i$'s local observation and action choice, that is, $Q_{tot} = f_{concave}(Q_1, ..., Q_n)$. We propose a deep neural network representation of the concave mixing function $f_{concave}$. It enables a novel concave value-function factorization in MARL.

Due to the lack of IGM under concave mixing functions, the global optimal joint actions are not necessarily consistent with the optimal actions of local agents. ConcaveQ tackles this problem through an iterative action-selection algorithm during training, which attains optimality using the concavity property of the proposed value-function factorization. To support fully decentralized execution, we note that the optimal joint action cannot be obtained directly from maximizing the local agent utilities $Q_i$. ConcaveQ adopts a soft actor-critic structure with local policy networks for distributed execution, such that each local agent can select the best action according to its own local policy network, which are corrected by the concave value networks during training. It also allows auxiliary information such as entropy maximization to be exploited for effective learning.



For evaluation, we demonstrate the performance of ConcaveQ on SMAC maps and predator-prey tasks. We compare ConcaveQ with several state-of-the-art value factorization methods on various MARL datasets. The experimental results show that ConcaveQ outperforms multiple competitive value factorization methods, especially on difficult tasks that require more coordination among the agents since monotonic factorization restricts global value-function estimate and hinders the effective training of agents.
Our numerical results show significant improvement over SOTA baselines in StarCraft II micromanagement challenge tasks regarding higher reward and convergence speed.
