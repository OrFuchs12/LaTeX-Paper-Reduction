\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}

\bibitem[{B{\"o}hmer, Kurin, and Whiteson(2020)}]{deep_coori_graph}
B{\"o}hmer, W.; Kurin, V.; and Whiteson, S. 2020.
\newblock Deep coordination graphs.
\newblock In \emph{PMLR}.

\bibitem[{Borwein and Lewis(2006)}]{convex_conjugate}
Borwein, J.; and Lewis, A. 2006.
\newblock \emph{Convex Analysis and Nonlinear Optimization: Theory and Examples}.
\newblock Springer.

\bibitem[{Boyd and Vandenberghe(2004)}]{convex_optimization}
Boyd, S.; and Vandenberghe, L. 2004.
\newblock \emph{Convex Optimization}.
\newblock Cambridge university press.

\bibitem[{Chen et~al.(2022{\natexlab{a}})Chen, Chen, Lan, and Aggarwal}]{chen2022multi}
Chen, J.; Chen, J.; Lan, T.; and Aggarwal, V. 2022{\natexlab{a}}.
\newblock Multi-agent covering option discovery based on kronecker product of factor graphs.
\newblock \emph{IEEE Transactions on Artificial Intelligence}.

\bibitem[{Chen et~al.(2022{\natexlab{b}})Chen, Chen, Lan, and Aggarwal}]{NEURIPS2022_c40d1e40}
Chen, J.; Chen, J.; Lan, T.; and Aggarwal, V. 2022{\natexlab{b}}.
\newblock Scalable Multi-agent Covering Option Discovery based on Kronecker Graphs.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~35, 30406--30418. Curran Associates, Inc.

\bibitem[{Chen and Lan(2023)}]{chen2023minimizing}
Chen, J.; and Lan, T. 2023.
\newblock Minimizing return gaps with discrete communications in decentralized pomdp.
\newblock \emph{arXiv preprint arXiv:2308.03358}.

\bibitem[{Czumaj(2004)}]{balls_into_bins}
Czumaj, A. 2004.
\newblock Lecture notes in Approximation and Randomized Algorithms.
\newblock \url{https://www.ic.unicamp.br/~celio/peer2peer/math/czumaj-balls-into-bins.pdf}.

\bibitem[{Foerster et~al.(2018)Foerster, Farquhar, Afouras, Nardelli, and Whiteson}]{coma}
Foerster, J.~N.; Farquhar, G.; Afouras, T.; Nardelli, N.; and Whiteson, S. 2018.
\newblock Counterfactual Multi-Agent Policy Gradients.
\newblock In \emph{AAAI}.

\bibitem[{Gogineni et~al.(2023)Gogineni, Mei, Lan, Wei, and Venkataramani}]{gogineni2023accmer}
Gogineni, K.; Mei, Y.; Lan, T.; Wei, P.; and Venkataramani, G. 2023.
\newblock Accmer: Accelerating multi-agent experience replay with cache locality-aware prioritization.
\newblock In \emph{2023 IEEE 34th International Conference on Application-specific Systems, Architectures and Processors (ASAP)}, 205--212. IEEE.

\bibitem[{Hu et~al.(2021)Hu, Jiang, Harding, Wu, and Liao}]{marl}
Hu, J.; Jiang, S.; Harding, S.~A.; Wu, H.; and Liao, S.-w. 2021.
\newblock Rethinking the implementation tricks and monotonicity constraint in cooperative multi-agent reinforcement learning.
\newblock In \emph{ICLR}.

\bibitem[{Iqbal and Sha(2019)}]{maac}
Iqbal, S.; and Sha, F. 2019.
\newblock Actor-Attention-Critic for Multi-Agent Reinforcement Learning.
\newblock In \emph{ICML}.

\bibitem[{Karush(1939)}]{concaveopt}
Karush, W. 1939.
\newblock \emph{Minima of Functions of Several Variables with Inequalities as Side Conditions}.
\newblock Master's thesis, Department of Mathematics, University of Chicago.

\bibitem[{Lowe et~al.(2017)Lowe, Wu, Tamar, Harb, Abbeel, and Mordatch}]{maddpg}
Lowe, R.; Wu, Y.; Tamar, A.; Harb, J.; Abbeel, P.; and Mordatch, I. 2017.
\newblock Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.
\newblock In \emph{NeuralIPS}.

\bibitem[{Mei, Zhou, and Lan(2023)}]{mei2023remix}
Mei, Y.; Zhou, H.; and Lan, T. 2023.
\newblock ReMIX: Regret Minimization for Monotonic Value Function Factorization in Multiagent Reinforcement Learning.
\newblock \emph{arXiv preprint arXiv:2302.05593}.

\bibitem[{Mei et~al.(2023)Mei, Zhou, Lan, Venkataramani, and Wei}]{mei2022mac}
Mei, Y.; Zhou, H.; Lan, T.; Venkataramani, G.; and Wei, P. 2023.
\newblock MAC-PO: Multi-Agent Experience Replay via Collective Priority Optimization.
\newblock In \emph{Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems}, AAMAS '23, 466â€“475. International Foundation for Autonomous Agents and Multiagent Systems.

\bibitem[{Rashid et~al.(2020)Rashid, Farquhar, Peng, and Whiteson}]{WQMIX}
Rashid, T.; Farquhar, G.; Peng, B.; and Whiteson, S. 2020.
\newblock Weighted {QMIX:} Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning.
\newblock In \emph{NeuralIPS}.

\bibitem[{Rashid et~al.(2018)Rashid, Samvelyan, de~Witt, Farquhar, Foerster, and Whiteson}]{QMIX}
Rashid, T.; Samvelyan, M.; de~Witt, C.~S.; Farquhar, G.; Foerster, J.~N.; and Whiteson, S. 2018.
\newblock {QMIX:} Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning.
\newblock In \emph{{ICML}}.

\bibitem[{Samvelyan et~al.(2019{\natexlab{a}})Samvelyan, Rashid, De~Witt, Farquhar, Nardelli, Rudner, Hung, Torr, Foerster, and Whiteson}]{samvelyan2019starcraft}
Samvelyan, M.; Rashid, T.; De~Witt, C.~S.; Farquhar, G.; Nardelli, N.; Rudner, T.~G.; Hung, C.-M.; Torr, P.~H.; Foerster, J.; and Whiteson, S. 2019{\natexlab{a}}.
\newblock The starcraft multi-agent challenge.
\newblock \emph{arXiv preprint arXiv:1902.04043}.

\bibitem[{Samvelyan et~al.(2019{\natexlab{b}})Samvelyan, Rashid, De~Witt, Farquhar, Nardelli, Rudner, Hung, Torr, Foerster, and Whiteson}]{smac}
Samvelyan, M.; Rashid, T.; De~Witt, C.~S.; Farquhar, G.; Nardelli, N.; Rudner, T.~G.; Hung, C.-M.; Torr, P.~H.; Foerster, J.; and Whiteson, S. 2019{\natexlab{b}}.
\newblock The starcraft multi-agent challenge.
\newblock \emph{arXiv preprint arXiv:1902.04043}.

\bibitem[{Shen et~al.(2022)Shen, Qiu, Liu, Liu, Fu, Liu, and Wang}]{ResQ}
Shen, S.; Qiu, M.; Liu, J.; Liu, W.; Fu, Y.; Liu, X.; and Wang, C. 2022.
\newblock ResQ: A Residual Q Function-based Approach for Multi-Agent Reinforcement Learning Value Factorization.
\newblock In \emph{{NeurIPS}}.

\bibitem[{Son et~al.(2019)Son, Kim, Kang, Hostallero, and Yi}]{QTRAN}
Son, K.; Kim, D.; Kang, W.~J.; Hostallero, D.; and Yi, Y. 2019.
\newblock {QTRAN:} Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning.
\newblock In \emph{ICML}.

\bibitem[{Su, Adams, and Beling(2021)}]{VDAC}
Su, J.; Adams, S.; and Beling, P. 2021.
\newblock Value-decomposition multi-agent actor-critics.
\newblock In \emph{AAAI}.

\bibitem[{Sunehag et~al.(2018)Sunehag, Lever, Gruslys, Czarnecki, Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, and Graepel}]{VDN}
Sunehag, P.; Lever, G.; Gruslys, A.; Czarnecki, W.~M.; Zambaldi, V.~F.; Jaderberg, M.; Lanctot, M.; Sonnerat, N.; Leibo, J.~Z.; Tuyls, K.; and Graepel, T. 2018.
\newblock Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward.
\newblock In \emph{{AAMAS}}.

\bibitem[{Wang et~al.(2021{\natexlab{a}})Wang, Ren, Liu, Yu, and Zhang}]{QPLEX}
Wang, J.; Ren, Z.; Liu, T.; Yu, Y.; and Zhang, C. 2021{\natexlab{a}}.
\newblock {QPLEX:} Duplex Dueling Multi-Agent Q-Learning.
\newblock In \emph{ICLR}.

\bibitem[{Wang et~al.(2021{\natexlab{b}})Wang, Han, Wang, Dong, and Zhang}]{dop}
Wang, Y.; Han, B.; Wang, T.; Dong, H.; and Zhang, C. 2021{\natexlab{b}}.
\newblock DOP: Off-Policy Multi-Agent Decomposed Policy Gradients.
\newblock In \emph{ICLR}.

\bibitem[{Yang et~al.(2020)Yang, Hao, Liao, Shao, Chen, Liu, and Tang}]{qatten}
Yang, Y.; Hao, J.; Liao, B.; Shao, K.; Chen, G.; Liu, W.; and Tang, H. 2020.
\newblock Qatten: A general framework for cooperative multiagent reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.03939}.

\bibitem[{Zhang et~al.(2021)Zhang, Li, Wang, Xie, and Lu}]{fop}
Zhang, T.; Li, Y.; Wang, C.; Xie, G.; and Lu, Z. 2021.
\newblock FOP: Factorizing Optimal Joint Policy of Maximum-Entropy Multi-Agent Reinforcement Learning.
\newblock In \emph{ICML}.

\bibitem[{Zhou, Lan, and Aggarwal(2022)}]{pac}
Zhou, H.; Lan, T.; and Aggarwal, V. 2022.
\newblock PAC: Assisted Value Factorization with Counterfactual Predictions in Multi-Agent Reinforcement Learning.
\newblock In \emph{NeurlIPS}.

\bibitem[{Zhou, Lan, and Aggarwal(2023)}]{zhou2022value}
Zhou, H.; Lan, T.; and Aggarwal, V. 2023.
\newblock Value Functions Factorization With Latent State Information Sharing in Decentralized Multi-Agent Policy Gradients.
\newblock \emph{IEEE Transactions on Emerging Topics in Computational Intelligence}, 7(5): 1351--1361.

\end{thebibliography}
