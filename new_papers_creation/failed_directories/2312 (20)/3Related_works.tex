\section{Related Work}

\noindent {\bf Value Factorization Approaches.}
Value Factorization approaches are widely adopted in value-based MARL  \cite{marl, dop,zhou2022value,gogineni2023accmer,mei2022mac}. 
Current value factorization methods mostly adhere to the
monotonic IGM principle, such as VDN\cite{VDN} and QMIX\cite{QMIX}. VDN \cite{VDN} represents the joint state-action value function $Q_{tot}$ as a sum of per-agent utilities $Q_i$. QMIX\cite{QMIX} employs a mixing network to factorize the $Q_{tot}$ in a monotonic manner.  The monotonicity constraint ensures that the optimal joint action for the global reward $Q_{tot}$ is also optimal for each agent’s local utility $Q_i$. However, these two decomposition methods suffer from structural constraints, limiting the range of joint action-value functions they can effectively represent. 


To compensate for the restricted expressiveness of monotonic representation, recent works have explored several mitigation strategies. Specifically, WQMIX \cite{WQMIX} applies a weighted projection to QMIX, which attaches more importance to the optimal joint actions when minimizing training errors. In WQMIX, finding the optimal weight remains an open problem. Furthermore, the approximation errors are high for non-optimal actions. QTRAN \cite{QTRAN}  incorporates an additional advantage value estimator and imposes a set of linear constraints.  QPLEX \cite{QPLEX}  leverages a dueling structure involving value and advantage functions. ResQ \cite{ResQ} masks some state-action value pairs and introduces an additional residual function network. 
Apart from mitigation strategies, there are also various MARL actor-critic methods, such as MADDPG \cite{maddpg}, MAAC \cite{maac}, and COMA \cite{coma}, which use centralized critics and decentralized actors. PAC \cite{pac} decouples individual agents’ policy networks from value function networks to leverage the benefits of assisted value function factorization.   VDAC \cite{VDAC} combined actor-critic structure with QMIX for the joint state-value function estimation. DOP \cite{dop}  employs a network similar to Qatten \cite{qatten} for policy gradients with off-policy tree backup and on-policy TD.

From the previous works mentioned above, we can see a series of works conducted within the confines of the monotonic assumption. However, few of them consider the non-monotonic case. To the best of our knowledge, this paper is the first one considering the MARL in the concave scenario, which is a general case in non-monotonic scenarios.
