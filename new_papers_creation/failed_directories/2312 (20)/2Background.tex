
\section{Background}

 \textbf{Value Decomposition.} In cooperative multi-agent reinforcement learning, value function decomposition is a paradigm for cooperative multi-agent reinforcement learning (MARL) that aims to learn a centralized state-action value function that can be decomposed into individual agent values. Value decomposition approaches can reduce the complexity and improve the scalability of MARL problems. Two prominent examples of value function decomposition methods are VDN and QMIX, which both assume that the centralized action value $Q_{tot}$ is additive or monotonic with respect to the agent values $Q_i$. VDN simply sums up the agent values to obtain $Q_{tot} = \Sigma_{i=1}^nQ_i(\tau_i, u_i)$, while QMIX uses a mixing network to combine the local agent utilities in a monotonic way. QMIX enforces the monotonicity through a constraint on the relationship between $Q_{tot}(s, \boldsymbol{u}) = f_\theta(s, Q_1(\tau_1, u_1), ..., Q_n(\tau_n, u_n))$ and each $Q_i$, that is, $\frac{\partial{Q_{tot}}(\boldsymbol{\tau}, \boldsymbol{u})}{\partial{Q_i}(\tau_i, u_i)} > 0, \forall i \in \mathcal{N}$. The monotonicity constraint ensures that the optimal joint action for the global reward $Q_{tot}$ is also optimal for each agentâ€™s local utility $Q_i$, where the mixing function $f_\theta$ is formulated as a feed-forward network parameterized by $\theta$. The weights of the mixing network are produced by independent hyper-networks, which take the global state as input and use an absolute activation function to ensure that the mixing network weights are non-negative to enforce the monotonicity. Then QMIX is trained end-to-end to minimize the squared TD error on \textcolor{black}{ mini-batch of $b$ samples from the replay buffer as $\Sigma^b_{i=1}\left(Q_{tot}(\boldsymbol{\tau}, \boldsymbol{u}, s; \theta) - y_{tot} \right)^2$, where $y^{tot} = r + \gamma max_{u'} Q_{tot}(\boldsymbol{\tau'}, \boldsymbol{u'}, s'; \theta')$, $r$ is the global reward and $\theta'$ is the parameters of the target network whose parameters are periodically copied from $\theta$ for training stabilization.}

\noindent {\bf Weighted QMIX Projection.}  QMIX projects $Q_{tot}$ to $\qmixSpace$ by minimizing the projection loss:
%The monotonic mixing $f_s$  can be viewed as a projection into 
\begin{equation}
\argmin_{q \in \qmixSpace} \sum_{\U \in \mathbf{U}} (\optimOper \qtot(s,\U) - q 
(s,\U))^2.
\label{eq:qmix_objective}
\end{equation}
Weighted QMIX \cite{WQMIX} extends the idea of monotonic mixing by introducing a weighting function into the QMIX projection operator to bias the learning process towards the best joint action. The weighted projection can be described as:
\begin{equation}
\mathop{\arg\min}\limits_{q \in Q^{mix}} \sum\limits_{\textbf{u}\in \textbf{U}}(w(s, \textbf{u})Q(s, \textbf{u}) - q(s, \textbf{u}))^2
\end{equation}
where $w$ is the weighting function that is added to place more importance on optimal joint actions, while still anchoring down the value estimates for other joint actions.  Weighted QMIX can be viewed as a projection onto monotonic mixing function space using weighted distance.



