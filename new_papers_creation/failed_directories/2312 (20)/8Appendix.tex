% \newpage
\onecolumn
% \appendix
% \renewcommand{\thesection}
%   {\arabic{section}.\hskip-0.6em}
% \renewcommand{\thesubsection}
%   {\arabic{section}.\arabic{subsection}\hskip-0.6em}
% % \renewcommand{\thesection}{\Alph{section}}

\begin{center}
	% \Large\textbf{Appendix}
 \Large\textbf{Appendix \\
% ConcaveQ: Non-Monotonic Value Function Factorization via Concave Representations in Deep Multi-Agent Reinforcement Learningz
}
\end{center}



% \renewcommand{\thesection}{\Alph{section}}
% \renewcommand{\thetheorem}{A.\arabic{theorem}}
\setcounter{section}{0}
\setcounter{theorem}{0}
\setcounter{equation}{0}
\setcounter{prop}{0}


\section{Organization of the Appendix}
In Section A.1, we present the theorems and their corresponding proofs concerning ConcaveQ. Section A.2 describes the experimental setup in detail. We show Predator Prey Environment in Section A.2.1 and show SMAC Environment in Section A.2.2.  We demonstrate implementation details in Section A.3.
% , while in Section A.4 we show more   experiment results.



\section{A.1 \ \ Proof of Theorems} \label{subsec:Theorems}
In this section, we show the proof of all the Theorems of this work.

\begin{theorem}
 When $n \geq log_{|S|}(2|A|\cdot log_2|A|) + 1$ and the optimal action choices in $Q_{jt}$ are uniformly distributed, for any monotonic factorization, and for some constant $\delta\in(0,1)$, we have

 \begin{equation}
     \frac{E(|S_{mono}|)}{|S|^n} \leq \frac{e+1}{|A|}\cdot \delta^{n-1}
 \end{equation}

\end{theorem}


\noindent\textit{Proof. \ }The key idea of the upper bound proof of monotonic representation limitations is to convert the problem into a classic max-load problem.

\noindent \textit{\textbf{Step1}: Formulate as max-load bin-ball problem.} For each agent $i$ and state $\mathbf{s}$, we consider the optimal action of $Q_{mono}$ as \textit{ball $i$}. Thus, $Q_{jt}$ and $Q_{mono}$ have the same optimal action for  agent $i$ if ball $i$ is placed in the bin corresponding to the optimal action of $Q_{jt}$. 

%the \textit{bin $i$} is the optimal action of $Q_{jt}$.

Let $X_i$ denotes that ball $i$ is in bin $i$, that is,  $Q_{jt}$ and $Q_{mono}$ have the same optimal action for  agent $i$, then we have: 
%. Our goal is to analyze $max_{1 \leq  i \leq n}Xi$
\begin{equation}
    E(|S_{mono}|) = \Sigma_sP(X_1)\cdot P(X_2|X_1) \cdot...\cdot P(X_n|X_{n-1}...X_1) \nonumber
\label{S_mono}
\end{equation}

Define $Y_i$ as the load of bin $i$, that is, the state space such that $Q_{jt}$ and $Q_{mono}$ have the same optimal action for agents except for agent $i$.
Note that the global maximum of $Q_{jt}$ is uniformly distributed over different states, we then analyze $P(X_1)$:
\begin{equation}
P(X_1) =  \frac{E(\Sigma_{s_2...s_n}X_1)}{|S|^{n-1}}
\leq \frac{E(max_{i}Y_i)}{|S|^{n-1}}
\label{P_X_1}
\end{equation}

\noindent \noindent\textit{\textbf{Step2}: Analyze the probability distribution of the load.} 
By the union bound, we have:
\begin{equation} 
P(max_{i}Y_i) \geq \frac{e \cdot {|S|^{n-1}}}{|A|} \leq \Sigma_iP(Y_i \geq \frac{e \cdot {|S|^{n-1}}}{|A|})
\label{P_maxY_union}
\end{equation}
By the Chernoff bound, for any $\alpha$, we have,
\begin{equation} 
P((max_{i}Y_i) \geq (1+\alpha)EY_i) \leq ( \frac{e^\alpha }{(1+\alpha)^{(1+\alpha)}})^{EY_i}
\label{P_maxY_union}
\end{equation}
Let us assume that $\alpha \geq e-1 $. Then, we have,
\begin{equation} 
\begin{aligned}
\frac{e^\alpha }{(1+\alpha)^{(1+\alpha)}} = &\frac{1}{1+\alpha}\cdot (\frac{e}{1+\alpha})^\alpha \\
\leq &\frac{1}{1+\alpha} \\
\leq &\frac{1}{2}
\end{aligned}
\label{P_maxY_union}
\end{equation}

\begin{equation} 
\begin{aligned}
\frac{e^\alpha }{(1+\alpha)^{(1+\alpha)}} = \frac{1}{1+\alpha}\cdot (\frac{e}{1+\alpha})^\alpha 
\leq \frac{1}{1+\alpha} 
\leq \frac{1}{2}
\end{aligned}
\label{P_maxY_union}
\end{equation}
Since  $EY_i \geq \frac{|S|^{n-1}}{|A|}$,  we have,

\begin{equation} 
P(max_{i}Y_i \geq \frac{e\dot|S|^{n-1}}{|A|}) \leq ( \frac{1}{2})^\frac{|S|^{n-1}}{|A|}
\label{P_maxY_union}
\end{equation}

\begin{equation}
\begin{aligned}
E(max_{i}Y_i) =  &E(max_{i}Y_i|Y_i \geq \frac{e\dot|S|^{n-1}}{|A|}) \cdot P(Y_i \geq \frac{e\dot|S|^{n-1}}{|A|}) \\ 
+ &E(max_{i}Y_i|Y_i < \frac{e\dot|S|^{n-1}}{|A|}) \cdot P(Y_i < \frac{e\dot|S|^{n-1}}{|A|}) \\
\leq &\frac{|A|\cdot|S|^{n-1}}{2^\frac{|S|^{n-1}}{|A|}} + \frac{e\cdot |S|^{n-1}}{|A|} 
\end{aligned}
\label{P_maxY_union}
\end{equation}
When $\frac{1}{2^\frac{|S|^{n-1}}{|A|}} \leq \frac{1}{|A|^2}$ or $n \geq log_{|S|}(2|A|\cdot log_2|A|) + 1$, we have,
\begin{equation}
E(max_{i}Y_i) \leq \frac{(e+1)|S|^{n-1}}{|A|}
\label{E_maxY}
\end{equation}
where $|A| \geq 1$ is the size of action space. 
%count of actions that are both optimal for $Q_{jt}$ and $Q_{mono}$ for agent $i$.
Applying Eq. (\ref{E_maxY}) to Eq. (\ref{P_X_1}), we have:
\begin{equation}
P(X_1) \leq \frac{e+1}{|A|}
\end{equation}
Using the same argument repeatedly for agents $i=2,\ldots,n$, we can choose $\delta = min_i(P(X_i|X_{i-1}...X_1))$ and $0 < \delta < 1$. Plugging these inequalities into $ E(|S_{mono}|) $, we have, 
\begin{equation}
\begin{aligned}
E(|S_{mono}|) &\leq \Sigma_s\frac{e+1}{|A|}\cdot \delta^{n-1}  \\
&= \frac{|S|^n\cdot(e+1)}{|A|}\cdot \delta^{n-1}
\end{aligned}
\end{equation}
\begin{equation}
\frac{E(|S_{mono}|)}{|S|^n} \leq \frac{e+1}{|A|}\cdot \delta^{n-1} 
\end{equation}


\iffalse
 \begin{equation}
% E(|S_{mono}|) \leq \sum_s\frac{e+1}{|\overline{u}|}\cdot \delta^{n-1}
E(|S_{mono}|) \leq \frac{e+1}{|\overline{u}|}\cdot \delta^{n-1} \cdot |S|^n
\end{equation}
where $\delta = min_i(P(X_i|X_{i-1}...X_1)$ and $0 < \delta < 1$.


\noindent\textit{\textbf{Step3}: Calculate the upper bound of the fraction}
The final fraction of $E(|S_{mono}|)$ over the overall state space satisfies:
 \begin{equation}
\frac{E(|S_{mono}|)}{|S|^n} \leq \frac{e+1}{|\overline{u}|}\cdot \delta^{n-1}
\end{equation}
when $n \geq log_{|S|}(2|\overline{u}|\cdot log_2|\overline{u}|) + 1$.
\fi 


{\begin{prop}
For any state $s$, there is always a concave function $Q_{c}(s, \textbf{u})$ that recovers the global maximum of  $Q_{jt}(s, \textbf{u})$ with the same optimal action.
\end{prop}

\noindent\textit{Proof. \ } For any state $s$, let $f_1(\textbf{u})$ = $Q_{jt}(, \textbf{u})$ and $f_2(\textbf{u})$ = -$Q_{jt}(, \textbf{u})$. According to Fenchel–Moreau theorem \cite{convex_conjugate}, the biconjugate function of $f_2(\textbf{u})$ is a convex function $g(\textbf{u}) = f_2^{**}(\textbf{u})$ and $h(\textbf{u}) = -f_2^{**}(\textbf{u})$  is a concave function. $g(\textbf{u})$ and $h(\textbf{u})$ satisfies:
 \begin{equation}
    g(\textbf{u}) \leq f_2(\textbf{u}), 
\end{equation}
 \begin{equation}
    -g(\textbf{u}) \geq -f_2(\textbf{u}), 
\end{equation}
 \begin{equation}
    h(\textbf{u}) \geq f_1(\textbf{u}).
\end{equation}
Suppose the optimal joint action of $f_1(\textbf{u})$ and $h(\textbf{u})$ are $\textbf{u}_{f_1}^*$ and $\textbf{u}_h^*$ respectively,  if we shift $h(\textbf{u})$ by $(-\textbf{u}_{f_1}^* + \textbf{u}_h^*)$, the shifted concave function has the same optimal action as $f_1(\textbf{u})$. In other words, for any state $s$, there is always a concave function $h(\textbf{u}-(\textbf{u}_{f_1}^* + \textbf{u}_h^*)$ that has the same optimal action as $Q_{jt}(s, \textbf{u})$. 
}



\begin{theorem}
The mixing network $f$ is concave in $x$ provided that all $W^{(z)}_{0:k-1}$ are non-negative, and all functions $a_i$ are convex and non-decreasing.
\end{theorem}

\noindent\textit{Proof. \ } 
Since linear functions are concave and convex, $z_1$ is a convex function of $x$. Note that t non-negative sums of convex functions are also convex and that the composition of a convex and convex non-decreasing function is also convex, $z_{i+1}, i = 1, ..., k-2$ is also a convex function of $x$. Following the fact that the negative
of a convex function is a concave, $z_k$ is a concave function of $x$. 

\section{A2 \ \ Algorithm Analysis}

\subsection{Iterative Action Selection}
~\\
\begin{algorithm} 
	\caption{Iterative action selection} 
    \label{alg_action_selction} 
	\begin{algorithmic}
        \STATE Initialize action and action-value function with greedy action search, and let $\mathbf{u}_{opt} = \mathbf{u}_{init}$, $Q_{tot} = Q_{init}$
        \FOR{$agent_i = 0$ to $max\_agents$}
        \FOR{ $action_i = 0$ to $max\_actions$}
        \STATE replace $(\boldsymbol{u}_{opt})^{(agent_i)}$ with $action_i$ and get $\boldsymbol{u}_{current}$        
        \IF {$Q(s, \boldsymbol{u}_{current}) > Q_{tot}$}
        \STATE $Q_{tot} = Q(s, \boldsymbol{u}_{current})$
        \STATE $\boldsymbol{u}_{opt} = \boldsymbol{u}_{current}$
        
        \ENDIF
        \ENDFOR
        \ENDFOR \\
        Return $\boldsymbol{u}_{opt}$
	\end{algorithmic} 
\end{algorithm}
\subsection{Factorized soft policy iteration}
Our factorized soft policy is based on Boltzmann exploration policy iteration. Previous works have demonstrated that within the MARL domain, Boltzmann exploration policy iteration is proven to enhance the policy as well as converge with a certain number of iterations. In this context, the Boltzmann policy iteration is  defined as:


\begin{equation}
J(\pi)=\sum_{t} \mathbb{E}\left[r\left(\mathbf{s}_{t}, \mathbf{u}_{t}\right)+\alpha \mathcal{H}\left(\pi\left(\cdot | \mathbf{s}_{t}\right)\right)\right]
\end{equation}

 The gradient for factorized soft policy can be given via:

\begin{equation}
\begin{array}{l}
\begin{aligned}
\mathcal{L}_{\pi}(\pi) &=\mathbb{E}_{\mathcal{D}}\left[\alpha \log \boldsymbol{\pi}\left(\boldsymbol{u}_{t} | \boldsymbol{\tau}_{t}\right)-Q^{\pi}_{tot}\left(\boldsymbol{s_{t}}, \boldsymbol{\tau_{t}}, \boldsymbol{u}_{t}\right)\right] \\
&= -q^{\pi}\left(\boldsymbol{s}_{t}, \mathbb{E}_{\pi^{i}}\left[q^{i}\left(\tau_{t}^{i}, u_{t}^{i}\right)-\alpha \log \pi^{i}\left(u_{t}^{i} | \tau_{t}^{i}\right)\right]\right)
\end{aligned}
\end{array}
\end{equation}



Let  $q^{\pi}$ be the operator of a one-layer mixing network with no activation functions at the end whose parameters are generated from the hyper-network with input $\boldsymbol{s}_{t}$, then
\begin{equation}
\begin{aligned}
& q^{\pi}(\boldsymbol{s}_{t}, \boldsymbol{q}(\tau_{t}, a_{t}) - \boldsymbol{\alpha} \log \boldsymbol{\pi(a_{t}| \tau_{t}})) 
\\  \\
& = \sum_{i}[ w^{i}(\boldsymbol{s}) \mathbb{E}_{\pi} [q^{i}(\tau_{t}^{i}, a_{t}^{i})]  - \sum_{i}[ w^{i}(\boldsymbol{s})\alpha^{i} \log {\pi^{i}}({a}_{t} |{\tau}_{t})] + b(\boldsymbol{s}) 
\label{q_pi_1}
\end{aligned}
\end{equation}


where $ w^{i}(s)$ and $b^{i}(s)$ are the corresponding weights and biases of $q^{\pi}$ conditioned on $\boldsymbol{s}$).
Analyze  the first  and second item in Eq. \ref{q_pi_1} respectively, and we have,
\begin{equation}
q^{\pi}(\boldsymbol{s}_{t}, \boldsymbol{q}(\tau_{t}, a_{t})) = \sum_{i}[ w^{i}(\boldsymbol{s}) \mathbb{E}_{\pi} [q^{i}(\tau_{t}^{i}, a_{t}^{i})] + b(\boldsymbol{s}) = \mathbb{E}_{\pi} [Q_{tot}(\boldsymbol{\tau}, \boldsymbol{a} ; \boldsymbol{\theta})] 
\label{q_pi_2}
\end{equation}

\begin{equation}
\mathbb{E}_{\pi} [Q_{tot}(\boldsymbol{\tau}, \boldsymbol{a} ; \boldsymbol{\theta})] = \sum_{\boldsymbol{a}} {\pi^{i}}({a}_{t}^{i} |{\tau}_{t}^{i}) \mathbb{E}_{\pi} [Q_{tot}(\boldsymbol{\tau}, \boldsymbol{a} ; \boldsymbol{\theta})] 
\label{q_pi_3}
\end{equation}

Applying Eq. \ref{q_pi_2} and Eq. \ref{q_pi_1} to Eq. \ref{q_pi_1}, we have,
\begin{equation}
\begin{aligned}
q^{\pi}(\boldsymbol{s}_{t}, \boldsymbol{q}(\tau_{t}, a_{t}) - \boldsymbol{\alpha} \log \boldsymbol{\pi(a_{t}| \tau_{t}})) &= \mathbb{E}_{\pi} [Q_{tot}(\boldsymbol{\tau}, \boldsymbol{a} ; \boldsymbol{\theta})] - \sum_{i}[ w^{i}(\boldsymbol{s})\mathbb{E}_{\pi} [\alpha^{i} \log {\pi^{i}}({a}_{t} |{\tau}_{t})]] \\
  &= \mathbb{E}_{\pi} [Q_{tot}(\boldsymbol{\tau}, \boldsymbol{a} ; \boldsymbol{\theta})] -\sum_{i} \mathbb{E}_{\pi} [\boldsymbol{\alpha} \log {\pi^{i}}({a}_{t}^{i} |{\tau}_{t}^{i})] \\
& \text{\hspace{3em}(let $\alpha^{i} =  \frac{\boldsymbol\alpha}{w^{i}(\boldsymbol{s})}$)} \\
&= \mathbb{E}_{\pi} [Q_{tot}(\boldsymbol{\tau}, \boldsymbol{a} ; \boldsymbol{\theta})] - \sum_{i}\sum_{\pi} [\boldsymbol{\alpha}\pi^{i} ({a}_{t}^{i} |{\tau}_{t}^{i})\log {\pi^{i}}({a}_{t}^{i} |{\tau}_{t}^{i})] \\ 
& = \mathbb{E}_{\pi} [Q_{tot}(\boldsymbol{\tau}, \boldsymbol{a} ; \boldsymbol{\theta})] - \sum_{\pi} \boldsymbol{\alpha} \log \boldsymbol{\pi}(\boldsymbol{a}_{t} |\boldsymbol{\tau}_{t}) \\
& \text{\hspace{3em}(Assume $\boldsymbol\pi = \prod \pi^{i}$, then $\sum_{i}\sum_{\pi} [\boldsymbol{\alpha}\pi^{i} ({a}_{t}^{i} |{\tau}_{t}^{i})\log {\pi^{i}}({a}_{t}^{i} |{\tau}_{t}^{i})] = \sum_{\pi} \boldsymbol{\alpha} \log \boldsymbol{\pi}(\boldsymbol{a}_{t} |\boldsymbol{\tau}_{t})$}) \\
& = \mathbb{E}_{\pi}[ Q^{\pi}_{tot}(\boldsymbol{s_{t}}, \boldsymbol{\tau_{t}}, \boldsymbol{a}_{t})-\boldsymbol\alpha \log \boldsymbol{\pi}(\boldsymbol{a}_{t} | \boldsymbol{\tau}_{t})]\\
\end{aligned}
\end{equation}

We leverage the derivation above to demonstrate that utilizing  $\boldsymbol{q^\pi}(\tau_{t}, a_{t}) - \boldsymbol{\alpha} \log \boldsymbol{\pi(a_{t}| \tau_{t}})$ directly as input for the mixing network can serve as soft-actor-critic policy update policy in a value factorization approach. This holds when applying a single-layer mixing network without an activation function. Although this condition provides insights into the proposed design, using a ReLU activation function allows this expression to serve as a lower bound for optimization.






\section{A2 \ \ Environment Details}

\begin{algorithm}
    \caption{pseudocode for training CONCAVEQ  }
    \begin{algorithmic}
        \FOR{$k = 0 $ to $max\_train\_steps$}                    
            \STATE {Initialize the environment, mixing network  $Q^*, Q_{tot}$, critic network $q$, policy network $\pi$}
            \STATE {Initialize the Replay buffer $\mathcal{D}$}
            \FOR{$t = 0 $ to $max\_episode\_limits$}    
              \STATE Execute joint action $\mathbf{a}$, observe reward $r$, and observation $\boldsymbol{\tau}$, next state $s_{t+1}$
              
              \STATE Store ($\boldsymbol{a}$, r, $\boldsymbol{\tau}$, $\boldsymbol{\tau^{'}}$) pair in replay buffer  $\mathcal{D}$
            \ENDFOR
        \FOR{t = 1 to N}
            \STATE Sample trajectory minibatch $\mathcal{B}$ from $\mathcal{D}$
            \STATE Take action $a_{i}\sim \pi_{i}$
            
            \STATE Calculate Loss 
            \STATE \quad\quad\quad $\mathcal{L}(\theta) = \mathcal{L}_{\pi} +  \mathcal{L}_{\hat{Q^{*}}} + \mathcal{L}_{{\text{ConcaveQ}}}$
            
            \STATE Update parameters of the critic network and mixing networks
            \STATE \quad\quad\quad $\boldsymbol{\theta(q, Q^*, Q_{tot})} \gets \beta\nabla\mathcal{L}(\theta)$ 
            \STATE Update policy network 
            \STATE \quad\quad\quad $\boldsymbol{\theta}(\pi) \gets \beta\nabla\mathcal{L}(\pi)$

            % \STATE Update encoding network 
            % \STATE \quad\quad\quad $\boldsymbol{\theta}_{m}(m) \gets \beta\nabla\mathcal{L}(\theta)$
            \STATE  Update the temperature parameter
            \STATE \quad\quad\quad  $\alpha \gets \beta \nabla\alpha$ 
            \IF{$t$ mod T = 0}
                \STATE Update target networks: $\boldsymbol{\theta^{\prime}} \gets \boldsymbol{\theta} $
            \ENDIF
        \ENDFOR
        \ENDFOR
        % \STATE Return $\boldsymbol{\pi}$
    \end{algorithmic}
\end{algorithm}
% \section{Environment Details}\label{Environment_setup}
For evaluation, we adopt state-of-the-art baselines that are closely related to our work and the most recent, such as   QMIX (baseline for value-based factorization methods), WQMIX (uses weighted projections to enhance representation ability), RESQ \cite{ResQ} (which is the most advanced value-based method), PAC \cite{pac}, QPLEX \cite{QPLEX} and  FOP \cite{fop} (SOTA actor-critic based method).   Our code implementation is available on GitHub.

% \href{https://github.com/hanhanAnderson/PAC-MARL}{Github}.

\subsection{A2.1 \ Predator Prey}\label{predator}

 Predator-prey Task is widely adopted to simulate a  partially observable environment on a  grid-world setting to study relative overgeneralization problem \cite{deep_coori_graph}. In this setup, a grid of size 10 × 10 is utilized, housing 8 agents tasked with capturing 8 prey. Agents possess the options of moving in any of the four cardinal directions, staying stationary, or attempting to ensnare adjacent prey. 
 Impossible actions, such as moving into an already occupied target position or catching when no adjacent prey exists, are designated as unavailable. Upon the execution of a catch action by two neighboring agents, a prey is successfully captured and both the prey and capturing agents are eliminated from the grid. The observation afforded to an agent comprises a $5 × 5$ sub-grid with the agent at the center, encompassing two distinct channels: one indicating the presence of agents and the other denoting the presence of prey.  An episode concludes either when all agents are removed from the grid or after 200 timesteps have transpired. The act of capturing prey garners a reward of $r$ = 10, whereas unsuccessful solo attempts incur a negative penalty of $p$. This study undertakes four sets of experiments, each with varying $p$ values: $p$ = 0, $p$ = -0.5, $p$ = -1.5, and $p$ = -2.



\subsection{A2.2 \ SMAC}\label{smac}
We conducted experiments on StarCraft II micromanagement following the instructions in \cite{smac} with open-source code including QMIX \cite{QMIX}, WQMIX \cite{WQMIX},  RESQ \cite{ResQ}, PAC \cite{pac}, FOP \cite{fop}, and QPLEX \cite{QPLEX}. We consider combat scenarios where the enemy units are controlled by the built-in AI in StarCraft II, while the friendly units are in the control of agents trained by MARL algorithms. The built-in AI difficulties cover a large range: Very Easy, Easy, Medium, Hard, Very Hard, and Insane, ranging from 0 to 7. We carry out the experiments with the highest difficulty for built-in AI: difficulty = 7 (Insane). Depending on the specific scenarios(maps), the units of the enemy and friendly can be either symmetric or asymmetric.  Each agent selects one action from discrete action space, like no-op, move[direction], attack[enemy\_id], and stop at each time step. Dead units are restricted to choosing the no-op action. Eliminating an enemy unit yields a reward of 10 while achieving victory by eliminating all enemy units results in a reward of 200. The global state information is solely available to the centralized critic. The map scenarios used in experiments are shown in Table \ref{tab:my-table} We train each baseline algorithm with 3 distinct random seeds. Evaluation is performed every 10,000 training steps with 32 testing episodes for the main results.  For ablation results, evaluation is carried out with three random seeds.  The experimentation is conducted on a Nvidia GeForce RTX 3080 Ti workstation, and on average, it takes approximately 2 hours to finish one run for the 3s5z map on the SMAC environment.



\begin{table*}[ht!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ccc@{}}
\toprule
map          & Ally Units                          & Enemy Units                         \\ \midrule
1c3s5z       & 1 Colossus, 3 Stalkers \& 5 Zealots & 1 Colossus, 3 Stalkers \& 5 Zealots \\
3m           & 3 Marines                           & 3 Marines                           \\
3s5z         & 3 Stalkers \& 5 Zealots             & 3 Stalkers \& 5 Zealots             \\
8m           & 8 Marines                           & 8 Marines                           \\ \midrule
3s\_vs\_5z   & 3 Stalkers                          & 5 Zealots                           \\
5m\_vs\_6m   & 5 Marines                           & 6 Marines                           \\
6h\_vs\_8z   & 6 Hydralisks                        & 8 Zealots                           \\
27m\_vs\_30m & 27 Marines                          & 30 Marines                          \\
Corridor     & 6 Zealots                           & 24 Zerglings                        \\
MMM2         & 1 Medivac, 2 Marauders \& 7 Marines & 1 Medivac, 3 Marauders \& 8 Marines \\\bottomrule
\end{tabular}%
}
\caption{Concise overview of SMAC Map scenarios employed in experimental settings}
\label{tab:my-table}
\end{table*}




\section{A3 \ \ Implementation Details}


We utilize one set of hyper-parameters across each environment, abstaining from tailored adjustments for specific maps.  In the absence of explicit specification, uniform configurations for general hyperparameters, such as learning rate, are maintained across all algorithms, while algorithm-specific hyperparameters remain at their default values. For action selection, we employ an epsilon-greedy strategy with an annealing schedule from an initial $\epsilon$ value of 0.995, gradually decreasing to $\epsilon$ = 0.05 over the course of 100,000 training steps in a linear fashion.
We use epsilon greedy for action selection with annealing from $\epsilon$ = 0.995 decreasing to $\epsilon$ = 0.05 in 100000 training steps linearly. 
Batch size set as $bs$ = 128, with a replay buffer capacity of 10,000 instances. And the learning rate is denoted as $lr$ = 0.001.
$\beta$ value is set to 0.001, considering the comparable dimensions of $o_i$ and $\hat{u}^*_i$, thus mitigating the need for excessive compression.
Weighting functions employ weights $w$ = 0.5 and the temporal Difference lambda parameter $\lambda$ set at 0.6.
Initial entropy term logarithmically set to log$\alpha$ = -0.07, its learning rate designated as $lr_{\alpha}$ = 0.0003.
Target network update interval established at every 200 episodes. And
algorithm performance evaluation conducted over 32 episodes every 1000 training steps.

% \subsection{A.4 Experimental Results}\label{results}
