%File: formatting-instructions-latex-2024.tex
%release 2024.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{eso-pic}
\usepackage{array}
\usepackage{bbding}
\usepackage{amsmath}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
\nocopyright
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
% \iffalse
\title{CityPulse: Fine-Grained Assessment of Urban Change with\\ Street View Time Series}
\author{
    %Authors
    % All authors must be in the same font size and format.
    % Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    % AAAI Style Contributions by Pater Patel Schneider,
    % Sunil Issar,\\
    Tianyuan Huang\textsuperscript{\rm 1}\equalcontrib,
    Zejia Wu\textsuperscript{\rm 2}\equalcontrib,
    Jiajun Wu\textsuperscript{\rm 1},
    Jackelyn Hwang\textsuperscript{\rm 1},
    Ram Rajagopal\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Stanford University
    \textsuperscript{\rm 2}University of California San Diego\\
    \{tianyuah, jihwang, ramr\}@stanford.edu, 
    zew024@ucsd.edu, 
    jiajunwu@cs.stanford.edu
}
% \affiliations {
%     % Affiliations
%     \textsuperscript{\rm 1}Stanford University\\
%     % zhecheng@stanford.edu,
%     % rajanie@stanford.edu,
%     % tianyuah@stanford.edu, 
%     % jiajunwu@cs.stanford.edu,
%     % ramr@stanford.edu
%     \{zhecheng, rajanie, tianyuah, ramr\}@stanford.edu, jiajunwu@cs.stanford.edu
% }
% \affiliations{
%     %Afiliations
%     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2}, 
    % J. Scott Penberthy\textsuperscript{\rm 3}, 
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    % 1900 Embarcadero Road, Suite 101\\
    % Palo Alto, California 94303-3310 USA\\
    % email address must be in roman text type, not monospace or sans serif
    % proceedings-questions@aaai.org
%
% See more examples next
% }

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{CityPulse: Fine-Grained Assessment of Urban Change with\\ Street View Time Series}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry
\newtheorem{definition}{Definition}
\begin{document}

\maketitle

\begin{abstract}
Urban transformations have profound societal impact on both individuals and communities at large. Accurately assessing these shifts is essential for understanding their underlying causes and ensuring sustainable urban planning. Traditional measurements often encounter constraints in spatial and temporal granularity, failing to capture real-time physical changes. While street view imagery, capturing the heartbeat of urban spaces from a pedestrian point of view, can add as a high-definition, up-to-date, and on-the-ground visual proxy of urban change. We curate the largest street view time series dataset to date, and propose an end-to-end change detection model to effectively capture physical alterations in the built environment at scale. We demonstrate the effectiveness of our proposed method by benchmark comparisons with previous literature and implementing it at the city-wide level. Our approach has the potential to supplement existing dataset and serve as a fine-grained and accurate assessment of urban change.

\end{abstract}

\section{Introduction}
% \textbf{Significance of the problem: The social impact problem considered by this paper is significant and has not been adequately addressed by the AI community}\\
% Urbanization has led to profound changes in the physical, social, and economic landscapes of cities worldwide. 
Our cities are evolving, and understanding how cities change at a granular level has far-reaching societal impact --- from facilitating better urban planning and infrastructure assessment to enabling more sustainable social and environmental interventions \cite{Daniel2015Goal1M,Seto2017SustainabilityIA}. 
Current measurements of urban change rely on datasets ranging from survey data such as American Community Survey (ACS), to government open data like construction permits, to remote sensing data such as satellite and aerial imagery. However, survey data often fall short of spatial and temporal granularity \cite{hwang14}, and top-down perspectives from the remote sensing data may not adequately represent the street-level changes that directly impact the daily lives of urban residents. And some construction permits data are not universally accessible.
Street view imagery, on the other hand, offers a high-resolution and frequently updated visual representation of urban environments from a ground-level perspective \cite{Huang2022DetectingNG}. By curating and analyzing the time series data of street view imagery, we can establish a more precise and direct proxy for how cities evolve over time.
\input{figure/main}
Previous studies on street view change detection have primarily focused on comparing pairwise images from identical locations but at different times, utilizing pixel-level annotations \cite{Sakurada2015ChangeDF,sakurada2020weakly}, which is similar to change detection tasks using satellite imagery \cite{Leenstra2021SelfsupervisedPE,Shi2021ADS}. Recent works have also demonstrated the applicability of street view pairwise change detection by collecting large-scale historical street view datasets and apply them on a range of urban applications, such as mapping out physical improvements and declines in cities, as well as correlating with socio-economic attributes and neighborhood gentrification status \cite{Naik7571, Huang2022DetectingNG}. 

However, unlike satellite imagery, street view imagery can be more susceptible to noisy signals, such as varying camera angles and noisy background elements like shadows and lighting changes. Additionally, existing street view datasets for change detection tasks are often limited in both spatial and temporal scales due to the fact that pixel-level semantic annotations can be costly. Such constraints hamper the model's generalizability and scalability, making it challenging to directly apply them to downstream tasks and thus restricting their potential social impacts. 
% Additionally,  In, these works fall short in providing an open, transferable model and corresponding benchmark datasets. 

To address these challenges, we first introduce a comprehensive multi-city street view time series dataset with image-level semantic labels, and then propose an end-to-end framework to detect urban change with street view data. We demonstrate the effectiveness of our approach with a fine-grained assessment of urban change across Seattle, Washington. Specifically, our major contributions in this study are threefold: 
\begin{itemize}
    \item We collect and curate a Google Street View (GSV) time series dataset, covering more than $1000$ coordinates across $6$ different cities, which is the largest street view change detection dataset available up to date. Each street view time series is labeled with change or no change on the image level, and each series has an average length of $10$ images, covering a time interval of $16$ years (from $2007$ to $2023$). We further validate the benefits of the time series data over pairwise data in our experiment.
    \item We propose an end-to-end change detection pipeline that effectively learns feature representations with semantic contexts from street view time series data, which allows the model to not only extract object shape, color, and structural information of the built environment, but also mitigate noisy effects from lighting changes and angle misalignment, enhancing the overall robustness of the change detection.
    \item Our method enables scalable applications for urban scene change detection, providing a more accurate proxy for assessing neighborhood socio-economic status changes. We demonstrate the efficacy of our approach by evaluating its correlation with social-demographic data and comparing against construction permits through a case study in Seattle.
\end{itemize}
% Figure showing the assessment visualization \todo{Figure}.

\section{Related work}
% \textbf{Engagement with literature: Shows an excellent understanding of other literature on the problem, including that outside computer science.}
\subsection{Urban change assessment}
Measuring physical change in urban environments offers profound insights into urban policies and economics, illuminating housing value trends, shifts in urban areas' roles, and spatial segregation effects \cite{Temkin1996NeighborhoodCA,Hwang2020UnequalDG}. It also has significant values in a various downstream tasks, such as detecting neighborhood gentrification \cite{hwang_gentrification}, monitoring the disaster recovery \cite{Stevenson2010UsingBP}. Prior research primarily utilizes satellite imagery for large-scale urban change detection \cite{Pandey2015UrbanizationAA,Daudt2018UrbanCD}. However, satellite data are hindered by constraints in spatial and temporal resolution, and lack detail on fine-grained and street-level changes. Several researchers use building permits data as a fine-grained proxy for physical urban change \cite{Stevenson2010UsingBP,Strauss2013DoesHD}. While these data also have limitations in availability and spatial coverage and may not accurately represent actual changes due to potential delays, as indicated by our evaluations.

\subsection{Street view imagery}
Street view imagery have been used in a wide range of applications in urban studies, such as quantifying urban greenery \cite{Li2015AssessingSU}, indicating region functions \cite{Gong2019ClassifyingSS}, revealing economic and social-demographic patterns\cite{gebru2017using,wang2020urban2vec,tian2021}, predicting populace' well-beings \cite{Lee2021PredictingLI} and estimating building energy efficiency \cite{MAYER2023120542}. 
It demonstrates substantial value as a medium closely reflecting human perception of the city. 
Moreover, recent studies analyzed historical street view data in the temporal dimension to map physical improvement and decline in the built environment and uncover how cities changed over time \cite{Naik2014StreetscoreP,Naik7571,Huang2022DetectingNG}. However, existing methods rely on comparing pairs of historical street views for each location rather than a comprehensive time series of street view data, which is limited in tracking the complete range of transformations within urban environments.

\subsection{Change detection}
Change detection is commonly presented in the field of remote sensing, as the task to identify changes between the pixel-level features of two temporally separated images. Previous works have trained convolutional network, recurrent network and Siamese network for change detection task on satellite imagery \cite{Gong2016ChangeDI,Daudt2018FullyCS,Lyu2016LearningAT}. Recent works also explored self-supervised pretraining and unsupervised learning in change detection to rely less on labels and generate meaningful representations for other downstream tasks \cite{Cong2022SatMAEPT,Mall2022ChangeED}. Unlike satellite imagery, Street view change detection faces additional challenges such as noisy signals including shades and angle misalignments due to the less fixed and more variable nature of acquiring street-level visual data. Previous works introduced benchmarking datasets for scene change detection and adopted deconvolutional networks or temporal attention networks \cite{Alcantarilla2016StreetviewCD,Sakurada2015ChangeDF,Chen2021DRTANetDR}. However, current research not only lacks comprehensive benchmark datasets with expansive spatial and temporal coverage, but also falls short of the model generalizability, limiting their societal impact. To tackle this, we introduce the largest image-level change detection dataset to date, featuring a complete time series of street views for each sampled location, applied at the city scale.

\input{figure/data_geo}
% \subsection{Self-supervised Pretraining}
\section{Methods}
% \textbf{Novelty of approach:Introduces a new model, data gathering technique, algorithm, and/or data analysis technique.}
\subsection{Problem Statement}
\begin{definition}
\textbf{Street view time series.}
Each street view time series, comprises $n$ street view images depicting the consistent street-level scene $s^{(i)}=(s^{(i)}_{1}, s^{(i)}_{2}, ... , s^{(i)}_{n})$. These images are chronologically arranged such that $s^{(i)}_{k}$ corresponds to the timestamp $t^{(i)}_k$.
\end{definition}

\begin{definition}
\textbf{Urban change point.}
In the street view time series $s^{(i)}$, the image $s^{(i)}_{c}$ is identified as an urban change point if the built environment in $s^{(i)}_{c}$ exhibits deviations (e.g., building constructions) relative to preceding images.
\end{definition}

Our objective is to accurately and efficiently detect urban change points in street view time series. To achieve this, we begin by creating a large street view time series dataset, followed by proposing an end-to-end training and evaluation pipeline.

\subsection{Street view time series dataset}
To start, we sample the geospatial coordinate for each street-level scene $s^{(i)}$. Specifically, the coordinate of each building is determined by computing the centroid of its footprint polygon, using the Microsoft building footprint dataset.
After locating scene $s^{(i)}$, we gather all the available historical street view metadata and subsequently download the associated images using their panoid ID. For each scene $s^{(i)}$, we retrieve the nearest-photographed panorama. The image heading is subsequently determined, facing the building from the panorama's coordinate. All our street view images and meta data are sourced from the Google Static Street View API.
\input{table/dataset}

In total, we select $931$ locations and retrieve their corresponding street view time series, which consist of $10,878$ images. We then annotate each time series $s^{(i)}$ to identify the urban change points $s_c^{(i)}$. Among them, $371$ time series have been labeled with a total of $433$ urban change points, while the remaining ones exhibit no substantial urban change. Figure \ref{fig:data_geo} demonstrates the geo-spatial distribution of our sampled street views. 
As is shown in table \ref{table:data_compare}, our dataset not only consists of more image pairs compared with previous scene change detection datasets TSUNAMI \cite{Sakurada2015ChangeDF} and PSCD \cite{sakurada2020weakly}, but also covers a significantly broader spatial and temporal scope. 


\subsubsection{Data partitioning.}
To train our change detection model on street view time series dataset labeled with urban change points, we introduce a partitioning scheme for generating training and evaluation sets as is shown in Figure \ref{fig:data_method}. For every street view time series $s^{(i)}$, we segment the views from $s_1^{(i)}$ to $s_n^{(i)}$ based on their occurrence relative to $s_c^{(i)}$. More precisely, suppose the time series $s^{(i)}$ has $q$ urban change points denoted as $s_{c_1}^{(i)}$, $\ldots$, $s_{c_q}^{(i)}$, we allocate the street view $s_j^{(i)}$ ($1\leq j\leq n$) to segment $seg(s_j^{(i)})$ as follows:
\begin{equation}
    seg(s_j^{(i)})=
    \begin{cases}
    1 & \text{if}\ j<c_1\\
    k & \text{if}\ c_1\leq j < c_q\ \text{and}\ c_{k-1}\leq k<c_k\\
    q+1 & \text{if}\ j \geq c_q
    \end{cases}
\end{equation}
\input{figure/data_method}
\input{figure/change_model}
For each street view time series $s^{(i)}$, we then generate a set of pairwise street view pairs by considering all combinations from $s_1^{(i)}$ to $s_n^{(i)}$. Each pair of samples is sorted in chronological order based on their timestamps, resulting in pairs like $(s_1^{(i)}, s_2^{(i)})$. The total number of such combinations for the time series $s^{(i)}$ with $n$ street views is $\binom{n}{2}$. The labeling of these pairwise samples is determined by their associated segments as follows:
\begin{equation}
    \textsc{label}{(s_a^{(i)},s_b^{(i)})}=
    \begin{cases}
        1 & \text{if}\ seg(s_a^{(i)})\neq seg(s_b^{(i)})\\
        0 & \text{if}\ seg(s_a^{(i)}) = seg(s_b^{(i)})
    \end{cases}
\end{equation}
\subsection{Change detection model}
To classify each pairwise pair $(s_a^{(i)},s_b^{(i)})$, we adopt a Siamese network to include a twin DINOv2 \cite{Oquab2023DINOv2LR} backboned module to realize a non-linear embedding from the input domain and a final linear layer transforming the concatenation of both images' hidden vectors and their distance, represented by their element-wise difference, into a scalar predictor as follows:
\begin{equation} \label{eq: sia_concat}
\mathbf{h}^{(i)}_{L} = \left[(\mathbf{h}^{(i)}_{l, L})^\top, (\mathbf{h}^{(i)}_{e, L})^\top, (\mathbf{h}^{(i)}_{l, L}-\mathbf{h}^{(i)}_{e, L})^{\top}\right]^{\top}
\end{equation}
Figure \ref{fig:change_model} visualizes the model architecture. We adopt a cross-entropy loss function to train such a urban change classifier, and let $\textsc{label}{(s_a^{(i)},s_b^{(i)})}$ be the label for the street view pair $(s_a^{(i)},s_b^{(i)})$.

% in the following form \todo{finalize loss function}:
% \begin{multline} \label{eq: bce}
%     \mathcal{L_S}(t^{(i)}) = \mathbf{y}(t^{(i)})\log\mathbf{p}(t^{(i)})+\\ (1-\mathbf{y}(t^{(i)}))\log(1-\mathbf{p}(t^{(i)})).
% \end{multline}

\section{Experiments}
% \textbf{Justification of approach: Thoroughly and convincingly justifies the approach taken, explaining strengths and weaknesses as compared to other alternatives.}\\
% \textbf{Quality of evaluation: Evaluation was exemplary: data described the real world and was analyzed thoroughly.}
To evaluate the efficacy of our proposed method, we conduct experiments from three perspectives: 1) Backbone models --- we benchmark the performance of selected visual foundational models in the context of street view change detection tasks. 2) Street view time series data --- we employ experiments to substantiate the advantage of time series data as a natural form of data augmentation \cite{seco}, compared with results achieved through artificial data augmentation. 3) Self-supervised pre-training --- we explore 2 pre-train methodologies using a larger-scale unlabeled street view dataset in order to evaluate the performance of a domain-specific pre-trained models for our change detection task.
% Firstly, we evaluate the performance of diverse generic visual foundational models in the context of street view image change detection tasks. 
% Subsequently, we employ experiments to substantiate the advantage of time series imagery, a natural form of data augmentation within our collected dataset, which not only showcases superiority over pairwise datasets but also significantly outperforms results achieved through artificial data augmentation. Lastly, we introduce two pre-train methodologies, which are applied to a larger-scale unlabeled dataset of street-level images. This enables a comparative analysis between domain-specific pre-trained models and generic foundational models concerning their respective performances in the street view image change detection task.
\subsection{Training details}
% Our labeled dataset comprises a total of $371$ street view time series, comprised of $4,465$ images, sampled from $371$ distinct locations across $5$ different cities. By employing the data partitioning methodology described in Figure \ref{fig:data_method}, the dataset can be transformed into a collection of $25,423$ image pairs with binary labels. 
Street view images differ from satellite imagery and high-quality object images in that they often have a lower signal-to-noise ratio, primarily due to varying camera positions and environment conditions as shown earlier. As a result, evaluating on a small-scale test set could suffer from a significant variance. To ensure a robust assessment, we randomly select $50\%$ of the street view time series in our dataset as the test set. It includes $25$ locations in Seattle, $13$ locations in San Francisco, $21$ locations in Oakland, $97$ locations in Los Angeles, and $29$ locations in Boston, constituting a total of $12,221$ image pairs. The remaining data are allocated with $90\%$ as the training set for model fine-tuning and $10\%$ as the validation set.
% which to $185$ temporal sequences, as the test set. It includes $25$ locations in Seattle, $13$ locations in San Francisco, $21$ locations in Oakland, $97$ locations in Los Angeles, and $29$ locations in Boston, constituting a total of $12,221$ image pairs. The remaining data are allocated with $90\%$ as the training set for model fine-tuning and $10\%$ as the validation set.
During fine-tuning, we employ the Adam optimizer to train models with a learning rate set at $1 \times 10^{-5}$ and a batch size of $16$. The global norm of gradients is clipped to be $\leq 0.5$, and we use random weight averaging for optimization. Our training and evaluation are conducted on $4$ Nvidia Tesla T4 GPUs. For all the backbone models, we experimented with two common approaches: global fine-tuning and linear probing, i.e. training with the backbone network frozen.

\subsubsection{Backbone models.}
We initiate our evaluation by assessing the performance of 4 pre-trained generic visual models—ResNet101 \cite{he2016deep}, DINO \cite{dino}, CLIP \cite{radford2021learning}, and DINOv2 \cite{Oquab2023DINOv2LR}—as backbone networks. For ViT-based models such as DINO and DINOv2, we experiment using the CLS Token as the backbone output.
% For ViT-based models such as DINO and DINOv2, we experiment using either the CLS Token or the patch features as the backbone output, retaining the best results on the test set. When utilizing patch features as the output, we incorporate both a linear and a non-linear convolutional module ($\textit{Conv2d} \rightarrow \textit{BatchNorm} \rightarrow \textit{ReLU} \rightarrow \textit{Conv2d}$). Their outputs are then summed and dimensionally reduced to $100$ using a $1 \times 1$ convolutional kernel size.
% Specifically when using patch features as the output, we introduce a linear and a non-linear convolutional module, followed by the summation of their outputs, to reduce the dimensionality of the patch features to $100$, with a $1 \times 1$ convolutional kernel size. % The non-linear convolutional module comprises the sequence: $Conv2d \rightarrow BatchNorm \rightarrow ReLU \rightarrow Conv2d$.
% For the prediction layers after the backbone model, we explored both linear prediction layers and non-linear prediction heads composed of $\textit{Linear }\rightarrow \textit{BatchNorm} \rightarrow \textit{ReLU} \rightarrow \textit{Linear}$ sequences. 
% Table \ref{table:backbones} presents a comprehensive display of the best performances achieved by each respective backbone network on our test set.
\input{table/backbone}
\input{table/timeseries}
\subsubsection{Time series data.}
% with DINOv2 as the backbone network, we utilize the architecture in Figure \ref{fig:change_model} to fine-tune the model. 
To validate the advantages of our proposed street view time series dataset, we constructed a pair-based dataset similar to TSUNAMI \cite{Sakurada2015ChangeDF} and PSCD \cite{sakurada2020weakly} dataset. Specifically, for each street view time series, we randomly sample $2$ images as a pair. We conduct model fine-tuning on this pair-based dataset and evaluated its performance on the test set described earlier.
% In sequences where changes occurred (positive sequences), we ensured the formation of one positive sample pair and one negative sample pair. This resulted in a pairwise dataset comprising $336$ sets of image pairs. 
To align the pair-based dataset with the size of our time series dataset, we randomly apply a combination of standard image augmentation techniques, including horizontal flip, color jitter, grayscale, and Gaussian blur. It seeks to validate our hypothesis that time series images, serving as natural augmentation, are more effective than artificial augmentations to supervise change detection model amidst noisy signals, thus bolstering its robustness.
% Assuming that we randomly sample $n$ pairs of images from time series $s^{(i)}$ (where $n\ mod\ 2 = 0$), the quantity for augmentation $a_{s^{(i)}}$ in $s^{(i)}$ can be determined using:
% \begin{equation}
% \begin{aligned}
%     a_{s^{(i)}} = \frac{n}{2} &\times [\mathrm{Ceil}(\frac{2 \times \text{\# positive image pairs}}{n})\\
%     &+\mathrm{Ceil}(\frac{2 \times \text{\# negtive image pairs}}{n})-2]\\
% \end{aligned}
% \end{equation}
% if $s^{(i)}$ is a positive sequence, or:
% \begin{equation}
% \begin{aligned}
%     &a_{s^{(i)}} = n \times [\mathrm{Ceil}(\frac{\text{\# image pairs}}{n})-1]
% \end{aligned}
% \end{equation}
% if $s^{(i)}$ is a negative sequence.

% We delved into different combinations of random data augmentation techniques and progressively increased the sampling quantity n from $2$ to $10$. This exploration aims to verify our hypothesis that, in contrast to artificial data augmentation, time series images as a form of natural augmentation can better enable the model to adapt to environmental noise, thereby enhancing its robustness. The results are presented in Table \ref{table:timeseries}.

%\subsection{Baselines}
%\begin{itemize}
%    \item[1] DINO (**freeze Resnet**, **freeze CLIP**, **DINO\_v2**) (DINO+?) (heading+batch norm) - zero-shot
%    \item[2] Time Series (vs random augmentation)
%    \item[3] Pretrain (BYOL, segmentation+BYOL, **SatMAE**) (**$10\%$ samples**) 
%\end{itemize}

\subsubsection{Self-supervised pre-training.}
% \todo{Model structure digram}\\
% With the advancement of Natural Language Processing (NLP), an increasing body of research is dedicated to the training of foundational models in Computer Vision. These models possess the capacity to generate generic visual features and enhance performance across downstream tasks. The majority of these foundational models are established through large-scale self-supervised training, 
Recent studies on self-supervised pre-training highlight its efficacy to extract image features when labels are limited and enhance performance in downstream tasks. Specifically, 2 primary branches of self-supervised pre-training are pursued: intra-image self-supervised training \cite{He2022masked,satmae2022} and discriminative self-supervised learning \cite{grill2020bootstrap}. Correspondingly, we adapt 2 pre-training procedures on street view data and benchmark their performance on the change detection task --- StreetMAE and StreetBYOL. StreetMAE uses masked autoencoders \cite{He2022masked} to reconstruct randomly masked patches in street view imagery. It also incorporates temporal encoding to represent each street view time series as a contextual sequence. StreetBYOL, on the other hand, is a self-distillation approach building upon the online and target networks \cite{grill2020bootstrap}. While retaining pivotal components such as the prediction head and the stop gradient mechanism, we try add an unsupervised segmentation head \cite{hamilton2021unsupervised} to identify building pixels and feed them alongside the original images into the networks in experiment seg+StreetBYOL. We adopt the ViT-B/16 architecture as the backbone network and initialize it with the parameters from DINO pre-trained model. Pre-training is conducted on an unlabeled dataset comprising 150,000 street view images randomly sampled in our studied areas. To mitigate noise interference, we apply a filtering process to remove images where the proportion of building pixels was less than $2\%$. After the pre-training phase, we plug it into the Siamese network and fine-tune the model on our labeled training set. 
% Figure \todo{Figure} shows a diagram of two self-supervised learning methods.

% We also introduce a self-distillation approach building upon the BYOL framework \cite{grill2020bootstrap}, which is in the second line of self-supervised computer vision work, i.e., discriminative self-supervised learning. While retaining pivotal components such as the prediction head and the stop gradient mechanism, we employ an unsupervised semantic segmentation method named STEGO \cite{hamilton2021unsupervised} to segment images, isolating architectural elements. Specifically, we input images containing solely the architectural segments, alongside unaltered complete images, into student and teacher networks, respectively. This scheme serves to guide the model in extracting features closely associated with architectural attributes. Additionally, we also explored an alternative approach using diverse random transformations on the original images, separately as inputs to the teacher and student networks. However, empirical results distinctly demonstrate the superiority of the former approach.

% The first approach  based on MAE, involves the random masking of certain patches within images and training the model to reconstruct these masked regions. We extend the MAE framework to sequence level, considering independent patch masking across different images while incorporating temporal encoding during training. 
% Figure \ref{MAE_visual} shows the masked and reconstructed patches. We anticipated that the sequence-level GSVMAE can effectively capture the inter-image contextual information within sequences, thereby attenuating noise that fluctuate temporally. 
% For instance, intuitively, when a patch within an image is masked while the corresponding patch in preceding or succeeding images remains unmasked, the model can leverage information from other images to facilitate patch reconstruction. Consequently, the model becomes adept at encoding more stable features present within the temporal sequence, such as those closely tied to architectural structures.

% i.e. design of pretext tasks \cite{He2022masked}, or through text-guided training \cite{radford2021learning}. Conversely, there exist studies that substantiate the superiority of domain-specific pre-trained models in contrast to generic foundational models, particularly within specific domains and tasks, showcasing heightened performance and superior generalization capabilities \cite{satmae2022}.


% In our work, we embark upon an exploration centered around the domain-specific pre-trained models' potential enhancements vis-à-vis the generic foundation models in the context of street view image change detection tasks.

% As previously elucidated, street view images exhibit a comparatively lower signal-to-noise ratio when contrasted with high-quality object image datasets. While our specific focus on detecting temporal changes in architectural structures, variations stemming from factors such as capturing perspective, lighting conditions, color variations, and vegetation dynamics engender substantial interference in the detection process. Thus, our aspiration is for the SSL methods to acquire an enhanced proficiency in learning object attributes like shape and structure, which facilitates the encoding of visual features that encompass a more comprehensive representation of object structure and spatial positioning. 
\input{table/pretrain}
\input{figure/res_noise}
\input{figure/seattle}
\section{Results and discussion}
As shown in Table \ref{table:backbones}, DINOv2 has demonstrated the best performance in our evaluation, achieving $88.85\%$ accuracy through fine-tuning. Notably, considering the presence of challenges like shadows and occlusions in the images, human performance in this change detection task is approximately around $90\%$ during our labeling process. This observation suggests that fine-tuning DINOv2 as the backbone network has enabled the model to approach human-level performance. Furthermore, freezing the DINOv2 network and training only the linear layers surpass the fine-tuning outcomes of all other backbone networks, strongly affirming the capacity of DINOv2 to generate potent visual features suitable for change detection. 
% It is worth noting that, when using patch features as the output and employing non-linear prediction heads, DINO achieves the best result as indicated in the table. On the other hand, DINOv2 shows distinct behavior; its optimal performance is attained when using the CLS Token as the output and employing a single linear layer as the prediction head.

We find the performance of the change detection model fine-tuned on the pairwise dataset is significantly lower than its performance attained after fine-tuning on our time series dataset, as illustrated in table \ref{table:timeseries}. 
Moreover, augmenting the dataset using artificial techniques can lead to adverse effects. 
% This phenomenon could be attributed to the introduction of additional noise and interference into the building elements of the street view images. Typically, artificial data augmentation involves transformations applied to the entire image, resulting in alterations in the appearance of buildings within the images, which subsequently disrupts the ability of the model to detect architectural changes.
As a form of natural data augmentation, time series data equips the model with sufficient information to identify and filter out irrelevant variations that occur over time, such as changes in lighting, vegetation, and vehicles as is shown in Figure \ref{fig:res_noisy}, which guides the model to focus on more temporally stable elements such as building structures. These results validate the critical role of our proposed time series data in the context of street view image change detection task.

% By progressively increasing the number of sampled image pairs from the sequences, we observe a gradual improvement in model performance. This underscores the essential significance of time series data. 
The performance of the street-view pre-trained models is presented in Table \ref{table:pretrain}. The results of StreetMAE are significantly lower compared to those of StreetBYOL. This may be because patch reconstruction process is more prone to learning color and texture information, which aligns with the noise we aim to eliminate in change detection, rather than building structure. The addition of the semantic segmentation module leads to a performance enhancement in StreetBYOL. Nevertheless, domain-specific pre-trained models, whether StreetMAE or StreetBYOL, do not surpass the performance of the generic visual model DINOv2. This can be attributed to the smaller training data size used for domain-specific pre-training compared to generic visual models. Specifically, the inherently noisy street view images, with their complex and cluttered scenes, make it difficult for models to grasp fundamental concepts like shape, location, and architecture from limited data.
% This could be attributed, on one hand, to the considerably smaller training data volume used for domain-specific pre-training compared to the vast data utilized by generic visual models. On the other hand, this is also caused by the inherently noisy street view images as training data, often comprising complex and cluttered scenes that make it challenging for models to learn fundamental concepts about shape, location, environment, and architecture from a limited set of data. 
% Given the relatively high annotation cost of street view images and the substantial amount of unannotated data that persists, the task of curating the data and designing larger-scale pre-training methods to enhance the accuracy of change detection remains a noteworthy avenue for exploration.

\section{Case study: Assessing urban change in Seattle}
% \textbf{Scope and promise for social impact: Likelihood of social impact is extremely high: the paper’s ideas are already being used in practice or could be immediately.}
To evaluate the generalizability of our proposed change detection model on a large scale, we prepare a large-scale street view time series data for the city of Seattle, Washington. Figure \ref{fig:seattle} demonstrates the sampling process. We then apply our change detection model to identify urban change points: In total, we detect $11,838$ change points from $795,919$ sampled images in Seattle. 
% Our data shows significant correlation with change in median household income and population size in each census tract.
% \begin{itemize}
%     \item visualization
%     sampled results (corner cases)
%     \item correlation benchmark with permits data
%     \item ACS prediction
% \end{itemize}
\subsubsection{Construction permits data.}
% We obtain permits data fro Seattle 
% As mentioned earlier, construction permits data are often adopted as a fine-grained proxy for urban change in literatures. To compare with our change detection model, We fetch construction permit data from the online permit center of the Seattle city government\footnote{Available at \url{https://data.seattle.gov/Permitting/Building-Permits}}. 
As previously noted, previous works frequently rely on construction permit data as a detailed proxy for urban evolution. To further compare them with the results from our proposed change detection model, we obtain construction permit data from the Seattle city government's online permit center.
Each entry in the permit data provides details such as the date of issuance, permit category, geospatial coordinates, estimated cost, and other requisite information as mandated by the government.
As a data pre-processing step, we keep the ``new'', ``alteration'' and ``addition'' categories to align with the definition of urban change points. Additionally, we curated a subset of permits that had a total estimated cost exceeding $\$100,000$ within a single year. This approach allows us to compare our findings with both the complete permit dataset and the high-value permits, the latter of which are more probable to signify visible physical alterations.

% This way, urban change recorded by our filtered permits is significant enough to be viewed as potential signals of physical change.
\input{figure/acs}
\subsubsection{Correlation with social-demographic data.} 
We prepare social-demographic data at the census tract level from the American Community Survey (ACS) 5-year estimates. Specifically, we select population size and median household incomes as our target variables, and calculate their relative percentage change from 2009 to 2021 for each census tract in Seattle. 
To quantify the linear correlation between proxies for urban change and shifts in socio-demographics, we compare three distinct proxies: the entire set of permits, high-value permits (those exceeding $\$100$k), and the percentage of locations with urban change points identified using our proposed methodology.
As is shown in Figure \ref{fig:acs}, both the entire set of permits and the high-value permits fail to show a linear correlation with the change of median household income and population size in each census tract with $p$ value larger than $0.05$ and $R^2$ close to $0$. 
While the change points results from our proposed method reach an $R^2$ of $0.19$ and $0.15$ for median household income change and population size change respectively, and achieve a $p$ value much less than $0.05$ supporting the statistical significance.
These results not only indicate that the detected urban change points provide a more accurate assessment of real-world urban transformations and socio-economic shifts, but also validates that our proposed change detection model can effectively complement existing proxies as a credible indicator of urban change.

\section{Conclusion and Future Work}
In this work, we propose a framework to assess fine-grained urban change at scale with street view time series. We have curated the largest street-level scene change detection dataset by far, and proposed an end-to-end change detection pipeline to identify urban change points at scale. We validate the proposed model by correlating with social-demographic data and prove its potential as a high-definition, up-to-date, and on-the-ground visual proxy of urban change.

While our data-driven approach provides a novel method to assess urban change, it is still subjective to a few limitations: 1) Street view data focus on changes observable at the street level, excluding alterations that might be non-visible, such as interior renovations. 2) The spatial-temporal distribution of Google Street View data is not consistent. Since its debut in 2007, Google has frequently updated its imagery in countries such as the US, but has been less consistent in updating images in many other regions, especially in developing countries. 
Despite these limitations, we believe our proposed method offer a comprehensive and extensive resource for urban change detection task, helping expand its social impact and advance sustainable development goals. 
% More broadly, identifying fine-grained urban change will benefit urban planing and advancing sustainable development goals. 
In future works, we can explore multi-task models to identify changes in a wider array of objects, 
% Additionally, our method can be integrated with satellite change detection systems, 
enhancing the applicability to a broader range of downstream tasks in cities.

\section{Acknowledgments}
This project was supported by the Google Cloud Grant from the Stanford Institute for Human-Centered Artificial Intelligence. The author would like to thank Zhecheng Wang, Sarthak Kanodia and Timothy Dai for their extensive guidance.
\bibliography{aaai24}

\end{document}
