\section{Introduction}
%%% motivation - general explanation of RL agents behavior
With the maturing of reinforcement learning (RL) methods, RL-based agents are
being trained to perform complex tasks in various domains, including robotics,
healthcare and transportation. Importantly, these agents do not operate in a
vaccum  -- people interact with agents in a wide range of settings. Effective
interaction between an agent and a user requires from the latter the ability to
anticipate and understand its behavior. For example, clinicians should
understand treatment regimes recommended by agents to determine their viability.

To facilitate improved user understanding of agents' behavior, a
range of explainable RL methods have been developed
\cite{XRL_survey,heuillet2021explainability}. These can be divided into two explanation categories: (1) ``local'' explanation approaches explaining why an agent chose
a particular action in a given state, e.g., saliency maps highlighting the
information an agent attends to~\cite{greydanus2017visualizing}, and (2)
``global'' explanation methods that describe the policy of an agent more
generally, such as strategy summaries that demonstrate the agents' behavior in a
selected set of states~\cite{amir2019summarizing}. While such global approaches have been shown to improve people's understanding of agent behavior,
they are not optimized for specific user needs.

%%% more specific motivation
In this work, we aim to enhance users' ability to distinguish between the
behavior of different agents. Such scenarios arise when people are required to
select an agent from a set of alternatives. E.g., a user might need to choose
from a variety of smart home assistants or self-driving cars available on the
market. Importantly, there often isn't a clear ``ground-truth'' for which agent
is superior, as agents may prioritize different outcomes and users may differ in their preferences. For example, some users may prefer self-driving cars that
value safety very highly, while others might be willing to relax safety
considerations to a degree to allow for faster driving. The ability to
distinguish policies is also important for model developers, as different
configurations of reward functions and algorithm parameters can lead to
different behaviors in unexpected ways, especially in domains where the reward
function is not obvious, such as healthcare \cite{gottesman2019guidelines}. 

%%% existing policy summaries do not fully address the task of supporting agent
%comparison
One possible approach for helping users distinguish between policies of agents
is to use strategy summarization
methods~\cite{amir2018agent,amir2019summarizing}. 
% These methods demonstrate the behavior of agents in a selected set of
% world-states based on some criteria, such as the importance of a
% decision~\cite{amir18highlights,huang2018establishing}. Ofra: removed as we'll
% discuss this in related work or an ability to reconstruct the agents'
% policy~\cite{lage2019exploring,huang2017enabling}. 
Using these methods, a summary is generated for each agent, allowing the user to
compare their behavior. 
% While these approaches have been shown to be effective in portraying agent
% behavior, they are not optimized for
However, these approaches are not optimized for the task of agent comparison, as
each summary is generated \emph{independently}. For instance, the HIGHLIGHTS
algorithm~\cite{amir18highlights} selects states based on their importance as
determined by the differences in Q-values for alternative actions in a given
state. If two high-quality agents are compared, it is possible that they will
consider the same states as most important, and will choose the same (optimal)
action in those states, resulting in similar strategy summaries. In such a case,
even if the agents' policies differ in numerous regions of the state-space, it
will not be apparent from the summaries.

%%% the disagreements-based summary approach
This work presents the \disalg~ algorithm which is optimized for comparing
agent policies.
%%% we use Disagreements to...
Our algorithm compares agents by simulating them in parallel and identifying
disagreements between them, i.e. situations where the agents' policies differ.
These disagreement states constitute behavioral differences between agents
and are used to generate visual summaries optimized towards showcasing the most
prominent conflicts, thus providing contrastive information. Our
approach assumes access to the agent’s strategy, described using a Markov
Decision Process (MDP) policy, and quantifies the importance of disagreements by
making use of agents’ Q-values.

%%% experimental results
To evaluate \disalg~, we conducted two human-subject experiments to test the following properties of the algorithm: Firstly, whether it improves users' ability to
identify a superior agent when one exists, i.e. given ground-truth performance measures. Secondly, does it help convey differences in the behavior of agents to users.
% i.e.
% when superiority is not the name of the game.
Both
experiments make use of HIGHLIGHTS summaries as a baseline for comparison.
Results indicate a significant improvement in user performance for the agent
superiority identification task, while not falling short in the behaviour conveying task, as compared to HIGHLIGHTS.


% generated summaries comparing agents playing the game of Frogger
% \cite{Sequeira2020} % from the InterestingnessXRL framework
% \cite{Sequeira2020} and evaluated them in a human-subject experiment. We
% compare these summaries to ones produced by HIGHLIGHTS, which we use as a
% baseline. In the experiment, participants were shown summaries of different
% Frogger agents which varied in their performance. They were asked to select
% the better performing agent and answered explanations satisfaction questions.
% % They were also % asked to rate their satisfaction from the explanation
% method for the task of evaluating the agent’s % capabilities. Results show
% that \disalg~ led to improved user performance on the agent selection task,
% compared to HIGHLIGHTS. % ability of users to identify % the better performing
% agent compared to HIGHLIGHTSץ % , and increased explanation satisfaction
% ratings. % and increased the explanation satisfaction of % participants.

%%% summary of contributions
Our contributions are threefold: \textit{i)} We introduce and formalize the
problem of comparing agent policies;  \textit{ii)} we develop \disalg~, an
algorithm for generating visual contrastive summaries of agents' behavioral
conflicts, and \textit{iii)} we conduct human-subject experiments, demonstrating
that summaries generated by \disalg~ lead to improved user performance compared
to HIGHLIGHTS summaries.
% were preferred by participants and improved their ability to assess agent
% performance compared to the baseline summaries.\\
