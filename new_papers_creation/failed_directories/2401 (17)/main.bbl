\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Abbasi-yadkori, P\'{a}l, and
  Szepesv\'{a}ri(2011)}]{NIPS2011_e1d5be1c}
Abbasi-yadkori, Y.; P\'{a}l, D.; and Szepesv\'{a}ri, C. 2011.
\newblock Improved Algorithms for Linear Stochastic Bandits.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}.

\bibitem[{Achiam et~al.(2017)Achiam, Held, Tamar, and
  Abbeel}]{achiam2017constrained}
Achiam, J.; Held, D.; Tamar, A.; and Abbeel, P. 2017.
\newblock Constrained policy optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}.

\bibitem[{Altman(1999)}]{altman1999constrained}
Altman, E. 1999.
\newblock \emph{Constrained Markov decision processes}, volume~7.
\newblock CRC Press.

\bibitem[{Amani, Thrampoulidis, and Yang(2021)}]{amani2021safe}
Amani, S.; Thrampoulidis, C.; and Yang, L. 2021.
\newblock Safe reinforcement learning with linear function approximation.
\newblock In \emph{International Conference on Machine Learning (ICML)}.

\bibitem[{Ames et~al.(2019)Ames, Coogan, Egerstedt, Notomista, Sreenath, and
  Tabuada}]{ames2019control}
Ames, A.~D.; Coogan, S.; Egerstedt, M.; Notomista, G.; Sreenath, K.; and
  Tabuada, P. 2019.
\newblock Control barrier functions: Theory and applications.
\newblock In \emph{European control conference (ECC)}.

\bibitem[{Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman,
  and Man{\'e}}]{amodei2016concrete}
Amodei, D.; Olah, C.; Steinhardt, J.; Christiano, P.; Schulman, J.; and
  Man{\'e}, D. 2016.
\newblock Concrete problems in {AI} safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}.

\bibitem[{Asadi, Misra, and Littman(2018)}]{asadi2018lipschitz}
Asadi, K.; Misra, D.; and Littman, M. 2018.
\newblock Lipschitz continuity in model-based reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}.

\bibitem[{Auer and Ortner(2007)}]{auer2007logarithmic}
Auer, P.; and Ortner, R. 2007.
\newblock Logarithmic online regret bounds for undiscounted reinforcement
  learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}.

\bibitem[{Bennett, Misra, and Kallus(2023)}]{bennett2023provable}
Bennett, A.; Misra, D.; and Kallus, N. 2023.
\newblock Provable Safe Reinforcement Learning with Binary Feedback.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTAT)}.

\bibitem[{Buckman, Gelada, and Bellemare(2020)}]{buckman2020importance}
Buckman, J.; Gelada, C.; and Bellemare, M.~G. 2020.
\newblock The importance of pessimism in fixed-dataset policy optimization.
\newblock \emph{arXiv preprint arXiv:2009.06799}.

\bibitem[{Bura et~al.(2022)Bura, HasanzadeZonuzy, Kalathil, Shakkottai, and
  Chamberland}]{bura2022dope}
Bura, A.; HasanzadeZonuzy, A.; Kalathil, D.; Shakkottai, S.; and Chamberland,
  J.-F. 2022.
\newblock DOPE: Doubly optimistic and pessimistic exploration for safe
  reinforcement learning.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}.

\bibitem[{Cheng et~al.(2019)Cheng, Orosz, Murray, and Burdick}]{cheng2019end}
Cheng, R.; Orosz, G.; Murray, R.~M.; and Burdick, J.~W. 2019.
\newblock End-to-end safe reinforcement learning through barrier functions for
  safety-critical continuous control tasks.
\newblock In \emph{AAAI conference on artificial intelligence (AAAI)}.

\bibitem[{Chow et~al.(2017)Chow, Ghavamzadeh, Janson, and
  Pavone}]{chow2017risk}
Chow, Y.; Ghavamzadeh, M.; Janson, L.; and Pavone, M. 2017.
\newblock Risk-constrained reinforcement learning with percentile risk
  criteria.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 18(1):
  6070--6120.

\bibitem[{Dulac-Arnold et~al.(2021)Dulac-Arnold, Levine, Mankowitz, Li,
  Paduraru, Gowal, and Hester}]{dulac2021challenges}
Dulac-Arnold, G.; Levine, N.; Mankowitz, D.~J.; Li, J.; Paduraru, C.; Gowal,
  S.; and Hester, T. 2021.
\newblock Challenges of real-world reinforcement learning: definitions,
  benchmarks and analysis.
\newblock \emph{Machine Learning}, 1--50.

\bibitem[{Faury et~al.(2020)Faury, Abeille, Calauz{\`e}nes, and
  Fercoq}]{faury2020improved}
Faury, L.; Abeille, M.; Calauz{\`e}nes, C.; and Fercoq, O. 2020.
\newblock Improved optimistic algorithms for logistic bandits.
\newblock In \emph{International Conference on Machine Learning (ICML)}.

\bibitem[{Filippi et~al.(2010)Filippi, Cappe, Garivier, and
  Szepesv{\'a}ri}]{filippi2010parametric}
Filippi, S.; Cappe, O.; Garivier, A.; and Szepesv{\'a}ri, C. 2010.
\newblock Parametric bandits: The generalized linear case.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}.

\bibitem[{Garc{\i}a and Fern{\'a}ndez(2015)}]{garcia2015comprehensive}
Garc{\i}a, J.; and Fern{\'a}ndez, F. 2015.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 16(1):
  1437--1480.

\bibitem[{Jin, Yang, and Wang(2021)}]{jin2021pessimism}
Jin, Y.; Yang, Z.; and Wang, Z. 2021.
\newblock Is pessimism provably efficient for offline rl?
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Koller et~al.(2018)Koller, Berkenkamp, Turchetta, and
  Krause}]{koller2018learning}
Koller, T.; Berkenkamp, F.; Turchetta, M.; and Krause, A. 2018.
\newblock Learning-based model predictive control for safe exploration.
\newblock In \emph{IEEE conference on decision and control (CDC)}.

\bibitem[{Li, Lu, and Zhou(2017)}]{li2017provably}
Li, L.; Lu, Y.; and Zhou, D. 2017.
\newblock Provably optimal algorithms for generalized linear contextual
  bandits.
\newblock In \emph{International Conference on Machine Learning (ICML)}.

\bibitem[{Lin et~al.(2021)Lin, Hu, Shi, Sun, Qu, and
  Wierman}]{lin2021perturbation}
Lin, Y.; Hu, Y.; Shi, G.; Sun, H.; Qu, G.; and Wierman, A. 2021.
\newblock Perturbation-based regret analysis of predictive control in linear
  time varying systems.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:
  5174--5185.

\bibitem[{Ok, Proutiere, and Tranos(2018)}]{ok2018exploration}
Ok, J.; Proutiere, A.; and Tranos, D. 2018.
\newblock Exploration in structured reinforcement learning.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 31.

\bibitem[{Rockafellar, Uryasev et~al.(2000)}]{rockafellar2000optimization}
Rockafellar, R.~T.; Uryasev, S.; et~al. 2000.
\newblock Optimization of conditional value-at-risk.
\newblock \emph{Journal of risk}, 2: 21--42.

\bibitem[{Roderick, Nagarajan, and Kolter(2021)}]{roderick2021provably}
Roderick, M.; Nagarajan, V.; and Kolter, Z. 2021.
\newblock Provably safe PAC-MDP exploration using analogies.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTAT)}.

\bibitem[{Stooke, Achiam, and Abbeel(2020)}]{stooke2020responsive}
Stooke, A.; Achiam, J.; and Abbeel, P. 2020.
\newblock Responsive safety in reinforcement learning by pid lagrangian
  methods.
\newblock In \emph{International Conference on Machine Learning (ICML)}.

\bibitem[{Strehl and Littman(2008)}]{strehl2008analysis}
Strehl, A.~L.; and Littman, M.~L. 2008.
\newblock An analysis of model-based interval estimation for {Markov} decision
  processes.
\newblock \emph{Journal of Computer and System Sciences}, 74(8): 1309--1331.

\bibitem[{Tessler, Mankowitz, and Mannor(2018)}]{tessler2018reward}
Tessler, C.; Mankowitz, D.~J.; and Mannor, S. 2018.
\newblock Reward constrained policy optimization.
\newblock \emph{arXiv preprint arXiv:1805.11074}.

\bibitem[{Tsukamoto, Chung, and Slotine(2021)}]{tsukamoto2021contraction}
Tsukamoto, H.; Chung, S.-J.; and Slotine, J.-J.~E. 2021.
\newblock Contraction theory for nonlinear stability analysis and
  learning-based control: A tutorial overview.
\newblock \emph{Annual Reviews in Control}, 52: 135--169.

\bibitem[{Turchetta, Berkenkamp, and Krause(2016)}]{turchetta2016safe}
Turchetta, M.; Berkenkamp, F.; and Krause, A. 2016.
\newblock Safe exploration in finite {Markov} decision processes with
  {Gaussian} processes.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}.

\bibitem[{Wachi et~al.(2023)Wachi, Hashimoto, Shen, and
  Hashimoto}]{wachi2023safe}
Wachi, A.; Hashimoto, W.; Shen, X.; and Hashimoto, K. 2023.
\newblock Safe Exploration in Reinforcement Learning: A Generalized Formulation
  and Algorithms.
\newblock \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Wachi and Sui(2020)}]{wachi2020safe}
Wachi, A.; and Sui, Y. 2020.
\newblock Safe reinforcement learning in constrained Markov decision processes.
\newblock In \emph{International Conference on Machine Learning (ICML)}.

\bibitem[{Wachi, Wei, and Sui(2021)}]{wachi2021safe}
Wachi, A.; Wei, Y.; and Sui, Y. 2021.
\newblock Safe Policy Optimization with Local Generalized Linear Function
  Approximations.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}.

\bibitem[{Yang and Wang(2020)}]{pmlr-v119-yang20h}
Yang, L.; and Wang, M. 2020.
\newblock Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and
  Regret Bound.
\newblock In \emph{International Conference on Machine Learning (ICML)}.

\end{thebibliography}
