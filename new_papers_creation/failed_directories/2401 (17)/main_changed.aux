\relax 
\bibstyle{aaai24}
\citation{garcia2015comprehensive}
\citation{dulac2021challenges}
\citation{amodei2016concrete}
\citation{altman1999constrained}
\citation{altman1999constrained}
\citation{rockafellar2000optimization}
\citation{stooke2020responsive}
\citation{wachi2020safe}
\citation{amani2021safe}
\citation{wachi2021safe}
\citation{bennett2023provable}
\citation{wachi2020safe,amani2021safe,roderick2021provably,wachi2023safe}
\citation{wachi2021safe}
\citation{bennett2023provable}
\newlabel{sec:introduction}{{1}{1}{}{}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:concept}{{1}{1}{Even if safety is guaranteed at time $t$ based on the instantaneous evaluation, safe behavior may not exist a few steps ahead. This paper requires an agent to guarantee long-term safety (i.e., constraint satisfaction from the time the current time step $t$ to the terminal time step $T$) in CMDPs with stochastic state transition and binary safety feedback.}{}{}}
\citation{achiam2017constrained}
\citation{tessler2018reward}
\citation{chow2017risk}
\citation{pmlr-v119-yang20h}
\citation{stooke2020responsive}
\citation{turchetta2016safe}
\citation{wachi2021safe}
\citation{amani2021safe}
\citation{bennett2023provable}
\citation{ames2019control}
\citation{cheng2019end}
\citation{koller2018learning}
\citation{NIPS2011_e1d5be1c}
\citation{li2017provably}
\newlabel{tab:problem}{{1}{2}{Comparison among existing work regarding their assumptions on a state transition, safety function, and others. In the above table, D means deterministic state transition, and S means stochastic state transition.}{}{}}
\newlabel{sec:related}{{2}{2}{}{}{}}
\newlabel{sec:problem}{{3}{2}{}{}{}}
\citation{filippi2010parametric,li2017provably,faury2020improved}
\citation{wachi2021safe}
\newlabel{eq:constraint}{{2}{3}{}{}{}}
\newlabel{eq:short_constraint}{{3}{3}{}{}{}}
\newlabel{assumption:linear}{{1}{3}{}{}{}}
\newlabel{eq:gl_safety}{{4}{3}{}{}{}}
\citation{wachi2021safe}
\citation{asadi2018lipschitz,ok2018exploration}
\citation{lin2021perturbation,tsukamoto2021contraction}
\citation{strehl2008analysis,auer2007logarithmic}
\citation{jin2021pessimism,buckman2020importance}
\citation{bura2022dope}
\citation{li2017provably}
\newlabel{assumption:link}{{2}{4}{}{}{}}
\newlabel{assumption:lipschitz_feature}{{3}{4}{}{}{}}
\newlabel{assumption:stabilizing}{{4}{4}{}{}{}}
\newlabel{sec:preliminary}{{4}{4}{}{}{}}
\newlabel{sec:bound_glm}{{4.1}{4}{}{}{}}
\citation{wachi2021safe}
\newlabel{lemma:confidence_bound}{{1}{5}{}{}{}}
\newlabel{sec:bound_lipschiz}{{4.2}{5}{}{}{}}
\newlabel{eq:policy_x_t}{{8}{5}{}{}{}}
\newlabel{lemma:f_t_T}{{2}{5}{}{}{}}
\newlabel{lemma:f_1_t}{{3}{5}{}{}{}}
\newlabel{lemma:lower_bound}{{4}{5}{}{}{}}
\newlabel{eq:f_t}{{10}{5}{}{}{}}
\newlabel{lemma:f_ell_T}{{5}{5}{}{}{}}
\newlabel{corollary:safety}{{1}{5}{}{}{}}
\newlabel{eq:condition_safety}{{12}{5}{}{}{}}
\newlabel{theorem:safety}{{1}{5}{}{}{}}
\newlabel{fig:point_return}{{2a}{6}{Conservative policy}{}{}}
\newlabel{sub@fig:point_return}{{a}{6}{Conservative policy}{}{}}
\newlabel{fig:point_avecost}{{2b}{6}{Early phase (Lipschitz)}{}{}}
\newlabel{sub@fig:point_avecost}{{b}{6}{Early phase (Lipschitz)}{}{}}
\newlabel{fig:point_maxcost}{{2c}{6}{Later phase (GLM)}{}{}}
\newlabel{sub@fig:point_maxcost}{{c}{6}{Later phase (GLM)}{}{}}
\newlabel{fig:long_term_safety}{{2}{6}{(a) Bounds by Lipschitz continuity for the conservative policy. (b) In the early phase of training, the lower bound of the safety linear predictor at time $t$ is typically characterized by the Lipschitz continuity, which decreases depending on the $x_1, x_2, \ldots  , x_{t-1}$. Depending on the safety margin at time $t$, we need to control $x_{t}, x_{t+1}, \ldots  , x_T$ for ensuring future safety. (c) As the training proceeds, the lower bound of the safety linear predictor can potentially be characterized by the GLMs, and the safety margin may increase.}{}{}}
\newlabel{sec:method}{{5}{6}{}{}{}}
\newlabel{alg:algorithm}{{1}{6}{Long-term Binary Safe RL (\textsf  {LoBiSaRL})}{}{}}
\newlabel{eq:opt}{{14}{6}{}{}{}}
\newlabel{eq:safety_constraint_checked}{{15}{6}{}{}{}}
\citation{amani2021safe}
\citation{wachi2021safe}
\newlabel{fig:reward}{{3a}{7}{Reward function.}{}{}}
\newlabel{sub@fig:reward}{{a}{7}{Reward function.}{}{}}
\newlabel{fig:safety}{{3b}{7}{Binary safety function.}{}{}}
\newlabel{sub@fig:safety}{{b}{7}{Binary safety function.}{}{}}
\newlabel{fig:value}{{3c}{7}{Value function with safety.}{}{}}
\newlabel{sub@fig:value}{{c}{7}{Value function with safety.}{}{}}
\newlabel{fig:experiment}{{3}{7}{Example reward, binary safety, and value functions. In this paper, we consider a safe RL problem with binary safety feedback; thus, there is an unsafe region (the white region in the (c)) where the agent is not allowed to visit.}{}{}}
\newlabel{tab:result}{{2}{7}{Experimental results. Reward is normalized with respect to \textsc  {Unsafe} agent.}{}{}}
\bibdata{ref}
\newlabel{appendix:proof}{{A}{22}{}{}{}}
\newlabel{lemma:7}{{6}{22}{}{}{}}
\newlabel{eq:diff_phi_t_tp1}{{16}{22}{}{}{}}
\newlabel{lemma:diff_phi}{{8}{22}{}{}{}}
\newlabel{proof:A_2}{{A.2}{23}{}{}{}}
\newlabel{lemma:9}{{9}{23}{}{}{}}
\newlabel{proof:A_3}{{A.3}{23}{}{}{}}
\newlabel{lemma:11}{{11}{24}{}{}{}}
\newlabel{appendix:A_5}{{A.4}{24}{}{}{}}
\newlabel{eq:Y}{{22}{25}{}{}{}}
\newlabel{eq:theorem_condition}{{23}{25}{}{}{}}
\gdef \@abspage@last{25}
