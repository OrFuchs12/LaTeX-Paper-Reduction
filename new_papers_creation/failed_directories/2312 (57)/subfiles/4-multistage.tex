\section{Multistage FedGM and Convergence}
\label{sec:stagewise}

\subsection{Proposed Algorithm: Multistage FedGM}

One major limitation in FedGM \eqref{fedgm_formulation} is that all server-side hyperparameters are held constant, which are inconsistent with common practice. Adaptively adjusting hyperparameters throughout the training is key to the success of many optimizers. Learning rate scheduling has been thoroughly studied in non-FL settings, e.g., \citep{Krizhevsky12ImageNet, He16Res, GoyalDGNWKTJH17LargeMinibatch, Smith17Cyclic}. Scheduling other hyperparameters (e.g. momentum factor and batch size) is also shown to be very effective in many settings. For example, \cite{Sutskever13Init,Smith18Bayesian,Smith18DontDecay} show a slowly increasing schedule for the momentum factor $\beta$ is crucial in training deep models faster.


\begin{algorithm2e}[tb]
\SetAlgoVlined
\KwIn{\\
Initialization $x_0$, number of rounds $T$, \textbf{local} learning rate $\eta_l$, \textbf{local} updating number $K$\;
\colorbox{babyblueeyes}{Number of stages $S$, stage lengths $\{T_s\}_{s=1}^{S}$}\;
\colorbox{babyblueeyes}{Stagewise hyperparameters $\{\eta_s,\beta_s,\nu_s\}_{s=1}^{S}$;}
}
% \colorbox{cyan}{Stagewise (\textbf{global} learning rate, momentum factor, instant discount factor) $\{\eta_s,\beta_s,\nu_s\}_{s=0}^{S-1}$;}}
\SetAlgoLined
\For{$s\in\{1,...,S\}$}
{
\For{$t $ \text{in stage} $s$}
% \For{$t\in\{T_0+\dots+T_{s-1}+1,\dots,T_0+\dots+T_s\}$}
{
    {   
        Randomly sample a subset $\mathcal{S}_t$ of clients
    
        Server sends $x_t$ to subset $\mathcal{S}_t$ of clients
    
        \For{each client $i \in \mathcal{S}_t$}
        {
        $\Delta_t^i=\text{LocalOPT}\left(i,\eta_l,K,x_t\right)$
        }
    
        Server aggregates $\Delta_t=\frac{1}{\lvert \mathcal{S}_t\rvert}\sum_{i\in \mathcal{S}_t}\Delta_t^i$
        
        \colorbox{babyblueeyes}{$d_{t+1}=(1-\beta_s)\Delta_{t}+\beta_s d_{t}$}

        \colorbox{babyblueeyes}{$h_{t+1}=(1-\nu_s)\Delta_{t}+\nu_s d_{t+1}$}
        
        \colorbox{babyblueeyes}{Update $x_{t+1}=x_t-\eta_s h_{t+1}$}
        
    }
}
}
return $x_T$
\caption{\colorbox{babyblueeyes}{Multistage FedGM}}
\label{multistage_FedGM_algorithm}
\end{algorithm2e}

We focus on a simple yet effective hyperparameter scheduler, ``constant and drop'' (a.k.a. ``step decay''). In its non-FL SGD version (a.k.a. multistage SGD), with a prespecified set of learning rates $\{\eta_s\}_{s=1}^{S}$ and training lengths $\{T_s\}_{s=1}^{S}$ (measured by number of iterations/epochs), the training process is divided into $S$ stages, and SGD with $\eta_s$ is applied for $T_s$ iterations/epochs at $s$-th stage, where $\{\eta_s\}_{s=1}^{S}$ is usually a non-increasing sequence \footnote{The name ``constant and drop'' refers to learning rate is dropped by some constant factor after each stage.}. We concentrate on ``constant and drop'' as it is the de-facto learning rate scheduler in most large-scale neural networks \citep{Krizhevsky12ImageNet,Sutskever13Init,He16Res,Huang2017DenseNet}, and has been theoretically shown to achieve near-optimal rate in non-FL settings \citep{ge19stepdecay,wang21stepdecay}.

The intuition behind ``constant and drop'' is straightforward: a large learning rate is held constant for a reasonably long period to take advantage of faster convergence until it saturates, and then the learning rate is dropped by a constant factor for more refined training.

We extend ``constant and drop'' to FedGM in Algorithm \ref{multistage_FedGM_algorithm}, which we refer to as Multistage FedGM. In Multistage FedGM (Algorithm \ref{multistage_FedGM_algorithm}), each stage has length $T_s$ ($T=\sum_{s=1}^{S}T_s$), and has its triplet of stagewise hyperparameters $\{\eta_s,\beta_s,\nu_s\}_{s=1}^{S}$. The convergence analysis in Sec \ref{subsec:convergence_analysis} also applies to single-stage FedGM by $S=1$.

To our best knowledge, there is no prior work giving definitive theoretical guarantee or empirical performances of any hyperparameter schedule in FL, especially considering multistage FedGM is an extremely flexible framework that allows both learning rate and momentum factor to evolve.


\subsection{Convergence Analysis of Multistage FedGM}
\label{subsec:convergence_analysis}

We now analyze the convergence of Algorithm \ref{multistage_FedGM_algorithm} under both full and partial participation settings. 

We aim to optimize objective \eqref{fed_min_objective}. Each local loss $f_i$ (and therefore $f$) may be general nonconvex function. We study the general \textit{non-i.i.d.} setting, i.e. $\mathcal{D}_i\neq\mathcal{D}_j$ when $i\neq j$. We state the assumptions that are needed in the analysis.

\begin{assumption}[Smoothness]
\label{smoothness_assumption}
Each local loss $f_i(x)$ is differentiable and has $L$-Lipschitz continuous gradients, i.e., $\forall x, x^\prime\in \mathbb{R}^d$, we have $\left\|\nabla f_i(x)-\nabla f_i(x^\prime)\right\| \leq L \left\| x-x^\prime\right\|$. And $f^\ast\triangleq\min_x f(x)$ exists, i.e., $f^\ast>-\infty$.
\end{assumption}

\begin{assumption}[Bounded Local Variance]
\label{bounded_local_assumption}
$\forall t, i$, LocalOPT can access an unbiased estimator $g_{t,k}^i=\nabla f_i(x_{t,k}^i, \xi_{t,k}^i)$ of true gradient $\nabla f_i(x_{t,k}^i)$, where $g_{t,k}^i$ is the stochastic gradient estimated with minibatch $\xi_{t,k}^i$. And each stochastic gradient on the $i$-th client has a bounded local variance, i.e., we have $\mathbb{E}\left[\left\| g_{t,k}^i - \nabla f_i(x_{t,k}^i) \right\|^2\right] \leq \sigma^2_l$.
\end{assumption}

\begin{assumption}[Bounded Global Variance]
\label{bounded_global_assumption}
The local loss $\{f_i(x)\}$ across all clients have bounded global variance, i.e., $\forall x$, we have $\frac{1}{n}\sum_{i=1}^{n}\left\| \nabla f_i(x)-\nabla f(x)\right\|^2\leq\sigma_g^2$.
\end{assumption}

Assumption \ref{smoothness_assumption}-\ref{bounded_global_assumption} are standard assumptions in nonconvex optimization and FL research, and have been universally adopted in most existing works \citep{reddi18adam_convergence,Li2020Fed-Non-IID,reddi2020adaptive,bao2020fast,yang2021achieving,wang22adaptive,wu2023federated,wu2023solving}. $\sigma_g^2=0$ in Assumption \ref{bounded_global_assumption} corresponds to the \textit{i.i.d.} setting. And we do not require the restrictive bounded gradient assumption \citep{reddi18adam_convergence,Avdiukhin21arbitrarycommunication,wu2023faster}.

Recall $T=\sum_{s=1}^{S}T_s$ is the number of rounds. Denote the expected gradient square as $\{\mathcal{G}_t\triangleq\mathbb{E}\left[\left\| \nabla f(x_t)\right\|^2\right]\}_{t\leq T}$. Define the average expected gradient square at $s$-th stage as $\Bar{\mathcal{G}}_s\triangleq\frac{1}{T_s}\Sigma_{t=T_1+\dots+T_{s-1}+1}^{T_1+\dots+T_s} \mathcal{G}_t$ and the average expected gradient square across $S$ stages as $\Bar{\mathcal{G}}\triangleq\frac{1}{S}\sum_{s=1}^S \Bar{\mathcal{G}}_s$. Bounding $\Bar{\mathcal{G}}$ generalizes from bounding $\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}\left[\left\|\nabla f(x_t)\right\|^2\right]$ in single-stage to multistage setting. 

To reflect the common hyperparameter scheduling practice that is adopted by existing works e.g. \cite{Sutskever13Init,Smith18DontDecay,liu2020improved}, We request the stagewise hyperparameters fulfill the following constraints,
\begin{equation}
    \begin{gathered}
    \eta_{S}\leq\eta_{S-1}\leq\dots\leq\eta_{1} \quad \beta_1\leq\beta_2\leq\dots\leq\beta_{S}<1\\
    W_1 \equiv \frac{\eta_s\beta_s\nu_s}{1-\beta_s} \quad \text{and} \quad  W_2\equiv T_s\eta_s
    \end{gathered}
\label{stagewise_hyper_constraints}
\end{equation}
where $W_1$ and $W_2$ are two constants. Constraint \eqref{stagewise_hyper_constraints} essentially requires learning rate to be non-increasing and momentum factor to be non-decreasing at a similar rate, which is consistent with common practice, e.g. for SHB and NAG, \cite{Sutskever13Init,Smith18DontDecay,liu2020improved} propose a scheduler for $\beta$ to increase and close to 1 for faster convergence. And it is also natural for \eqref{stagewise_hyper_constraints} to require $T_s\eta_s$ as a constant. As the learning rate is decaying, more rounds in later stages are necessary for sufficient refined training. 

We now state the convergence guarantee of the multistage training regime in FL framework.

\subsubsection{Full Participation}
\label{subsec:full_participation}

If all clients are required to participate in each round, i.e. $\mathcal{S}_t=\{1,2,\dots,n\}$, we have,

\begin{theorem}
\label{multistage_fedgm_full_participation_convergence_theorem}
We optimize $f(x)$ with Algorithm \ref{multistage_FedGM_algorithm} (Full Participation) under Assumptions \ref{smoothness_assumption}-\ref{bounded_global_assumption}. Denote $\Bar{\eta} \triangleq \frac{1}{S}\sum_{s=1}^S \eta_s$ as the average server learning rate and $C_\eta\triangleq\frac{\eta_1}{\eta_S}$. Under the condition \footnote{The condition could be fulfilled by typical value assignment, and would recover the typical $\eta_l\leq\min\left\{\frac{1}{8KL},\frac{1}{KL\eta}\right\}$ constraint in FedAvg analysis \citep{yang2021achieving}, by setting $S=1$.} $\eta_l\leq\min\left\{\frac{1}{8KL},\frac{1}{K S C_\eta \left(L \Bar{\eta}  + 1 + L^2 W_1^2 C_\eta \right)}\right\}$, we would have:

\begin{gather*}
\Bar{\mathcal{G}}  \triangleq \frac{1}{S} \sum_{s=1}^{S} \frac{1}{T_s} \sum_{t=T_0+\dots+T_{s-1}}^{T_0+\dots+T_s-1} \mathbb{E}\left[\left\|\nabla f(x_t)\right\|^2\right] \\
 \leq \frac{64}{17}\frac{f(x_0)-f^\ast}{S W_2 \eta_l K} + \Psi_l \sigma_l^2 + \Psi_g  \sigma_g^2
\label{multistage_fedgm_full_participation_bound}
\end{gather*}
where $\Psi_l  \triangleq \frac{32}{17}\frac{L^2 W_1^2 T \Bar{\eta} \eta_l}{n W_2} +\frac{32}{17} \frac{L \Bar{\eta} \eta_l }{n} +\frac{32}{17}\frac{\eta_l}{n}+\frac{160}{17} \eta_l^2L^2K$, and $\Psi_g \triangleq \frac{960}{17} \eta_l^2L^2K^2$.


\end{theorem}

\begin{corollary}[Convergence Rate of Multistage FedAvg]
Suppose $\nu_s=0$, i.e., the FedAvg algorithm that allows learning rate vary across $S$ stages. By setting $\Bar{\eta}=\Theta\left(\sqrt{nK}\right)$ and $\eta_l=\Theta\left(\frac{1}{\sqrt{T}K}\right)$, $W_2=\Theta\left(\frac{T\sqrt{nK}}{S}\right)$, i.e. $T\Bar{\eta}$ equally divided into $S$ stages. $W_1=0$ as $\nu_s=0$. Suppose $T$ is sufficiently large, i.e. $T\ge nK$, we have a $\mathcal{O}\left(\frac{1}{\sqrt{TKn}}\right)$ convergence rate.
\label{corollary:multistage_fedavg_full_participation}
\end{corollary}

\iffalse
\begin{remark}[Same Rate as Vanilla FedAvg]
Corollary \ref{corollary:multistage_fedavg_full_participation} indicates multistage FedAvg matches the best known rate for general FL nonconvex optimization approaches, e.g. SCAFFOLD \citep{karimireddy2020scaffold} and FedAdam \citep{reddi2020adaptive}. Note single-stage FedAvg with two-sided learning rates also achieves the same rate \citep{yang2021achieving}. However, we do observe multistage FedAvg converges empirically much better than single-stage FedAvg. It is unclear whether we could devise a scheduler such that with its corresponding $\Bar{\eta}$ and $W_2$, we could show an accelerating effect of stage $S$ and we will leave it as an interesting future direction. 
\label{remark:full_participation_same_rate}
\end{remark}
\fi

\begin{remark}[Why Multistage Helps?]
Corollary \ref{corollary:multistage_fedavg_full_participation} indicates multistage FedAvg recovers the best-known rate for general FL nonconvex optimization approaches, e.g. SCAFFOLD \citep{karimireddy2020scaffold} and FedAdam \citep{reddi2020adaptive}. Note single-stage FedAvg with two-sided learning rates also achieves the same rate \citep{yang2021achieving}. However, we do observe multistage FedAvg empirically converges much better than single-stage. We can obtain insights from Theorem \ref{multistage_fedgm_full_participation_convergence_theorem} why multistage helps. We note that $\Psi_l$ is only related to average learning rate $\Bar{\eta}$ (instead of initial learning rate $\eta_1$). At initial rounds, the first term with $f(x_0)-f^\ast$ dominates, and thus we could select a relatively large $\eta_1$ to ensure a more dramatic decay of this term. At later rounds, when $f(x_t)-f^\ast$ plateaus, we could enable smaller learning rate to control $\Bar{\eta}$. Thus, Theorem \ref{multistage_fedgm_full_participation_convergence_theorem} indicates a less stringent reliance on $\eta_1$, which enables us to flexibly select suitable $\eta$ depending on which training stage we are in, that can still guarantee convergence.
\label{remark:full_participation_why_multistage_helps}
\end{remark}

\begin{corollary}[Convergence Rate of Multistage FedGM]
Suppose $S>1$, i.e. the multistage regime, by setting $\Bar{\eta} = \Theta\left(\sqrt{nK}\right)$, $\eta_l=\Theta\left(\frac{1}{\sqrt{T}K}\right)$, $W_2=\Theta\left(\frac{T\sqrt{nK}}{S}\right)$. Let $W_1^2=\mathcal{O}\left(\frac{\sqrt{nK}}{S}\right)$ \footnote{It holds by setting an infinitesimal $\beta$ or $\nu$ at early stages when $\eta$ is large, but $\beta$ or $\nu$ can go to 1 when $\eta$ is reduced to $o\left( \frac{\sqrt[\leftroot{-3}\uproot{3}4]{nK}}{\sqrt{S}} \right)$.}. When $T\ge Kn$, we have a $\mathcal{O}\left(\frac{1}{\sqrt{TKn}}\right)$ convergence rate. 
\label{corollary:multistage_fedgm_full_participation}
\end{corollary}


\begin{remark}[Why Momentum Helps?]
We attribute the empirically superior performances of momentum to two reasons. (a) When clients are dynamically heterogeneous, historical gradient information has regularization effect to avoid the search direction from going wild. (b) Server learning rate $\eta$ acts like a multiplier to client learning rate $\eta_l$ in FedAvg, i.e. $\eta>1$ effectively enhances the reliance on current round gradient. Due to the same reason as in (a), such reliance can harm convergence. In contrast, in FedGM, $\beta$ and $\nu$ act as a buffer that could to some extent absorb the impact from a large $\eta$. We empirically observe in Appendix \ref{subsec:appendix_more_results_fedgm}, with same $\eta_l$, FedGM could sustain a much larger $\eta$, while FedAvg diverges very easily with a moderately large $\eta$.
\label{remark:full_participation_why_momentum_helps}
\end{remark}



\subsubsection{Partial Participation}
\label{subsec:partial_participation}

Full participation rarely holds in reality, thus we further analyze multistage FedGM in partial participation setting \footnote{In each round $t$, the server samples a subset of clients $\mathcal{S}_t$ (suppose $\left | \mathcal{S}_t \right | = m < n$) uniformly at random without replacement, i.e. $\mathbb{P}\left\{i\in\mathcal{S}_t\right\}=\frac{m}{n}$ and $\mathbb{P}\left\{i,j\in\mathcal{S}_t\right\}=\frac{m\left(m-1\right)}{n\left(n-1\right)}$.}.

\begin{theorem}
\label{multistage_fedgm_partial_participation_convergence_theorem}
We optimize $f(x)$ with Algorithm \ref{multistage_FedGM_algorithm} (Partial Participation) under Assumptions \ref{smoothness_assumption}-\ref{bounded_global_assumption}. Denote $\Bar{\eta}$ and $C_\eta$ as in Theorem \ref{multistage_fedgm_full_participation_convergence_theorem}. Under the condition $\eta_l\leq\frac{1}{8KL}$, and $\eta_l \left( C_\eta + L \Bar{\eta} C_\eta + L^2 W_1^2 C_\eta \right) SK \leq \min\left\{ \frac{m(n-1)}{n(m-1)}, \frac{17m}{282} \right\}$, we would have:

\begin{gather*}
\Bar{\mathcal{G}} \leq \frac{64}{17}\frac{f(z_0)-f^\ast}{S W_2 \eta_l K}  
+ \Psi_l \sigma_l^2
+ \Psi_g \sigma_g^2
\label{multistage_fedgm_partial_participation_bound}
\end{gather*}


where $\Psi_l  \triangleq \frac{\eta_l}{m}\Phi +\frac{15\left(n-m\right)K^2L^3\eta_l^3}{m\left(n-1\right)}\Phi + \frac{160}{17} \eta_l^2L^2K$, $\Psi_g \triangleq  \frac{90\left(n-m\right)K^3L^3\eta_l^3}{m\left(n-1\right)}\Phi  +  \frac{3\eta_l\left(n-m\right)K}{m\left(n-1\right)}\Phi     + \frac{960}{17} \eta_l^2L^2K^2$, and $\Phi \triangleq \frac{32 T \Bar{\eta} + 32 L T \hat{\eta}^2 + 32 L^2 W_1^2 T \Bar{\eta}}{17 W_2}$.

\iffalse
where 
\begin{gather*}
\Psi_l  \triangleq \frac{\eta_l}{m}\Phi +\frac{15\left(n-m\right)K^2L^3\eta_l^3}{m\left(n-1\right)}\Phi + \frac{160}{17} \eta_l^2L^2K\\ 
\Psi_g \triangleq  \frac{90\left(n-m\right)K^3L^3\eta_l^3}{m\left(n-1\right)}\Phi  +  \frac{3\eta_l\left(n-m\right)K}{m\left(n-1\right)}\Phi     + \frac{960}{17} \eta_l^2L^2K^2\\
\Phi \triangleq \frac{32 T \Bar{\eta} + 32 L T \hat{\eta}^2 + 32 L^2 W_1^2 T \Bar{\eta}}{17 W_2}
\nonumber
\end{gather*}
\fi

\end{theorem}

\begin{corollary}[Convergence Rate of Multistage FedGM]
Suppose $S>1$, i.e. the multistage regime, by setting $\Bar{\eta} = \Theta\left(\sqrt{mK}\right)$, $\hat{\eta}^2 = \Theta\left(mK\right)$, $\eta_l=\Theta\left(\frac{1}{\sqrt{T}K}\right)$, $W_2=\Theta\left(\frac{T\sqrt{mK}}{S}\right)$ and $W_1^2=\mathcal{O}\left(\sqrt{mK}\right)$, we have convergence rate as $\mathcal{O}\left(\sqrt{\frac{K}{Tm}}\right)$.
\label{corollary:fedgm_partial_participation_convergence_rate}
\end{corollary}



\begin{remark}
$\mathcal{O}\left(\sqrt{\frac{K}{Tm}}\right)$ recovers the best known convergence rate for FL nonconvex optimization \citep{yang2021achieving}. Similar to Remark \ref{remark:full_participation_why_multistage_helps}, Theorem \ref{multistage_fedgm_partial_participation_convergence_theorem} shows an reliance on average learning rate, which explains why multistage scheme helps empirically. $\mathcal{O}\left(\sqrt{\frac{K}{Tm}}\right)$ indicates a slowdown effect from more local computation, which is supported by some existing works \citep{Li2020Fed-Non-IID}, while others observe a different effect of $K$ \citep{lin2020dont}. The exact impact of $K$ on convergence warrants further investigation.
\end{remark}


\iffalse

\begin{remark}
$\mathcal{O}\left(\sqrt{\frac{K}{Tn}}\right)$ indicates a slowdown effect from more local computation, which is supported by some existing works \citep{Li2020Fed-Non-IID}, while others observe a different effect of $K$ \citep{stich2018localSGD,lin2020dont}. The exact impact of $K$ on convergence warrants further investigation.
\end{remark}

\fi