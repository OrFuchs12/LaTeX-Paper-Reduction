\section{Introduction}
\label{sec:intro}

Federated Averaging (FedAvg) \citep{McMahan2017FedAvg}, which runs multiple epochs of Stochastic Gradient Descent (SGD) locally in each client and then averages the local model updates once in a while on the server, is probably the most popular algorithm to solve many federated learning (FL) problems, mainly due to its low communication cost and appealing convergence property.

Though it has seen great empirical success, vanilla FedAvg experiences an unstable and slow convergence when encountering \textit{client drift}, i.e., the local client models move away from globally optimal models due to client heterogeneity \citep{karimireddy2020scaffold}. On the server side, FedAvg is in spirit similar to an SGD with a constant learning rate one and updates the global model relying only on the averaged model update from the current round, thus extremely vulnerable to client drift. Note that in non-FL settings, SGD in its vanilla form has long been replaced by some momentum scheme (e.g. heavy ball momentum (SHB) and Nesterov's accelerated gradient (NAG)) in many tasks, as momentum schemes achieve an impressive training time saving and generalization performance boosting compared to competing optimizers \cite{Sutskever13Init,Wilson2017Generalization}, which promises a great potential to apply momentum in FL settings as well. Incorporating server momentum essentially integrates historical aggregates into the current update, which could conceptually stabilize the global update against dramatic local drifts. 

Though various efforts have been made to understand the role of server momentum in FL, e.g. \citep{Hsu2019MeasuringTE,rothchild20fetchsgd}, it is still largely an under-explored problem due to the following reasons:

(1) Lack of diversity in momentum schemes. Most existing server momentum works only focus on SHB (e.g. FedAvgM \citep{Hsu2019MeasuringTE}). It is unclear whether many momentum schemes that outperformed SHB in non-FL settings can also perform better in FL, and there is no unified analysis for momentum schemes other than SHB. 

(2) No hyperparameter schedule. Properly scheduling hyperparameters is key to train deep models more efficiently and an appropriate selection of server learning rate $\eta_t$ is also important in obtaining optimal convergence rate \citep{yang2021achieving}. Existing works either still employ a constant server learning rate one or consider a $\eta_t$ schedule that is uncommonly used in practice, e.g., polynomially decay (i.e., $\eta_t\propto\frac{1}{t^\alpha}$) \citep{khanduri2021stem}. Moreover, it is known that increasing momentum factor $\beta$ is also a critical technique in deep model training \citep{Sutskever13Init,Smith18DontDecay}, while to our best knowledge, there is no prior work considering time-varying $\beta$ in FL. 

(3) Ignoring client system heterogeneity. Existing works make unrealistic assumptions on system homogeneity and client synchrony, e.g., clients are sampled uniformly at random, all participating clients synchronize at each round $t$, and all clients run identical number of local epochs, none of which holds in most cross device FL deployments \citep{Kairouz21AdvancesProblems}. System heterogeneity (i.e., the violation of above assumptions), alongside with data heterogeneity, is also a main source client drift \citep{karimireddy2020scaffold}. Thus, ignoring it would provide an incomplete understanding of the role of server momentum. 

To address the above limitations, we propose a novel formulation which we refer to as Federated General Momentum (FedGM). FedGM includes the following hyperparameters, learning rate $\eta$, momentum factor $\beta$, and instant discount factor $\nu$. With different specifications of $(\eta,\beta,\nu)$, FedGM subsumes the FL version of many popular momentum schemes, most of which have never been explored in FL yet. 

We further incorporate a widely used hyperparameter scheduler ``constant and drop'' (a.k.a. ``step decay'') in FedGM. We refer to this framework as multistage FedGM. Specifically, with a prespecified set of hyperparameters $\{\eta_s,\beta_s,\nu_s\}_{s=1}^{S}$ and training lengths $\{T_s\}_{s=1}^{S}$, the training process is divided into $S$ stages, and at stage $s$, FedGM with $\{\eta_s,\beta_s,\nu_s\}$ is applied for $T_s$ rounds. Compared to many unrealistic schedule in existing works, ``constant and drop'' is the de-facto scheduler in most model training \citep{Sutskever13Init,He16Res,Huang2017DenseNet}. Multistage FedGM is extremely flexible, as it allows the momentum factor to vary stagewise as well, and subsumes single-stage training as a special case. We provide the convergence analysis of multistage FedGM. Our theoretical results reveal why stagewise training can provide empirically faster convergence. 

Furthermore, in order to understand how server momentum behaves in the presence of system heterogeneity, we propose a framework that we refer to as Autonomous Multistage FedGM, in which clients can do heterogeneous and asynchronous computing. Specifically, we allow each client to (a) update local models based on an asynchronous view of the global model, (b) run a time-varying, client-dependent number of local epochs, and (c) participate at will. We provide convergence analysis of Autonomous Multistage FedGM. Autonomous Multistage FedGM is a more realistic characterization of real-world cross-device FL applications.

Finally, we conduct extensive experiments that validate, (a) FedGM is a much more capable momentum scheme compared with existing FedAvgM in both with and without system heterogeneity settings; and (b) multistage hyperparameter scheduler further improves FedGM effectively.

Our main contributions can be summarized as follow,

\begin{itemize}[leftmargin=*]
    \item We propose FedGM, which is a general framework for server momentum and covers a large class of momentum schemes that are unexplored in FL. We further propose Multistage FedGM, which incorporates a popular hyperparameter scheduler to FedGM.
    \item We show the convergence of multistage FedGM in both full and partial participation settings. We also empirically validate the superiority of multistage FedGM. To our best knowledge, this is the first work that provides convergence analysis of server-side hyperparameter scheduler.
    \item We propose Autonomous Multistage FedGM, which requires much less coordination between server and workers than most existing algorithms, and theoretically analyze its convergence. Our work is the first to study the interplay between server momentum and system heterogeneity.
\end{itemize}

The rest of the paper is organized as follows. In Section \ref{sec:background}, we formally introduce federated optimization. In Section \ref{sec:fedgm}, we introduce Federated General Momentum (FedGM), followed by multistage FedGM and its convergence analysis in Section \ref{sec:stagewise}. In Section \ref{sec:autonomous}, we introduce Autonomous multistage FedGM and provide its convergence analysis. Section \ref{sec:experiments} presents the experimental results. Due to the page limit, we leave related work, all proofs, and additional experimental results to the Appendix.