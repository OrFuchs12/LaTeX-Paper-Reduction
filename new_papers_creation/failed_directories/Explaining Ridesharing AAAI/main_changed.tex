\def\year{2021}\relax
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai21}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{graphicx}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS


% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{booktabs}
%\newcommand{\citep}{\cite}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{observation}{Observation}

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

	%/Author (David Zar, Noam Hazon, Amos Azaria)
}

%An Agent for Providing Explanations that Increase User Satisfaction from a Shared Ride
\title{Explaining Ridesharing: Selection of Explanations for Increasing User Satisfaction}

%\author{Paper \#26}
% copyright notice AAAI 2020 removed in .sty

\author{Paper Id: \#1254}
% \and
% \IEEEauthorblockN{Noam Hazon}
% \IEEEauthorblockA{Ariel University}
% \IEEEauthorblockA{Computer Science Department and Data Science Center}\\
% \textit{Ariel University, Israel}
% \and
% \IEEEauthorblockN{Amos Azaria}
% \IEEEauthorblockA{Ariel University}
% \IEEEauthorblockA{Computer Science Department and Data Science Center}\\
% \textit{Ariel University, Israel}
% }


\begin{document}
\maketitle
\begin{abstract}
Transportation services play a crucial part in the development of modern smart cities.
In particular, on-demand ridesharing services, which group together passengers with similar itineraries, are already operating in several metropolitan areas.
These services can be of significant social and environmental benefit, by reducing travel costs, road congestion and $CO_2$ emissions.

Unfortunately, despite their advantages, not many people opt to use these ridesharing services. We believe that increasing the user satisfaction from the service will cause more people to utilize it, which, in turn, will improve the quality of the service, such as the waiting time, cost, travel time, and service availability. One possible way for increasing user satisfaction is by providing appropriate explanations comparing the alternative modes of transportation, such as a private taxi ride and public transportation. For example, a passenger may be more satisfied from a shared-ride if she is told that a private taxi ride would have cost her $50\%$ more. Therefore, the problem is to develop an agent that provides explanations that will increase the user satisfaction.

%Talk about theoretical solution
We model our environment as a signaling game and show that a rational agent, which follows the perfect Bayesian equilibrium, must reveal all of the information regarding the possible alternatives to the passenger.
In addition, we develop a machine learning based agent that, when given a shared-ride along with its possible alternatives, selects the explanations that are most likely to increase user satisfaction.
Using feedback from humans we show that our machine learning based agent outperforms the rational agent and an agent that randomly chooses explanations, in terms of user satisfaction.
\end{abstract}


\section{Introduction}
More than $55\%$ of the world’s population are currently living in urban areas, a proportion that is expected to increase up to $68\%$ by 2050 \cite{united2018}. Sustainable urbanization is a key to successful future development of our society. A key inherent goal of sustainable urbanization is an efficient usage of transportation resources in order to reduce travel costs, avoid congestion, and reduce greenhouse gas emissions.

%The proposed project EC-RIDER will contribute to this goal by studying and creating novel advanced and explainable Artificial Intelligence (AI) methods for solving hard coordination and optimization problems in shared urban mobility [2], and by validating these AI methods and their societal impact on shared mobility services. Specifically, we consider on-demand ridesharing by carpooling (driver is passenger), taxi ridesharing (ridehailing) with human or automated drivers, and multimodal ridesharing (see [3] for a taxonomy of service types).

While traditional services---including buses and taxis---are well established, large potential lies in shared but flexible urban transportation. On-demand ridesharing, where the driver is not a passenger with a specific destination, appears to gain popularity in recent years, and big ride-hailing services such as Uber and Lyft are already offering such services. However, despite the popularity of Uber and Lyft \cite{likeUber2018}, their ridesharing services, which group together multiple passengers (Uber-Pool and Lyft-Line), suffer of low usage \cite{motherboard2016,resons2017}.

% : Pilot projects such as
% Volkswagen's MOIA offer ridesharing trips where the route is adapted to travelers' demands. Daimler and Bosch recently announced to pilot driverless ridehailing in San José \cite{Daimler2019}. However, as per today, shared mobility platforms fail to build and maintain sufficient active user bases. A 2018 ING Think report \cite{Ing2018} points out that, while the number of carsharing users in Europe has doubled from 2016 to 2018, absolute figures are still marginal. Furthermore, while ride-hailing services such as Uber and Lyft have recently gained popularity, their ridesharing services, which group together multiple passengers (Uber-Pool and Lyft-Line), suffer of low usage \cite{motherboard2016,resons2017}.
%Another recent study [6] indicates that only
%$5\%$ of the population of Berlin have already used ridesharing offerings.

In this paper we propose to encourage the usage of ridesharing services by increasing the user satisfaction. It is well-known that one of the most influencing factors for driving people to utilize a specific service is to increase their satisfaction form the service (see for example, \cite{singh2006importance}). Moreover, if people will be satisfied and use the the service more often it will improve the quality of the service, such as the waiting time, cost, travel time, and service availability, which in turn further increase the user satisfaction.

One possible way for increasing user satisfaction is by providing appropriate explanations \cite{bradley2009dealing}. Indeed, in recent years there is a growing body of literature that deals with explaining decisions made by AI systems \cite{gunning2017explainable}.
%, a field termed XAI \cite{gunning2017explainable}.
In our ridesharing scenario, a typical %XAI
approach would attempt to explain the entire assignment of all passengers to all vehicles. Clearly, a passenger is not likely to be interested in such an explanation, since she is not interested in the assignment of other passengers to other vehicles.
A passenger is likely to only be interested with her own current shared-ride when compared to other alternative modes of transportation, such as a private taxi ride or public transportation.

Comparing the shared-ride to other modes of transportation may provide many different possible explanations.
For example, consider a shared-ride that takes $20$ minutes and costs $\$10$. The passenger could have taken a private taxi that would have cost $\$20$. Alternatively, the passenger could have used public transportation, and such a ride would have taken $30$ minutes. A passenger is not likely to be aware of the exact costs and riding times of the other alternatives, but she may have some estimations. The agent, on the other hand, has access to many sources of information, and it can thus provide the exact values as explanations. Clearly, the agent is not allowed to provide false information. The challenge is to design an agent that provides the appropriate explanation in any given scenario.

We first model our environment as a signaling game~\cite{spence1974market}, which models the decision of a rational agent whether to provide the exact price (i.e., the cost or the travel time) of a possible alternative mode of transportation, or not.
In this game there are three players: nature, the agent and the passenger.
Nature begins by randomly choosing a price from a given known distribution, which corresponds to the estimation of the passenger.
The agent then decides whether to disclose this price to the passenger or not. The passenger then determines her current expectation over the price of the alternative. The goal of the agent is to increase the passenger satisfaction, and thus it would like the passenger to believe that the price of the alternative is higher than the price of the shared-ride as much as possible. We use the standard solution concept of Perfect Bayesian Equilibrium (PBE)~\cite{fudenberg1991perfect} and show that a rational agent must reveal all of the information regarding the price of the possible alternative to the passenger.

%However, algorithmic approaches that use a pure theoretically analytic objective often perform poorly with real humans~\cite{Peledetal11,azaria2015strategic,nay2016predicting}.
Interacting with humans and satisfying their expectations is a very complex task. Research into humans' behavior has found that people often deviate from what is thought to be the rational behavior, since they are affected by a variety of (sometimes conflicting) factors: a lack of knowledge of one's own preferences, framing effects, the interplay between emotion and cognition, future discounting, anchoring and many other effects~\cite{tversky81,Loewenstein00,ArielyAnchor,camerer03}.
Therefore, algorithmic approaches that use a pure theoretically analytic objective often perform poorly with real humans~\cite{Peledetal11,azaria2015strategic,nay2016predicting}.
We thus develop an Automatic eXplainer for Increasing Satisfaction (AXIS) agent, that when given a shared-ride along with its possible alternatives selects the explanations that are most likely to increase user satisfaction.

For example, consider again the setting in which a shared-ride takes $20$ minutes and costs $\$10$.
The passenger could have taken a private taxi that would have taken $15$ minutes, but would have cost $\$20$. Alternatively, the passenger could have used public transportation. Such a ride would have taken $30$ minutes, but would have cost only $\$5$.
A \emph{human} passenger may be more satisfied from the shared-ride if she is told that a private taxi would have cost her $100\%$ more. Another reasonable explanation is that a public transportation would have taken her $10$ minutes longer. It may be even better to provide both explanations. However, providing an explanation that public transportation would have cost $50\%$ less than the shared-ride is less likely to increase her satisfaction. Indeed, finding the most appropriate explanation depends on the specific parameters of the scenario. For example, if public transportation still costs $\$5$ but the shared ride costs only $\$6$, providing an explanation that public transportation would have cost only $\$1$ less than the shared-ride may now become an appropriate explanation.

%Our AXIS agent utilizes a different approach.
For developing the AXIS agent we utilize the following approach.
We collect data from human subjects on which explanations they believe are most suitable for different scenarios. AXIS then uses a neural network to generalize this data in order to provide appropriate explanations for any given scenario. Using feedback from humans we show that AXIS outperforms the PBE agent and an agent that randomly chooses explanations. That is, human subjects that were faced with shared-ride scenarios, were more satisfied from the ride given the explanations selected by AXIS, than by the same ride when shown all explanations and when the explanations were randomly selected.

The contributions of this paper are threefold:
\begin{itemize}
    \item The paper introduces the problem of automatic selection of explanations in the ridesharing domain, for increasing user satisfaction. The set of explanations consists of alternative modes of transportation.
    \item We model the explanation selection problem as a signaling game and determine the unique set of Perfect Bayesian Equilibria (PBE).
    \item We develop the AXIS agent, which learns from how people choose appropriate explanations, and show that it outperforms the PBE agent an agent that randomly chooses explanations, in terms of user satisfaction.
\end{itemize}

%Ride sharing receives much attention. The benefits of ride sharing are less damage to environment and less roads congestion.
%Carbon dioxide emission of private vehicles is big cause of air pollution. [?]
%The switch to ride sharing and decreasing the use of private vehicle also helps with parking problems, which are serious problem in big cities, like New York, ... [?]
%By decreasing the ride cost, ride sharing service also can increase the satisfaction of the users and increase the benefit to the providers.
%
%Surveys show that the use of ride sharing services is very low. The vehicle occupancy is only XX, and this low number shows that there is big hidden potential of decrease road congestion, which in turn, of course, decrease exponentially the waiting time in traffic jams.
%
%In addition, the deployment of autonomous cars in the near future will surely change the way people are traveling. It is even more promising for a ridesharing service, since it will be easier and cheaper for a company to handle a fleet of autonomous cars that can serve the demands of different passengers.
%
%The basic problem of ride sharing is given some requests of users (passengers) for a ride and some vehicles we have, we want to assign them to vehicles and define the route of each vehicle, in a way the optimize some objective. This problem belongs to generic class, Vehicle Routing and scheduling Problems (VRPs), which have been extensively studied over the past $50$ years, mainly in the operation research and transportation science communities. In this generic class there are some variants. The initial definition of the VRP assumes that the environment is \textit{static}, i.e., all requests are known before-hand and do not change thereafter [?].
% (copy-pasted: )
%The more complex variants, including the ridesharing problem, are \textit{dynamic}, where real-time requests are gradually revealed along the service operating time. In this setting the assignment of passengers to vehicles and the determination of vehicles' routes may be adjusted when they are already in transit~\cite{psaraftis2016dynamic,shen2016dynamic}. Arguably, a major criterion that characterizes each variant of the VRP is the objective function. It is very common to consider objectives from the service provider's perspective, for example, minimizing the total distance travelled~\cite{secomandi2000comparing,agatz2011dynamic}, minimizing the fleet size~\cite{diana2006model,secomandi2009reoptimization}, or maximizing the service provider's profit~\cite{campbell2005decision,parragh2014dial}. However, as noted by~\cite{cordeau2003tabu}, one should be interested not only in minimizing the operating costs for the service provider but also in maximizing the quality of the service and the user satisfaction.
%
%Levinger, Azaria and Hazon show that by taking the user satisfaction as the objective, and developing complex model to compute user satisfaction it is possible to increase the satisfaction over the simple models (i.e. minimize time, minimize cost).
%
%In this work we study the influence of the explanations the user get on the satisfaction. We show that selecting the right explanation, improves the satisfaction.
%
%One motivation for explanations is Explainable AI (XAI). Our model selects explanations independently of the way the assignments of the requests were. So given black box that had objective to increase satisfaction, we can say that our model explains well the assignment, since the model is also focused in user satisfaction.





% we show that it is possible to increase satisfaction by selecting the right explanation to provide. Using a deep learning method and data gathered from Mechanical Turk we trained an agent that selects the explanation to show. We compared it to providing random explanations.

\section{Related Work}



Most work on ridesharing has focused on the assignment of passengers to vehicles. See the comprehensive surveys by \citeauthor{parragh2008a}~\cite{parragh2008a,parragh2008b}, and a recent survey by \citeauthor{psaraftis2016dynamic}~\cite{psaraftis2016dynamic}.
In particular, the dial-a-ride problem (DARP) is traditionally distinguished from other problems of ridesharing since transportation cost and user inconvenience must be weighed against each other in order to provide an appropriate solution~\cite{cordeau2003tabu}. Therefore, the DARP typically includes more quality constraints that aim at capturing the user's inconvenience. We refer to a recent survey on DARP by Molenbruch et al.~\cite{molenbruch2017}, which also makes this distinction.
Similar to the DARP we are interested in the satisfaction of the passenger, but instead of developing assignment algorithms (as common in this field) we emphasize the importance of explanations of a given assignment.

A domain closely related to ridesharing is car-pooling. In this domain, ordinary drivers, may opt to take an additional passenger on their way to a shared destination. The common setting of car-pooling is within a long-term commitment between people to travel together for a particular purpose, where ridesharing is focused on single, non-recurring trips.
Indeed, several works investigated car-pooling that can be established on a short-notice, and they refer to this problem as ridesharing~\cite{agatz2012optimization}.
In this paper we focus on ridesharing since it seems that our explanations regarding the alternative modes of transportation are more suitable for this domain (even though they might be also helpful for car-pooling).
%We stress that in our ridesharing problem we do not consider the drivers as a customer of the system, that is, the drivers do not have their own travel plans (see~\cite{furuhata2013ridesharing} that also makes this distinction). %Therefore, the problem is also sometimes referred as taxi ridesharing~\cite{ma2013t,ma2015real}.


% We model our problem as signalling games, as done in other problems, for example ~\cite{shen2011signaling}. We choose the strategy in this model by looking for Perfect Bayesian Equilibrium, which is defined by ~\cite{fudenberg1991perfect}.

Explainable AI (XAI) is another domain related to our work \cite{core2006building,gunning2017explainable,carvalho2019machine}. In a typical XAI setting, the goal is to explain the output of the AI system to a human. This explanation is important for allowing the human to trust the system, better understand, and to allow transparency of the system's output \cite{adadi2018peeking}. Other XAI systems are designed to provide explanations, comprehensible by humans, for legal or ethical reasons \cite{doran2017does}. For example, an AI system for the medical domain might be required to explain its choice for recommending the prescription of a specific drug \cite{holzinger2017we}. %Amos: add an example! A doctor providing an prognosis to a patient ...
Despite the fact that our agent is required to provide explanations to a human, our work does not belong to the XAI settings. In our work the explanations do not attempt to explain the output of the system to a passenger but to provide additional information that is likely to increase the user's satisfaction from the system. Therefore, our work can be seen as one of the first instances of x-MASE \cite{Kraus2019ai}, explainable systems for multi-agent environments.

In our work we build an agent that provides information to a human passenger in order to increase her satisfaction. There are other works in which an agent provides information to a human user (in the context of the roads network) for different purposes. For example, Azaria et al.~\shortcite{azaria2012giving,azaria2012strategic,azaria2015strategic} develop agents that provide information or advice to a human user in order to convince her to take a certain route. Bilgic and Mooney \cite{bilgic2005explaining} present methods for explaining the decisions of a recommendation system to increase the user satisfaction. In their context, user satisfaction is interpreted only as an accurate estimation of the item quality.



\section{The PBE Agent}

We model our setting with the following signaling game.
We assume that there is a given random variable $X$ with a prior probability distribution over the possible prices of a given alternative mode of transportation. The possible values of $X$ are bounded within the range $[min,max]$\footnote{Without loss of generality, we assume that $Pr(X=min)>0$ for a discrete distribution, and $F_X(min+\epsilon)>0$ for a continuous distribution, for every $\epsilon > 0$.}.

The game is composed of three players: nature, player 1 (agent) and player 2 (passenger). It is assumed that both players are familiar with the prior distribution over $X$.
Nature randomly chooses a number $x$ according to the distribution over $X$.
The agent observes the number $x$ and her possible action, denoted $a_1$, is either  $\varphi$ (quiet) or $x$ (say).
The passenger observes the agent's action and her action, denoted $a_2$, is any number in the range $[min,max]$.
In our setting the agent would like the passenger to think that the price of the alternative is as high as possible, while the passenger would like to know the real price. Therefore, we set the utility for the agent to $a_2$ and the utility of the passenger to $-(a_2-x)^2$. Note that we did not define the utility of the passenger to be simply $-|a_2-x|$, since we want the utility to highly penalize a large deviation from the true value.

We first note that if the agent plays $a_1 \neq \varphi$ then the passenger knows that $a_1$ is nature's choice. Thus, a rational passenger would play $a_2=a_1$. On the other hand, if the agent plays $a_1=\varphi$ then the passenger would have some belief about the real price, which can be the original distribution of nature, or any other distribution. We show that the passenger's best response is to play the expectation of this belief. Formally,
\begin{lemma}
\label{lemma:belief}
Assume that the agent plays $a_1=\varphi$, and let $Y$ be a belief over $x$. That is, $Y$ is a random variable with a distribution over $[min,max]$. Then, $\argmax_{a_2} E[-(a_2-Y)^2] = E[Y]$. \end{lemma}
\begin{proof}
Instead of maximizing $E[-(a_2-Y)^2]$ we can minimize $E[(a_2-Y)^2]$. In addition, $E[(a_2-Y)^2] = E[(a_2)^2] -2E[a_2 Y] + E[Y^2] = (a_2)^2 -2a_2 E[Y] + E[Y^2]$. By differentiating we get that
\[ \frac d {da_2} \left((a_2)^2 -2a_2 E[Y] + E[Y^2]\right) = 2a_2 -2E[Y].\]
The derivative is $0$ when $a_2 = E[Y]$ and the second derivative is positive; this entails that
\[\argmin_{a_2} \left((a_2)^2 -2a_2 E[Y] + E[Y^2]\right) = E[Y]\]
\end{proof}

Now, informally, if nature chooses a ``high'' value of $x$, the agent would like to disclose this value by playing $a_1=x$. One may think that if nature chooses a ``low'' value of $x$, the agent would like to hide this value by playing $a_1=\varphi$. However, Theorem~\ref{thm:pbe} shows that a rational agent should always disclose the true value of $x$ (unless $x=min$). We begin by applying the definition of PBE to our signaling game.
\begin{definition}
A tuple of strategies and a belief, $(\sigma_1, \sigma_2, \mu_2)$, is said to be a perfect Bayesian equilibrium in our setting if the following hold:
\begin{enumerate}
    \item The strategy of player 1 is a best response strategy. That is, given $\sigma_2$ and $x$, deviating from $\sigma_1$ does not increase player 1's utility. %$\forall x \forall a_1\, u_1(a_1, \sigma_2, x) \leq u_1(\sigma_1(x), \sigma_2, x)$
    \item The strategy of player 2 is a best response strategy. That is, given $a_1$, deviating from $\sigma_2$ does not increase player 2's expected utility according to her belief.
    %: $\forall x,a_1 \forall a_2 E[u_2(a_2, a_1, x) | \mu_2] \leq E[u_2(\sigma_2(a_1), a_1, x) | \mu_2]$
    \item $\mu_2$ is a consistent belief.
    That is, $\mu_2$ is a distribution over $x$ given $a_1$, which is consistent with $\sigma_1$ (following Bayes rule, where appropriate).
    %of the value of $x$, when $a_1$ is given. For $a_1$ such $\exists x_0 Pr(a_1 | x=x_0)>0$, Bayes rule must hold: $\mu(x|a_1)=\frac{Pr(a_1|x)Pr(x)}{\sum_{x_0} Pr(a_1 | x=x_0)}$
\end{enumerate}
\end{definition}

\begin{theorem}
\label{thm:pbe}
%A tuple $(S_1,S_2,B)$ is a PBE if and only if:
%\begin{itemize}
%    \item $S_1$: if $x>min$ then $a_1=x$, otherwise $a_1$ is any strategy.
%    \item $S_2$: if $a_1 \neq \varphi$ then $a_2=a_1$, otherwise $a_2=min$.
%    \item $B$: if $a_1 \neq \varphi$ then $P(x=a_1)=1$, otherwise $P(x=min)=1$.
%\end{itemize}
A tuple of strategies and a belief, $(\sigma_1, \sigma_2, \mu_2)$, is a PBE if and only if:
\begin{itemize}
    \item $\sigma_1(x)=\begin{cases} x: & x>min \\ \text{anything}: & x=min \end{cases}$
    \item $\sigma_2(a_1)= \begin{cases} a_1: & a_1 \neq \varphi \\ min: & a_1=\varphi \end{cases}$
    \item $\mu_2(x=a_1| a_1\neq \varphi) = 1$ and $\mu_2(x=min| a_1=\varphi)=1$.
\end{itemize}
\end{theorem}
\begin{proof}
%Clearly, the passenger's belief is consistent with the agent's strategies. Furthermore, the passenger's strategy %maximizes her expected utility under her belief. Given the passenger's strategy, the agent has no incentive to %deviate. Therefore, $(S_1,S_2,B)$ is a PBE.
($\Leftarrow$) Such a tuple is a PBE: $\sigma_1$ is a best response strategy, since the utility of player 1 is $x$ if $a_1=x$ and $min$ if $a_1=\varphi$. Thus, playing $a_1=x$ is a weakly dominating strategy.
$\sigma_2$ is a best response strategy, since it is the expected value of the belief $\mu_2$, and thus it is a best response  according to Lemma \ref{lemma:belief}.
Finally, $\mu_2$ is consistent:
If $a_1=\varphi$ and according to $\sigma_1$ player 1 plays $\varphi$ with some probability (greater than 0), then according to Bayes rule $\mu_2 (x=min| a_1=\varphi)=1$. Otherwise, Bayes rule cannot be applied (and it is thus not required).
If $a_1\neq \varphi$, then by definition $x=a_1$, and thus $\mu_2(x=a_1| a_1\neq \varphi)=1$.

($\Rightarrow$)
Let $(\sigma_1, \sigma_2, \mu_2)$ be a PBE.
It holds  that $\mu_2(x=a_1| a_1\neq \varphi) = 1$ by Bayes rule, implying that if $a_1\neq\varphi$, $\sigma_2(a_1)=a_1$. Therefore, when $a_1=x$ the utility of player 1 is $x$. %payoff of player 1 is $u_1(a_1,\sigma_2, x)=x$.

%We first show that in every PBE, where $a_1=\varphi$ it holds that $a_2=min$.
We now show that $\sigma_2(a_1=\varphi) = min$.
%Assume by contradiction that $a_2=c>min$, or $E[a_2]=c$ if the passenger plays a mixed strategy.
Assume by contradiction that $\sigma_2(a_1=\varphi)\neq min$ (or $p(\sigma_2(a_1=\varphi)=min) < 1$), then $E[\sigma_2(\varphi)]=c>min$.
We now imply the strategy of player 1. There are three possible cases: if $x>c$, then $a_1=x$ is a strictly dominating strategy. If $x < c$, then $a_1=\varphi$ is a strictly dominating strategy.
If $x=c$, there is no advantage for either playing $\varphi$ or $x$; both options give player 1 a utility of $c$, and thus she may use any strategy.
That is,
$$\sigma_1(x)=\begin{cases}x: & x>c \\ \varphi: & x<c \\ \text{anything}: & x=c.\end{cases}$$
%Using the implied agent's strategy, let $c^\prime$ be the expected value of $X$ given that the agent played $\varphi$, that is $c^\prime = E[X| a_1=\varphi]$. If when $x=c$ the agent plays $\varphi$, $c^\prime = E[X| X\leq c]$. In the more general case it holds that $c^\prime \leq E[X| X\leq c]$. Since $c>min$,  $E[X| X\leq c] < c$.
%
Given this strategy, we need to apply Bayes rule to derive $\mu_2(x| a_1=\varphi)$. By $\sigma_1$, it is possible that $a_1=\varphi$ only if $x\leq c$. That is, $\mu_2(x>c| a_1=\varphi)=0$ and $\mu_2(x \leq c| a_1=\varphi)=1$. Therefore, the expected value of the belief, $c^\prime = E[\mu_2(x|a_1=\varphi)]$, and according to Lemma \ref{lemma:belief}, $\sigma_2(\varphi) = c^\prime$. However, $c^\prime = E[\mu_2(x|a_1=\varphi)] \leq E[x | x\leq c]$ which is less than $c$, since $c>min$. That is,
$E[\sigma_2(\varphi)]=c^\prime < c$, which is a contradiction.
%
%According to the consistency of beliefs in PBE, the expected value over the passenger's belief given that $a_1=\varphi$ must equal $c^\prime$. Therefore, according to Lemma \ref{lemma:belief}, $a_2 = c^\prime < c$. Contradiction.
Therefore, the strategy for player 2 in every PBE is determined. In addition, since $\sigma_2(\varphi) = E[\mu_2(x| a_1=\varphi)]$ according to Lemma~\ref{lemma:belief}, then $\mu_2(x| a_1=\varphi)=min$, and the belief of player 2 in every PBE is also determined.

We end the proof by showing that for $x>min$, $\sigma_1(x)=x$. Since $\sigma_2$ is determined, the utility of player 1 is $min$ if $a_1=\varphi$ and $x$ if $a_1=x$. Therefore, when $x>min$, playing $a_1=x$ is a strictly dominating strategy.

%Now, since the passenger strategy that $a_2=min$ whenever $a_1=\varphi$, therefore,
%if $x>min$ then $a_1=x$ is a strictly dominating strategy for the agent. If $x=min$ then the passenger for any strategy

%In addition, if $x>min$
%Now, assume $x>min$. The utility of playing $a_1=\varphi$ is $min$, which is less than playing $a_1=x$. So for every $x>min$ in PBE, the agent plays $a_1=x$. This ends our proof.
%the passenger's belief, given that the agent played $\varphi$
%playing $c$ (expected) is not the best response, as $E[X| X<c]<c$ is better. Contradiction.
\end{proof}

The provided analysis can be applied to any alternative mode of transportation and to any type of price (e.g. travel-time or cost). We thus conclude that the PBE agent must provide all of the possible explanations.










% Ridesharing
% Automatic
% Selection
% Explanations
% Passengers / Human / User
% Alternatives
% Satisfaction
%
%SEAR
%ERS
%ERIS (Explaining of Ridingsharing for Increasing Satisfaction)
% AXIS (Automatic eXplainer for Increasing Satisfaction)
%

\section{The AXIS Agent}
\label{sec:AXIS}
The analysis in the previous section is theoretical in nature. However, several studies have shown that algorithmic approaches that use a pure theoretically analytic objective often perform poorly with real humans. Indeed, we conjecture that an agent that selects a subset of explanations for a given scenario will perform better than the PBE agent.
In this section we introduce our Automatic eXplainer for Increasing Satisfaction (AXIS) agent.
The AXIS agent has a set of possible explanations, and the agent needs to choose the most appropriate explanations for each scenario. Note that we do not limit the number of explanations to present for each scenario, and thus AXIS needs also to choose how many explanations to present. AXIS was built in $3$ stages.
%We begin by describing how the database of explanations was built.

First, an initial set of possible explanations needs to be defined. We thus consider the following possible classes of factors of an explanation. Each explanation is a combination of one factor from each class:
\begin{enumerate}
    \item Mode of alternative transportation: a private taxi ride or public transportation.
    \item Comparison criterion: time or cost.
    \item Visualization of the difference: absolute or relative difference.
    \item Anchoring: the shared ride or the alternative mode of transportation perspective.
\end{enumerate}
For example, a possible explanation would consist of a private taxi for class $1$, cost for class $2$, relative for class $3$, and an alternative mode of transportation perspective for class $4$. That is, the explanation would be ``a private taxi would have cost $50\%$ more than a shared ride''. Another possible explanation would consist of public transportation for class $1$, time for class $2$, absolute for class $3$, and a shared ride perspective for class $4$. That is, the explanation would be ``the shared ride saved $10$ minutes over  public transportation''. Overall, there are $2^4 = 16$ possible combinations. In addition, we added an explanation regarding the saving of $CO_2$ emission of the shared ride, so there will be an alternative explanation for the case where the other options are not reasonable.
Note that the first two classes determine which information is given to the passenger, while the later two classes determine how the information is presented. We denote each possible combination of choosing form the first two classes as a \textit{information setting}. We denote each possible combination of choosing form the latter two classes as a \textit{presentation setting}.
%
%For example, a possible explanation from the alternative mode of transportation perspective (anchoring) is that a private taxi (mode of alternative transportation) would have cost (comparison criterion) $50\%$ more (visualization of the difference) than a shared ride. An explanation from the shared ride perspective (anchoring) is that the shared ride saved $10$ (visualization of the difference) minutes (comparison criterion) over a public transportation (mode of alternative transportation).

Presenting all $17$ possible explanations with the additional option of ``none of the above'' requires a lot of effort from the human subjects to choose the most appropriate option for each scenario. Thus, in the second stage we collected data from human subjects regarding the most appropriate explanations, in order to build a limited subset of explanations. Recall that there are $4$ possible information settings and $4$ possible presentation settings. We selected for each information setting the corresponding presentation setting that was chosen (in total) by the largest number of people. We also selected the second most chosen presentation setting for the information setting that was chosen by the largest number of people. Adding the explanation regarding the $CO_2$ emissions we ended with $6$ possible explanations.

In the final stage we collected again data from people, but we presented only the $6$ explanation to choose from. This data was used by AXIS to learn which explanations are appropriate for each scenario. AXIS uses a neural network with two hidden layers, one with $8$ neurons and the other one with $7$ neurons, and the logistic activation function (implemented using Scikit-learn \cite{scikit-learn}). The number of neurons and hidden layers was determined based on the performance of the network.
AXIS used $10\%$ of the input as a validation set (used for early stopping) and $40\%$ as the test set. AXIS predicts which explanations were selected by the humans (and which explanations were not selected) for any given scenario.

%Describe the process:
%describe the options
%say that we run an experiment for selecting the top 6 (one from every group) + from the top group we select 2 best options.
%use machine learning to predict the best explanations.


\section{Experimental Design}
%last mile
In this section we describe the design of our experiments. Since AXIS generates explanations for a given assignment of passengers to vehicles, we need to generate assignments as an input to AXIS. To generate the assignments we first need a data-set of ride requests.

%Query Generation with NYC taxi data
To generate the ride requests we use the New York city taxi trip data-set \footnote{\url{https://data.cityofnewyork.us/Transportation/2016-Green-Taxi-Trip-Data/hvrh-b6nb}}, which was also used by other works that evaluate ridesharing algorithms (see for example, \cite{lin2016model,biswas2017profit}). We use the data-set from 2016, since it contains the exact GPS locations for every ride. %To calculate the duration of each ride we use Google Maps (through Google Maps API).

%assignment
We note that the data-set contains requests for taxi rides, but it does not contain a data regarding shared-rides. We thus need to generate assignments of passengers to taxis, based on the requests from the data-set. Now, if the assignments are randomly generated, it may be hard to provide reasonable explanations, and thus the evaluation of AXIS in these setting is problematic. We thus concentrate on requests that depart from a single origin but have different destinations, since a brute force algorithm can find the optimal assignment of passengers to taxis in this setting.

%assignment brute force algorithm
We use the following brute force assignment algorithm. The algorithm receives $12$ passengers and outputs the assignment of each passenger to vehicle that minimizes the overall travel distance. We assume that every vehicle can hold up-to four passengers.
%Finding optimal assignment in the term of cost using Bell recursion and exhaustive search in the TSP.
The brute force assignment algorithm recursively considers all options to partition the group of $12$ passengers to subsets of up to four passengers. We note that there are $3,305,017$ such possible partitions. %(this is obtained using the Bell number, when limiting the number of each subset to $4$).
The algorithm then solves the
%In order to get groups of 2-4 riders to share a ride we took each 12 requests and checked all of the possibilities, using Bell recursion to get all partitions (with sets up to size 4), and solving the
Travel Salesman Problem (TSP) in each group, by exhaustive search, to find the cheapest assignment.
%
Solving the TSP problem on 4 destinations (or less) is possible using exhaustive search since there are only $4!=24$ combinations. The shortest path between each combination is solved using a shortest distance matrix between all locations.
In order to compute this matrix we downloaded the graph that represents the area of New York from Open Street Map (using OSMnx \cite{boeing2017osmnx}), and ran the Floyd-Warshall's algorithm.
%This brute force algorithm is feasible, since there are only up to  such options.

%We define $B(A, k)$ as the set of all partitions of $A$ with subsets of up to size $k$.
%If $A$ is empty then we finish with only one partition with no sets, so $A(\phi, k) = \{\{\}\}$.
%otherwise, we take $x\in A$ and build its subset in the partition. The possibilities are ${A\setminus \{x\} \choose k-1}$ then the rest of the set is partitioned with recursion, and each partition we merge with the set of $x$. So $B(A, k) = \bigcup_{X\in {$

We set the origin location to JFK Station, Sutphin Blvd-Archer Av, and the departing time to 11:00am. See Figure \ref{fig:destinations} where the green location is the origin, and the blue locations are the destinations.

\begin{figure}[hbpt]
\centering
\includegraphics[width=3.5in]{map2.pdf}
\caption{A map depicting the origin (in green) and destinations (in blue) of all rides considered.}
\label{fig:destinations}
\end{figure}

%Get the cost of rides from Taxi Fare Finder, durations from Google Maps, and cost of public transportation 2.5 * number of rides, got by Google Maps.
In order to calculate the duration of the rides we use Google Maps (through Google Maps API). Specifically, the duration of the private taxi ride was obtained using ``driving'' mode, and the duration of the public transportation was obtained using ``transit'' mode. The duration of the shared-ride was set as the duration of the ride to the destination of the last passenger (using ``driving'' mode) with the destinations of the other passengers as way-points.

In order to calculate the cost of the private ride we use Taxi Fare Finder (through their API)\footnote{\url{https://www.taxifarefinder.com/}}. The cost for public transportation was calculated by the number of buses required (as obtained through Google Maps API), multiplied by $\$2.5$ (the bus fare). The cost for the shared-ride was obtained from Taxi Fare Finder. Since this service does not support a ride with way-points, we obtained the cost of multiple taxi rides, but we included the base price only once. Note that this is the total cost of the shared-ride.
%Proportional cost divided to the riders of each vehicle.
The cost for a specific passenger was determined by the proportional sharing pricing function \cite{fishburn1983fixed}, which works as follows. Let $c_{p_i}$ be the cost of a private ride for passenger $i$, and let $total_s$ be the total cost of the shared ride. In addition, let $f=\frac{total_s}{\sum_i c_{p_i}}$. The cost for each passenger is thus $f \cdot c_{pi}$.

We ran 4 experiments in total. Two experiments were used to compose AXIS (see Section~\ref{sec:AXIS}), and the third and fourth experiments compared the performance of AXIS with that of other agents (see below).
All experiments used the Mechanical Turk platform, a crowd-sourcing platform that is widely used for running experiments with human subjects \cite{amir2012economic,paolacci2010running}.
Unfortunately, since participation is anonymous and linked to monetary incentives, experiments on a crowd-sourcing platform can attract participants who do not fully engage in the requested tasks \cite{turner2012using}. Therefore, the subjects were required to have at least $99\%$ acceptance rate and were required to have previously completed at least $500$ Mechanical Turk Tasks (HITs). In addition, we added a sanity check question for each experiment, which can be found in the Appendix.

In the first two experiments, which were designed for AXIS to learn what people believe are good explanations, the subjects were given several scenarios for a shared ride. The subjects were told that they are representatives of a ride sharing service, and that they need to select a set of explanations that they believe will increase the customer's satisfaction. Each scenario consists of a shared-ride with a given duration and cost.

In the third experiment we evaluate the performance of AXIS against the PBE agent. %, which provides the $6$ facts.
%Create random explanations by choosing uniformly 1-4 explanations from the total 17 (but with out dups)
The subjects were given $2$ scenarios. Each scenario consists of a shared-ride with a given duration and cost and it also contains either the explanations that are chosen by AXIS or the information that the PBE agent provides:
the cost and duration a private ride would take, and the cost and the duration that public transportation would have taken. The subjects were asked to rank their satisfaction from each ride on a scale from 1 to 7.

In the forth experiment we evaluate the performance of AXIS against a random baseline agent.
The random explanations were chosen as follows: first, a number between $1$ and $4$ was uniformly sampled. This number determined how many explanations will be given by the random agent. This range was chosen since over $93\%$ of the subjects selected between $1$ and $4$ explanations in the second experiment.
Recall that there are 4 classes of factors that define an explanation, where the fourth class is the anchoring perspective (see Section~\ref{sec:AXIS}). The random agent sampled explanations uniformly, but it did not present two explanations that differ only by their anchoring perspective.
%The explanations were then uniformly sampled from the 9 possible explanations, and for each explanation (except $CO_2$) the agent randomly chose the anchoring mode.
%
The subjects were again given $2$ scenarios. Each scenario consists of a shared-ride with a given duration and cost and it also contains either the explanations that are chosen by AXIS or the explanations selected by the random agent. The subjects were asked to rank their satisfaction from each ride.
The exact wording of the instructions for the experiments can be found in the Appendix.

$953$ subjects participated in total, all from the USA.
The number of subjects in each experiment and the number of scenarios appear in Table~\ref{tbl:participants}.
Tables \ref{tbl:gender} and \ref{tbl:education} include additional demographic information on the subjects in each of the experiments. The average age of the subjects was $39$. % in all four experiments.

\begin{table}
\centering
\begin{tabular}{ c|c c c c c }
\hline
 & \#1 & \#2 & \#3 & \#4 & Total\\
 \hline
Number of subjects & 343 & 180 & 156 & 274 & 953\\
Scenarios per subject & 2 & 4 & 2 & 2 & -\\
Total scenarios & 686 & 720 & 312 & 548 & 3266\\
 \hline
\end{tabular}
\caption{Number of subjects and scenarios in each of the experiments.}
\label{tbl:participants}
\end{table}


\begin{table}
\centering
\begin{tabular}{ c|c c c c c}
\hline
 & \#1 & \#2 & \#3 & \#4 & Total\\
 \hline
Male & 157 & 66 & 52 & 117 & 392\\
Female & 183 & 109 & 104 & 153 & 549\\
Other or refused & 3 & 5 & 0 & 4 & 12\\
 \hline
\end{tabular}
\caption{Gender distribution for each of the experiments.}
\label{tbl:gender}
\end{table}


\begin{table}
\centering
\begin{tabular}{ c|c c c c c }
\hline
 & \#1 & \#2 & \#3 & \#4 & Total\\
 \hline
High-school & 72 & 39 & 38 & 80 & 229\\
Bachelor & 183 & 86 & 84 & 131 & 484\\
Master & 60 & 29 & 37 & 46 & 172\\
PhD & 15 & 2 & 0 & 10 & 27\\
Trade-school & 8 & 4 & 5 & 10 & 27\\
Refused or did not respond & 5 & 3 & 0 & 6 & 14\\
 \hline
\end{tabular}
\caption{Education level for each of the experiments.}
\label{tbl:education}
\end{table}


%Amos: I think that all this (until the end of the section) should be moved to the results section (some of it might need to be removed, as it repeats what we wrote in the AXIS section.

% %First questionnaire to determine the potential explanations out of 17
% To select explanations we firstly made 17 possible explanations, as follows.
% We compare the cost and duration of the shared ride to those of private taxi ride and public transportation, this give us four properties to compare. We compare each of them by difference and by percents. Each of these 8 comparisons we write one time the shared ride relative to the other, and one time the other relative to the shared ride, like "The shared ride took only X minutes more than what a private taxi would have take" or "With a private taxi would have saved only X minutes".
% One more explanation is the amount of $CO_2$ emission was saved. This amount is calculated by the amount of the emission if the passenger would has use private taxi minus the emission now.
% Each user asked about 2 scenarios, in each one she got origin and destination of some user that just finish a shared ride, the time and cost this ride took and given that, we asked to select the best explanations she think would be good to show to the user.

% % sanity

% %Second the priquestionnaire to gather information which rides lead to which information
% With the results of the first questionnaire we made second questionnaire with 6 possible explanations. We took from each property the most chosen one, which was difference relative to the shared ride, $CO_2$ emission, and for the highest class, private cost, we took also the second most chosen, which was percents, relative to the private taxi.
% As the first questionnaire, the responders asked to give the best explanation to the user of shared ride. They given the time and cost of shared ride and now had 6 explanations.

% %Deep learning to train model to predict explanations similar to the answers in the second questionnaire

% We had data of 562 rows of answers to scenarios in the second questions. We used 240 of them to train deep learning model with one layer of size 6. 10\% of them used for validation. We got accuracy of 18\% of rows that predicted perfectly, and total of 75\% of the checks were correct.



%Compare it with random explanations
% We took the predictions of our model to compare it against random explanations.
% We made two independent questionnaire, in one we describe the shared ride with origin, destination, time and cost and some random explanations, and in the other, the same, but with our predicted explanations.



%explain how we implemented the random agent

\begin{figure}[hbpt]
\centering
\includegraphics[width=\columnwidth]{bar_q1.pdf}  %//5in
\caption{The percent of scenarios that every explanation was selected in the first experiment.
The explanations marked in green were selected for the second experiment.}
\label{fig:humanResults1}
\end{figure}

\section{Results}
%describe the number of people + statistics

%graph of first experiment (columns) - separated by groups.
Recall that the first experiment was designed to select the most appropriate explanations (out of the initial $17$ possible explanations). The results of this experiment are depicted in Figure~\ref{fig:humanResults1}. The x-axis describes the possible explanations according to the $4$ classes. Specifically, the factor from the anchoring class is denoted by s-p or p-s; s-p means that the explanation is from the shared-ride perspective, while p-s means that it is from the alternative (private/public) mode of transportation. The factor from the comparison criterion class is denoted by $\Delta$ or $\%$; $\Delta$ means that the explanation presents an absolute difference while $\%$ means that a relative difference is presented. We chose $6$ explanations for the next experiment, which are marked in green.



As depicted by Figure \ref{fig:humanResults1}, the subjects chose explanations that compare the ride with a private taxi more often than those comparing the ride with public transportation. We believe that this is because from a human perspective a shared-ride resembles a private taxi more than public transportation.
Furthermore, when comparing with a private taxi, the subjects preferred to compare the shared-ride with the \emph{cost} of a private taxi, while when comparing to public transportation, the subjects preferred to compare it with the travel time. This is expected, since the travel time by a private taxi is less than the travel time by a shared ride, so comparing the travel time to a private taxi is less likely to increase user satisfaction.
We also notice that with absolute difference the subjects preferred the shared ride perspective, while with relative difference the subjects preferred the alternative mode of transportation perspective. We conjecture that this is due to the higher percentages when using the alternative mode prospective. For example, if the shared ride saves $20\%$ of the cost when compared to a private ride, the subjects preferred the explanation that a private ride costs $25\%$ more.
%the subjects preferred, in most cases, to mention the difference relative to the shared ride. For example, in time saving over public transportation, while 40\% selected the explanation of difference relative to the shared ride, only 28\% selected the explanation of difference relative to public transportation, and only 25\% selected the explanation of percent relative to the shared ride.


\begin{figure}[hbpt]
\centering
\includegraphics[width=\columnwidth]{bar_q2.pdf}
\caption{The percent of scenarios that every explanation was selected in the second experiment. The obtained data-set was used to train AXIS.}
\label{fig:humanResults2}
\end{figure}

The second experiment was designed to collect data from humans on the most appropriate explanations (out of the $6$ chosen explanations) for each scenario. The results are depicted in Figure~\ref{fig:humanResults2}. This data was used to train AXIS.
%describe machine learning results here
The accuracy of the neural network on the test-set is $74.9\%$. That is, the model correctly predicts whether to provide a given explanation in a given scenario in almost $75\%$ of the cases.

\begin{figure}[hbpt]
\centering
\includegraphics[width=0.85\columnwidth]{Performance.pdf}
\caption{A comparison between the performance of AXIS, the PBE agent and the random agent. The bars indicate the $95\%$ confidence interval.
AXIS significantly outperformed both baseline agents $(p<0.001)$.}
\label{fig:humanResults3}
\end{figure}

The third experiment was designed to evaluate AXIS against the PBE  agent; the results are depicted in Figure~\ref{fig:humanResults3}.
AXIS outperforms the PBE agent; the difference is statistically significant $(p<10^{-5})$, using the student t-test.
We note that achieving such a difference is non-trivial since the ride scenarios are identical and only differ by the information that is provided to the user.

The forth experiment was designed to evaluate AXIS against the random baseline agent; the results are depicted in Figure~\ref{fig:humanResults3}.
AXIS outperforms the random agent; the difference is statistically significant $(p<0.001)$, using the student t-test.
We note that AXIS and the random agent provided a similar number of explanations on average ($2.551$ and $2.51$, respectively). That is, AXIS performed well not because of the number of explanations it provided, but since it provided appropriate explanations for the given scenarios.
%
%We note that while the difference between the two agents is $0.48$, this difference is significant, since the ride scenarios are identical and only differ by the selected explanations.

We conclude this section by showing an example of a ride scenario presented to some of the subjects, along with the information provided by the PBE agent, and the explanations selected by the random agent and by AXIS.
In this scenario the subject is assumed to travel by a shared ride from JFK Station to 102-3 188th St, Jamaica, NY.
The shared ride took $13$ minutes and cost $\$7.53$.
The PBE agent provided the following information:
\begin{itemize}
    \item ``A private ride would have cost $\$13.83$ and would have taken $12$ minutes''.
    \item ``Public transportation costs \$2.5 and would have taken 26 minutes''.
\end{itemize}
The random agent provided the following explanations:
\begin{itemize}
    \item ``A private taxi would have cost $\$6.3$ more''.
    \item ``A ride by public transportation would have saved you only $\$5.03$''.
\end{itemize}
Instead, AXIS selected the following explanations:
\begin{itemize}
    \item ``The shared ride had saved you $\$6.3$ over a private taxi''.
    \item ``A private taxi would have cost $83\%$ more''.
    \item ``The shared ride saved you $4$ minutes over public transportation''.
\end{itemize}
Clearly, the explanations provided by AXIS seem much more compelling.


\section{Conclusions and Future Work}

In this paper we took a first step towards the development of agents that provide explanations in a multi-agent system with a goal of increasing user satisfaction.
We first modeled the explanation selection problem as a signaling game and determined the unique set of Perfect Bayesian Equilibria (PBE). We then presented AXIS, an agent that, when given a shared-ride along with its possible alternatives, selects the explanations that are most likely to increase user satisfaction.
We ran four experiments with humans. The first experiment was used to narrow the set of possible explanations, the second experiment collected data for the neural network to train on, the third experiment was used to evaluate the performance of AXIS against that of the PBE agent, and the fourth experiment was used to evaluate the performance of AXIS against that of an agent that randomly chooses explanations. We showed that AXIS outperforms the other agents in terms of user satisfaction.

% future:
In future work we will consider natural language generation methods for generating explanations that are likely to increase user satisfaction. We also plan to extend the set of possible explanations, and to implement user modeling in order to provide explanations that are appropriate not only for a given scenario but also for a given specific user.

% Run experiment without any explanations - doesn't make much sense, as the participants have no reference point.
% Update all information with those that passed the sanity check
% Non-last mile: add waiting time, pick-up and drop-off order, number of other passengers.
% Interactive explanations: suggest single explanation and then update according to user response. Probably need more than the 17 explanations we had (maybe by using non-last mile).
% Think of a theoretical model of the problem (and show that it doesn't work in practice, when running experiments with humans)
% read the two papers from EC-rider


% \section{Acknowledgment}
% This research was supported in part by Volkswagen Foundation (Volkswagenstiftung) under grant "EC-RIDER: Explainable AI Methods for Human-Centric Ridesharing" and the Ministry of Science, Technology \& Space, Israel.





\clearpage
%\bibliographystyle{aaai}
\clearpage
\bibliography{explaining}

 %\clearpage
% \section*{Appendix}
%\subsection*{Sanity Check Question}
%Which of the following claims do you agree with?
%\begin{itemize}
%    \item A shared ride may take longer than a private ride.
%    \item A shared ride is supposed to be more expensive than a private ride.
%    \item The cost of public transportation is usually less than the cost of a private ride.
% \item In a private ride there are at least 3 passengers.
% \item Public transportation usually takes longer than a  private ride.
%\end{itemize}

%\subsection*{The Text for the First Two Experiments}
%In this survey we would like to learn which explanations increase the satisfaction from a ride sharing service.
%Suppose that you are a representative of a ride sharing service. This service assigns multiple passengers with different destinations to a shared taxi and divides the cost among them. Assume that the customer of your service has just completed the shared ride.
%Below are given few scenarios for a shared ride. For each of the scenarios you should choose one or more suitable explanation(s) that you believe will increase the customer's satisfaction.
%%%%%The subjects were then given a set of scenarios and were required to select a set of explanations see Section~\ref{sec:AXIS}.

%\subsection*{The Text for the Third and Fourth Experiments}
%%%%In the final experiment the subjects were received the following text:
%In this survey we would like to evaluate your satisfaction from using shared taxi services.
%The following questions contain description of some shared rides. Please provide your rate of satisfaction from each ride on a scale from 1 to 7.
%Please read the details carefully and try to evaluate your satisfaction in each scenario as accurate as possible. Good luck!


%commented out....
%The subjects were then given a set of scenarios, along with the explanations of one of the agents, and were required to report their satisfaction level from each scenario.

%Suppose that you have got a shared taxi ride from Sutphin Blvd-Archer Av-JFK Station, Queens, NY 11435, USA to 102-3 188th St, Jamaica, NY 11423, USA.
%The shared ride took 13 minutes and cost \$7.53.

%A private taxi would have cost \$6.3 more.
%A ride by public transportation would have saved you only \$5.03.
%How satisfied are you from this ride?
% ''

\end{document}


%received the following text:
% ``In this survey we would like to learn which explanations increase the satisfaction from a ride sharing service.
% Suppose that you are a representative of a ride sharing service. This service assigns multiple passengers with different destinations to a shared taxi and divides the cost among them. Assume that the customer of your service has just completed the shared ride.
% Below are given few scenarios for a shared ride. For each of the scenarios you should choose one or more suitable explanation(s) that you believe will increase the customer's satisfaction.''
% The subjects were then given a set of scenarios and were required to select a set of explanations see Section~\ref{sec:AXIS}.
% In the final experiment, in which we compared AXIS to the random agent, the subjects were received the following text:
% ``In this survey we would like to evaluate your satisfaction from using shared taxi services.
% The following questions contain description of some shared rides. Please provide your rate of satisfaction from each ride on a scale from 1 to 7.
% Please read the details carefully and try to evaluate your satisfaction in each scenario as accurate as possible. Good luck!''
% The subjects were then given a set of scenarios, along with the explanations of one of the agents, and were required to report their satisfaction level from each scenario.