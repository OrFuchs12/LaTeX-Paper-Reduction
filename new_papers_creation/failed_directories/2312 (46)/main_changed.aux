\relax 
\bibstyle{aaai24}
\citation{orig_adam}
\citation{carlini2019secret,carlini2021extracting,carlini2022membership,balle2022reconstructing}
\citation{dwork2006calibrating,abadi2016deep}
\citation{wasserman2010statistical}
\citation{li2021}
\citation{daigavane2022nodelevel}
\citation{orig_adam}
\citation{kunstner2023heavytailed}
\citation{tang2023dpadambc}
\citation{orig_adam}
\providecommand \oddpage@label [2]{}
\newlabel{sec:intro}{{1}{1}{}{}{}}
\citation{kunstner2023heavytailed}
\citation{abadi2016deep}
\citation{li2021}
\citation{tang2023dpadam}
\newlabel{sec:background}{{2}{2}{}{}{}}
\newlabel{sec:motiv}{{3}{2}{}{}{}}
\newlabel{sec:motiv-bias}{{3}{2}{}{}{}}
\newlabel{eq:mt}{{1}{2}{}{}{}}
\newlabel{eq:vt}{{2}{2}{}{}{}}
\citation{orig_adam}
\citation{orig_adam}
\citation{kunstner2023heavytailed}
\citation{balles2020dissecting}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:motiv_10001}{{1}{3}{Histogram of \textbf  {Left:} un-noised ($\hat  {m}_t^c$) and private ($\hat  {m}_t^p$) first moment estimates, \textbf  {Right:} un-noised ($\hat  {v}_t^c$) and private ($\hat  {v}_t^p$) second moment estimates near end of training at $t=10000$, using the SNLI dataset with $B=256, C=0.1, \sigma =0.4, \beta _2=0.999$, $\Phi \approx \textnormal  {2.441e-8}$ for large $t$.}{}{}}
\newlabel{assumption:stationarity}{{1}{3}{}{}{}}
\newlabel{sec:motiv-sgdm}{{3}{3}{}{}{}}
\newlabel{eq:lr_convert}{{3}{3}{}{}{}}
\citation{mohapatra2021role}
\citation{abadi2016deep}
\newlabel{fig:motiv_sgdm}{{2}{4}{DP-Adam behaves similarly to DP-SGDM with a specific learning rate (lr) schedule. \textbf  {Left: }The implied lr schedule of DP-SGDM. \textbf  {Middle: } DP-Adam and DP-SGDM with the specific lr schedule has similar training performance with a mean squared difference of 0.015 in training loss. \textbf  {Right: }Performance degrades when adding a larger constant bias to un-noised (clipping-only) DP-Adam as it transitions to behave more like DP-SGDM.}{}{}}
\newlabel{sec:method}{{4}{4}{}{}{}}
\newlabel{eq:adam_corr}{{4}{4}{}{}{}}
\newlabel{alg:dp_adam}{{1}{4}{}{}{}}
\newlabel{prop:privacy_guarantee}{{1}{4}{}{}{}}
\newlabel{proposition:consistent}{{2}{4}{}{}{}}
\citation{défossez2022simple}
\citation{li2023dp2}
\citation{cifar10_data}
\citation{bowman-etal-2015-large}
\citation{wang2019glue}
\citation{hu2021open}
\citation{bert_paper}
\citation{daigavane2022nodelevel}
\newlabel{prop:bound_fixed}{{3}{5}{}{}{}}
\newlabel{prop:bound_martingale}{{4}{5}{}{}{}}
\newlabel{sec:exp}{{5}{5}{}{}{}}
\citation{kunstner2023heavytailed}
\citation{kunstner2023heavytailed}
\citation{wilson2018marginal}
\citation{daigavane2022nodelevel}
\citation{lidp2}
\newlabel{eval:empirical-effect-correction}{{5}{6}{}{}{}}
\newlabel{tab:res_multi_eps}{{1}{6}{Accuracy under different optimizers, for several privacy budgets. Hyper-parameters are tuned for each target $\epsilon $ and optimizer. Mean (standard deviation) over 5 runs for the best hyper-parameters.}{}{}}
\newlabel{fig:compare_dp2_snli_only}{{3}{6}{\textbf  {Left:} Comparison between DP2RMSProp, DP-AdamBC, DP-Adam and DP-SGD and \textbf  {Right:} the performance of DP2RMSProp with different phase switching frequency $s$ on SNLI with Bert-base.}{}{}}
\citation{Reddi2019}
\citation{orig_adam}
\newlabel{tab:exp2}{{2}{7}{Moment estimates with un-noised and noised gradient, w/ and w/o bias correction, at step $t=10000$.}{}{}}
\newlabel{fig:mt_vt_hist}{{4}{7}{Histogram of private ($\hat  {v}_t^p$), un-noised ($\hat  {v}_t^c$) and corrected ($\hat  {v}_t^{\textnormal  {corr}}$) second moment estimates with \textbf  {Left: } $\gamma '=3e$-12, \textbf  {Middle: }$\gamma '=3e$-10. \textbf  {Right: } private ($\Delta _t^p$) and corrected ($\Delta _t^{\textnormal  {corr}}$) Adam updates with respect to $m_t^{p}$. }{}{}}
\newlabel{fig:exp3_4}{{5}{7}{Performance when \textbf  {Upper Left: }subtracting different (fake) values of $\Phi $, \textbf  {Upper Right: }tuning $\gamma '$ in DP-AdamBC, \textbf  {Lower Left: }tuning $\gamma $ in DP-Adam, \textbf  {Lower Right: }tuning $\beta $s in DP-Adam. Tuning hyperparameters in DP-Adam cannot replace DP-AdamBC's bias correction. }{}{}}
\bibdata{main}
\citation{cifar10_data}
\citation{bowman-etal-2015-large}
\citation{wang2019glue}
\citation{hu2021open}
\citation{papernot2020tempered}
\citation{bert_paper}
\citation{daigavane2022nodelevel}
\citation{opacus}
\citation{deepmind2020jax}
\citation{opacus}
\citation{pytorch}
\citation{daigavane2022nodelevel}
\citation{daigavane2022nodelevel}
\newlabel{apdix:exp_setup}{{A}{11}{}{}{}}
\newlabel{apdix:consistent}{{B}{11}{}{}{}}
\citation{wainwright2019high}
\newlabel{apdix:bound}{{C}{12}{}{}{}}
\newlabel{lemma:zk2_sub_exponential}{{1}{12}{}{}{}}
\newlabel{appendix:bound_fixed}{{C}{12}{}{}{}}
\newlabel{appendix:bound_martingale}{{C}{12}{}{}{}}
\citation{wainwright2019high}
\newlabel{appendix:num-analysis}{{C}{13}{}{}{}}
\newlabel{apdix:more_res}{{D}{13}{}{}{}}
\newlabel{tab:num_bound_fixed_grad}{{3}{14}{Numerical values for relevant quantities of the bound as in Proposition \ref {prop:bound_fixed}.}{}{}}
\newlabel{tab:num_bound}{{4}{14}{Numerical values for relevant quantities of the bound as in Proposition \ref {prop:bound_martingale}.}{}{}}
\newlabel{apdix:additional_exp}{{D}{14}{}{}{}}
\citation{lidp2}
\citation{lidp2}
\citation{lidp2}
\citation{kunstner2023heavytailed}
\citation{lidp2}
\citation{lidp2}
\newlabel{tab:empirical_bound}{{5}{15}{Empirically measured values for deviance of the observed DP bias from $\Phi $.}{}{}}
\newlabel{fig:res_multi_eps}{{6}{15}{(From left to right) Comparing the performance of DP-Adam, DP-AdamBC and DP-SGD on QNLI and SNLI dataset (nlp), CIFAR10 (images) and obgn-arxiv (node classification) at different target pribacy budget ($\epsilon $). Each result is tuned separately. We report the mean (standard deviation) over 5 runs for the best parameters.}{}{}}
\newlabel{apdix:compare_dp2}{{D}{15}{}{}{}}
\newlabel{apdix:privacy_proof}{{E}{15}{}{}{}}
\citation{abadi2016deep}
\citation{abadi2016deep}
\citation{dwork2006calibrating}
\citation{dwork2006calibrating}
\citation{abadi2016deep}
\citation{Mironov_2019}
\citation{dwork2006calibrating}
\citation{abadi2016deep}
\newlabel{fig:exp_rel_sgdm_more}{{7}{16}{Compare train loss, test loss and test accuracy between DP-Adam and the DP-SGDM with the converted learning rate schedule as in Equation \ref {eq:lr_convert} on SNLI (top row) and CIFAR10 with relatively small $\Phi $ (middle row) and large $\Phi $ (bottom row) respectively. }{}{}}
\newlabel{fig:exp1}{{8}{16}{(From left to right) Comparing the performance of DP-Adam, DP-AdamBC and DP-SGD on QNLI and SNLI dataset (nlp), CIFAR10 (images) and obgn-arxiv (node classification). At the end of training $\epsilon \textnormal  {-DP} \approx 7$ for CIFAR10, QNLI and SNLI and $\epsilon \textnormal  {-DP} \approx 12$ for obgn-arxiv. Each optimizer is tuned separately. The x-axis is the step over a single training trajectory converted to privacy budget $\epsilon $ to make results comparable for different optimizers.}{}{}}
\citation{défossez2022simple}
\citation{défossez2022simple}
\citation{li2023dp2}
\citation{défossez2022simple}
\citation{wainwright2019high}
\newlabel{fig:additional_exp}{{9}{17}{(\textbf  {Left: }) Comparison between DP-AdamBC and DP-Adam when target $\epsilon =3$ on the SNLI dataset. DP-AdamBC shows $2.6\%$ advantage in final mean test accuracy. The final mean(standard deviation) test accurary for DP-AdamBC and DP-Adam are $50.1\%(1.6\%)$, $47.5\%(1.8\%)$ respectively. (\textbf  {Right: }) Comparison between DP-AdamBC and DP-Adam on the SST2 dataset with Bert-Large. DP-AdamBC shows 2.4\% advantage in final test accuracy compared to DP-Adam (68.80\% vs 66.4\%).}{}{}}
\newlabel{fig:compare_dp2_cifar_only}{{10}{17}{\textbf  {Left:} Comparison between DP2RMSProp, DP-AdamBC, DP-Adam and DP-SGD on CIFAR10 with CNN, \textbf  {Right:} The performance of DP2RMSProp with different phase switching frequency $s$ on CIFAR10 with CNN.}{}{}}
\newlabel{apdix:convergence_analysis}{{F}{17}{}{}{}}
\newlabel{assmption:f_bounded_below}{{2}{17}{}{}{}}
\newlabel{assmption:bounded_stochastic_grad}{{3}{17}{}{}{}}
\newlabel{lemma:gaussian_concentration_on_z}{{3}{17}{}{}{}}
\citation{défossez2022simple}
\citation{défossez2022simple}
\citation{défossez2022simple}
\newlabel{lemma:union_bound_on_v}{{1}{18}{}{}{}}
\newlabel{assmption:f_smooth}{{4}{18}{}{}{}}
\citation{défossez2022simple}
\citation{défossez2022simple}
\citation{défossez2022simple}
\citation{défossez2022simple}
\citation{défossez2022simple}
\citation{défossez2022simple}
\citation{défossez2022simple}
\citation{défossez2022simple}
\citation{défossez2022simple}
\citation{daigavane2022nodelevel}
\gdef \@abspage@last{21}
