\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Ahmed et~al.(2021)Ahmed, Hu, Acharya, and Ding}]{AhmedHAD21}
Ahmed, I.; Hu, X.~B.; Acharya, M.~P.; and Ding, Y. 2021.
\newblock Neighborhood Structure Assisted Non-negative Matrix Factorization and
  Its Application in Unsupervised Point-wise Anomaly Detection.
\newblock \emph{Journal of Machine Learning Research}, 22: 34:1--34:32.

\bibitem[{Ahookhosh, Themelis, and Patrinos(2021)}]{AhookhoshTP21}
Ahookhosh, M.; Themelis, A.; and Patrinos, P. 2021.
\newblock A {B}regman Forward-Backward Linesearch Algorithm for Nonconvex
  Composite Optimization: {S}uperlinear Convergence to Nonisolated Local
  Minima.
\newblock \emph{SIAM Journal on Optimization}, 31(1): 653--685.

\bibitem[{Auslender and Teboulle(2006)}]{AuslenderT06}
Auslender, A.; and Teboulle, M. 2006.
\newblock Interior Gradient and Proximal Methods for Convex and Conic
  Optimization.
\newblock \emph{SIAM Journal on Optimization}, 16(3): 697--725.

\bibitem[{Bauschke, Bolte, and Teboulle(2017)}]{BauschkeBT17}
Bauschke, H.~H.; Bolte, J.; and Teboulle, M. 2017.
\newblock A Descent Lemma Beyond {L}ipschitz Gradient Continuity: First-Order
  Methods Revisited and Applications.
\newblock \emph{Mathematics of Operations Research}, 42(2): 330--348.

\bibitem[{Bauschke, Dao, and Lindstrom(2018)}]{BauschkeDL18}
Bauschke, H.~H.; Dao, M.~N.; and Lindstrom, S.~B. 2018.
\newblock Regularizing with {B}regman-{M}oreau Envelopes.
\newblock \emph{SIAM Journal on Optimization}, 28(4): 3208--3228.

\bibitem[{Bolte, Sabach, and Teboulle(2014)}]{BolteST14}
Bolte, J.; Sabach, S.; and Teboulle, M. 2014.
\newblock Proximal alternating linearized minimization for nonconvex and
  nonsmooth problems.
\newblock \emph{Mathematical Programming}, 146(1-2): 459--494.

\bibitem[{Bolte et~al.(2018)Bolte, Sabach, Teboulle, and
  Vaisbourd}]{BolteSTV18First}
Bolte, J.; Sabach, S.; Teboulle, M.; and Vaisbourd, Y. 2018.
\newblock First Order Methods Beyond Convexity and {L}ipschitz Gradient
  Continuity with Applications to Quadratic Inverse Problems.
\newblock \emph{SIAM Journal on Optimization}, 28(3): 2131--2151.

\bibitem[{Bottou(2010)}]{Bottou10}
Bottou, L. 2010.
\newblock Large-Scale Machine Learning with Stochastic Gradient Descent.
\newblock In \emph{19th International Conference on Computational Statistics,
  {COMPSTAT}}, 177--186.

\bibitem[{Bregman(1967)}]{Bregman67The}
Bregman, L. 1967.
\newblock The relaxation method of finding the common point of convex sets and
  its application to the solution of problems in convex programming.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics}, 7(3):
  200--217.

\bibitem[{Cai(2011)}]{litekmeans}
Cai, D. 2011.
\newblock Litekmeans: the fastest {MATLAB} implementation of kmeans.
\newblock \emph{Available at:
  \url{http://www.cad.zju.edu.cn/home/dengcai/Data/Clustering.html}}.

\bibitem[{Cai et~al.(2011)Cai, He, Han, and Huang}]{Cai11GNMF}
Cai, D.; He, X.; Han, J.; and Huang, T.~S. 2011.
\newblock Graph Regularized Non-negative Matrix Factorization for Data
  Representation.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 33(8): 1548--1560.

\bibitem[{Che and Wei(2020)}]{CheW20}
Che, M.; and Wei, Y. 2020.
\newblock Multiplicative Algorithms for Symmetric Nonnegative Tensor
  Factorizations and Its Applications.
\newblock \emph{Journal of Scientific Computing}, 83(3): 53.

\bibitem[{Comon et~al.(2008)Comon, Golub, Lim, and Mourrain}]{ComonGLM08}
Comon, P.; Golub, G.~H.; Lim, L.; and Mourrain, B. 2008.
\newblock Symmetric Tensors and Symmetric Tensor Rank.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 30(3):
  1254--1279.

\bibitem[{Defazio, Bach, and Lacoste{-}Julien(2014)}]{DefazioBL14}
Defazio, A.; Bach, F.~R.; and Lacoste{-}Julien, S. 2014.
\newblock {SAGA:} {A} Fast Incremental Gradient Method With Support for
  Non-Strongly Convex Composite Objectives.
\newblock In \emph{Advances in Neural Information Processing Systems 27},
  1646--1654.

\bibitem[{Defazio, Domke, and Caetano(2014)}]{DefazioDC14}
Defazio, A.; Domke, J.; and Caetano, T.~S. 2014.
\newblock Finito: {A} faster, permutable incremental gradient method for big
  data problems.
\newblock In \emph{Proceedings of the 31th International Conference on Machine
  Learning}, volume~32, 1125--1133.

\bibitem[{Donoho(1995)}]{Donoho95}
Donoho, D.~L. 1995.
\newblock De-noising by soft-thresholding.
\newblock \emph{IEEE Transactions on Information Theory}, 41(3): 613--627.

\bibitem[{Dragomir et~al.(2022)Dragomir, Taylor, d'Aspremont, and
  Bolte}]{DragomirTdB22}
Dragomir, R.; Taylor, A.~B.; d'Aspremont, A.; and Bolte, J. 2022.
\newblock Optimal complexity and certification of {Bregman} first-order
  methods.
\newblock \emph{Mathematical Programming}, 194(1): 41--83.

\bibitem[{Driggs et~al.(2021)Driggs, Tang, Liang, Davies, and
  Sch{\"{o}}nlieb}]{DriggsTLDS2020}
Driggs, D.; Tang, J.; Liang, J.; Davies, M.~E.; and Sch{\"{o}}nlieb, C. 2021.
\newblock A Stochastic Proximal Alternating Minimization for Nonsmooth and
  Nonconvex Optimization.
\newblock \emph{SIAM Journal on Imaging Sciences}, 14(4): 1932--1970.

\bibitem[{Fan and Li(2001)}]{FanL01}
Fan, J.; and Li, R. 2001.
\newblock Variable selection via nonconcave penalized likelihood and its oracle
  properties.
\newblock \emph{Journal of the American Statistical Association}, 96:
  1348--1360.

\bibitem[{Gillis(2020)}]{Gillis20}
Gillis, N. 2020.
\newblock \emph{Nonnegative Matrix Factorization}.
\newblock SIAM.

\bibitem[{Hasannasab et~al.(2020)Hasannasab, Hertrich, Neumayer, Plonka,
  Setzer, and Steidl}]{HasannasabHNPSS2020}
Hasannasab, M.; Hertrich, J.; Neumayer, S.; Plonka, G.; Setzer, S.; and Steidl,
  G. 2020.
\newblock Parseval Proximal Neural Networks.
\newblock \emph{Journal of Fourier Analysis and Applications}, 26(59).

\bibitem[{He et~al.(2011)He, Xie, Zdunek, Zhou, and Cichocki}]{HeXZZC11}
He, Z.; Xie, S.; Zdunek, R.; Zhou, G.; and Cichocki, A. 2011.
\newblock Symmetric Nonnegative Matrix Factorization: {A}lgorithms and
  Applications to Probabilistic Clustering.
\newblock \emph{IEEE Transactions on Neural Networks}, 22(12): 2117--2131.

\bibitem[{Kolda and Bader(2009)}]{KoldaB09}
Kolda, T.~G.; and Bader, B.~W. 2009.
\newblock Tensor Decompositions and Applications.
\newblock \emph{SIAM Review}, 51(3): 455--500.

\bibitem[{Lan(2020)}]{Lan2020First}
Lan, G. 2020.
\newblock \emph{First-Order and Stochastic Optimization Methods for Machine
  Learning}.
\newblock Springer.

\bibitem[{Latafat et~al.(2022)Latafat, Themelis, Ahookhosh, and
  Patrinos}]{LatafatTAP22}
Latafat, P.; Themelis, A.; Ahookhosh, M.; and Patrinos, P. 2022.
\newblock Bregman {F}inito/{MISO} for Nonconvex Regularized Finite Sum
  Minimization without {L}ipschitz Gradient Continuity.
\newblock \emph{SIAM Journal on Optimization}, 32(3): 2230--2262.

\bibitem[{Laude, Ochs, and Cremers(2020)}]{LaudeOC20}
Laude, E.; Ochs, P.; and Cremers, D. 2020.
\newblock Bregman Proximal Mappings and {B}regman-{M}oreau Envelopes Under
  Relative Prox-Regularity.
\newblock \emph{Journal of Optimization Theory and Applications}, 184(3):
  724--761.

\bibitem[{Lee and Seung(1999)}]{LeeS99}
Lee, D.~D.; and Seung, H.~S. 1999.
\newblock Learning the parts of objects by non-negative matrix factorization.
\newblock \emph{Nature}, 788--791.

\bibitem[{Li et~al.(2022)Li, Wang, Zhang, and Cheng}]{LiWZC22}
Li, W.; Wang, Z.; Zhang, Y.; and Cheng, G. 2022.
\newblock Variance reduction on general adaptive stochastic mirror descent.
\newblock \emph{Machine Learning}, 111: 4639--4677.

\bibitem[{Lin, Li, and Fang(2020)}]{LinLF2020book}
Lin, Z.; Li, H.; and Fang, C. 2020.
\newblock \emph{Accelerated Optimization for Machine Learning}.
\newblock Springer.

\bibitem[{Lu, Freund, and Nesterov(2018)}]{LuFN18}
Lu, H.; Freund, R.~M.; and Nesterov, Y.~E. 2018.
\newblock Relatively Smooth Convex Optimization by First-Order Methods, and
  Applications.
\newblock \emph{SIAM Journal on Optimization}, 28(1): 333--354.

\bibitem[{Luss and Teboulle(2013)}]{LussT13}
Luss, R.; and Teboulle, M. 2013.
\newblock Conditional Gradient Algorithms for Rank-One Matrix Approximations
  with a Sparsity Constraint.
\newblock \emph{SIAM Review}, 55(1): 65--98.

\bibitem[{Ma, Lou, and Huang(2017)}]{MaLH17}
Ma, T.; Lou, Y.; and Huang, T. 2017.
\newblock Truncated $l_{1-2}$ Models for Sparse Recovery and Rank Minimization.
\newblock \emph{SIAM Journal on Imaging Sciences}, 10(3): 1346--1380.

\bibitem[{Mairal(2015)}]{Mairal15}
Mairal, J. 2015.
\newblock Incremental Majorization-Minimization Optimization with Application
  to Large-Scale Machine Learning.
\newblock \emph{SIAM Journal on Optimization}, 25(2): 829--855.

\bibitem[{Malitsky and Tam(2020)}]{MalitskyT20}
Malitsky, Y.; and Tam, M.~K. 2020.
\newblock A Forward-Backward Splitting Method for Monotone Inclusions Without
  Cocoercivity.
\newblock \emph{SIAM Journal on Optimization}, 30(2): 1451--1472.

\bibitem[{Mukkamala and Ochs(2019)}]{MukkamalaO19}
Mukkamala, M.~C.; and Ochs, P. 2019.
\newblock Beyond Alternating Updates for Matrix Factorization with Inertial
  {B}regman Proximal Gradient Algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems 32},
  4268--4278.

\bibitem[{Mukkamala et~al.(2020)Mukkamala, Ochs, Pock, and
  Sabach}]{MukkamalaOPS20}
Mukkamala, M.~C.; Ochs, P.; Pock, T.; and Sabach, S. 2020.
\newblock Convex-Concave Backtracking for Inertial {B}regman Proximal Gradient
  Algorithms in Nonconvex Optimization.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2(3): 658--682.

\bibitem[{Nesterov(1983)}]{Nesterov1983}
Nesterov, Y.~E. 1983.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence ${O}(1/k^{2})$.
\newblock \emph{Soviet Mathematics Doklady}, 27(2): 372--376.

\bibitem[{Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'{a}}c}]{NguyenLST17}
Nguyen, L.~M.; Liu, J.; Scheinberg, K.; and Tak{\'{a}}c, M. 2017.
\newblock {SARAH:} {A} Novel Method for Machine Learning Problems Using
  Stochastic Recursive Gradient.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, 2613--2621.

\bibitem[{Polyak(1964)}]{Polyak64}
Polyak, B.~T. 1964.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics}, 4(5):
  1--17.

\bibitem[{Reem, Reich, and Pierro(2019)}]{ReemRP19}
Reem, D.; Reich, S.; and Pierro, A. R.~D. 2019.
\newblock A Telescopic {B}regmanian Proximal Gradient Method Without the Global
  Lipschitz Continuity Assumption.
\newblock \emph{Journal of Optimization Theory and Applications}, 182(3):
  851--884.

\bibitem[{Robbins and Monro(1951)}]{RobbinsM1951}
Robbins, H.; and Monro, S. 1951.
\newblock A Stochastic Approximation Method.
\newblock \emph{Annals of Mathematical Statistics}, 22(3): 400--407.

\bibitem[{Rockafellar(1970)}]{Rockafellar1970}
Rockafellar, R.~T. 1970.
\newblock \emph{Convex Analysis}.
\newblock Princeton, NJ, USA: Princeton University Press.

\bibitem[{Shahnaz et~al.(2006)Shahnaz, Berry, Pauca, and
  Plemmons}]{ShahnazBPP06}
Shahnaz, F.; Berry, M.~W.; Pauca, V.~P.; and Plemmons, R.~J. 2006.
\newblock Document clustering using nonnegative matrix factorization.
\newblock \emph{Information Processing and Management}, 42(2): 373--386.

\bibitem[{Teboulle and Vaisbourd(2020)}]{TeboulleV20}
Teboulle, M.; and Vaisbourd, Y. 2020.
\newblock Novel Proximal Gradient Methods for Nonnegative Matrix Factorization
  with Sparsity Constraints.
\newblock \emph{SIAM Journal on Imaging Sciences}, 13(1): 381--421.

\bibitem[{Wang and Han(2023)}]{WangH23}
Wang, Q.; and Han, D. 2023.
\newblock A {B}regman Stochastic Method for Nonconvex Nonsmooth Problem Beyond
  Global {L}ipschitz Gradient Continuity.
\newblock \emph{Optimization Methods and Software}, 38(5): 914--946.

\bibitem[{Wang et~al.(2022)Wang, Themelis, Ou, and Wang}]{WangTOW22}
Wang, Z.; Themelis, A.; Ou, H.; and Wang, X. 2022.
\newblock A mirror inertial forward-reflected-backward splitting: {G}lobal
  convergence and line search extension beyond convexity and {L}ipschitz
  smoothness.
\newblock \emph{arXiv: 2212.01504}.

\bibitem[{Yin et~al.(2015)Yin, Lou, He, and Xin}]{YinLHX15}
Yin, P.; Lou, Y.; He, Q.; and Xin, J. 2015.
\newblock Minimization of $l_{1-2}$ for Compressed Sensing.
\newblock \emph{SIAM Journal on Scientific Computing}, 37(1): A536--A563.

\bibitem[{Zhang(2010)}]{Zhang10}
Zhang, C.-H. 2010.
\newblock Nearly unbiased variable selection under minimax concave penalty.
\newblock \emph{The Annals of Statistics}, 38(2): 894--942.

\bibitem[{Zhang and He(2018)}]{ZhangH18}
Zhang, S.; and He, N. 2018.
\newblock On the Convergence Rate of Stochastic Mirror Descent for Nonsmooth
  Nonconvex Optimization.
\newblock \emph{arXiv: 1806.04781}.

\bibitem[{Zhang et~al.(2019)Zhang, Barrio, Carballo, Jiang, and
  Cheng}]{ZhangBM0C19}
Zhang, X.; Barrio, R.; Carballo, M. A.~M.; Jiang, H.; and Cheng, L. 2019.
\newblock Bregman Proximal Gradient Algorithm With Extrapolation for a Class of
  Nonconvex Nonsmooth Minimization Problems.
\newblock \emph{{IEEE} Access}, 7: 126515--126529.

\bibitem[{Zhao et~al.(2022)Zhao, Dong, Rassias, and Wang}]{ZhaoDRW22}
Zhao, J.; Dong, Q.; Rassias, M.~T.; and Wang, F. 2022.
\newblock Two-step inertial {B}regman alternating minimization algorithm for
  nonconvex and nonsmooth problems.
\newblock \emph{Journal of Global Optimization}, 84(4): 941--966.

\bibitem[{Zhu et~al.(2021)Zhu, Deng, Li, and Zhao}]{ZhuDLZ21}
Zhu, D.; Deng, S.; Li, M.; and Zhao, L. 2021.
\newblock Level-Set Subdifferential Error Bounds and Linear Convergence of
  {B}regman Proximal Gradient Method.
\newblock \emph{Journal of Optimization Theory and Applications}, 189(3):
  889--918.

\end{thebibliography}
