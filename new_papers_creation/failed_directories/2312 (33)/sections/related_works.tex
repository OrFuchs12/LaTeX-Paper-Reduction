
\textbf{Language conditioned manipulation.} 
Significant work has been performed in  learning concepts and tasks for robots in interactive settings~\cite{gopalan2018sequence,gopalan2020simultaneously,tellex2020robots} even with the use of dialog~\cite{chai2018language,matuszek2012joint}.
Our work differs from previous works as it learns visual concepts for manipulation one-shot, and improves generalization by updating other known concepts.
Moreover, our approach can learn a concept hierarchy starting from zero known concepts, displaying the adaptability of our model under a continual learning setup.
%Our work differs from previous works as it is attempting to learn visual concepts for manipulation one-shot, while updating other known concepts to improve generalization.
%Moreover, our approach is completely differentiable and can start with zero known concepts, which is important for a continual learning setup. 
Previous work has focused on language conditioned manipulation~\citep{shridhar2021cliport, liu2021structformer, brohan2023rt1, brohan2023rt2}. \citealt{shridhar2021cliport} computes a pick and place location conditioned on linguistic and visual inputs. 
\citealt{liu2021structformer} focuses on semantic arrangement on unseen objects. 
Other works train on large scale linguistic and visual data and can perform real-life robotic task based on language instructions~\citep{ahn2022i, brohan2023rt1, brohan2023rt2}. Our work focuses on interactive teaching of tasks and concepts instead of focusing on the emergent behaviors from large models. 
% \citealt{ahn2022i} uses a pre-trained to propose plans to complete task and executes the feasible grounded plan. 
% None of these approaches discussed above focus on one shot teaching of concepts and task types to solve novel tasks in a zero-shot setting.
\citealt{daruna2019robocse} learns a representation of a knowledge graph by predicting directed relations between objects allowing a robot to predict object locations. 
To the best of the author's knowledge, our work  is the first that learns concepts and tasks one-shot to generalize to novel task scenarios on a robot, making our contributions significant compared to other related works.   


\noindent\textbf{Visual reasoning and visual concept learning.} Our work is related to visual concept learning \citep{mei2022falcon, mao2018the, yi2019neuralsymbolic, han2020visual, li2020competenceaware} and visual reasoning \citep{Mascharka_2018, DBLP:journals/corr/abs-1807-08556, DBLP:journals/corr/JohnsonHMHLZG17, DBLP:journals/corr/abs-1803-03067}. To perform the visual reasoning task, traditional methods \citep{Mascharka_2018, DBLP:journals/corr/abs-1807-08556, DBLP:journals/corr/JohnsonHMHLZG17, DBLP:journals/corr/abs-1803-03067} decompose the visual reasoning task into visual feature extraction and reasoning by parsing the queries into executable neuro-symbolic programs.  On top of that, many concept learning frameworks \citep{mei2022falcon, mao2018the, yi2019neuralsymbolic, han2020visual, li2020competenceaware} learn the representation of concepts by aligning concepts onto objects in the visual scene. 
% \ngnote{\citealt{yi2019neuralsymbolic} parses the visual scene into a structural scene representation, which makes the results of the neural network more interpretable.  \citealt{mao2018the} presents a concept learner that jointly learns a visual feature extractor, visual concept representation, and semantic parsing. \citealt{han2020visual} shows that introducing the relationships between concepts and higher-level concepts is helpful in learning the conceptâ€™s representation. \citealt{mei2022falcon} even shows that it is possible to train a module that learns a new concept with a very limited number of examples and its conceptual relationship to known concepts.} 
As far as we know, \citealt{mei2022falcon}'s FALCON is the most similar work to our work in this line of research. However, when introducing a new concept, our work continually updates the representation of all related concepts, whereas \citealt{mei2022falcon} does not, which makes it ill-suited for continual learning settings. Our work is also related to the area of few-shot learning~\citep{snell2017prototypical, tian2020rethinking, vinyals2017matching}, which learns to recognize new objects or classes from only a few examples but does not represent a concept hierarchy which is useful in robotics settings.



% \textbf{Few-shot learning.} Our work is also related to the area of few-shot learning, which learns to recognize new objects or classes from only a few examples. \citealt{vinyals2017matching} and \citealt{snell2017prototypical} use the visual features of a small number of annotated images as representation for the new class. 
% While the existing frameworks use Euclidean distance or cosine similarity to compute similarity between classes, \citealt{sung2018learning} trains a learnable module to predict the similarity between examples. Provided the relationship between the new class and known class, \citealt{wang2018zeroshot} and Kampffmeyer et al. \cite{kampffmeyer2019rethinking} learn the representation of a new class with no visual example. \citealt{tian2020rethinking} trains a transferable embedding model that can generalize to new classes. 

% \textbf{Continual learning and knowledge graphs.} Another area of work related to ours is continual learning and knowledge graphs. \citealt{daruna2019robocse} learn a representation of a knowledge graph by predicting whether a directed edge between two vertices is within the graph or not. On top of the existing framework, \citealt{daruna2021continual} shows that it is possible to continually update the knowledge graph whenever a new edge or a new vertex appears while avoiding catastrophic forgetting. 

\noindent\textbf{Scene graph.} Scene graphs are  structural representations of all objects and their relationships within an image. The scene graph representation~\cite{Chang_2023} of images is widely used in the visual domains for various tasks, such as image retrieval\cite{DBLP:journals/corr/JohnsonHMHLZG17}, image generation\citep{johnson2018image}, and question answering\citep{teney2017graphstructured}.
This form of representation has also been used in the robotics domains 
% combines geometric scene graph and symbolic scene graph as a representation of the scene 
for long-horizon manipulation~\citep{zhu2021hierarchical}.
