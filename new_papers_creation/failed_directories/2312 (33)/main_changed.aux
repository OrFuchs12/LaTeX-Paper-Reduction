\relax 
\bibstyle{aaai24}
\citation{mei2022falcon}
\citation{shridhar2021cliport,brohan2023rt2}
\citation{mei2022falcon,snell2017prototypical,vinyals2017matching,sung2018learning,wang2018zeroshot,tian2020rethinking}
\providecommand \oddpage@label [2]{}
\citation{gopalan2018sequence,gopalan2020simultaneously,tellex2020robots}
\citation{chai2018language,matuszek2012joint}
\citation{shridhar2021cliport,liu2021structformer,brohan2023rt1,brohan2023rt2}
\citation{shridhar2021cliport}
\citation{liu2021structformer}
\citation{ahn2022i,brohan2023rt1,brohan2023rt2}
\citation{daruna2019robocse}
\citation{mei2022falcon,mao2018the,yi2019neuralsymbolic,han2020visual,li2020competenceaware}
\citation{Mascharka_2018,DBLP:journals/corr/abs-1807-08556,DBLP:journals/corr/JohnsonHMHLZG17,DBLP:journals/corr/abs-1803-03067}
\citation{Mascharka_2018,DBLP:journals/corr/abs-1807-08556,DBLP:journals/corr/JohnsonHMHLZG17,DBLP:journals/corr/abs-1803-03067}
\citation{mei2022falcon,mao2018the,yi2019neuralsymbolic,han2020visual,li2020competenceaware}
\citation{mei2022falcon}
\citation{mei2022falcon}
\citation{snell2017prototypical,tian2020rethinking,vinyals2017matching}
\citation{Chang_2023}
\citation{DBLP:journals/corr/JohnsonHMHLZG17}
\citation{johnson2018image}
\citation{teney2017graphstructured}
\citation{zhu2021hierarchical}
\citation{mei2022falcon}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:interactive_task_teaching}{{1}{2}{This figure demonstrates how Hi-Viscont learns from users interactively. (a) First, the user demonstrates a structure, say a ``house,'' with its sub-components such as a ``roof'' and the concepts used to make the ``roof'' such as a ``yellow curve block''. (b) The user then teaches a novel concept such as a ``green curve block'' and describes its properties. (c) The user can now ask the robot to create a new structure (``house with green roof'') zero-shot with the taught component without explicitly asking for the object of interest. }{}{}}
\citation{vilnis2018probabilistic}
\newlabel{fig:concept_net_model}{{2}{3}{We demonstrate the updates to the box embedding space and the parent concepts when a novel concept is taught to our robot using Hi-Viscont. Existing approaches only edit the leaf nodes as those represent novel concepts.}{}{}}
\citation{mei2022falcon}
\citation{kirillov2023segment}
\citation{zeng2021transporter}
\newlabel{fig:node_classifier}{{3}{4}{Our pipeline decides objects for each node in the scene graph one at a time. The node's context and the request phrase are fed into a node classifier, which is composed of a BERT encoder and an multilayer perceptron, to decide the concepts applicable for the current node. Hi-Viscont then decides the object to pick in the current scene based on the extracted concepts. In this example, the object chosen for Node 5 is "blue rectangular tile" as it existed in the original demonstration and was not changed given the novel task's linguistic request. Notice that the new structure has red floor tiles which were never demonstrated to the robot.}{}{}}
\citation{liu2021structformer,shridhar2021cliport}
\citation{Bartneck2009MeasurementIF}
\citation{trust_survey}
\citation{article}
\newlabel{tab:test_three}{{1}{6}{The average F1 score and standard deviation of Hi-Viscont and FALCON on the test concepts across all the three domains.}{}{}}
\newlabel{tab:house_type}{{3}{6}{The average F1 score and standard deviation of Hi-Viscont and FALCON on the house construction domain by type of concepts.}{}{}}
\newlabel{tab:CUB_depth}{{2}{6}{The average F1 score and standard deviation of Hi-Viscont (HV) and FALCON (FCN) on the CUB dataset by the depth of concepts in the hierarchy.}{}{}}
\newlabel{tab:zoo_type}{{4}{7}{The average F1 score and standard deviation of Hi-Viscont and FALCON on the zoo domain by type of concepts.}{}{}}
\newlabel{tab:human_subject}{{5}{7}{The results of the human-subjects experiment. Success Rate and Node Accuracy are measured in percentage. Hi-Viscont is better than FALCON on all metrics with significance as described in the Human Subjects Study Section.}{}{}}
\bibdata{custom}
\newlabel{app:training_details}{{}{10}{}{}{}}
\citation{WahCUB_200_2011}
\citation{Sullivan2009eBirdAC}
\citation{kampffmeyer2019rethinking}
\newlabel{fig:sampled_figures}{{4}{11}{Sample images from the three domains in this work.}{}{}}
\newlabel{app:dataset_stats}{{}{11}{}{}{}}
\newlabel{app:detailed_results}{{}{11}{}{}{}}
\newlabel{fig:concept_type}{{5}{12}{The number of concepts of each type for all the three domains.}{}{}}
\newlabel{tab:all_stats}{{6}{12}{Statistics for the three domains. The number of train and test concepts of the CUB-200-2011 dataset comes from the average of five different splits, resulted in decimal numbers.}{}{}}
\newlabel{tab:detailed_test}{{7}{13}{The mean and standard deviation of precision and recall of Hi-Viscont and FALCON on the test concepts across all the three domains.}{}{}}
\newlabel{tab:detailed_CUB}{{8}{13}{The mean and standard deviation of precision and recall of the two models on CUB-200-2011 dataset by depth of concepts.}{}{}}
\newlabel{tab:detailed_house}{{9}{13}{The mean and standard deviation of precision and recall of the two models on the house domain by type of concepts.}{}{}}
\newlabel{tab:detailed_zoo}{{10}{13}{The mean and standard deviation of precision and recall for the two models on the zoo domain by type of concepts.}{}{}}
\gdef \@abspage@last{13}
