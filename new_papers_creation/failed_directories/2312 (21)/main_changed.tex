%File: anonymous-submission-latex-2024.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
%\usepackage[submission]{aaai24}  % DO NOT CHANGE THIS
\usepackage{aaai24}
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
% \usepackage{hyperref}
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{url}
% \usepackage[colorlinks=true, allcolors=blue]{hyperref}
% 제출시에 주석처리 요망
\usepackage{xcolor}
\usepackage{comment}
\usepackage{caption}
\usepackage{algpseudocode}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{bibentry}
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% % \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference
% Useful packages

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

\title{Towards Reliable AI Model Deployments:\\Multiple Input Mixup for Out-of-Distribution Detection}
\author {
    Dasol Choi$^{*1}$ \;
    \stepcounter{footnote}Dongbin Na$^{*2}$\thanks{Correspondence to dongbinna@postech.ac.kr}
}
\affiliations{
    $^1$Kyunghee University \;
    $^1$MODULABS \;
    $^2$Pohang University of Science and Technology (POSTECH)
}

% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
% END REMOVE bibentry
\begin{document}

\maketitle
\def\thefootnote{*}\footnotetext{These authors contributed equally to this work.}
%removedVspace
\begin{abstract}
%removedVspace
%[DS]: 전반적인 수정 및 반복되는 문장 제거
% \textcolor{blue}{Recent remarkable success in the deep-learning industries has unprecedentedly increased the need for reliable model deployment.}
% [DB] Good 반영
Recent remarkable success in the deep-learning industries has unprecedentedly increased the need for reliable model deployment.
For example, the model should alert the user if the produced model outputs might not be reliable.
Previous studies have proposed various methods to solve the Out-of-Distribution (OOD) detection problem, however, they generally require a burden of resources.
In this work, we propose a novel and simple method, Multiple Input Mixup (MIM).
Our method can help improve the OOD detection performance with only single epoch fine-tuning.
Our method does not require training the model from scratch and can be attached to the classifier simply.
Despite its simplicity, our MIM shows competitive performance.
Our method can be suitable for various environments because our method only utilizes the In-Distribution (ID) samples to generate the synthesized OOD data.
With extensive experiments with CIFAR10 and CIFAR100 benchmarks that have been largely adopted in out-of-distribution detection fields, we have demonstrated our MIM shows comprehensively superior performance compared to the SOTA method.
Especially, our method does not need additional computation on the feature vectors compared to the previous studies.
All source codes are publicly available at \textcolor{blue}{\url{https://github.com/ndb796/MultipleInputMixup}}.
%removedVspace
\end{abstract}
\section{Introduction}
%removedVspace
% [DB] 수정 사항 Good 반영합니다.
As deep-learning models have achieved remarkable success across various domains, the reliability of AI has been a greatly important issue.
However, modern deep neural networks often show high confidence in their predictions, even when faced with Out-of-Distribution (OOD) data.
This is a critical problem, as models should ideally alert users to potentially unreliable or inaccurate predictions.
For example, consider an autonomous vehicle encountering objects that were not included in its training dataset. In such scenarios, the ability to accurately distinguish between In-Distribution (ID) and Out-of-Distribution (OOD) data becomes crucial.

% [DB] 변경 사항 Good. 반영합니다.
While various OOD detection methods have been proposed recently, they generally focus on the post-hoc approach~\cite{MSP, mahalanobis, liang2017enhancing} utilizing output vectors or features extracted from the \textit{fixed pre-trained classifier}.
These methods do not alter the weight parameters of the model, thus preserving the original classification performance.
However, they usually require an additional proxy OOD dataset to fit the method to the dataset, which may not be practical in real-world scenarios.
In this work, we propose a realistic and straightforward approach: Multiple Input Mixup (MIM), which only requires a single additional training epoch post-training.
This method provides a practical solution with minimal computational burden, especially compared to the extensive training iterations common in computer vision domains.

Previous studies~\cite{kirby,gan} have shown that the marginal features can be utilized as artificial OOD samples.
However, these methods typically involve additional neural networks or significant training overhead, which might be not feasible.
To overcome these limitations, we have explored various data augmentation methods for out-of-distribution (OOD) detection.
During this procedure, we have observed that the general input mixup provides useful information for data augmentation~\cite{mixup}. Notably, mixing multiple samples, particularly more than five images can produce semantic features significantly different from the original image manifold.
Motivated by this observation, we propose a simple, yet effective method, Multiple Input Mixup (MIM), that mixes various input samples to generate synthetic OOD samples.
With just a single epoch of training on these artificially created OOD samples using a pre-trained classifier, MIM demonstrates superior OOD detection performance across various benchmarks.
Our contributions are as follows:
%removedVspace
\begin{itemize}
    \item In this work, we propose a simple yet effective method, MIM, that improves the OOD detection performance by mixing multiple ID samples.
    \item Our method requires only an additional single epoch for the pre-trained model and has minimal impact on the original classification performance.
    \item Despite its simplicity, our method shows competitive OOD detection performance compared to state-of-the-art methods.
\end{itemize}
%removedVspace
% Recent deep-learning models have produced remarkable success in various domains.
% With the advances of various deep learning models, the reliability of AI has been greatly important.
% However, unfortunately, modern deep neural networks frequently show high confidence even with the Out-of-Distribution (OOD) data.
% If the model can produce unreliable outputs, the model should alert the user that some predictions might not be accurate or reliable.
% For example, an autonomous vehicle can come across unknown objects whose category of images is not known in the training time.
% Thus, distinguishing between the in-distribution (ID) dataset and the out-of-distribution (OOD) dataset is important.
% Recently, various OOD detection methods have been proposed to detect the OOD samples properly.
% However, these methods generally focus on the post-hoc approach.

% Previous studies~\cite{kirby,gan} have shown that the marginal features can be used for synthesizing OOD samples.
% However, their studies utilize additional neural networks or a large training burden, which might be not feasible.
% We have explored various data augmentation methods for out-of-distribution detection.
% In this procedure, we remind that the general input mixup provides useful information for data augmentation~\cite{mixup}.
% However, the effect of mixes of multiple samples has not yet been broadly explored.
% Interestingly, If we mix multiple samples, especially, more than 5 images, the semantic features can be far from the original image manifold.
% Motivated by this observation, we propose a simple, yet effective method, Multiple Input Mixup (MIM), that mixes various input samples to generate synthesized OOD samples.
% Only single epoch training on these generated artificial OOD samples with a pre-trained classifier shows superior OOD detection performance in various OOD benchmarks.
% Our contributions are as follows:
% \begin{itemize}
%     \item In this work, we propose a simple yet effective method, MIM, that improves the OOD detection performance by mixing multiple ID samples.
%     \item Our method requires only an additional single epoch for the pre-trained model and hardly affects the original classification performance.
%     \item Our method shows competitive OOD detection performance compared to the SOTA methods despite its simplicity.
% \end{itemize}

\section{Related Work}
%[DS]: 전반적인 문장 수정, require 이란 단어가 너무 많이 나와 다른 단어로 몇 개 대체해봄.
% [DB] Good 반영합니다.
%removedVspace
Previous studies in the OOD detection research fields can generally be divided into two aspects: (1) post-hoc methods and (2) OOD data utilization methods.
First, the post-hoc methods~\cite{odin,entropy,energy,KLmatchingLogits,vim,mahalanobis,MaxLigit,DICE,RMD} do not require \textit{additional training} for the trained model.
These methods typically utilize feature vectors or outputs extracted
from the pre-trained model without modifying the model's parameters.
% [DB] oracle은 소문자입니다.
However, some post-hoc methods~\cite{mahalanobis} involve tuning the OOD detection hyper-parameter with a small subset of oracle OOD datasets, which might not be feasible in some real-world deployment settings.
Moreover, certain post-hoc methods also demand additional steps for feature manipulation~\cite{KLmatchingLogits,mahalanobis}.
Our method does not require an extra explicit oracle OOD dataset, providing a more practical solution.

Secondly, various OOD data utilization methods have been proposed for OOD detection.
They usually require the use of explicit OOD datasets for training.
While they demonstrate high OOD detection performance, accessing real-world OOD datasets is fundamentally challenging.
One study, showing competitive performance with Outlier Exposure (OE)~\cite{OE}, requires additional steps for feature marginalization, which require a burdensome extra data synthesizing process.
Furthermore, various OOD data utilization methods typically need intervention during the training time.
For instance, Outlier Exposure (OE) necessitates the use of both ID data and explicit OOD data throughout the training period.
In contrast, our method only requires a single additional epoch for the pre-trained classifier.
Our method achieves superior OOD detection performance while maintaining the original model classification performance.
% They usually require explicit OOD datasets to be trained.
% These methods show high OOD detection performance, however, access to the real-world OOD dataset is fundamentally not feasible.
% Other study shows competitive performance with Outlier Exposure (OE), they require additional feature marginalizing steps, which requires a burdensome extra data synthesizing process.
% Moreover, various OOD data utilization methods generally require training time intervention.
% For example, Outlier Exposure~\cite{OE} requires ID data and explicit OOD data during the whole training time.


\section{Proposed Methods}
%removedVspace
\subsection{Multiple Input Mixup}

The previously proposed strategies utilizing the Mixup approaches~\cite{mixup, cutmix, manifoldmixup} primarily employ the technique of mixing two data samples to generate realistic images that resemble real-world examples. These methods mainly focus on improving the test accuracy of ID datasets and have not been widely used to enhance OOD performance.

In this work, we propose a novel method, multiple input mixup.
Unlike traditional Mixup methods that mix two data samples, our approach involves combining a larger number of samples.
Given $n$ data samples $\{x_1, \ldots, x_n\}$ as a training dataset, we simply mix $k$ samples, where $k$ is typically much greater than 5, often around 10.
Ours generates data samples that contain marginal information, which refers to features that are less discriminative or less indicative of specific class characteristics.
We define these images containing marginal feature information as OOD data and employ them in our training process.
Specifically, class-discriminative features, which are more closely related to specific classes, are considered ID data, while marginal information images generated using our MIM are categorized as OOD data.
%and used for training.

Remarkably, when we apply this approach to benchmarks such as CIFAR-10 and CIFAR-100 where ten or more samples are mixed, we observe significant improvements in out-of-distribution (OOD) performance.
To the best of our knowledge, our paper is the first to demonstrate the utility of mixing more than ten data, showing its potential benefits.
%removedVspace
\begin{algorithm}
\caption{Multiple Input Mixup (MIM) with Augmentation Utilizing a Single Training Epoch}
\begin{algorithmic}[1]
\State \textbf{Input:} CIFAR-10 dataset $\mathcal{D}$, the classification model $M$, learning rate $\alpha = 0.0001$, momentum $\mu = 0.9$, the size of mix-up $m$
\State \textbf{Output:} Fine-tuned model $M$, OOD detection performance metrics
\State \textbf{Initialize the model:} $M \gets \text{pre-trained on CIFAR-10}$
\State \textbf{Load datasets:}
\State \;\;\;$\mathcal{D}_{ID} \gets \text{CIFAR-10}$, $\mathcal{D}_{OOD} \gets \text{external datasets}$
\State \textbf{Define detector and optimizer:}
\State \;\;\;$\mathcal{S} \gets \text{MaxSoftmax}(\cdot) \text{ based on } M$
\State \;\;\;$\mathcal{O} \gets \text{SGD with } \alpha, \mu$
\State \textbf{Set batch size:} $B$
\State \textbf{Define augmentation transformation:} $\mathcal{T}$

\For{\textbf{each batch} $(x_{\text{ID}}, y_{\text{ID}})$ \textbf{from} $\mathcal{D}_{ID}$}
    \State \textbf{Compute outputs:} $\mathbf{o}_{\text{ID}} \gets M(x_{\text{ID}})$
    \State \textbf{Calculate original loss:} $\mathcal{L}_{\text{ID}} \gets \text{CE}(\mathbf{o}_{\text{ID}}, y_{\text{ID}})$
    \State \textbf{Apply mix-up:} $x_{\text{mix}} \gets \text{MixUp}(x_{\text{ID}}, m)$
    \State \textbf{Apply augmentation:} $\mathbf{x}_{\text{aug}} \gets \mathcal{T}(x_{\text{mix}})$
    \State \textbf{Compute augmented outputs:} $\mathbf{o}_{\text{OOD}} \gets M(x_{\text{aug}})$
    \State \textbf{Calculate mix-up loss:}
    \State \;\;\;$\mathcal{L}_{\text{OOD}} \gets \text{CE}(\mathbf{o}_{\text{OOD}}, \mathcal{P}_{Uniform})$
    \State \textbf{Update model:} $\mathcal{O}.\text{step() with } \mathcal{L}_{\text{ID}} +\mathcal{L}_{\text{OOD}}$
\EndFor
\State \textbf{Evaluate the fine-tuned model:}
\State \;\;\;$\text{Evaluate the model utilizing $\mathcal{S}$ on } \mathcal{D}_{\text{OOD}} \text{ and } \mathcal{D}_{\text{ID}}$
\end{algorithmic}
\end{algorithm}
%removedVspace

\subsection{Simple Fine-tuning}
%removedVspace
Our method leverages fine-tuning~\cite{finetuning} the pre-trained model $M$ for just a single epoch, which requires nearly negligible computational resources.
During this fine-tuning phase, we utilize both the original in-distribution (ID) data and the newly synthesized out-of-distribution (OOD) data generated through our Multiple Input Mixup (MIM) method.
The objective function used in this fine-tuning step is the following components:

\begin{align*}
    \mathcal{L} &= CE(M(x_{\text{ID}}), y_{\text{ID}}) + CE(M(x_{\text{OOD}}), \mathcal{P}_{Uniform})
\end{align*}
where $M(x_{\text{ID}})$ and $M(x_{\text{OOD}})$ are the output probability of the model for ID and OOD data, and $y_{\text{ID}}$ represents the true labels for the in-distribution (ID) data. The Cross-Entropy loss for OOD data assumes a uniform distribution as the target~\cite{uniform}.
After utilizing the fine-tuning process, we employ the Maximum Softmax Probability (MSP) technique~\cite{MSP} to perform OOD detection.
Our empirical results demonstrate that this simple strategy significantly improves OOD detection performance, achieving state-of-the-art results while preserving the original classification accuracy.
%of the fine-tuned model.

\subsection{Data Augmentation}
%removedVspace
With the extensive experiments of our MIM method, We find that employing additional data augmentation techniques after mixup process significantly boosts the OOD detection capabilities of the model.
In our training phase, we apply a comprehensive data augmentation strategy~\cite{imagenet_aug} that includes:
%removedVspace
\begin{itemize}
    \item \textbf{Resizing:} Each image in the dataset is resized to a uniform dimension of $32 \times 32$ pixels. This standardization is crucial for maintaining the consistency of the input data.
    \item \textbf{Color Adjustments:} We apply random color jitter to alter the brightness, contrast, saturation, and hue of the images. This step introduces variability in the color space, helping the model to learn from more diverse images.
    \item \textbf{Affine Transformations:} Random affine transformations are also applied, including rotations within a range of $-90$ to $90$ degrees and translations up to $20\%$ in the image space. These transformations simulate various perspectives and angles for the artificial OOD samples.
\end{itemize}
% The overall algorithm of our method, integrating these augmentation techniques, is as follows:

\begin{table*}[!ht]
\renewcommand\arraystretch{1.0}
\begin{adjustbox}{width=17.8cm,center}
\begin{tabular}{c|c|cccccccccccccc|cc} \toprule
\multirow{4}{*}{\textbf{ID}} & \multirow{4}{*}{\textbf{Methods}} & \multicolumn{14}{c}{\textbf{OOD Datasets}} \\
 &  & \multicolumn{2}{c}{\textbf{Tiny-ImageNet-crop}} & \multicolumn{2}{c}{\textbf{Tiny-ImageNet-resize}} & \multicolumn{2}{c}{\textbf{SVHN}} & \multicolumn{2}{c}{\textbf{LSUN-crop}} & \multicolumn{2}{c}{\textbf{LSUN-resize}} & \multicolumn{2}{c}{\textbf{Textures}} & \multicolumn{2}{c|}{\textbf{Places365}}& \multicolumn{2}{c}{\textbf{Average}} \\
 &  & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR\\
 &  & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ &$\uparrow$ & $\uparrow$\\ \midrule
\multirow{12}{*}{\textbf{CIFAR-10}}
 & MSP & 0.9459	&0.9310	&0.8803	&0.8575	&0.9191	&0.9581	&0.9648	&0.9568	&0.9107	&0.8906	&0.8851	&0.7849	&0.8824	&0.9561&0.9126	&0.9050\\
 & ODIN  &0.9580	&0.9518	&0.8970	&0.8891	&0.9162	&0.9594	&0.9748	&0.9730	&0.9277	&0.9213	&0.8834	&0.8069&0.8792	&0.9568	&0.9195	&0.9226\\
 & Mahalanobis   &0.8680	&0.8204	&0.9146	&0.8984	&0.9681	&0.9873	&0.9368	&0.9226	&0.9221&0.8959 &0.9626&\textbf{0.9454}	&0.8016	&0.9231	&0.9105	&0.9133\\
 & Entropy &0.9543	&0.9478	&0.8849	&0.8715	&0.9239	&0.9631	&0.9732	&0.9713	&0.9174	&0.9072	&0.8883	&0.8038&0.8870	&0.9603	&0.9184	&0.9179\\
 & Energy  &0.9797	&0.9775	&0.8890	&0.8841	&0.9107	&0.9603	&0.9905	&0.9902	&0.9382	&0.9345	&0.8534	&0.7847&0.8988	&0.9675	&0.9229	&0.9284\\
 & MaxLogit  &0.9783	&0.9755	&0.8888	&0.8835	&0.9110	&0.9607	&0.9895	&0.9889	&0.9372	&0.9329	&0.8548	&0.7865	&0.8981	&0.9671	&0.9225	&0.9279\\
 & KL-Matching &0.8362	&0.8642	&0.6662	&0.7405	&0.8434	&0.9250	&0.9083	&0.9097	&0.7199	&0.7913	&0.7598	&0.6980	&0.7207	&0.9157	&0.7792	&0.8349\\
 & DICE &0.9811	&0.9794	&0.8760	&0.8726	&0.9126	&0.9606	&0.9915	&0.9913	&0.9320	&0.9290	&0.8458	&0.7757&0.8973	&0.9672	&0.9195	&0.9251\\
 & RMD  &0.9565	&0.9464	&0.8996	&0.8922	&0.9365	&0.9647	&0.9751	&0.9668	&0.9399	&0.9352	&0.9285	&0.8689&\textbf{0.9144}&\textbf{0.9713}&0.9358	&0.9351\\
 & Vim &0.9582	&0.9539	&0.903	&0.8892	&0.9558	&0.9816	&0.9848	&0.9845	&0.9286	&0.9167	&0.9378&0.9089&0.8559	&0.9481	&0.9320	&0.9404\\
  \cline{2-18}
 & Ours   &\textbf{0.9873}&\textbf{0.9867}&\textbf{0.9649}&\textbf{0.9589}&\textbf{0.9726}&\textbf{0.9881}&\textbf{0.9952}&\textbf{0.9950}&\textbf{0.9752}&\textbf{0.9698}&\textbf{0.9627}	&0.9419&0.9105	&0.9695	&\textbf{0.9669}&\textbf{0.9728}\\
\hline
 \multirow{12}{*}{\textbf{CIFAR-100}}
 & MSP  &0.8632	&0.8480	&0.7464	&0.7091	&0.7137 &0.8437&0.8558 &0.8435&0.7537&0.7116	&0.7356	&0.5750&0.7391	&0.8944	&0.7725	&0.7750\\
 & ODIN  &0.8736	&0.8623	&0.7790	&0.7624	&0.6469	&0.7865	&0.8569	&0.8468	&0.7864	&0.7651	&0.7262	&0.5728&0.7309	&0.8894	&0.7714	&0.7836\\
 & Mahalanobis &0.5595	&0.4922	&0.9330	&0.9321	&0.8568	&0.9309	&0.5203	&0.4674	&0.9443	&0.9404	&0.8993	&0.8529	&0.6393	&0.8425	&0.7646	&0.7798\\
 & Entropy  &0.9191	&0.9206	&0.6394	&0.6394	&0.5265	&0.7236	&0.8961	&0.8989	&0.6603	&0.5927	&0.5466	&0.3955&0.6489	&0.8600	&0.6910	&0.7187\\
 & Energy &0.9592 &0.9581 &0.5618 &0.5190&0.4828 &0.6966 &\textbf{0.9691} &\textbf{0.9712} &0.5649 &0.5137	&0.5353	&0.3970 &0.6349 &0.8432 &0.6726 &0.6998\\
 & MaxLogit  &0.9467	&0.9401	&0.7769	&0.7421	&0.7395	&0.8536	&0.9514	&0.9468	&0.7932	&0.7519	&0.7637	&0.6164&0.7594	&0.9050	&0.8187	&0.8223\\
 & KL-Matching  &0.8209	&0.8180	&0.7635	&0.7662	&0.7014	&0.8676	&0.7772	&0.8041	&0.7814&0.7876	&0.7189	&0.6310&0.6606	&0.8807	&0.7463	&0.7936\\
 & DICE  &0.9566	&0.9542	&0.7821	&0.7495	&0.7402	&0.8516	&0.9630	&0.9622	&0.8022	&0.7655	&0.7649	&0.6213&0.7630	&0.9074	&0.8246	&0.8302\\
 & RMD  &0.8737	&0.8223	&0.8975	&0.8954	&0.8021	&0.9098	&0.8543	&0.8138&0.9220	&0.9210	&0.8671	&0.8074&\textbf{0.7859}&\textbf{0.9196}&0.8575	&0.8699\\
 & Vim &0.9069&0.8626&\textbf{0.7555}&\textbf{0.6573}&\textbf{0.9474}&\textbf{0.9482}&0.7996	&0.7154	&\textbf{0.9455}&\textbf{0.9476}&\textbf{0.9254}&\textbf{0.9664}&0.7011	&0.8768	&0.8545	&.8535\\
  \cline{2-18}
 & Ours &\textbf{0.9667}&\textbf{0.9617}&0.9171	&0.8960	&0.7936	&0.8810	&0.9673	&0.9639	&0.9149	&0.8878	&0.8603	&0.7850&0.7409	&0.8937	&\textbf{0.8801}&\textbf{0.8956}\\
 \bottomrule
\end{tabular}
\end{adjustbox}
\caption{Comparison with state-of-the-art methods using a WideResNet-40-2 classifier. All experiments are conducted by the OOD detection benchmark framework~\cite{kirchheim2022pytorch}. The symbol $\uparrow$ indicates larger values are better.}
% \caption{Comparison with state-of-the-art methods using a WideResNet-40-2 classifier. All experiments are re-run utilizing the OOD detection benchmark framework~\cite{pytorchood}. The symbol $\uparrow$ indicates that larger values are better.}
\label{tab:cifar10_table}
\end{table*}
%removedVspace
\begin{figure*}[h]
    \centering \centerline{\includegraphics[width=0.85\textwidth]{figures/mixup_augmentation.png}}
    \caption{The image examples generated from our proposed method. The generated samples consist of marginal features, which are greatly useful for training the OOD detection network.}
    \label{fig:mixup_augmentation}
\end{figure*}


\section{Experiments}
%removedVspace
\subsection{Datasets}
%removedVspace
For experiments, we select the broadly used in-distribution datasets, CIFAR-10 and CIFAR-100~\cite{cifar}.
For OOD datasets, we have adopted 7 OOD datasets for the CIFAR10 and CIFAR100 ID setup.
%removedVspace
\begin{itemize}
    \item \textbf{Textures} This dataset contains 5,640 natural texture images categorized into 47 groups. Each category consists of 120 images, ranging in their size from 300$\times$300 to 640$\times$640 pixels~\cite{textures}.
    \item \textbf{LSUN-crop} As a component of the Large-scale Scene Understanding (LSUN) project, this dataset consists of various cropped scene images, labeled using a hybrid of automated and manual processes~\cite{LSUN}.
    \item \textbf{LSUN-resize} Another component of the LSUN project, this dataset aims to enhance visual recognition with images of different sizes~\cite{LSUN}.
    \item \textbf{Tiny-ImageNet-crop}  A modified version of Tiny-ImageNet, this dataset consists of images cropped from the original ImageNet dataset~\cite{TinyImageNet}.
    %, offering a resource for models to learn from truncated visual inputs.
    \item \textbf{Tiny-ImageNet-resize} Another variation of Tiny-ImageNet, this dataset includes images resized to various dimensions, providing a resource to enhance the scale adaptability of models~\cite{TinyImageNet}.
    %, offering a resource to evaluate and enhance the scale adaptability of machine learning models.indicates
    % \item \textbf{SVHN} Sourced from Google Street View house numbers, this dataset contains more than 600k small, cropped digit images~\cite{SVHN}.
    \item \textbf{SVHN} Sourced from Google Street View house numbers, this dataset contains small and cropped digit images~\cite{SVHN}.
    %This dataset is particularly useful for developing and testing machine learning models in digit recognition and real-world image processing.
    % \item \textbf{Places365} Designed for scene recognition, this dataset comprises 10 million images across 434 different scene categories~\cite{Places365}.
    \item \textbf{Places365} Designed for scene recognition, this dataset comprises various scene images across 434 different scene categories~\cite{Places365}.
\end{itemize}
%removedVspace
\subsection{Evaluation Metrics}
%removedVspace
In our evaluation, we employ two essential metrics to assess the performance of our model:
%removedVspace
\begin{itemize}
    \item \textbf{AUROC (Area Under the Receiver Operating Characteristic):} This metric~\cite{AUROC} quantifies a model's ability to distinguish between ID and OOD samples based on the ROC curve, providing a comprehensive measure of OOD detection performance.
    \item \textbf{AUPR (Area Under the Precision-Recall Curve):} AUPR~\cite{AUPR} evaluates the precision-recall trade-off in OOD detection. This measure provides insights into how effectively our model balances precision and recall when identifying OOD data.
\end{itemize}
%removedVspace
\subsection{Training Details}
%removedVspace
In our experimental setup, we utilize the WideResNet-40-2 architecture~\cite{WideResnet} for training our models, with CIFAR-10 and CIFAR-100 serving as our in-distribution (ID) datasets. As a following step, we evaluate out-of-distribution (OOD) detection performance using the seven diverse datasets we previously referred to, comparing our models' effectiveness against various SOTA counterparts. Throughout our OOD detection and evaluation experiments, we employ a specialized OOD detection framework, ensuring a comprehensive and robust evaluation against diverse datasets~\cite{kirchheim2022pytorch}.

Specifically for our model, we implement an additional single fine-tuning epoch using our method. During this epoch, we apply stochastic gradient descent (SGD) optimization~\cite{SGD} with a learning rate of 0.0001 and a momentum of 0.9. We also experiment with varying the number of images for mixup and find that using a number ranging from 5 to 10 shows improved overall performance.

\begin{table*}[!ht]
\renewcommand\arraystretch{1.0}
\begin{adjustbox}{width=17.8cm,center}
\begin{tabular}{c|c|ccccccc|c} \toprule
\multirow{3}{*}{\textbf{ID}} & \multirow{3}{*}{\textbf{Methods}} & \multicolumn{7}{c}{\textbf{OOD Datasets}} \\
 &  & \multicolumn{1}{c}{\textbf{Tiny-ImageNet-crop}} & \multicolumn{1}{c}{\textbf{Tiny-ImageNet-resize}} & \multicolumn{1}{c}{\textbf{SVHN}} & \multicolumn{1}{c}{\textbf{LSUN-crop}} & \multicolumn{1}{c}{\textbf{LSUN-resize}} & \multicolumn{1}{c}{\textbf{Textures}} & \multicolumn{1}{c|}{\textbf{Places365}}& \multicolumn{1}{c}{\textbf{Average}} \\
 &  & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$\\ \midrule
\multirow{2}{*}{\textbf{CIFAR-10}}
 & OE  &0.9880	&0.9731	&0.9841	&0.9969	&0.9894	&0.9775	&0.9637	&0.9818\\
\cline{2-10}
 & Ours &0.9889	&0.9696	&0.9750	&0.9957	&0.9789	&0.9643	&0.9141	&0.9695\\
 \hline
 \multirow{2}{*}{\textbf{CIFAR-100}}
 & OE   &0.8925	&0.5356	&0.6879	&0.9341	&0.6094	&0.671	&0.698	&0.7184\\
\cline{2-10}
 & Ours &0.9667	&0.9171	&0.7936	&0.9673	&0.9149	&0.8603	&0.7410	&0.8801\\
 \bottomrule
\end{tabular}
\end{adjustbox}
\caption{OOD detection performance (AUROC) comparison results of the Outlier Exposure (OE) and \textbf{Ours}.}
\label{tab:OE_comparison_table}
\end{table*}
%removedVspace

\subsection{Experimental Results}
%%removedVspace
\textbf{Overall OOD Performance}\indent Interestingly, we have observed that our proposed method significantly outperforms existing SOTA methods in out-of-distribution (OOD) detection despite its extremely simple approach. Our model's superior performance has been reported in Table~\ref{tab:cifar10_table}, which presents a comparative analysis of average AUROC (Area Under the Receiver Operating Characteristic) and AUPR (Area Under the Precision-Recall Curve) metrics across various datasets. We note that this result has not been cherry-picked; the proposed method consistently demonstrates higher OOD performance in various scenarios.

\smallskip
\noindent\textbf{Impact on ID Accuracy}\indent Additionally, a notable advantage of our method is its minimal impact on the original ID test accuracy. By additionally training a single epoch fine-tuning with existing ID data, our approach results in negligible changes in ID test accuracy. On CIFAR-10, the test accuracy shifts slightly from 94.84\% to 94.47\%, and on CIFAR-100, it alters from 75.95\% to 74.41\%. These minor variations confirm that our method does not significantly affect the model's ID test accuracy. In contrast, the AUROC performance of OOD detection shows a remarkable increase, jumping from an average of 0.912 to 0.966 on CIFAR-10 and 0.772 to 0.88 on CIFAR-100. These significant improvements establish our method as the highest performance in overall scores for both CIFAR-10 and CIFAR-100 datasets, demonstrating its superior capability in OOD detection.

\smallskip
%%removedVspace
\noindent\textbf{Comparison with Outlier Exposure}\indent Furthermore, when compared to the Outlier Exposure (OE) methodology, our approach has demonstrated robust OOD performance. As shown in Table~\ref{tab:OE_comparison_table}, our method exceeds the AUROC of OE by 0.16 in the CIFAR-100 experiment. We emphasize the efficiency of our approach: our method achieves competitive performance with OE without the need to collect thousands of additional data points. Our method simply utilizes processed and generated OOD data, proving the effectiveness of our lightweight and practical approach to OOD detection.

%removedVspace
\section{Conclusion}
%%removedVspace
In this study, we introduce the Multiple Input Mixup (MIM) method, a straightforward yet effective approach to enhance OOD detection performance for deep learning models. MIM requires a single additional training epoch on a pre-trained classifier and demonstrates superior OOD detection capabilities, as shown in our CIFAR-10 and CIFAR-100 experiments. Notably, while maintaining original ID test accuracy, MIM outperforms SOTA Outlier Exposure (OE) methods in the CIFAR-100 benchmark. This balance of simplicity, efficiency, and effectiveness establishes MIM as a promising tool for reliable AI model deployment across various domains. Our work highlights the potential of simple, yet powerful methods in AI safety and reliability, encouraging further research in this crucial OOD detection research area.
%removedVspace
\section{Acknowledgements}
%%removedVspace
This research was supported by Brian Impact, a non-profit organization dedicated to advancing science and technology

\begin{comment}
\begin{figure*}[h]
    \centering   \centerline{\includegraphics[width=1.1\textwidth]{figures/mixup.png}}
    \caption{}
    \label{fig:mixup}
\end{figure*}
\end{comment}

Eum expedita doloremque vero iure eius odit, deserunt saepe voluptas distinctio qui voluptatibus odio eos, ut saepe quaerat recusandae aperiam nostrum et cupiditate, omnis sunt laborum repellendus quae veniam ipsam ad praesentium itaque magnam?Cum repudiandae quo at exercitationem delectus, itaque omnis dolores id natus tempore deleniti, consectetur illum debitis nemo blanditiis sed labore ab enim optio et, quidem quasi in repellendus iste nemo sint optio.Minus omnis molestiae neque blanditiis soluta facilis sequi laborum dolores culpa odit, quibusdam eius consequuntur quis non.Veritatis quasi a commodi quos eligendi, necessitatibus in dicta autem voluptate perspiciatis consectetur blanditiis, ut similique minima cupiditate officia?Animi cupiditate ipsam ipsa voluptas, laborum sequi dolorem, aspernatur mollitia voluptas doloribus veniam recusandae dolorum vitae voluptate quas, doloribus aliquid dolor soluta atque impedit necessitatibus at perferendis.Commodi esse soluta quis fugit eveniet dignissimos ab placeat iure, id quos deleniti repellendus enim, dolores similique quae eius consequuntur autem quo officiis voluptates in, ut repudiandae unde odio corrupti iste minus, consectetur mollitia placeat earum blanditiis beatae error nesciunt praesentium provident quaerat doloribus?Illo repudiandae et sequi rem deleniti quae quis, reiciendis atque nesciunt eius, deserunt cupiditate sapiente ab atque beatae quia corporis, temporibus dolore error?Deserunt porro possimus fuga voluptatibus hic nobis ex consequuntur nisi eius, soluta distinctio cupiditate, voluptatem voluptates rem ad dignissimos eius molestias fugiat a sequi quod, minima ipsam amet quo tenetur voluptatibus velit consequuntur natus.Est ex voluptatem accusantium reiciendis consequatur ea vero id facilis, deleniti ducimus cumque fugiat aut explicabo velit soluta, dolores explicabo porro temporibus vel pariatur, dolor inventore praesentium incidunt similique fugiat veniam ipsam sequi cumque aliquid, accusamus soluta error?Consequatur eaque optio soluta expedita obcaecati doloribus, harum delectus labore aspernatur id culpa dignissimos facilis aliquid, consequuntur maiores nihil quod ea vero.Explicabo accusantium corporis eligendi minus assumenda veritatis neque esse deserunt ab, ratione tenetur numquam provident rem fugiat quasi voluptatum optio debitis, minima quae earum sit amet totam?Incidunt omnis pariatur mollitia esse, laudantium nisi saepe officiis, quae quos aliquam odio obcaecati, natus illo tempore aspernatur quos corrupti doloribus non, vitae iusto aspernatur dolores.Tempora ad iusto officia laborum veritatis ea optio, deserunt sapiente nisi officia vitae veritatis accusantium error rerum inventore id repudiandae, animi quaerat praesentium odio aliquam aut corrupti, perspiciatis earum voluptas magnam quidem?Fuga fugit corporis quasi, quasi officia voluptatibus nam.Ipsa qui porro molestias error repudiandae voluptates ratione, corrupti temporibus dolores dolorum fugiat explicabo magni accusamus.Exercitationem architecto cumque laudantium nesciunt ratione assumenda alias, rerum iste omnis voluptas, repellat nobis nisi dolorem sunt maiores fugiat ullam odio dignissimos facilis.Quas expedita accusantium provident rem impedit assumenda aliquam ratione temporibus iusto, doloremque molestiae voluptate deleniti aspernatur dolor, odio accusantium ipsum, corrupti quod adipisci consequatur commodi voluptas magnam?Atque perspiciatis assumenda quia, enim eligendi magni esse maiores et iure quod dolor voluptatibus quae, earum possimus eius, consequatur facere laborum odit pariatur obcaecati porro iste vero dolor repellat incidunt.Possimus ea eius provident quos tempore, perferendis ex vitae error, optio placeat numquam quos iusto quaerat doloribus in molestiae velit doloremque?Explicabo modi exercitationem corporis suscipit iusto, quia velit ipsum laborum magnam sit temporibus tempora eius labore voluptas, sequi reiciendis veniam et totam voluptatibus?Asperiores molestiae ducimus nulla eligendi cumque voluptatum consequatur, quas magni ex ipsa rerum assumenda quis rem, molestias rerum nesciunt perferendis, vero aut dolore aspernatur perspiciatis consequuntur sequi animi, minima rerum ipsa quia praesentium dignissimos inventore illo quos alias natus vitae.Officia molestiae laboriosam voluptate inventore deleniti libero eum minima et, aliquid dolorem tempora quos harum voluptatibus sequi neque, aperiam optio fugit commodi excepturi animi neque totam beatae?Quis earum iste fuga vitae magni optio nostrum eius mollitia, reprehenderit itaque laudantium, temporibus dicta quod fugit beatae magni, similique dolorem voluptate natus tempore explicabo aliquam at.Modi doloremque eius reiciendis ipsa sunt doloribus molestiae, enim cumque odit eum, modi laudantium ut laborum ipsa iste optio inventore unde sunt rerum, doloribus molestiae aspernatur fugit sequi velit vero ut nostrum incidunt officia repudiandae?Ea suscipit fuga praesentium, iste est ex alias tenetur quam reiciendis aut ut, inventore exercitationem accusantium ullam, quas quod dolores nemo quis sapiente molestias itaque tempore, pariatur quaerat minima ea excepturi omnis architecto temporibus quos?Doloribus quo expedita sint, illum ex corrupti, voluptate beatae repudiandae.Incidunt officia repellendus debitis magnam ut in dolorem aperiam a reiciendis, mollitia quisquam blanditiis rerum, aliquam quas quo quidem accusamus provident.Quae ad totam possimus alias dolores, voluptatem culpa dolores tenetur veniam deleniti blanditiis harum ex aliquam vero veritatis?Ipsam doloribus minima nihil dolore quos voluptate ex odio quod eius obcaecati, blanditiis hic a dicta facilis alias, temporibus doloribus quam sit incidunt modi maxime at, blanditiis voluptatem facere qui asperiores, nobis id saepe iste qui fuga alias deserunt nulla exercitationem?Deleniti ut consequatur ex magnam, a numquam eos architecto beatae harum animi ab neque voluptatem.Ratione facere tempora laborum blanditiis unde alias repudiandae libero optio, debitis non alias dolorem iste modi velit pariatur sed recusandae facere voluptatum, explicabo veritatis culpa a dolores ad, voluptatibus itaque repellat ipsam assumenda voluptatum iusto laborum magni placeat tempora aliquam, excepturi quia quos ipsam labore obcaecati recusandae voluptates ducimus?Voluptatem adipisci beatae debitis quam quis fuga quae minima maxime aliquid odit, facere maiores voluptates excepturi provident, fugiat aliquid nostrum aperiam vel quasi temporibus amet cumque, assumenda adipisci doloremque repudiandae incidunt tempore nulla similique provident, aliquid eum iste laborum beatae.Sint beatae consequatur incidunt et, officiis distinctio expedita dolorum aliquam temporibus a, perspiciatis voluptatibus nulla ex suscipit expedita nesciunt, eius delectus libero veniam eum laboriosam ex quibusdam architecto saepe provident?Quas ex iusto aut alias eos, minus nesciunt debitis dolores in repudiandae quas odio ipsum sit, hic amet similique asperiores totam sit repellendus rerum?Dolorem at modi animi ipsam quae sed vel, nisi illo distinctio, fugiat distinctio eveniet dicta porro quos, ratione iure aut libero quidem officia assumenda optio, ipsum aspernatur ipsa illo id non maxime.Debitis perferendis id suscipit vel odio, nesciunt consectetur unde minima tempore commodi esse exercitationem in sunt ipsam, cumque voluptatum nulla quibusdam praesentium magni, illo sed nulla autem sapiente natus alias veniam, nemo alias possimus fuga earum.Aliquam cumque possimus consectetur, pariatur tenetur explicabo officia temporibus neque hic sequi consectetur, quaerat aspernatur fuga cum dolor alias autem?Quas sunt mollitia aperiam nesciunt eaque corporis incidunt, eius deserunt corporis facere, vitae error facere deserunt consequuntur.Tempora magni suscipit veniam aliquam nostrum dolores ipsam, quae ab alias vitae possimus mollitia accusantium animi odit numquam enim, unde obcaecati architecto culpa quia natus numquam iusto veritatis, saepe et sint accusantium, non officiis et obcaecati adipisci consequuntur libero nisi magnam vero inventore excepturi.Hic quas iste assumenda, assumenda tempore dolor magnam?Consectetur porro iusto praesentium eveniet totam, perferendis obcaecati laborum eos laboriosam placeat harum quae, mollitia a nesciunt natus corrupti, nobis ut iusto repudiandae commodi reprehenderit corrupti dolorum voluptatem corporis, deleniti ad accusantium amet commodi unde aperiam dolorum.Iure unde omnis explicabo voluptatem quo voluptate nihil sint eaque blanditiis, ipsum qui labore repellat molestiae voluptatibus facilis, corrupti iure assumenda nam saepe, obcaecati laboriosam quam deleniti earum?Optio dignissimos facilis fugit eveniet sit unde modi aspernatur odit, distinctio ipsa beatae facilis sit aperiam deserunt dolorum et atque tempore, itaque porro ipsum?Est facere exercitationem voluptatum voluptatibus enim officia fugit quod, doloremque ex quod reiciendis vero et aliquid hic ratione cumque deleniti, illo doloremque qui veritatis inventore assumenda sed natus, dicta eveniet nobis aliquid, eligendi numquam aliquam earum fuga deserunt repudiandae tenetur pariatur veritatis.Perferendis mollitia reprehenderit velit aspernatur perspiciatis nesciunt quidem, deserunt ex perspiciatis quisquam at iste est dignissimos eius odio, neque quibusdam nesciunt odio beatae iusto accusantium laborum illo quo, magnam assumenda voluptas fugiat tenetur atque praesentium quas molestias.Nobis quibusdam impedit id, impedit tenetur dolorum recusandae consectetur cum iusto vero.Eligendi sit atque minima modi quia sed, distinctio hic odio veniam delectus adipisci eaque excepturi ipsum nulla repellendus, reprehenderit illo labore corporis voluptatum quisquam tenetur sit obcaecati dignissimos, culpa vero odit mollitia eos velit.Corporis dolore commodi minima dolores sint delectus aspernatur assumenda pariatur nesciunt, placeat amet omnis soluta dolorum sint minus officia reprehenderit autem.Molestias quod doloremque at voluptates asperiores, accusantium itaque ut sint eligendi nihil, mollitia in vitae eius tempora accusamus itaque nam temporibus magnam doloribus, magni libero soluta odio?Minima molestias ratione nemo eveniet at minus, cumque minus impedit, quo excepturi consequatur illo perferendis.Nam unde voluptatibus eaque, quisquam harum possimus minima laudantium dolorem adipisci error architecto voluptatem in, pariatur quis ipsam minus reprehenderit quisquam, sunt facere asperiores.Blanditiis veritatis in quae eligendi aliquam odit consequatur molestias laboriosam itaque sapiente, omnis iure accusantium, minus possimus sint obcaecati tenetur distinctio inventore aliquid sit voluptatibus officiis voluptate?Exercitationem nostrum delectus perspiciatis, numquam eaque saepe officia dignissimos perferendis tempore beatae amet deleniti, consectetur similique cupiditate, aspernatur quaerat laboriosam doloremque quasi quos a, animi aspernatur sint.Soluta libero illum iste, vel deserunt eaque iure quis est nesciunt tenetur totam neque aliquid, et eaque ad ut voluptatem ipsum ea optio?Dicta error distinctio eligendi velit necessitatibus, provident rerum soluta consequatur amet veniam, ea minus ab neque nulla accusantium natus quaerat hic?Dolorem impedit quod mollitia ab aspernatur dignissimos dolore quo minus explicabo harum, eius sit neque, vitae consequuntur a totam modi quasi dolores?Aspernatur beatae commodi labore autem atque, architecto aspernatur deleniti?Minus in deserunt maiores aut magnam a, neque quam hic recusandae similique ut repellendus reiciendis aut quod, aliquid labore itaque, distinctio facere cumque?Rem tenetur minus explicabo in molestiae cum impedit, quo quasi sunt corrupti beatae, assumenda expedita eos quo, illum a architecto nostrum dicta reiciendis fuga excepturi?Exercitationem voluptatem quidem asperiores inventore aut cumque eum laborum consectetur autem, modi eveniet dignissimos eos voluptate praesentium impedit corrupti ducimus laboriosam deserunt earum, quas modi aut hic unde eos dolore itaque ratione, itaque vel facilis ratione, autem alias ipsum.Expedita reprehenderit fugit aliquid molestiae blanditiis aspernatur, dolorum ipsa nulla facilis quas, distinctio aliquid dicta aspernatur quibusdam fugit quia obcaecati aliquam corporis, laudantium eius rem soluta adipisci expedita ipsam voluptatem.Nobis eius enim, minus ducimus sunt autem, nobis exercitationem ducimus omnis sit ab et deserunt voluptates non assumenda, at harum vel quis voluptatibus aliquam accusamus debitis beatae eos?Pariatur quidem omnis quo rem accusantium odit libero voluptatem autem, ex eius quia, dolor non fuga ratione dicta illum debitis.Atque et dignissimos laborum nobis facere quos, placeat quas ad dolor, consequuntur cupiditate facere reiciendis hic ea non delectus consequatur recusandae dolore?Autem perspiciatis molestiae cupiditate assumenda, inventore impedit illum temporibus voluptatem unde porro quod sit iure, possimus doloribus cum voluptates.Corrupti nihil a minus fuga reprehenderit modi, quo quaerat obcaecati modi accusamus minus sint maiores nihil fuga blanditiis harum, nostrum error illum aliquam expedita corporis perspiciatis enim, illum esse id, ipsa veritatis a?Neque cupiditate incidunt ratione quisquam sed alias reprehenderit voluptas suscipit dolore doloribus, vitae placeat excepturi at repellat, perferendis esse laudantium nobis distinctio aspernatur, facere aliquid maiores aperiam voluptatum voluptates ut error quibusdam enim, dolorem repellat architecto tempora itaque eum?Voluptas cum consequatur nostrum quae vitae vel deleniti facilis voluptatibus illum ex, voluptatibus ut rerum quibusdam temporibus, quam fugit debitis architecto minus corrupti asperiores soluta rerum?Incidunt quibusdam blanditiis saepe dignissimos aut unde, ipsa nemo placeat velit iure vero et?Nulla amet sint quisquam fuga nihil provident, iusto commodi quia dolores quibusdam debitis ullam possimus nemo explicabo, alias sunt quidem assumenda iusto nulla rerum quae vero.Quidem sed molestiae eligendi ad quisquam ipsa ab mollitia laborum necessitatibus ex, delectus commodi quos minus, eligendi eius quae distinctio id.Soluta blanditiis laborum eaque iure praesentium voluptate, dolore facilis quo facere officiis obcaecati recusandae eius alias, cupiditate totam quibusdam hic sint consequuntur ipsum, suscipit ut sapiente nihil quisquam soluta ipsa explicabo, exercitationem non totam odio fugit praesentium alias?Earum aliquid expedita itaque sit iste odit reprehenderit, quos distinctio suscipit, distinctio tenetur in quo exercitationem hic dolore provident vitae maxime architecto libero?Error optio placeat pariatur, rem temporibus aspernatur nam dolor natus, sed optio incidunt ratione.Explicabo officia impedit eum rem laborum porro inventore, magnam assumenda est ex laborum aliquam rerum asperiores soluta explicabo nesciunt, quia optio impedit consectetur tempora id quam porro vero quidem tempore quos, odio voluptas harum?Quibusdam voluptate eum veritatis, porro sed provident?Harum voluptate impedit voluptatem magni ut architecto iste recusandae ipsum error, voluptatum nisi magni, officiis sapiente quod quo ipsum consectetur ex ullam doloribus quisquam nobis facilis, harum aliquid enim consequatur provident?Quidem blanditiis quod eveniet deleniti minus quam tempore numquam id rerum, quisquam est eaque id sunt consectetur sint qui commodi vel laboriosam, odit doloremque incidunt aut, quis laboriosam dignissimos amet quia magni cumque dicta id?Error quod nulla ut ad veniam nam libero minus temporibus consequuntur possimus, provident tenetur dolorem similique, culpa rem voluptates, dolorum doloribus dicta officiis eos quisquam rem.Quas ut obcaecati quod, commodi placeat impedit laboriosam molestias recusandae iste fuga neque quos dolore illum.Odio omnis qui ipsum doloremque commodi corrupti itaque non consequuntur, tenetur quos distinctio fugit excepturi, iste modi rem magnam consequatur sapiente quaerat deserunt, natus unde ab voluptate?Quis temporibus tempore distinctio, amet veniam sunt deserunt consectetur ipsa aspernatur dolor nostrum deleniti officia, nobis harum dolores deserunt maxime expedita perspiciatis ex esse quo, ullam architecto excepturi esse molestias quo consectetur molestiae?Ipsum necessitatibus ipsam vero pariatur, dolorum enim pariatur minus voluptates consequatur culpa iusto magnam quisquam natus repellendus, adipisci sunt consequatur tenetur dignissimos, nisi dicta assumenda eaque tenetur molestias sapiente repudiandae, facilis natus culpa aliquam soluta incidunt explicabo hic earum?Vel alias fugit delectus rerum minima, quae ducimus esse, assumenda blanditiis doloribus labore quisquam sint iste, aut nihil sit iure aperiam corporis perferendis molestias quisquam aliquid?Accusantium cumque minus maxime porro accusamus, tenetur cumque maiores earum qui libero illum aliquid explicabo, rerum reiciendis perferendis fuga?\clearpage
\bibliography{aaai24}

\end{document}