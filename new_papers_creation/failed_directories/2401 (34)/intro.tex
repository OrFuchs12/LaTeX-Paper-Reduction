%!TEX root =  main.tex
\section{Introduction}\label{sec:intro}

The problem of two-sided matching markets has been studied for a long history due to its wide range of applications in real life including the labor market and college admission \citep{gale1962college,roth1984evolution,roth1992two,abdulkadirouglu1999house,epple2006admission,fu2014equilibrium}. 
There are two sides of market participants, e.g., employers and workers in the labor market, and each side has a preference ranking over the other side. 
The matching reflects the bilateral nature of exchange in the market. For example, a worker works for an employer and the employer employs this worker.  
Stability is a key concept describing the equilibrium of a matching, which ensures the current bilateral exchange cannot be easily broken.  
A rich line of works study how to find a stable matching in the market \citep{gale1962college,kelso1982job,roth1984evolution,roth1992two,erdil2019efficiency}.
However, all of them assume the preferences of market participants are known \emph{a priori}, which may not be satisfied in practice. 
For example in labor markets, workers usually have unknown preferences over employers since they do not know whether they like the task type or the employer. 
With the emergence of online marketplaces such as online labor market Upwork and crowdsourcing platform Amazon Mechanical Turk where employers have numerous similar tasks to delegate, workers are able to learn the uncertain preferences during the iterative matching process with employers through these tasks. 



Multi-armed bandit (MAB) is a core problem that characterizes the learning process during iterative interactions when faced with uncertainty  \citep{lattimore2020bandit}. 
There are also two sides of agents: a player on one side and $K$ arms on the other side. The player has unknown preferences over arms. At each time, it selects an arm and receives a reward. 
The player's objective is to maximize the cumulative reward over a specified horizon. 
To better measure the performance of the player's strategy, an equivalent objective of minimizing the cumulative regret is widely studied, which is defined as the cumulative difference between the reward of the optimal arm and that of the selected arms.  




Recently, a rich line of works study the bandit learning problem in matching markets where more than one player and arms exist. 
These works study the case where players have unknown preferences over arms and arms can determine their preferences over players based on some known utilities such as the profile of workers in online labor markets.
To characterize the stability of the learned matching, the objective of stable regret is adopted and studied \citep{das2005two,liu2020competing,liu2021bandit,sankararaman2021dominate,basu21beyond,kong2022thompson,kong2023player,wang2022bandit}.
Previous works mainly focus on two types of objectives: the player-optimal stable regret and the player-pessimal stable regret. 
The former is defined as the cumulative difference between the reward of the arm in the players' most preferred stable matching and the accumulated reward by the player. 
The latter is defined compared with the reward of the arm in the players' least preferred stable matching. 
\citet{liu2020competing} first study the centralized version where a central platform assigns an allocation of arms to players in each round and provide theoretical guarantees. 
Since such a platform may not always exist in real applications, the following works mainly focus on the decentralized setting where each player makes her own decision \citep{liu2021bandit,sankararaman2021dominate,basu21beyond,kong2022thompson,zhang2022matching,maheshwari2022decentralized,kong2023player}. 
Most of these works achieve guarantees on the player-pessimal stable regret and until recently, \citet{zhang2022matching} and \citet{kong2023player} independently propose algorithms that can reach player-optimal stable matching. 



All of the above works study the one-to-one matching markets where each player proposes to one arm at a time and each arm could accept at most one player. 
The many-to-one setting is more general and common in real life such as in labor markets where an employer usually has a certain quota and can recruit a group of workers \citep{roth1984stability,roth1992two,abdulkadirouglu2005college,che2019stable}. 
\citet{wang2022bandit} initialize the study in many-to-one markets by considering that arms have responsive preferences. 
However, their algorithm is only able to achieve player-pessimal stable matching and lacks guarantees on incentive compatibility.  
Incentive compatibility is a crucial property in multi-player systems as it ensures players are incentivized to act in ways that align with desired system outcomes, thereby promoting cooperation and efficiency rather than encouraging competitive or destructive behaviors. Deriving algorithms that can achieve better regret and enjoy guarantees on this property is important in matching markets. 


In this paper, we aim to provide algorithms with improved regret guarantee and incentive compatibility for many-to-one markets. 
For the sake of the generality, we also study the decentralized setting.
We propose an adaptive explore-then-DA (AETDA) algorithm for markets with responsive preferences and derive $O(N\min\set{N,K}C\log T/\Delta^2)$  upper bound for the player-optimal stable regret as well as a guarantee of incentive compatibility, where $N$ is the number of players, $K$ is the number of arms, $C$ is arms' total capacities, $T$ is the horizon, and $\Delta$ is the players' minimum preference gap. 
To the best of our knowledge, it is the first guarantee for the player-optimal regret in decentralized many-to-one markets and is also the first that simultaneously enjoys such robust assurance in one-to-one markets. 
Since arms preferences may possess a combinatorial structure which may not be well characterized by responsiveness, we also consider a more general setting with \textit{substitutability}~\citep{roth1992two}, one of the most generally known conditions to ensure the existence of a stable matching and naturally holds under responsiveness \citep{roth1992two,abdulkadirouglu2005college}.
We design an online deferred acceptance (ODA) algorithm for this more general setting and prove that the regret against the player-pessimal stable matching is bounded by $O(NK\log T/\Delta^2)$.
As compared in Table \ref{table:comparison}, this result not only works under a more general setting but also achieves a great advantage over \citet{wang2022bandit}. 


\begin{table*}[htb!]
\centering
\caption{Comparisons of settings and regret bounds with most related works. $*$ represents the player-optimal stable regret and bounds without labeling $*$ are for player-pessimal stable regret, 
$\#$ represents the centralized setting.  
$N,K,\Delta,C, \varepsilon, C'$ are the number of players and arms, the minimum preference gap among all players, the total capacities of all arms under responsiveness, the hyper-parameter of algorithms which can be very small, and the parameter related to the unique stable matching condition which can grow exponentially in $N$, respectively. `Incentive' means that there is a guarantee for incentive compatibility. 
} 
\label{table:comparison}
\begin{tabular}{lll}
\toprule 
  & Regret bound       & Setting                     \\\hline
\rule{0pt}{13pt}\multirow{2}{*}{\citet{liu2020competing}} & $\displaystyle O\bracket{{K\log T/\Delta^2}} *\#$  & one-to-one, known $\Delta$, incentive\\
& $\displaystyle O\bracket{{NK\log T/\Delta^2}}\#$              & one-to-one, incentive \\\hline
\rule{0pt}{20pt}\citet{liu2021bandit}                                                                                 & $\displaystyle O\bracket{\frac{N^5K^2\log^2 T}{\varepsilon^{N^{4}}\Delta^2}}$                                                       & one-to-one                        \\ \hline
\rule{0pt}{12pt}\multirow{2}{*}{\citet{sankararaman2021dominate} }& $\displaystyle O\bracket{{NK\log T}/{\Delta^2}}$ & \multirow{2}{*}{one-to-one (serial dictatorship), incentive }   \\ &
              $\displaystyle\Omega\bracket{{N\log T}/{\Delta^2}}$                &                          \\\hline
\rule{0pt}{13pt}\multirow{2}{*}{\citet{basu21beyond}} & $\displaystyle O\bracket{K\log^{1+\varepsilon} T + 
2^{(\frac{1}{\Delta^2})^{\frac{1}{\varepsilon}}} } *$                 & one-to-one \\
                       &  $\displaystyle O\bracket{{NK\log T}/{\Delta^2}}$                &   one-to-one (uniqueness consistency)                                    \\\hline
\rule{0pt}{15pt}\citet{maheshwari2022decentralized}                                                                                 &   $O\bracket{C'NK\log T/\Delta^2}$                                                     & one-to-one ($\alpha$-reducible condition)                        \\ \hline
\rule{0pt}{20pt}\citet{kong2022thompson}                                                                                 & $\displaystyle O\bracket{\frac{N^5K^2\log^2 T}{\varepsilon^{N^{4}}\Delta^2}}$                                                       & one-to-one                        \\ \hline
\rule{0pt}{15pt}\citet{zhang2022matching}                                                                                 & $\displaystyle O\bracket{K\log T/\Delta^2}*$                                                       & one-to-one                    \\ \hline
% \rule{0pt}{20pt}\citet{kong2023player}                                                                                & $\displaystyle O\bracket{K\log T/\Delta^2}*$                                                       & one-to-one                        \\ \hline
\rule{0pt}{12pt}\multirow{2}{*}{\citet{kong2023player} }& \multirow{2}{*}{$\displaystyle O\bracket{{K\log T}/{\Delta^2}}*$} & one-to-one   \\ &   &  responsiveness \textbf{(our extension)}                         \\\hline
\rule{0pt}{12pt}\multirow{3}{*}{\citet{wang2022bandit}}& $\displaystyle O\bracket{{K\log T}/{\Delta^2}}*\#$ &  responsiveness, known $\Delta$                     \\
  & $\displaystyle O\bracket{{NK^3\log T}/{\Delta^2}}\#$  & responsiveness \\
& {$\displaystyle O\bracket{\frac{N^5K^2\log^2 T}{\varepsilon^{N^{4}}\Delta^2}}$} & responsiveness                        \\ \hline
\rule{0pt}{12pt}\multirow{2}{*}{\textbf{Ours}} & {$\displaystyle O\bracket{{N\min\set{N,K}C\log T}/{\Delta^2}}*$} & responsiveness, incentive 
% , incentive compatibility
\\ & $\displaystyle O\bracket{{NK\log T}/{\Delta^2}}$ &  substitutability, incentive
                      \\

          \bottomrule 
\end{tabular}
\end{table*}








% \fang{introduce stricted substi; \textit{restricted} footnote refer to; delete $,$}



% Specific to the responsiveness setting, we also propose a more efficient explore-then-DA (ETDA) algorithm and provide an $O(K\log T/\Delta^2)$ upper bound for the player-optimal stable regret. 
% This is the first player-optimal guarantee for decentralized many-to-one markets and achieves the same order as the state-of-the-art result in the one-to-one setting. 
% We also demonstrate the convergence of our algorithms and their advantage over baselines in a series of experiments.

