%!TEX root =  main.tex
\section{An Extension of \citet{kong2023player}
}\label{sec:etgs}

Recall that \citet{kong2023player} provide a near-optimal bound $O(K\log T/\Delta^2)$ for player-optimal stable regret in one-to-one markets. We first provide an extension of their algorithm, explore-then-deferred-acceptance (ETDA), for many-to-one markets with responsiveness and $N\le K\cdot \min_{j\in[K]} C_j$. 

The deferred acceptance (DA) algorithm is designed to find a stable matching when both sides of participants have known preferences. The algorithm proceeds in multiple steps. At the first step, all players propose to their most preferred arm and each arm rejects all but their favorite subset of players among those who propose to it. Such a process continues until no rejection happens. It has been shown that the final matching is the player-optimal stable matching under responsiveness \citep{kelso1982job,roth1992two}. 



Since players are uncertain about their preferences, the ETDA algorithm lets players first explore to learn this knowledge and then follow DA to find a stable matching. 
Specifically, each player first estimates an index in the first $N$ rounds (phase $1$); and then explores its unknown preferences in a round-robin way based on its index (phase $2$). After estimating a good preference ranking, it will follow DA to find the player-optimal stable matching (phase $3$). 
Compared with \citet{kong2023player}, the difference mainly lies in the first phase of estimating indices for players where multiple players can share the same index in many-to-one markets. For completeness, we provide the detailed algorithm in Appendix \ref{sec:etda:appendix} and the theoretical guarantees below. 

 
\begin{theorem}\label{thm:etda}
   Following ETDA, 
   the player-optimal stable regret of each player $p_i$ satisfies
   \begin{align}
        \overline{R}_i(T) &\le O\bracket{K\log T/\Delta^2}  \,.
    \end{align}
\end{theorem}
Due to the space limit, the proof of Theorem \ref{thm:etda} is deferred to Appendix \ref{sec:proof:etda}. Under the same decentralized setting, this player-optimal stable regret bound is even $O(N^5K\log T/\varepsilon^{N^4})$ better than the weaker player-pessimal stable regret bound in \citet{wang2022bandit}. 
Such a result also achieves the same order as the state-of-the-art analysis in the reduced one-to-one setting \citep{kong2023player}. 



Though achieving better regret bound, the ETDA algorithm is not incentive compatible. We can consider the market where the player-optimal stable arm of a player $p_i$ is its least preferred arm. 
If $p_i$ always reports that it does not estimate the preference ranking well, then the stopping condition of phase $2$ is never satisfied. In this case,  all of the other players fail to find a stable matching and suffer $O(T)$ regret, while this player is always matched with more preferred arms than that in the stable matching during phase $2$, resulting in $O(T)$ improvement in the cumulative rewards.  Thus player $p_i$ lacks the incentive to always act as the algorithm requires. 
To improve the algorithm in terms of incentive compatibility, we further propose a novel algorithm in the next section. 
