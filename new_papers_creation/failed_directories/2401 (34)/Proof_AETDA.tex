
\section{Analysis of The AETDA Algorithm (Algorithm \ref{alg:AETDA})}
\label{sec:proof:aetda}



\subsection{Proof of Theorem \ref{thm:aetgs:regret}}

    The player-optimal stable regret of each player $p_i$ by following our AETDA algorithm (Algorithm \ref{alg:AETDA}) satisfies
% \begin{align}
%     \overline{R}_i(T) &=\EE{\sum_{t=1}^T \bracket{\mu_{i,\overline{m}_i} - X_{i}(t)} } \notag \\
%     &\le \EE{\sum_{t=1}^T \bracket{\mu_{i,\overline{m}_i} - X_{i}(t)} \mid 
%     \urcorner \cF } + T \cdot \PP{\cF } \cdot \mu_{i,\overline{m}_i}\notag \\
%     &\le \EE{\sum_{t=1}^T \sum_{a_j\in \cK} \bOne{\bar{A}_i(t)=a_j} \Delta_{i,\overline{m}_i,j} \mid \urcorner \cF } + \EE{\sum_{t=1}^T \bOne{\bar{A}_i(t)=\emptyset} \mu_{i,\overline{m}_i} \mid \urcorner \cF } + 2NK \mu_{i,\overline{m}_i} \notag \\
%     &\le \frac{192\min\set{N^2, NK} C\log T}{\Delta^2}\cdot \mu_{i,\overline{m}_i}  + 2NK \mu_{i,\overline{m}_i}  \label{eq:aetda:regret}  \\
%     &= O\bracket{N\min\set{N,K}C\log T/\Delta^2} \notag \,,
% \end{align}
\begin{align}
    \overline{R}_i(T) &=\EE{\sum_{t=1}^T \bracket{\mu_{i,\overline{m}_i} - X_{i}(t)} } \notag \\
    &\le \EE{\sum_{t=1}^T \bracket{\mu_{i,\overline{m}_i} - X_{i}(t)} \mid 
    \urcorner \cF } + T \cdot \PP{\cF } \cdot \mu_{i,\overline{m}_i}\notag \\
    &\le \EE{\sum_{t=1}^T \bOne{\opt_i \neq -1}\bracket{\mu_{i,\overline{m}_i} - X_{i}(t)} \mid 
    \urcorner \cF } + \EE{\sum_{t=1}^T \bOne{\opt_i = -1}\bracket{\mu_{i,\overline{m}_i} - X_{i}(t)} \mid 
    \urcorner \cF } + T \cdot \PP{\cF } \cdot \mu_{i,\overline{m}_i}\notag \\
    % &\le \EE{\sum_{t=1}^T \sum_{a_j\in \cK} \bOne{\bar{A}_i(t)=a_j} \Delta_{i,\overline{m}_i,j} \mid \urcorner \cF } + \EE{\sum_{t=1}^T \bOne{\bar{A}_i(t)=\emptyset} \mu_{i,\overline{m}_i} \mid \urcorner \cF } + 2NK \mu_{i,\overline{m}_i} \notag \\
    &\le \frac{192\min\set{N^2, NK} C\log T}{\Delta^2}\cdot \mu_{i,\overline{m}_i}  + 2NK \mu_{i,\overline{m}_i}  \label{eq:aetda:regret}  \\
    &= O\bracket{N\min\set{N,K}C\log T/\Delta^2} \notag \,,
\end{align}
where Eq. \eqref{eq:aetda:regret} comes from Lemma \ref{lem:aetda:collision} and \ref{lem:aetda:sub-optimal}. 




\begin{lemma}\label{lem:aetda:collision}
    Following the AETDA algorithm, conditional on $\urcorner \cF$,  the regret of each player $p_i$ suffered when focusing on arms satisfies that 
    \begin{align*}
        % \EE{\sum_{t=1}^T \bOne{\bar{A}_i(t)=\emptyset} \mu_{i,\overline{m}_i} \mid \urcorner \cF } 
        \EE{\sum_{t=1}^T \bOne{\opt_i \neq -1}\bracket{\mu_{i,\overline{m}_i} - X_{i}(t)} \mid 
    \urcorner \cF } \le  \frac{96\min\set{N^2, NK} C\log T}{\Delta^2}\cdot \mu_{i,\overline{m}_i} \,.
    \end{align*}
\end{lemma}

\begin{proof}
Recall that conditional on $\urcorner \cF$, the AETDA algorithm is an online adaptive version of the offline DA algorithm and it will reach the player-optimal stable matching. 
Once $p_i$ focuses on an arm ($\opt_i\neq -1$), this arm must have a higher ranking than the player-optimal stable one.
So the regret in this part only happens when $p_i$ collides with others at arm $\opt_i$. 

Lemma \ref{lem:DA:steps} shows that the offline DA algorithm proceeds in at most $\min\set{N^2, NK}$ steps. Denote $t_s$ as the round index of the start of step $s$ in our AETDA. Then the regret caused when focusing on arms can be decomposed into these steps as Eq. \eqref{eq:aetda:explore:steps}. 
The total regret in this part satisfies
\begin{align}
        & \EE{\sum_{t=1}^T \bOne{\opt_i \neq -1}\bracket{\mu_{i,\overline{m}_i} - X_{i}(t)} \mid 
    \urcorner \cF } \notag \\ \le& \EE{\sum_{s=1}^{\min\set{N^2, NK}} \sum_{t=t_{s}}^{t_{s+1}-1} \bOne{\opt_i \neq -1, \bar{A}_i(t)=\emptyset} \mu_{i,\overline{m}_i} \mid \urcorner \cF } \label{eq:aetda:explore:steps} \\
        \le & \sum_{s=1}^{\min\set{N^2, NK}} \frac{96C\log T}{\Delta^2}\cdot \mu_{i,\overline{m}_i} \label{eq:aetgs:explore:steps:times} \\
        \le& \frac{96\min\set{N^2, NK} C\log T}{\Delta^2}\cdot \mu_{i,\overline{m}_i} \notag \,. 
    \end{align}
In each step, the regret occurs when $p_i$ focuses on the arm $\opt_i$ and other players round-robin explore this arm who is preferred more by $\opt_i$. Based on Lemma \ref{lem:cen:pulltime}, an arm is explored for at most $96\log T/\Delta^2$ times by another player $p_{i'}$ before the stopping condition holds, i.e., $\mathrm{opt}_{i'}\neq -1$. 
And when $N$ players explore $K$ arms, at most $C$ rounds are required to ensure each player can be matched with each arm once. 
That is why Eq. \eqref{eq:aetgs:explore:steps:times} holds. 
\end{proof}





\begin{lemma}\label{lem:aetda:sub-optimal}
    Following the AETDA algorithm, the regret of each player $p_i$ caused by exploring sub-optimal arms satisfies that 
    \begin{align*}
        % \EE{\sum_{t=1}^T \sum_{a_j\in \cK} \bOne{\bar{A}_i(t)=a_j} \Delta_{i,\overline{m}_i,j} \mid \urcorner \cF } \le \frac{96\min\set{N,K} C\log T}{\Delta^2}\cdot  \mu_{i,\overline{m}_i} \,.\\
        \EE{\sum_{t=1}^T \bOne{\opt_i = -1}\bracket{\mu_{i,\overline{m}_i} - X_{i}(t)} \mid 
    \urcorner \cF } \le \frac{96\min\set{N,K} C\log T}{\Delta^2}\cdot  \mu_{i,\overline{m}_i} \,.\\
    \end{align*}
\end{lemma}

\begin{proof}
Recall that $\opt_i=-1$ means that player $p_i$ explores to find its most preferred available arm. 
 % So the regret only occurs when $p_i$ explores arms in a round-robin manner. 
According to Lemma \ref{lem:DA:steps}, the player-optimal stable arm must be the first $\min\set{N,K}$ ranked, denote $t_{s,s}$ and $t_{s,e}$ as the start and end round index when $p_i$ explores to find the $s$-ranked arm, then the regret can be decomposed as Eq. \eqref{eq:aetgs:dueto:GS:explore}. The total regret caused by exploring sub-optimal arms satisfies that
    \begin{align}
        &\EE{\sum_{t=1}^T \bOne{\opt_i = -1}\bracket{\mu_{i,\overline{m}_i} - X_{i}(t)} \mid 
    \urcorner \cF } \notag \\
        \le &  \EE{ \sum_{s=1}^{\min\set{N,K}} \sum_{t=t_{s,s}}^{t_{s,e}}  \bracket{\mu_{i,\overline{m}_i} - X_{i}(t)} \mid \urcorner \cF } \label{eq:aetgs:dueto:GS:explore} \\
        \le& \sum_{s=1}^{\min\set{N,K}} \frac{96C\log T}{\Delta^2} \cdot \mu_{i,\overline{m}_i} \label{eq:aetgs:dueto:GS:explore:times} \\
        \le& \frac{96\min\set{N,K} C\log T}{\Delta^2}\cdot  \mu_{i,\overline{m}_i} \notag \,,
    \end{align}
    where Eq. \eqref{eq:aetgs:dueto:GS:explore:times} holds based on Lemma \ref{lem:cen:pulltime} and the fact that each player can match each arm once in at most $C$ rounds during round-robin exploration.
\end{proof}






\subsection{Proof of Theorem \ref{thm:aetgs:strategic}}


    For the offline DA algorithm, it has been shown that when all of the other players submit their true rankings, no single player can improve its final matched partner by misreporting its preference ranking \cite{roth1982economics,dubins1981machiavelli}. 

Recall that our algorithm is an adaptive online version of the GS algorithm and $\mathrm{opt}_i$ represents the estimated most preferred arm of player $p_i$ in the currently available arm set $S_i$.
    There are mainly two cases of misreporting. One is that $p_i$ wrongly reports an arm as its estimated optimal one which actually is not. And the other case is that $p_i$ has learned the optimal arm but reports $\mathrm{opt}_i$ as $-1$. 
     According to the property of the DA algorithm, no matter whether $p_i$ has estimated well its current most preferred arm, reporting a wrong one would finally result in a less-preferred arm. 
    And on the other hand, if $p_i$ has already estimated well its most preferred arm, misreporting $\mathrm{opt}_i=-1$ would keep it in the round-robin exploration process. 
    According to the property of GS, no matter whether all players enter the algorithm simultaneously, their final matched arm is always the player-optimal one. 
    So misreporting $\mathrm{opt}_i=-1$ is equivalent to the player delaying entry into the offline DA algorithm and the final matching would not change. 






