\section{Conclusion}
In this work, we present a simple end-to-end framework VLBase and VLCounter for zero-shot object counting that eliminates the need for the process of discovering exemplars.
Simply put, VLBase is built upon the pre-trained vision-language CLIP model.
Then, VLCounter introduces three key components that bring task-specificity and object-specificity.
Whereas the semantic-conditioned prompt tuning and learnable affine transformation fine-tune the encoding process to obtain counting-tailored representations, the segment-aware skip connection is designed to learn the generalizable decoder with the knowledge.
Our thorough experiments on FSC147 and cross-dataset benchmarks validate the effectiveness and efficiency of VLCounter.
% and intuitive \textbf{one-stage} zero-shot object counting method VLBase and VLCounter that builds upon the pre-trained vision-language CLIP models.
% Especially VLCounter involves three key components that further embody the localization capability of the visual-text interaction to suit the object-counting task while obtaining semantic-aware visual representations in an end-to-end manner
% To validate the efficacy of VLCounter, we conduct extensive evaluations on the FSC147 and CARPK benchmark datasets and show that our model outperforms previous state-of-the-art complicated two-stage methods.



% As we focused on designing and fine-tuning the effective end-to-end framework, 
% Although we have successfully achieved 
% From a technical standpoint, we successfully achieved direct interaction between visual and language modalities.
% We did not incorporate considerations for scale variations, as exemplar patches that could serve as references were not available to us.
% As a result, the model might encounter challenges in handling objects at different scales effectively.
% Addressing this issue and incorporating scale considerations could be an area of improvement for future work.