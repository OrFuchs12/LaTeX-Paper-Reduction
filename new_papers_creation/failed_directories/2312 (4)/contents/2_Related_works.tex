\section{Related Works}
% 08/06 03:00 - WJ : Class-specific, Zero-shot fix
%% TODO : Few-shot, Learnable Prompt Tuning


\subsection{Object Counting}
\paragraph{Class-specific Object Counting}
focuses on quantifying specific class samples, e.g., crowds~\cite{h0, h1, h2, h3}, cars~\cite{2016cars, 2017drone}, animals~\cite{2016animal}, and cells~\cite{2018cell}.
Most works fall into two main categories each employing detection~\cite{d0, 2017drone, d1} or regression~\cite{r0, r1, r2, r3} mechanism to measure the number of instances.
The former predicts the bounding box for every instance using an object detector, whereas the latter predicts the density distribution of the image instead, thereby being recognized as a more robust stream against partially occluded objects~\cite{2019GMN}.



\paragraph{Few-shot Object Counting}
To overcome the lack of generality of being constrained to a specific class, 
Generic Matching Network~(GMN)~\cite{2019GMN} first formalized class-agnostic object counting to count the desired objects provided by the human-annotated exemplar patches.
They introduced a two-stream architecture to encode each image and exemplar to handle the difference in their resolution. 
Following them, CFOCNet~\cite{2021CFOCNet} and BMNet~\cite{2022BMNet} also adopted and enhanced the two-stream approach by adding a layer-wise matching procedure and bilinear similarity metric.
Other works adhere to single-stream architecture. 
To be specific, FamNet~\cite{2021FAMNet} and RCAC~\cite{gong2022class} use ROI pooling after feature extraction to obtain exemplar prototypes.
However, the aforementioned studies suffer from the limitation that every inference requires human-annotated exemplars.



\paragraph{Zero-shot Object Counting}
has been proposed by RepRPN~\cite{2022RepRPN} to discard the duty of annotating target exemplars for counting.
To be specific, they trained the region proposal network~(RPN) to capture the patches containing the most frequently appeared objects to replace human-annotated exemplars.
Then, to further grant more applicability to exemplar-free object counter, ZSC~\cite{2023zsc} presented a method that takes guidance from semantic information.
By matching semantic information to randomly generated patches, they sampled the most semantically relevant patches to obtain target exemplars.
Our work shares the goal with ZSC in that we aim to train the counter that can count user-specified classes with only class names.
Yet, as mentioned methods adopt a two-stage pipeline that is prone to error propagation, we focus on mitigating such issues by proposing an end-to-end framework that localizes and counts at once.



\subsection{Prompt Tuning}
\label{Related:VPT}
Prompt tuning is a popular strategy to adapt pre-trained large models for downstream tasks due to its efficiency compared to conventional fine-tuning methods~\cite{wang2022learning, gu2021ppt, 2020lp, 2022vpt}.
Whereas fine-tuning updates all parameters, prompt tuning freezes the pre-trained large models and introduces only a small set of learnable prompts to optimize~\cite{li2021prefix, 2022vpt}.
Following these works, we utilize prompt tuning to efficiently exploit the quality of the visual-language understanding capability of pre-trained CLIP.
Yet, our work differs in using semantic information from the semantic embeddings to condition the prompts in the visual encoder to concentrate more on specification-relevant information.