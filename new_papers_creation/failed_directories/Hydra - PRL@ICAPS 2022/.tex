%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2022 (based on sample-sigconf.tex)
%%% Prepared by Ana L. C. Bazzan and Lucas N. Alegre, with the contribution of the AAMAS-2022 Program Chairs. Thanks to Natasha Alechina. (version 2022-07-08)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.
%%% Use the first variant below for the final paper.
%%% Use the second variant below for submission.

\documentclass[sigconf]{aamas} 
%\documentclass[sigconf,anonymous]{aamas} 

%%% Load required packages here (note that many are included already).

\usepackage{balance} % for balancing columns on the final page

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \usepackage[table]{xcolor} % ADDED !
% \usepackage{amsmath,amssymb,amsfonts}
\newcommand{\tuple}[1]{\ensuremath{\left \langle #1 \right \rangle }}
\newcommand{\reals}{\ensuremath{\mathbb{R}}}
\usepackage{todonotes}
\usepackage{xspace} % ADDED!
\newcommand{\sbirds}{Science Birds\xspace} % TODO Replace with acronym
\newcommand{\hydra}{\textsc{Hydra}\xspace} % TODO Replace with acronym



%%% AAMAS-2022 copyright block (do not change!)

\setcopyright{ifaamas}
\acmConference[AAMAS '22]{Proc.\@ of the 21st International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2022)}{May 9--13, 2022}
{Auckland, New Zealand}{P.~Faliszewski, V.~Mascardi, C.~Pelachaud,
M.E.~Taylor (eds.)}
\copyrightyear{2022}
\acmYear{2022}
\acmDOI{}
\acmPrice{}
\acmISBN{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{???}

%%% Use this command to specify the title of your paper.

\title{Model-Based Adaptation to Novelty in Open-World AI}

%%% Provide names, affiliations, and email addresses for all authors.
\author{Submission \#7490} % ijcai submission no. is 7490
% \author{Samwise Gamgee}
% \affiliation{
%   \institution{Long Journey University}
%   \city{Eriador}
%   \country{New Zealand}}
% \email{samwise.gamgee@lju.nz}

% \author{Merry Brandybuck}
% \affiliation{
%   \institution{The Ring Company Inc.}
%   \city{Lindon}
%   \state{The Great Land}
%   \country{New Zealand}}
% \email{merry@ring.com}

% \author{Frodo Baggins}
% \affiliation{
%   \institution{Two Towers University}
%   \city{Rohan}
%   \country{New Zealand}}
% \email{frodo.baggins@2tu.nz}

%%% Use this environment to specify a short abstract for your paper.

\begin{abstract}
Model-based agents, such as those usually presented in the automated planning literature, 
are often more reliable, explainable, and effective than agents created by model-free reinforcement learning techniques. 
However, to function in an open world, an automated agent must be able to adapt to novel situations.
Most model-based agents are ill-equipped to handle novel situations in which their model of the environment no longer sufficiently represent the world. 
We introduce \hydra, a model-based agent operating in open-world environments, that detects such novelties and adapts to them  automatically. 
\hydra uses a mixed continuous-discrete planning formalism, namely PDDL+, which allows modeling a broad range of domains. 
It detects novelties by analyzing inconsistencies between its PDDL+ model and observations, 
and quickly adapts to novelty by automatically repairing its model. 
The latter step uses model-based diagnosis to identify possible repairs -- diagnoses -- that are consistent with the collected observations.  
We demonstrate our approach on two domains: the well-known balancing cartpole problem and the popular Angry Birds game. Our results show that we are able to quickly and effectively detect and adapt to some classes of novelties.
\end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Open world learning, Automated planning, Novelty adaptation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction} 

% FROM THE IJCAI 2022 CALL FOR PAPERS

% Paper length: Papers must be no longer than 7 pages in total: 6 pages for the body of the paper (including all figures/tables), plus up to 1 additional page with references that do not fit within the six body pages. (For accepted papers, up to two additional pages may be purchased at an additional cost per page; note that, at the time of submission, papers are required to adhere to the 6+1 format above.) Overlength papers will be rejected without review.

% Supplementary file: Authors may submit up to 50MB of supplementary material, such as appendices, proofs, derivations, data, or source code; all supplementary material must be in PDF or ZIP format. Supplementary material should be material, created by the authors, that directly supports the submission content. Like submissions, supplementary material must be anonymized. To submit supplementary material, first upload your submission. You will then be able to upload supplementary material from the author console. There are two entries for supplementary files: one is “TechnicalAppendix”, and the other one is “ResubmissionFile”. The latter one can be ignored if you are not resubmitting paper rejected at any peer-reviewed conference within the past 6 months. Looking at supplementary material is at the discretion of the reviewers.


% Motivation: novelty is key to intelligence

While current AI systems perform with superhuman capability in many game domains, each of these domains is a closed world, and minor perturbations of the game can lead to significant drops in performance. 
Witty et al. demonstrated that even changes which made the game easier could cause catastrophic results for superhuman performing deep Q-learning agents \cite{witty2018measuring}. 
On the other hand, one hallmark of human cognition is our ability to function in an open-world. 
People navigate to previously unseen places, perform new tasks, and integrate new technology into their lives. In games, human flexibility supports inventing new strategies along with adapting to changing rules (e.g., consider chess players who play bughouse\footnote{\url{https://en.wikipedia.org/wiki/Bughouse_chess}}).
This mismatch between human cognitive abilities and machine capabilities indicates that adapting to novelty is a major problem in current AI systems. Our research goal is to impart machines with human-like learning capabilities to make them robust to novelty. 


% Concrete: we develop an agent that works in a mixed continuosu-discrete environment, can perform actions and get observations. 
As a step towards achieving this goal, we developed the \textbf{Hypothesis-Guided Model Revision over Multiple Aligned Representations} (\hydra) agent. 
\hydra is an autonomous agent that operates in a mixed continuous-discrete environment and can detect and adapt to novel changes in that environment. 
\hydra is designed to performs two key tasks: \textbf{novelty detection} and \textbf{novelty adaptation}.
Novelty detection is the problem of identifying from the collected observations whether something meaningfully novel has occurred~\cite{pimentel2014review}. 
Novelty response is the problem of adapting the behavior of the agent when novelty is introduced such that it continues to function effectively. 
While each of these tasks, individually, have been addressed in the literature to some extent, 
our agent design solves them in a synergetic manner and in the context of acting in a dynamically changing world. 
%In addition, performing these tasks in the mixed a continuous-discrete environment in which \hydra operates is very challenging. 

\todo[inline,color=green]{[[Roni: maybe add a bit more here in terms of related works]]}
% 


% Key: PDDL+ model of the world 
Central to our approach for both novelty detection and adaptation is the use of PDDL+~\cite{fox2006modelling}, a feature-rich domain-independent planning modeling language, to represent \hydra's beliefs about the environment dynamics. 
PDDL+ is an extension of the well-known Planning Domain Definition Language (PDDL)~\cite{mcdermott1998pddl} that allows defining discrete and continuous state variables as well as exogenous events and continuous processes. 
It has been used to model a range of complex planning problems, including Chemical Batch Plant~\cite{della2010pddl+}, Atmospheric Reentry~\cite{piotrowski2018heuristics}, Urban traffic Control~\cite{vallati2016efficient}, and Planetary Lander~\cite{della2010resource}.
% \todo[inline,color=green]{[[Roni: todo - fill with references of applications]]}
\hydra selects which actions to perform by
invoking a domain-independent PDDL+ planner over 
the agent's PDDL+ model of the environment and the task at hand. 
Novelty detection is performed by comparing the expected outcome of the actions it performed %according to \hydra's PDDL+ model and 
with the observed outcome.  
Novelty response is performed by automatically diagnosing and then repairing \hydra's PDDL+ model so that it is consistent with the observations. 



% Experiments
We have implemented \hydra on two domains: OpenAI Gym's balancing Cartpole domain~\cite{brockman2016openai}, and Science Birds, a version of the popular Angry Birds game developed for research purposes~\cite{renz2019ai}. 
In this work, we describe our implementation for Cartpole and report on a small-scale set of experiments in the Cartpole domain. The results show that \hydra can adapt to novelty much faster than standard Reinforcement Learning (RL) techniques, requiring only \todo[inline,color=green]{[[TODO: HOW MANY]]} interactions with the world to return to top performance after novelty has been introduced. Then, we describe key challenges that arise when using \hydra for ScienceBirds, and outline directions to meet these challenges. 


\section{Problem Definition}

% The general setting of the world
\hydra is designed to interact with the environment in a standard Reinforcement Learning (RL) setup. 
That is, the agent plays a sequence of \textbf{episodes}, 
every episode consists of a sequence of \textbf{steps},
and every step consists of observing a state, 
performing an action, 
observing the resulting state, 
and receiving reward. 
The reward of an episode is the sum of rewards in its constituent steps.
We assume that the \emph{environment} can be modeled as a transition system $E=\tuple{S, A, Tr, I, T, R}$
where $S$ is the possibly infinite set of states;
$A$ is the set of possible actions; 
$Tr$ is a transition function $Tr: S\times A \times S \rightarrow [0,1]$, 
where $Tr(s,a,s')$ returns the probability of reaching a state $s'$ when performing action $a$ in state $s$;
$S_I\subset S$ is a set of possible initial states, 
$S_T\subset S$ is set of terminal states, 
and $R$ is a reward function $R: S\rightarrow \reals$, where $R(s)$ returns the reward of reaching state $s$.\footnote{Other reward function formulations are also common, e.g., where the reward function includes both state and action ($R(s,a)$).}
The agent has a possibly partial view of the environment $E$ and the states it encounters, 
but this view includes the rewards it collects. 
An agent plays an episode in an environment $E=\tuple{S, A, Tr, S_I, S_T, R}$ 
means that the agent selects and performs an action in the environment starting 
from a randomly selected initial state $s_I\in S_I$ and ending when reaching a terminal state $s_T\in S_T$. 
The result of an agent playing an episode is a \emph{trajectory}, which comprises a sequence of tuples of the form $\tuple{s, a, s',r}$, representing that the agent performed action $a$ at state $s$, reached the state $s'$ and collected a reward $r$. 



% Defining novelty
Following the \emph{Theory of Environmental Change}~\cite{langley2020open}, 
we view novelty as a \emph{transformation} of the underlying environment $E$ that occurs some point in time.
Formally, we consider novelty here as a function $\varphi$ that can be applied to an environment $E$ and outputs an environment $\varphi(E)$ 
that is different from $E$ in some way.\footnote{Boult et al.~\cite{boult2021towards} referred to this type of novelty as a \emph{world novelty}.}
Introducing a novelty $\varphi$ in an environment $E$ means that the underlying environment $E$ changes to $\varphi(E)$. 
In an open world, sequences of novelties can, in general, be introduced at multiple points in time. 
However, in this work we simplify and assume that 
(1) novelty is not introduced during an episode, only between episodes;
(2) at most one novelty will be introduced;
(3) once a novelty is introduced all subsequent episodes are played in the modified environment. 
We refer to this setup as the \emph{single persistent novelty setup}. 

\begin{definition}[Single persistent novelty setup]
A single persistent novelty setup is defined by a tuple $\Pi=\tuple{E, \varphi, i}$ 
where $E$ is a transition system, $\varphi$ is a novelty function, 
and $i$ is a non-negative integer specifying the episode in which novelty $\varphi$ is introduced. 
In this setup, an agent plays $i$ episodes from environment $E$ 
and all subsequent episodes in environment $\varphi(E)$. 
\end{definition}

% Objective
A key aspect of this setup is that the agent does know the value of $i$ or $\varphi$, i.e., the agent is not given \emph{when} novelty will be introduced and \emph{how} it will change the environment. 
We focus on two challenges that arise in this setup when an agent plays multiple episodes: \emph{novelty detection} and \emph{novelty response}. 
The former challenge refers to identifying which episodes are played in $\varphi(E)$ 
and the latter challenge refers to maximizing the rewards collected over time. 
% Our objective is to play the game, \emph{detect} the novelty when it occurs, and \emph{respond} to it. 
% We define these objectives formally as follows. 
% Let $e_1,\ldots$ be a sequence of episodes, and let $e_n$ be the first episode in which novelty has been introduced. 
Note that to directly address the novelty detection challenge, the game playing agent outputs its belief about whether or not novelty has been introduced after playing every episode. 


% An example of novelty is the introduction of a new bird type with different dynamics and actions that would be available in future levels. 





% \begin{figure}[bth]
% \centering
%         \includegraphics[width=0.75\columnwidth]{figures/Metrics.pdf}
%         \caption{An illustration of the desired novelty response behavior. The $x$-axis is time --- number of episodes played --- and the $y$-axis is the reward collected in every played episode.}
%         \label{fig:hydra-metrics}
% \end{figure}

For novelty detection, performance can be measured using standard anomaly detection metrics (false positives, false negatives, etc.). 
For novelty response, performance can be measured by the cumulative reward collected by the agent after novelty has been introduced. %, i.e., after episode $i$. 


% \begin{figure*}[bth]
% \center{\includegraphics[scale=0.7]{figures/HYDRA.pdf}}
% \vspace{-0.3cm}
%         \caption{\label{fig:hydra} HYDRA draws on multiple model representations to plan actions, observe their effects, and focus learning.}
% \end{figure*}
% Figure~\ref{fig:hydra-metrics} illustrates the desired behavior of our agent, in terms of novelty response. 
% The $x$-axis is number of episodes played (``Time'') 
% and the $y$-axis is the reward collected in every played episode (``Score''). 
% The vertical dashed red line marks when novelty has been introduced. 
% The SOTA line illustrate how a standard state of the art learning agents is expected to behave before and after novelty has been introduced. 
% We expected the SOTA's performance to significantly decrease when some novelties are introduced, and that it would require significant re-training for the SOTA agent to return to high performance. This degraded performance between the time novelty has been introduced and the agent fully adapts to it is referred to as the \textbf{novelty recovery} penalty. Hydra is designed to minimize this penalty, at the cost of slightly reduced performance during the pre-novelty stage. This decreased performance is due to false detections of novelty that may occur. 
% % and HYDRA lines illustrate how a standard state of the art learning agents and our HYDRA are expected to behave before and after novelty has been introduced. 
% % SOTA is expected to learn how to act faster than HYDRA, since it does not consider the possibility of novelties. 
% % However, we expect HYDRA to be able to adapt to novel situations faster than SOTA.

% [[Roni: maybe all of the above about the metrics figure is redundant? ]]
% [[Roni: Ideally we remove this figure and show it in the real plots]]

% The agent has two goals: (1) to accurately detect when novelty has been introduced, 
% and (2) to maximize the cumulative reward it collects over time. 
% The cumulative reward collected by the agent after novelty has been introduced reflects the ability of our agent to respond to novelty. 



% To measure our agent's ability to detect novelty, the agent outputs its belief about 
% whether or not novelty has been introduced at the end of each episode. 
% To measure our agent's ability to respond to novelty, we 
% When episode $e_i$ ends, the agent must also output whether or not novelty has been introduced in this episode. 
% The latter is to address the novelty detection aspect of our problem. 
% The cumulative reward collected by the agent and how it changes 


%The result of this learning will enable our agent to mitigate the effects of the novelty on its performance and, when possible, take advantage of new opportunities available due to the change in the environment on future problems in the sequence. 
% Figure \ref{fig:metrics} illustrates this process and how we intend to measure performance against a state-of-the-art AI system that is not designed to respond to novelty.


% [[Roni: elaborate more on the figure -- what it says, why is it useful]]




\section{The \hydra Agent}
% \section{Problem Definition}
% \sbirds is a free version of the popular Angry Birds game. The player launches birds in sequence at a structure made out of different material blocks with the goal of destroying the pigs inside. Different birds and structures have different actions (e.g., yellow birds accelerate when the player taps the screen during flight) and properties (e.g., TNT objects explode when damaged by birds or other falling objects). \sbirds is a challenging domain for AI agents due to the continuous action space and the large state space of resulting block configurations, and has maintained a yearly competition since 2012 \cite{renz2019ai}.

% Our agent interacts with the game through a server with the following API. After the level is loaded, the agent is given a list of objects with their outer hull polygons and a color-map that specifies the amount of each color in inside the polygon\footnote{Raw pixels for the entire image are also available, but we do not use them in our system.}. The agent specifies shots by providing an $(X, Y)$ position to launch from and a time $t$ to tap the screen. The screen tap initiates actions based on bird type (e.g., a bomb bird will explode a few seconds after it is tapped). After each action, the score is updated.

% We are studying novelty as something that is introduced into the environment while an agent is performing tasks. In the context of \sbirds, the agent plays a sequence of levels. At some point in the sequence, novelty is introduced and all subsequent levels behave with the novelty. An example of novelty is the introduction of a new bird type with different dynamics and actions that would be available in future levels. Our objective is to play the game, \emph{detect} the novelty when it occurs, and \emph{respond} to it. The result of this learning will enable our agent to mitigate the effects of the novelty on its performance and, when possible, take advantage of new opportunities available due to the change in the environment on future problems in the sequence. Figure \ref{fig:metrics} illustrates this process and how we intend to measure performance against a state-of-the-art AI system that is not designed to respond to novelty.
% \section{Proposed Approach}

% \begin{figure*}[bth]
% \center{\includegraphics[scale=0.7]{figures/HYDRA.pdf}}
% \vspace{-0.3cm}
%         \caption{\label{fig:hydra} HYDRA draws on multiple model representations to plan actions, observe their effects, and focus learning.}
% \end{figure*}
% [[Roni: TODO Replace to a clearer figure that is more closely related to how we work]]
% Figure \ref{fig:hydra} shows an overview of Hydra. 
%Science Birds provides the score for the level and a description of the objects.


% HYDRA classifies these objects into types in its domain theory, and assesses if they have behaved consistently with the domain theory's expectations. These expectations could be driven by quantitative or qualitative composable models. Any inconsistencies are localized to model components using model-based diagnosis and learning problems are formulated. For example, if HYDRA does not understand why a structure has not fallen over, a possible explanation is that there is an unseen rigid object supporting it. Then, HYDRA may generate a plan to satisfy the learning goal by shooting a bird in that area and look for evidence of rigid object mechanics.


% Hydra, higher-level
To address the novelty detection and response challenges in the persistent single novelty setup, we propose the \hydra agent and describe it here in detail.
\hydra is a \emph{model-based agent} --- it uses a surrogate model of the environment to make predictions about the expected outcome of actions. 
In \hydra, this environment surrogate model, denoted 
$\hat{E}=\tuple{\hat{S}, \hat{A}, \hat{Tr}, \hat{I}, \hat{T}, \hat{R}}$, is to plan its actions, detect novelties, and adapt to them efficiently. 

% The Meta-Model 
Initially, the environment $E$ and \hydra's surrogate model $\hat{E}$ may be the same. 
But, when novelty is introduced $E$ changes and a gap between them is created. 
Moreover, $E$ and $\hat{E}$ may be different even before novelty is introduced due to modeling inaccuracies and different modeling choices. 
\hydra is designed for this by including an 
environment \emph{meta model}. The meta model serves as a bridge between the $E$ and $\hat{E}$. 
It supports the following core functionalities. 
\begin{enumerate}
    \item Create or update the surrogate model $\hat{E}$.
    \item Estimate state $\hat{s}\in\hat{S}$ matching the real current state $s\in S$. 
    \item Translate actions in $\hat{E}$ to $E$. 
\end{enumerate}

The first functionality is to create or update the surrogate model $\hat{E}$

to map observations from $and using them to create or update the surrogate model $\hat{E}$. 
The second functionality is to and identify a state $\hat{s}\in\hat{S}$ that best represents the current state $s$ of the real environment $E$. 
The second task of the meta model is to translate an action of the surrogate model $\hat{a}\in \hat{A}$ to an appropriate action $a\in A$ in the real environment $E$.  

 

But, as novelty
At the core of the \hydra agent is an environment \emph{meta model}, which allows states and actions 

serves as a bridge between the actual environment $E$ and the agent's internal representation of it, $\hat{E}$. 


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{hydra-cropped.pdf}
    \caption{An overview of the \hydra agent.}
    \label{fig:hydra-overview}
\end{figure}

\todo[inline,color=green]{[[Klenk: I think a gray box around all of the colored boxes here representing the \hydra agent would be helpful. The meta model does not perform actions.]]}


Figure~\ref{fig:hydra-overview} illustrates \hydra's design. 
\hydra has three algorithmic components:
\begin{itemize}
    \item \textbf{Planner.} The planner accepts a state $\hat{s}\in\hat{S}$ in the surrogate model and generates a plan that, according to $\hat{E}$, will maximize the expected reward. Then, it outputs the action $\hat{a}\in\hat{A}$ to perform now according to this plan.
    \item \textbf{Consistency checker (CC).} The consistency checker uses $\hat{E}$ to predict the expected states according to the previously performed actions and compares them with the observed states. 
    \item \textbf{Meta model repair (MMR).} The model repair accepts the same input as the consistency checker as well as the output of the consistency checker. It suggests which changes should be made to the meta model so that it yields an updated surrogate model $\hat{E}$ that better predicts the observations. %to make the expected and observed states be more similar. 
\end{itemize}
These components operate together in the following way. 
When a new episode starts, \hydra observes the current state and uses the meta-model to translates this observation $o$ to a state $\hat{s}$ in its environment model $\hat{E}$. 
Then, a planner generates a plan, aiming to maximize the expected reward collected starting from state $\hat{s}$ and operating in $\hat{E}$. 
Based on this plan, the planner outputs the next action to perform $\hat{a}$, which the meta model then translates to an action $a$ that can be applied in the real environment. 
This process continues until an episode is completed. 
At this stage, the \hydra CC analyzes the executed trajectory to see if it is consistent with the current surrogate environment model. 
This means analyzing every observed transition, i.e., every $\tuple{\hat{s}, \hat{a}, \hat{s}'}$ triple, where $\hat{s}$ is an observed state, $\hat{a}$ is the action \hydra performed in $\hat{s}$, and $\hat{s}'$ is the state observed immediately afterwards. 
If all states and transitions are consistent with the states and transitions specified in $\hat{E}$, then no change to the meta model is needed and \hydra will start a new episode. 


Otherwise, the CC outputs a real value $\gamma$ quantifying the likelihood that this consistency indeed indicates that novelty has been introduced. 
If this value passes some pre-defined threshold, \hydra declares that novelty has been detected. 
This inconsistency value is then passed to the MMR component along with the observed trajectory. 
%The MMR applies model-based diagnosis to suggest hypotheses explaining why the observed trajectory is inconsistent with $\hat{E}$. 
The MMR attempts to modify the current meta model so that it generates a surrogate environment model that is consistent with the observed trajectory. 
This is done by defining a set of \emph{Meta Model Manipulation Operators} (MMOs), denoted $\mathcal{M}$. 
Each MMO represents a possible change to the meta model. 
The MMR searches for a sequence of MMOs such that, when applied to the current meta model, yields a meta model that is most consistent with the observed trajectory (i.e., a meta model that generates a surrogate model consistent with the observed trajectory).  
Then, the selected MMO is applied to the meta model, and the process repeats for the next episode with the repaired meta model. 






\section{Implementing \hydra Agents using PDDL+}

Most of \hydra's design is domain independent. To demonstrate this, we implemented \hydra on two very different benchmark domains --- Cartpole and \sbirds --- and highlight in this section the details of this implementation. In particular, we highlight which parts of our implementation are domain-independent and which parts are design choices that are domain specific. 


The first design choice is the modeling language used to define the surrogate environment model $\hat{E}$. 
Both domains require dealing with mixed discrete and continuous dynamics. In addition, \sbirds dynamics are governed by chains of reactions triggered by the agent's actions, unlike many other planning problems where most world changes are the direct result of agent actions. 
Due to these properties, we chose PDDL+~\cite{fox2006modelling} as our modeling language for $\hat{E}$. 
%PDDL+ allows modeling of the environment, its dynamics and behavior, as well as the agent's interactions with the environment. 
The defining characteristic of PDDL+ is the ability to model exogenous behaviour with discrete events and continuous processes.
Events apply discrete effects instantaneously, whereas processes apply changes continuously while their preconditions hold. The agent has no direct control over processes and events, and can only interact with exogenous activity indirectly. As noted above, \sbirds is overwhelmingly governed by processes and events, making PDDL+ an appropriate modeling language for our setting.  
In addition, PDDL+ generalizes many previously proposed planning languages, which simplifies supporting additional domains. 

\subsection{Planner}
Following the choice of PDDL+ as a modeling language, \hydra requires a meta model that outputs a PDDL+ problem and domain file. To this end, we created two such meta models, one for each domain. 
The physics of Cartpole are well-known, and thus creating a meta model for such domain is straightforward.
\sbirds is a more challenging domain, with a complex dynamics and a large number of objects of different types. 
There is no known PDDL+ model for \sbirds and fully modeling the game is beyond the scope of this research. 
Nevertheless, we have created a PDDL+ model that solves a variety of \sbirds levels. 
Planning in this domain can result in search-space explosion, due to the tight integration of planning and scheduling over a continuous timeline. To improve the performance, our \sbirds model relies heavily on the \emph{Theory of Waiting}~\cite{mcdermott2003reasoning} and currently employs only one action responsible for the release of the bird from the slingshot at maximum initial velocity (i.e. at full stretch of the slingshot). This reduces the number of decision points in the search, which significantly reduces the branching factor. For the dynamics, events model collisions between birds, pigs, blocks, platforms, TNT blocks, and the ground, whereas processes capture the ballistic motion of birds under gravity and changing the possible angle of launch. 


%  \todo[inline,color=green]{[[Roni: Wiktor, can you put 2 small snippets of PDDL from the two domains here? nothing too large]]}

\begin{figure}
\begin{center}
% \begingroup
    \fontsize{8pt}{10pt}\selectfont
\begin{verbatim}
  (:process movement
    :parameters ()
    :precondition (and (ready)(not (total_failure)))
    :effect (and (increase (x) (* #t (x_dot)) )
        (increase (theta) (* #t (theta_dot)))
        (increase (x_dot) (* #t (x_ddot)) )
        (increase (theta_dot) (* #t (theta_ddot)) )
        (increase (elapsed_time) (* #t 1) ) ))
\end{verbatim}
% \endgroup
\caption{(CartPole) Continuous PDDL+ process updating over time the positions and velocities of the cart and pole.}
\label{fig:process-cartpole}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
% \begingroup
    \fontsize{8pt}{10pt}\selectfont
\begin{verbatim}
 (:action pa-twang
    :parameters (?b - bird)
    :precondition (and
        (= (active_bird) (bird_id ?b))
        (not (angle_adjusted))
        (not (bird_released ?b)) )
    :effect (and
        (assign (vy_bird ?b) 
            (* (v_bird ?b) SIN(angle)))
        (assign (vx_bird ?b) 
            (* (v_bird ?b) COS(angle)))
        (bird_released ?b)
        (angle_adjusted)) )
\end{verbatim}
% \endgroup
\caption{(Science Birds) Action releasing the bird from the slingshot, assigning initial velocities of the launched bird (approximations of sine and cosine functions are obscured).}
\label{fig:action-pa-twang}
\end{center}
\end{figure}

While each domain has its own meta model, the \hydra planner itself is a domain-independent PDDL+ planner. 
That is, both domains use the same planner, and adding a new domain would only require creating a corresponding meta model for it. 





% When our agent receives a \sbirds level to play, it automatically translates it to a PDDL+ planning problem under our \sbirds PDDL+ model. Then, we use an off-the-shelf PDDL+ planner, UPMurphi \cite{della2009upmurphi}, to obtain a plan. 




\subsection{Consistency Checking}
%\subsection{Adapting to Novelty as Search in the Space of PDDL+ Model Manipulation Operators}

Given a PDDL+ domain $D$, problem $\Pi$, and plan $\pi$, one can simulate the trajectory we \emph{expect} to observe when executing $\pi$ to solve $\Pi$ in domain $D$. 
Thus, we can apply our consistency checker as described above to detect novelties. 
That is, when playing an episode, \hydra observes the game and collects periodic states from the game API. 
Then, \hydra maps these states to PDDL+ states using the meta model. 
It determines whether the sequence of observed states is consistent with the sequence of states it expected to observe according to the model. 
A novelty is detected when the discrepancy between the observed and expected sequence of states exceeds a predefined threshold. 


This approach to detecting novelties is domain-independent. 
However, it relies on the accuracy of the PDDL+ model. 
In our implementation, this was sufficient for the Cartpole domain.
However, for \sbirds our PDDL+ model is not accurate enough to exactly match the observed state trajectory.
Note that it was accurate enough to allow the off-the-shelf PDDL+ planner to create plans that yielded state-of-the-art results in the non-novelty version of \sbirds. 
To address this, we have explored additions: (1) Using representation learning, we have trained a neural network over non-novelty levels to do next state prediction. (2) We created a domain-specific consistency checking component that incorporates qualitative analysis of the observed and expected trajectories. In particular, we matched the initial trajectory of the bird that was shot, the set of blocks destroyed at the end of each shot, and the types of objects that appears in the level. The current implementation of the \sbirds' consistency checker is a work-in-progress, and improving its accuracy is a topic for ongoing research.  \todo[inline,color=green]{[[Roni: not sure how to say all this in a nice way. Maybe push this to the experiments, Klenk: Agreed Just Replace the Sentence and say something like, \hydra combines these sources of information to perform novelty detection. Wiktor: I edited it to sound a bit more researchy, see what you think.]]}


% Roni: I can add a paragraph here with more details on how this consistency check was implemented currently. 

% % \subsection{Adapting to Novelty as Search in the Space of PDDL+ Models}
% Following Langley's recent \emph{Theory of Environmental Change}~\cite{langley2020open}, 
% we view novelty as a \emph{transformation} of the underlying world model. 
% To adapt to novelty, \hydra must update its domain model. To accomplish this, 
% it searches for a hypothesis 
% about the transformations that would be consistent with the observations. \hydra uses a set of \emph{Model Manipulation Operators} to transform the PDDL+ domain theory. To check if a sequence of MMOs is consistent with the observations, we apply them to the current PDDL+ model, 
% simulate the expected sequence of states according the modified model, 
% and check if this sequence of states is consistent with the sequence of states observed in the game. 
% After a consistent model has been found, it is used by HYDRA to generate future plans. 

\subsection{Meta Model Repair}


% \subsection{Adapting to Novelty as Search in the Space of PDDL+ Models}


In our implementation we limited the MMOs we consider to only modify the different constants used in the domain, e.g., gravity and pole length, by a fixed amount (either positive or negative). 
As noted above, to check if a sequence of MMOs is consistent with the observations, we apply them to the current PDDL+ model, simulate the expected sequence of states according the modified model, 
and check if this sequence of states is consistent with the sequence of states observed in the game. 
The space of possible MMO sequences is combinatorial. 
To search this space, we implemented a greedy best-first search that uses a heuristic a linear combination of the consistency score returned by the consistency checker and the size of the evaluated MMO sequence. 
The latter consideration biases the search towards simpler repairs. 


This implementation of the MMR is domain independent and is, in fact, the exact same source code for both domains.
However, in general, the set of possible MMOs to use for the meta-model repair is a domain-specific design choice. We discuss these choices later in the Discussion section. 

%There may be multiple models consistent with the current observations. Also, new novelties may occur over time. Therefore, the process of detecting novelties and adapting HYDRA's PDDL+ model to them is continuous: after every action HYDRA performs, it checks if the current observation is consistent with its model. If it is not, it searches for a sequence of MMOs that would yield a model that is consistent with the current and previously collected observations. 




% Domains description
\subsection{Domains}
The single persistent novelty setup and the novelty detection and response challenges can be studied on virtually any domain. In addition, the design of the agent is domain-independent, as well as most of its algorithmic components. 
However, we use the following two domains as running examples as well as a test bed for evaluating \hydra. 
For completeness, we provide a brief description of these domains.



% To demonstrate this, we implemented Hydra on two very different domains: \textbf{Cartpole} and \textbf{ScienceBirds}. 

\paragraph{Cartpole}
A classical RL benchmark domain in which a pole is connected to a cart and the task is to balance the pole in the upright position by pushing the cart either left or right.
There are multiple variants to the Cartpole domain. The variant we are using is defined as follows.
A state in this domain is represented by 4 state variables (velocities and positions of the cart and the pole).
In every step, the agent's action is to apply force to the cart in either left or right direction. 
An episode ends when either 200 steps have been performed or a limit is exceeded (i.e. pole angle or cart position). % [[TODO: min/max angle]]. 
Every step returns +1 reward, so the total reward for an episode is between 1 and 200. 
An example of a novelty in this domain can be as simple as a change in the mass of the pole or as difficult as adding wind or even an extra pole that interferes with the original pole. 

% [[Roni: maybe add some text about the optimal closed-loop control formulae? or the one we used? not sure about this]]
% @article{yu2008closed,
%   title={Closed-loop tracking control of a pendulum-driven cart-pole underactuated system},
%   author={Yu, Hongnian and Liu, Yang and Yang, Taicheng},
%   journal={Proceedings of the Institution of Mechanical Engineers, Part I: Journal of Systems and Control Engineering},
%   volume={222},
%   number={2},
%   pages={109--125},
%   year={2008},
%   publisher={SAGE Publications Sage UK: London, England}
% }
% @inproceedings{duan2016benchmarking,
%   title={Benchmarking deep reinforcement learning for continuous control},
%   author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
%   booktitle={International conference on machine learning},
%   pages={1329--1338},
%   year={2016},
%   organization={PMLR}
% }






\paragraph{Science Birds}
This is a free version of the popular Angry Birds game developed for research purposes~\cite{renz2019ai}. 
In this single-player game, the player launches a finite set of birds in sequence
at a 2D structure made out of different material blocks and occupied by one or more pigs. The goal is to destroy all the pigs. 
Different birds and structures have different actions (e.g., yellow birds accelerate when the player taps the screen during flight) and properties (e.g., TNT objects explode when damaged by birds or other falling objects).
\sbirds is a challenging domain for AI agents due to the continuous action space and the large state space of resulting block configurations, and has maintained a yearly competition since 2012 \cite{renz2019ai}.


The agent interacts with the game through a server with the following API. After the level is loaded, the agent is given a list of objects with their outer hull polygons and a color-map that specifies the amount of each color inside the polygon\footnote{Raw pixels for the entire image are also available, but we do not use them in our system.}. 
The agent specifies shots by providing an $(X, Y)$ position to launch from and a time $t$ to tap the screen. The screen tap initiates actions based on bird type (e.g., a black bird will explode when tapped, a yellow bird will speed up, etc.). 
Thus, a state in this domain is the list of objects specified above and the possible actions are ``shoot at angle $\alpha$'' and ``tap''. 
Note that after shooting a bird the shoot action is not available until the target bird has become inactive and the level elements have settled. 
Similarly, after tapping a bird the tap action is not available until the next bird is released. 
In some time steps, there are no available actions, in which case a dummy ``no-op'' action is assumed.  
Shots that result in killing pigs or breaking blocks increase the score, which constitutes our reward function. 
A step constitutes shooting and potentially tapping a single bird. 
An episode here constitutes playing a single level. 
Playing a level starts with a finite set of birds that can be shot, and ends when either all pigs have been killed or when all the given birds were shot and have exploded. 
Examples of novelties in this domain are an increase in the force of gravity or an addition of a new type of bird with unique powers. 



\section{Experimental Results}
We are interested in designing adaptive systems that can deal with novelties introduced during their deployment robustly. There are two desired properties of such systems. First, they should detect and react to novelty \emph{quickly} with few interactions with the environment. This is desired because often, it is infeasible to collect large amounts of data during deployment. 
Secondly, the adaptation in the system should be \emph{interpretable} by a human. This dimension is useful for adaptation during deployment as it enables a human designer or operator to inspect why the system behavior has evolved and ascertain if that adaptation is correct. Below we evaluate our proposed approach in \hydra along these dimensions and analyze its performance. 

 \todo[inline,color=green]{TODO: Shorten paragraph above}

\textbf{Experiments} Our experiments were conducted in the CartPole domain. This domain is defined by specific values of several parameters including mass of the cart, length of the pole, gravity, and friction coefficient. 
 \todo[inline,color=green]{Ideally, we would put here the analytical control solution for this, which uses all these parameters and explain them. Not critical}
Each trial starts with the non-novel environmental setup in which we measure an agent's performance when the environment reflects what it has been designed or trained for. After few episodes, we introduce a novelty in the environment by changing some of the parameters that define the dynamics of the domain. 
Our experiments had two conditions: \emph{system detection} in which the presence of novelty was not indicated to the agent and it is expected to infer the presence of novelty and react to it autonomously, and \emph{given detection} in which an oracle indicated the presence of novelty to the agent.

We ran our experiments for two novelties: 
(1) \emph{Type 1}: gravity increases from $9.8$ to $12$ and the pole length grows from $1.0$ to $1.1$;
(2) \emph{Type 2}: cart mass decreases from $1.0$ to $0.9$ and the pole length grows from $1.0$ to $1.1$. 
The selected novelty was introduced in the $8^{th}$ episode in a trial. 
Every trial consisted of $30$ episodes, and each experiment consisted of $5$ trials.

\textbf{Baselines} In addition to \hydra, we report performance to two baseline agents: 
a non-adaptive planning agent and a deep Q-network (DQN) reinforcement learning agent. 
We refer to these agents as the planning agent and DQN agent, respectively. 
These baseline agents were designed and trained to achieve perfect performance in the non-novelty case prior to the experiment. 
The planning agent does not attempt to learn or adapt to novelty, and so its behavior for both experiment conditions (system detection and given detection) is the same. 
The DQN agent makes Bellman updates to its trained Q-network when novelty was indicated to it in the given detection experiment condition.  
 \todo[inline,color=green]{What about the learning factor? do we reset it to learn from scratch?}

\textbf{Results}
The results are summarized in Figure \ref{fig:combined-results}. The $x$-axis shows the episodes in a trial and the $y$-axis shows the total reward collected by the agent per episode, represented as a proportion of its score with a perfect controller. 
The red line indicates the episode where the selected novelty was introduced. 
The blue line shows the results for the ``system detection'' condition and the orange line shows the results for the ``given detection'' condition. 


% Trend 1: Composable models are more robust
As shown in Figure \ref{fig:combined-results}, all agents begins at perfect performance at the beginning of the trial and then experience a significant drop in performance as soon as the novelty is introduced in episode $8$. 
%The behavior changes in characteristic ways after the novelty is introduced.
%Then, all agents experience a drop in performance as soon as the novelty is introduced in episode $8$. 
This shows the selected novelties are meaningful for all agents. 
%is expected because the agents' were designed to operate in the non-novelty settings. 
%Their models (and Q-functions) become outdated and lead to worse performance. 
The planning agent and \hydra drop to $50\%$ performance for both novelties and both experiment conditions. 
The performance drop in the DQN agent is more drastic at $~25\%$ and below for both conditions. 
This difference is expected -- both the planning agent and \hydra are built with \emph{composable} component models. 
Even in novel situations, a majority of the component models are still relevant and can be exploited to drive behavior. 
In contrast, the knowledge that drives behavior in end-to-end learning systems like the deep Q-network is distributed and cannot be re-purposed to drive behavior in an environment with changing dynamics. 
This supports the claim that composable component models can be more robust to novelties.  \todo[inline,color=green]{Last sentence is tricky?}


% Trend 2: Only hydra adapts fast
Since the planning agent does not react to novelties directly, so its performance does not improve over time in both experimental conditions (given detection and system detection). 
The DQN agent does attempt to adapt to the observed novelty. 
While it starts with worse performance in the given detection condition, it improves towards the later parts of the trial. 
This observation suggests that the agent is learning a new q-function that supports the new dynamics of the environment under novelty conditions. 
However, this learning is slow, requiring multiple interactions ($~75$ episodes, not shown in the figure) with the environment to return to optimal performance. 
\hydra adapts very quickly in less than $20$ episodes in both experimental conditions. 
This observation supports the central thesis in this paper that model-based methods enable localization of novelty adaptation, making the corresponding learning very efficient. 
\hydra performs similarly in both experimental conditions, suggesting that \hydra can reliably detect novelty even when it is not indicated using consistency checking. 
  \todo[inline,color=green]{Ideally, we would hav novelty detection results for these novelties to support this}


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/experiments/compiled-results.pdf} 
    \caption{Total reward (y-axis) for each episode (x-axis) in a trial. Red line indicates the episode where novelty has been introduced. 
    	The results clearly show that \hydra adapts very quickly to the novelties in our experiment, returning to optimal performance in ~10 episodes.
    	 \todo[inline,color=green]{The top line has the gravity 12 as the left plot but in the other lines it is reversed. Can you fix this? Not critical}}
    \label{fig:combined-results}
\end{figure}


% \subsection{Cartpole}

% \subsubsection{Novelty Detection}
% Ideal trend: a human-generated composable model is more effective in detecting novelty than black box approaches. 

% Task 1: Compare Hydra novelty detection against an off-the-shelf anomaly detection algorithm. 

% Concrete task:
% Compare the UPenn model with our Cartepole novelty detection over the benchmark
% Metrics: FP, FN, ...

% Shiwali will help here

% Question: can the UPenn model be considered an off-the-shelf anomaly detection?
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/repairing-cartpole.png}
%     \caption{Caption}
%     \label{fig:my_label}
% \end{figure}



% \subsubsection{Novelty Response}
% Ideal trend: a human-generated composable model with our diagnosis-based approach allows faster adaptation to novelty 

% Task 2: Compare Hydra performance after novelty with an off the shelf RL approach 
% and also a non-adaptive Hydra agent (e.g., just a planner)


% \subsection{ScienceBirds}
% \subsubsection{Novelty Detection}
% Task 3: the same but more refined: here the model is not perfect, but we need both system 


% To compare the different novelty detection methods --- NN-based, PDDL-based, and the NN-PDDL hybrid (denoted Hybrid hereonafter) --- we performed the following set of experiments. 
% In each experiment, we run our agent with each of the novelty detection methods (NN, PDDL, or Hybrid) on a trial with 20 levels. 
% Novelty is being introduced after the first 10 levels are played. 
% We repeated this experiments with each of the 13 novelties published by ANU in the ScienceBirds novelty competition track.\footnote{\url{aibirds.org/angry-birds-ai-competition/novelty-track.html}} 
% After every level, the agent reports whether it detected novelty or not, and we evaluted the novelty detection method by measuring the number of false positives (novelty detection reported by the agent although no novelty has been introduced) and false negatives (novelty introduced by not detected by the agent). 

% \begin{figure}
%     \centering
%     \includegraphics[width=0.7\columnwidth]{figures/SB-Novelty Detection.png}
%     \caption{Average false positives and false negatives rates in the ScienceBirds domain using different novelty detection methods.}
%     \label{fig:sb-detection-experiment}
% \end{figure}

% Figure~\ref{fig:sb-detection-experimentmy_label} shows the results of these experiments. The results show that the NN-based and PDDL-based detection alone yield a high false positive rate of over 0.3, while the NN-PDDL Hybrid detection has a false positive rate of approximately 0.1. 
% This comes at some cost to the amount of false negatives, which is slightly smaller for the NN-based detection. 


% \subsubsection{Novelty Response}
% Task 4: the same

% We don't have a standard RL approach

% Question: is there an off the shelf RL approach for SB
% Wiktor will look for this?




\section{Related Work}

The topic of how to detect and react to novelties has been gaining significant attention in the AI literature. 
The novely problems we address in this work has been described by Senator~\cite{senator2019sailon} 
and analyzed by Boult et al.~\cite{boult2021towards}
and Langely~\cite{langley2020open}. 
Boult et al.~\cite{boult2021towards,langley2020open} did not propose an agent design for this setting, but rather suggested a framework for characterizing different types of novelties. 


Langely also identified four elements an agent architecture needs to properly address novelty detection and response --- ``performance, monitoring, diagnosis, and repair.'' 
HYDRA implements these elements. 
It uses its meta model to generate plans, act (\emph{performance}), and detect novelties (\emph{monitor}). 
Then, it uses heuristic search to identify elements of the meta model that are incorrect (\emph{diagnosis}) and modifies the meta model accordingly (\emph{repair}). 



We are not the first to design a novelty-robust agent and novelty detection and response algorithms. DiscoverHistory~\cite{molineaux2012discoverhistory} is an algorithm for inferring the possible values of unobservable state variables from observations. While our current experiments are similar in nature, HYDRA is designed to be more general, including novelties such as adding new variables, process, and object types. MIDCA~\cite{paisner2014goal} is a cognitive architecture for designing novelty-robust agents, where novelty is limited to which goal to pursue next. 

% \todo[inline,color=green]{[[TODO: Mention Pat Langley's paper, Mention Terry's Boult's paper, also SIFTS work if they have one yet, Roni: I didn't see any recent paper by David M so I'm assuming they don't have anything yet]]}

%More recently, there have been attempts at defining more general frameworks for handling novelty in open-world environments, driven by our contemporaries from the DARPA-SAILON program.[[Roni: No no, we're an anonymous author]]
More recently, there have been attempts at defining more general frameworks for handling novelty in open-world environments. 
Muhammad et al. \cite{muhammad2021novelty} proposed an approach for a cognitive agent to detect, characterize, and accommodate novelties based on knowledge and inference from the agent's internal predictions and the observed ground truth. 
% As expected from a fellow research program participant, there are similarities in our approaches. 
While it also exploits a planning-centric architecture and defines the composition of the world in a planning paradigm, there exist differences, the key of which is the richness of the environments that our approaches operate in and reason with. Most notably, they do not consider scenarios with external activity (i.e. beyond the executive of the agent), or continuous change in the environment. 

Other recent developments in the area have tackled yet another class of application domains, such as the game of Monopoly \cite{gopalakrishnan2021integrating}, a multi-player game heavily skewed towards uncertainty (from dice rolls, card draws, or adversaries' actions). The agent behaves according to a policy with a state-value function based on a short-horizon lookahead approach, prioritizing robustness to novelties in an already unpredictable game. 


%Adapting to novelties can also be viewed as a special case of \emph{Life Long Learning}~\cite{thrun1998lifelong,eaton}. 

%thrun1998lifelong



\section{Conclusions and Future Work}
\todo[inline,color=green]


We presented a design for an agent called \hydra, which can detect and adapt to novel situations automatically. 
\hydra is a model-based agents, in the sense that to choose how to act in an environment it uses an internal surrogate model of the environment. Mapping observations and actions from the environment to the surrogate environment model is done using a \emph{meta model} that describes such conversions. 
To detect when novelty occurs, \hydra compares the observations it collects with the one it expects according to its surrogate environment model.
To adapt to novelties, \hydra automatically attempts to repair its meta model based on observations. 
This is done by diagnosing its meta model and employing meta model manipulation operators (MMOs) to change the meta model as needed. 
We demonstrate \hydra in the Cartpole domain, a well-known DQN benchmark.
Our results show that introducing novelties in this domain is possible and results in catastrophic performance for both planning and DQN agents. 
\hydra is able to detect and adapt to novelties in this domain very quickly, requiring as many as 20 episodes before returning to normal performance. 







\begin{table}[tbh!]
	\centering
	\footnotesize
	\begin{tabular}{p{0.3\columnwidth} | p{0.3\columnwidth} | p{0.3\columnwidth}}
		\hline
		\textbf{Novelty types} & \textbf{Domain adjustment} & \textbf{Novelty examples} \\
		\hline
		Spatio-temporal trans. & Fluent changes & Increased gravity \\
		Structures trans. & New objects and fluents & New type of bird \\
		Processes trans. & New and/or changing existing processes & Introduced wind \\
		Constraints trans. & New preconditions and/or changed events & Only explosions kill pigs \\
		
		%Spatio-temporal Transformation & Fluent changes & Increased the force of gravity \\
		%Structures Transformation & New objects and fluents & Introduced new type of bird \\
		%Processes Transformation & New and/or changing existing processes & Introduced wind \\
		%Constraints Transformation & New preconditions and/or changed events & Only explosions can kill pigs \\
		\hline
	\end{tabular}
	\caption{Description of example novelties that can be encountered in \sbirds, changes to the PDDL+ model required to accommodate them, and their corresponding novelty types  defined by~\protect\cite{langley2020open}.} % TODO RONI!!!!! \cite{langley2020open}.}
	\vspace{-0.4cm}
	\label{tab:novelties_pddl}
\end{table}

Most components in the \hydra agent is domain-independent and we are currently implementing it on two other domains. 
To implement \hydra in a new domain, one needs to only (1) create the appropriate meta model, and (2) select the appropriate MMOs for novelty repair.
%There are two domain-specific parts A challenge in the design of \hydra is how to select the appropriate set of MMOs. 
In our current implementation, we focused in MMOs that modify the value of constant fluents in the PDDL+ model such as gravity, pole length, and pole mass. 
% the force gravity applies on flying objects, the size of the birds, and the speed in which the slingshot's angle is adjusted.  
An open research challenge is how to identify the necessary and sufficient set of MMOs for a given domain and types of novelties. 
Table~\ref{tab:novelties_pddl} maps possible types of MMOs to types of novelties as defined by Langley~\cite{langley2020open} along with examples from \sbirds, one of the domains we are implementing \hydra for.
% [[Roni: I decided not to detail more here, mainly due to time. Let me know if you think more is needed]
% [[Roni: I copied some of the text about this to drafts.tex]
The number of MMOs may be very large and thus finding a sequence of MMOs that may yield a consistent model is a challenging combinatorial search problem.  We expect to need heuristics to guide the search in an efficient manner. 
In our current implementation, we run a Greedy Best-First Search algorithm that uses a heuristic that prefers shorter MMO sequences that yield models that are more consistent. Future work may explore more sophisticated search techniques for this task. % Roni: this is pretty vague, but it is what I did. If this was not an abstract and we had more time , I'd say more. Let me know what you think





% \subsection{Hydra Agent for Cartpole}

% How we implemented it in practice for cartpole


% \subsection{Hydra Agent for ScienceBirds}
% How we implemented it in practice for SB







% \subsection{Case Study: Auto-Tuning Gravity}

% \begin{figure*}[tb]
%     \centering
%     % \includegraphics[width=0.3\columnwidth]{figures/level.png}
%     \includegraphics[width=0.9\columnwidth]{figures/level-miss.png}\qquad
%         \includegraphics[width=0.9\columnwidth]{figures/level-solved.png}
%     \caption{Example of automated model-repair with HYDRA. The left figure shows the first shot, in which HYDRA assumes an incorrect gravity factor. The right figure shows the second shot, after HYDRA diagnosed its incorrect assumption about gravity and corrected it accordingly.}
%     \vspace{-0.3cm}
%     \label{fig:model-repair}
% \end{figure*}

% To demonstrate how HYDRA works, we performed the following case study. 
% The agent is given a simple \sbirds level shown in Figure~\ref{fig:model-repair}, in which it needs to hit a pig that is elevated on some platform. 
% We intentionally set the agent's PDDL+ model to be incorrect by setting the force it assumes gravity applies on objects to be significantly higher than its real value. 
% % Set to 175
% Using this incorrect PDDL+ model, the agent fails to create a plan that hits the pig, since it cannot throw the bird strong enough to overcome the force of gravity it assumes. 
% In such a case, the agent chooses an arbitrary action, which in this case was to throw the bird at a very high angle. 
% The resulting trajectory is shown in Figure~\ref{fig:model-repair} (left). 
% Then, HYDRA uses the observed trajectory of the bird to correct its PDDL+ model. 
% Specifically, the MMOs we used were to modify the gravity parameter by either adding or subtracting 30 from its value. 
% HYDRA uses these MMOs to search for a PDDL+ model that is consistent with the observed trajectory. In this case, HYDRA is able to find such a model, modifying its gravity parameter to a value that is much closer to the correct value. Using the revised model, HYDRA is now able to create a plan that accurately shoots the pig and wins the game, as shown in Figure~\ref{fig:model-repair} (right). 


% %To demonstrate how HYDRA works, we performed the following case study. In this case study, the agent is given a simple \sbirds level depicted in Figure~\ref{todo}. We intentionally set the gravity factor in the agents PDDL+ model to be incorrect, setting the force gravity applies on objects to be half its real value. Using this incorrect PDDL+ model, the agent creates a plan that misses the pig, as can be seen in Figure~\ref{todo}. Then, HYDRA uses the observed trajectory of the bird to repair its PDDL+ model. The MMOs we used where to modify the gravity parameter by either adding or subtracting 50 from its value. After [[TODO]] iterations, HYDRA is able to repair its current model, modifying its gravity parameter to [[TODO]]. Using the revised model, HYDRA creates a plan that accurately shoots the pig and wins the game, as shown in Figure~\ref{todo}. 


% \section{Discussion}
% This early stage work opens up a several research questions:
% \begin{enumerate}
%     \item Are MMOs and search heuristics domain independent? That is, as we transition the technique to other domains (e.g., Minecraft, inverted pendulum control, and simulated driving) will the MMO's change?
%     \item How much of the domain revisions will be done within the PDDL+ model versus in other models in the system? For example, while the classification task of mapping observations to types is not performed in PDDL+, the types themselves are.
%     \item How to incorporate agent experience in the model revision decisions? Since our model is an approximation of the world, constantly revising it due to noise would not make sense.
%     \item How to account for other agents? We propose to modeling the behavior of other agents through their changing configurations with other objects in the environment, a model representation we call \emph{comic graphs} \cite{klenk2017collaborative}.
%     \item How to integrate PDDL+ planning with reinforcement learning techniques? Parameterized skills \cite{rostami2020using} provide a method for learning detailed action models that may be organized using planning.
% \end{enumerate}
% As part of the DARPA SAIL-ON effort, we will explore these questions over the next three years.






% \section{Acknowledgments}
% This work was supported by the DARPA SAIL-ON program under contract HR001120C0040. The views and conclusions in this  document are those of the authors and should   not be   interpreted   as   representing   the   official policies,   either   expressly   or   implied,   of   the   Defense Advanced    Research    Projects    Agency    or    the    U.S. Government


% \section{Acknowledgments}
% AAAI is especially grateful to Peter Patel Schneider for his work in implementing the original aaai.sty file, liberally using the ideas of other style hackers, including Barbara Beeton. We also acknowledge with thanks the work of George Ferguson for his guide to using the style and BibTeX files --- which has been incorporated into this document --- and Hans Guesgen, who provided several timely modifications, as well as the many others who have, from time to time, sent in suggestions on improvements to the AAAI style. We are especially grateful to Francisco Cruz, Marc Pujol-Gonzalez, and Mico Loretan for the improvements to the Bib\TeX{} and \LaTeX{} files made in 2020.

% The preparation of the \LaTeX{} and Bib\TeX{} files that implement these instructions was supported by Schlumberger Palo Alto Research, AT\&T Bell Laboratories, Morgan Kaufmann Publishers, The Live Oak Press, LLC, and AAAI Press. Bibliography style changes were added by Sunil Issar. \verb+\+pubnote was added by J. Scott Penberthy. George Ferguson added support for printing the AAAI copyright slug. Additional changes to aaai22.sty and aaai22.bst have been made by Francisco Cruz, Marc Pujol-Gonzalez, and Mico Loretan.

% \bigskip
% \noindent Thank you for reading these instructions carefully. We look forward to receiving your electronic files!

% Thus, PDDL+ is an attractive language for the Science Birds domain theory. 




% We have implemented this concrete Hydra agent on our two, very different, benchmark domains --- Cartpole and ScienceBirds. 
% A major achievement of our design is that it is mostly domain-independent, the only main difference between the Hydra Cartpole agent and the Hydra ScienceBirds agent is that they have a different meta model. The planner, consistency checker, and meta model repair, can all be domain independent. 

% --- allowing us to highlight which elements are domain dependent in which are domain-indepe
% Initially, we created for each domain a meta-model component that accepts an initial state and creates a corresponding PDDL+ domain and problem that represents it. Building this meta-model was done by a human expert so that it fits the pre-novelty behavior of the domain. 


% \begin{acks}
% If you wish to include any acknowledgments in your paper (e.g., to 
% people or funding agencies), please do so using the `\texttt{acks}' 
% environment. Note that the text of your acknowledgments will be omitted
% if you compile your document with the `\texttt{anonymous}' option.
% \end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The next two lines define, first, the bibliography style to be 
%%% applied, and, second, the bibliography file to be used.

\bibliographystyle{ACM-Reference-Format} 
\bibliography{library}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

