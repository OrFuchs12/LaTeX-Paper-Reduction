\section{Related Work}
\label{related}
Continual learning has been an active area of research in which many researchers have proposed techniques for mitigating catastrophic forgetting. The main idea behind these methods is to balance stability (retaining knowledge) and plasticity (learning new concepts) for deep learning models. Techniques broadly fall into three categories~\cite{de2021continual}: regularisation-based~\cite{kirkpatrick2017overcoming, li2017learning, shin2017continual, wu2019large}, replay-based~\cite{rebuffi2017icarl, chaudhry2018efficient, ostapenko2019learning, buzzega2020dark}, and parameter-isolation methods~\cite{rusu2016progressive, serra2018overcoming}. State-of-the-art performance often combines these approaches~\cite{mittal2021essentials}.

\subsection{Continual Self-supervised Learning}
However, many existing techniques rely heavily on labelled data, which is often unavailable.  Continual self-supervised learning (CSSL) approaches leverage self-supervised learning to enable continual learning under limited supervision. Initial efforts focused narrowly on self-supervised pre-training combined with supervised continual learning~\cite{gallardo2021self, caccia2022special} or extending contrastive learning frameworks~\cite{cha2021co2l, madaan2021rethinking}. However, these techniques are often designed to work with specific self-supervised learning frameworks. The latest continual self-supervised learning frameworks proposed in existing literature~\cite{de2021continual, fini2022self} have begun investigating more overarching and flexible frameworks, where the model learns continually from a stream of unlabelled data. The work by \cite{tang2023practical} proposed a unified semi-supervised framework with continual fine-tuning, introducing mechanisms for knowledge distillation and new task learning in both representation and classification. These demonstrate progress towards practical solutions under realistic supervision assumptions, and we focus on these recent proposals which repurpose SSL methods for continual learning in this work.

\subsection{Human Activity Recognition}
Wearable-based human activity recognition is an important component in human-centric computing because of its ability to extract real-time information and context clues about user behaviours, which enables other computing applications \cite{choi2016understanding, jaimes2015corredor}. However, the utility of HAR systems has been limited by the scarcity of high-quality labelled data and the computational capabilities of wearable devices.

Since the adoption of deep learning models for HAR, researchers have looked at tackling catastrophic forgetting in HAR specifically \cite{jha2021continual, leite2022resource, schiemer2023online}, and they have demonstrated moderate success. Simultaneously, self-supervised learning has recently gained popularity in the wearable-based HAR community, and many approaches from the general machine learning community, as well as customised methods, have been proposed to leverage unlabelled data to overcome the limitations of labelled data \cite{multi_self_har, tang2020exploring, haresamudram2020masked, tang2021selfhar, haresamudram2021contrastive}. However, we have yet to see efforts in leveraing self-supervised learning techniques for HAR in continual learning. This is an important step towards practical and generalisable user models. CSSL paves the way for realistic solutions to continually learn from combinations of labelled and unlabelled data.
