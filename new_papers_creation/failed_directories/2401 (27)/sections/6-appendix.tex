\begin{appendix}
    \input{sections/2-related}
\section{Transformation Functions}
\label{appx:trans}

\noindent\textbf{Random 3D rotation} applies a random rotation in the 3D space by picking a random axis in 3D and a rotational angle from uniform distributions. This is to simulate common pose changes in wearable devices.

\noindent\textbf{Random scaling} alters the size of samples within a window by multiplying them with a randomly chosen scalar. We apply this transformation since a model that can handle these scaled signals creates better representations because it learns to be unaffected by changes in amplitude and offset.

\noindent\textbf{Time warping} locally stretches or warps the time series data, smoothly distorting the time intervals between sensor readings.
\section{Evaluation setup}
\label{appx:setup}
{
\subsection{Model Architecture}
In this work, we adopted a lightweight HAR model, TPN \cite{multi_self_har}, to replace the vision-based model.

\subsection{Evaluation Metrics}
For the evaluation metrics, we are adopting the same framework as introduced in Kaizen \cite{tang2023practical}: \textbf{Final Accuracy (FA)}, \textbf{Continual Accuracy (CA)}, \textbf{Forgetting (F)}, and \textbf{Forward Transfer (FT)} as metrics. This set of values can better reflect the performance of continual learning methods in different use cases.

\subsection{Self-supervised Learning Frameworks}
In this work we selected a contrastive-based method MoCoV2+~\cite{chen2020improved, he2020momentum}, and an asymmetric-model-based method
BYOL~\cite{grill2020bootstrap} as the self-supervised learning method for continual learning. These two methods have been shown to be well-performing in continual self-supervised learning settings~\cite{fini2022self, tang2023practical}, and our goal is to investigate whether different CSSL methods demonstrate different performance characteristics when using different SSL methods as the knowledge retention mechanism.

\subsection{Dataset}
We performed our evaluation using the WISDM2019 (WISDM Smartphone and Smartwatch Activity and Biometrics Dataset) \cite{weiss2019wisdm}, which is an activity recognition dataset collected by the WISDM (Wireless Sensor Data Mining) Lab in the Department of Computer and Information Science of Fordham Unversity. The dataset contains raw accelerometer and gyroscope data from a smartwatch (LG G Watch) and a smartphone (Google Nexus 5/5x or Samsung Galaxy S5) worn by 51 subjects, who performed 18 different activities for 3 minutes each. The smartphone is placed inside the participant's pocket, while the smartwatch is worn at the dominant hand. The data was collected at a sampling rate of 20Hz for the following activities: Walking, Jogging, Stairs, Sitting, Standing, Typing, Brushing Teeth, Eating Soup, Eating Chips, Eating Pasta, Drinking from Cup, Eating Sandwich, Kicking (Soccer Ball), Playing Catch with Tennis Ball, Dribbling (Basketball), Writing, Clapping, and Folding Clothes. The accelerometer data from the smartwatch was used in this study, and we selected this dataset for evaluation because it has a relatively high number of activities in HAR, which makes it suitable for continual learning evaluation.

The raw sensor data underwent minimal pre-processing. First, z-normalization was applied using the training data's mean and standard deviation for each sensor channel. Next, the data was segmented into \(384 \times 3\) sliding windows, representing 384 timestamps and 3 triaxial accelerometer channels. Consecutive windows do not overlap. 20-25\% of user data was held out unseen as the test set to evaluate model generalisability. 

The 18 classes of activities are randomly and evenly split into 6 tasks of 3 classes each with no overlap as follows: task 1 - \{Folding Clothes, Stairs, Walking\}, 
task 2 - \{Sitting, Drinking from Cup, Eating Chips\}, 
task 3 - \{Standing, Eating Sandwich, Clapping\}, 
task 4 - \{Brushing Teeth, Jogging, Eating Pasta\},
task 5 - \{Eating Soup, writing, Typing\}, and
task 6 - \{Playing Catch with Tennis Ball, Kicking (Soccer Ball), Dribbling (Basketball)\}. This follows the conventional setup of class-incremental learning, and how different (potentially qualitative) splitting of the classes affects model performance is left as future work.

\subsection{Baselines}
\label{subsection:baselines}
We followed the evaluation setup as proposed in \cite{tang2023practical}, in which we compare the performance of Kaizen, CaSSLe, as well as a \emph{No distill} baseline that fine-tunes the full model on new tasks with no explicit catastrophic forgetting mitigation. As CaSSLe and \emph{No distill} baselines are self-supervised learning methods without incorporating a classifier, the classifiers are trained separately after the feature extractor is trained, with data replay enabled.
}
\end{appendix}

\section{Additional Visualisations}
\label{subsection:additional_results}
\begin{figure}
    \centering
    \includegraphics[width=0.8 \linewidth ]{figures_new/Part_2/F3-WISDM2019-MoCoV2+-6Tasks-v2.pdf}
    % \vspace{-0.3in}
    \caption{Performance after training on different tasks with varying constant importance coefficients.}
\label{fig:kaizen_performance_across_time_constant_lamb}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.99 \linewidth ]{figures_new/Part_3/F3-WISDM2019-MoCoV2+-6Tasks-v2.pdf}
    % \vspace{-0.3in}
    \caption{Performance after training on different tasks with varying progressive importance coefficients.}
\label{fig:kaizen_performance_across_time_progressive_lamb}
\end{figure}

Fig.~\ref{fig:kaizen_performance_across_time_constant_lamb} and \ref{fig:kaizen_performance_across_time_progressive_lamb} illustrate the aggregated performance of models trained with Kaizen after each task with different importance coefficients.
