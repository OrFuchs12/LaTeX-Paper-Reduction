\section{Related Work}

\textbf{Question Answering and Large Language Models.} In recent years, advanced question-answering systems have evolved across various scenarios \cite{chen2023cityspecshield, chen2022intelligent, chen2022cityspec, diefenbach2018core}. Black-box abstractive QA systems like mBART and T5 \cite{chipman2022mbart, raffel2019t5} lack output control. Although large language models, especially for QA dialogues \cite{brown2020language, ouyang2022training, claude2_2023}, gain attention, we still argue that prompt-based models are unsuitable for emergency response due to compromised input preservation and advocate for a transparent, controllable offline approach prioritizing reliability and decision transparency.

% In recent years, significant efforts have been made to create advanced question-answering systems across various scenarios \cite{allam2012question, hirschman2001natural, diefenbach2018core}. While abstractive QA systems like fine-tuned mBART and T5 \cite{chipman2022mbart, raffel2019t5} are also widely used, they often lack output control as black-box models. Large language models, especially for QA dialogues, are gaining attention \cite{brown2020language, ouyang2022training, claude2_2023}. However, we contend that prompt-based models are unsuitable for emergency response due to their potential to compromise user utterance preservation, which is considered critical for accuracy and reliability in emergency response. Thus, a more transparent and controllable offline approach, prioritizing original input preservation and model decision elucidation, is necessary.



% Recent years have witnessed significant efforts to develop advanced question-answering systems for diverse scenarios \cite{allam2012question, hirschman2001natural, diefenbach2018core}. Abstractive QA systems, including fine-tuned mBART and T5\cite{chipman2022mbart, raffel2019t5}, are often considered text-to-text black-box models with limited output control. Large language models also gain increasing attention, especially for question-answering dialogues \cite{brown2020language, ouyang2022training, claude2_2023}. However, despite advances in prompt-based models, we argue they prove unsuitable for emergency response systems, as they may compromise preserving usersâ€™ initial utterances, critical for system accuracy and reliability. Thus, a more transparent, controllable approach prioritizing original input preservation and elucidating model decision-making should be adopted.



% In recent years, significant efforts have been dedicated to the development of advanced question-answering systems under various scenarios \cite{allam2012question, hirschman2001natural, diefenbach2018core}. Among them, abstractive question-answering (QA) systems, such as fine-tuned mBART \cite{chipman2022mbart}, are often considered as text2text black-box generation models, with limited control over the output. Large language models (LLMs) are also drawing more and more attention in Natural Language Processing applications, especially when the interactions are carried out in a question-answering dialogue \cite{brown2020language, ouyang2022training, claude2_2023}. Despite recent huge advancements in prompt-based language models, we argue that these methods are unsuitable for the current task of emergency response systems, as they may jeopardize the preservation of the initial user utterance, which is paramount for ensuring the accuracy and reliability of the system. Therefore, a more transparent and controllable approach should be adopted, which prioritizes the preservation of the user's initial input and provides a clearer understanding of the model's decision-making process.

\noindent\textbf{Confidence Scores in Machine Learning}. 
While significant efforts have been dedicated to assessing the model confidence \cite{poggi2017quantitative, hullermeier2021aleatoric, poggi2016learning}, we redefine confidence as internal consistency across identical inputs, deviating from common definitions. For text classification like call dispatching, this consistency is seen in distributional shifts. However, quantifying and analyzing output text changes across domains remains challenging in current information itemization setups. Most open-source QA models provide confidence scores for single runs, like HuggingFace's Bert-QA \cite{huggingface_bert_qa_2022}, which measures confidence in a single trial through simple multiplication of softmax distributions. Hence, a robust confidence measurement mechanism for Auto311 in incident type prediction and information itemization is crucial.

% Substantial excellent work aims to measure model confidence \cite{poggi2017quantitative, hullermeier2021aleatoric, poggi2016learning}. However, we redefine confidence as internal consistency across multiple trials with identical inputs, diverging from common definitions. For text classification like call dispatching, internal consistency manifests in distributional shifts. Moreover, in current information itemization setups, effective means to quantify and analyze output text perturbations across domains remain lacking. Most open-source QA models' built-in confidence scores focus on one single run, like HuggingFace's Bert-QA \cite{huggingface_bert_qa_2022} measure confidence within a single trial by a simple multiplication by the softmax distributions of the start and end index. Thus an effective confidence measurement mechanism for Auto311 in both incident type prediction and information itemization is crucial.

% While considerable work focuses on measuring model confidence \cite{poggi2017quantitative, hullermeier2021aleatoric, poggi2016learning}, we redefine confidence as internal consistency across repeated trials with identical inputs. This approach differs from common definitions and is particularly relevant for tasks like call dispatching in text classification. Existing open-source QA models' confidence scores mainly address single runs, like HuggingFace's Bert-QA \cite{huggingface_bert_qa_2022}. They calculate confidence within one trial using a simple multiplication with the softmax distribution of start and end indices. To enhance Auto311's incident type prediction and information itemization tasks, an effective mechanism for measuring confidence across trials is crucial.



% There has been plenty of excellent work aiming to measure the confidence of a machine learning model \cite{poggi2017quantitative, hullermeier2021aleatoric, poggi2016learning}. We conclude and adjust the definition of confidence score as ``the internal consistency of a model after multiple trails with same inputs'' instead. In text classification applications, similar to call dispatching, internal consistency can be obtained by observing the shifts in output distributions. However, in question-answering scenarios, there lack of common approaches to effectively quantify and analyze the perturbations in outputted text fields among different application domains. Build-in confidence scores in open-source QA models, like Bert-QA from Huggingface \cite{huggingface_bert_qa_2022}, simply measure the confidence by multiplying the two output distributions within a single trial, which conflicts with our definition of confidence score in the first place.

\noindent\textbf{Metrics for Text Comparison}. Many text comparison metrics are unsuitable for Auto311's information itemization. For our goal of concise, detailed outputs that allow deviations from the ground truth, metrics like Damerau-Levenshtein distance \cite{damerau1964technique} and BLEU \cite{papineni2002bleu} fall short. N-gram metrics like ROUGE \cite{lin-2004-rouge} and WER lack semantic understanding. Although end-to-end metrics like embeddings and learned metrics \cite{reimers-2019-sentence-bert, cer2018universal, artetxe-etal-2019-laser} consider semantics, they misalign with our criteria and lack interpretability and generalization in emergency response. Thus, a metric that gauges key information coverage from user utterances while meeting dispatch center requirements becomes essential.

% Numerous text comparison metrics are unsuitable for Auto311's information itemization. Since the goal is concise yet detailed outputs, allowing deviations from the ground truth, edit distance metrics like Damerau-Levenshtein distance \cite{damerau1964technique} and BLEU \cite{papineni2002bleu} are unsuitable. Other n-gram metrics like ROUGE \cite{lin-2004-rouge} and WER lack semantic consideration. While end-to-end metrics like embeddings and learned metrics \cite{reimers-2019-sentence-bert, cer2018universal, artetxe-etal-2019-laser} do consider semantics, they misalign with our criteria and lack interpretability and generalization in the emergency response context. Thus, a metric measuring key information coverage from user utterances while meeting dispatch center requirements is necessary.


% As preserving original user input without modification based on user utterances is paramount, edit distance metrics like Damerau-Levenshtein distance \cite{damerau1964technique} and BLEU \cite{papineni2002bleu} are inappropriate. Likewise, end-to-end metrics including embeddings and learned metrics \cite{reimers-2019-sentence-bert, cer2018universal, artetxe-etal-2019-laser} lack interpretability or generalization. Traditional n-gram metrics like ROUGE \cite{lin-2004-rouge} and WER insufficiently consider semantic importance. Thus, a metric measuring key information coverage from user utterances while meeting dispatch center requirements is necessary.

% \textbf{Text Classification}. The field of Natural Language Processing (NLP) has witnessed remarkable progress, leading to the development of numerous text multi-classification models that have demonstrated astonishing performance on various datasets. However, our specific scenario demands a system that can dynamically adapt to the caller's inputs, which may transition between different incident types or even encompass multiple incident types as the conversation progresses. To address this challenge effectively, we capitalize on the impressive performance of state-of-the-art (SOTA) text classification models and devise a multi-layer text classification structure. By employing this multi-layer approach, we aim to enhance the system's capacity to handle the dynamic nature of the conversation, allowing it to accurately identify and classify incident types as they evolve during the discourse. This model architecture leverages the strength of existing SOTA text classification models, while also incorporating adaptability and flexibility to accommodate the changing context and user input. As a result, our approach endeavors to provide a robust and efficient solution for the unique demands of the emergency response domain.

% With the rapid development of NLP models, there are plenty of text multi-classification models that have obtained astonishing performances on different datasets. However, in our scenario, we require our system to dynamically adjust to the caller's inputs, which might shift between different incident types or include multiple incident types as the conversation goes on. In this task, We leverage the strong performance of current SOTA text classification models and develop a multi-layer text classification structure to better deal with the challenge.