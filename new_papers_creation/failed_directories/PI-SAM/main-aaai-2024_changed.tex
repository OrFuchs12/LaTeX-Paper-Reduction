%File: anonymous-submission-latex-2024.tex
\documentclass[letterpaper]{article} % DO NO CHANGE HIS
\usepackage[submission]{aaai24}  % DO NO CHANGE HIS
\usepackage{times}  % DO NO CHANGE HIS
\usepackage{helvet}  % DO NO CHANGE HIS
\usepackage{courier}  % DO NO CHANGE HIS
\usepackage[hyphens]{url}  % DO NO CHANGE HIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage[algo2e]{algorithm2e}
\LinesNumbered

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\usepackage{amsthm}
\usepackage{paralist}
\usepackage{xspace}
\newcommand{\tuple}[1]{\ensuremath{\left \langle #1 \right \rangle }}
\newcommand{\pre}{\textit{pre}}
\newcommand{\params}{\textit{params}}
\newcommand{\eff}{\textit{eff}}
\newcommand{\name}{\textit{name}}
\newcommand{\type}{\textit{type}}
\newcommand{\cnf}{\textit{CNF}}
\newcommand{\conj}{\textit{Conj}}
\newcommand{\true}{\textit{true}}
\newcommand{\false}{\textit{false}}
\newcommand{\unobserved}{\textit{?}}
\newcommand{\realm}{\ensuremath{M^*}\xspace}
\newcommand{\liftf}{F}
\newcommand{\liftl}{L}
\newcommand{\lifta}{A}
\newcommand{\pisam}{\textit{PI-SAM}\xspace}
\newcommand{\sam}{\textit{SAM}\xspace}
\newcommand{\sgam}{\textit{SGAM}\xspace}
\newcommand{\bindings}{\textit{bindings}}
\newcommand{\iseff}{\textit{IsEff}}
\newcommand{\ispre}{\textit{IsPre}}
\newcommand{\state}{\textit{State}}
\usepackage{xcolor}
\newcommand{\brendan}[1]{{\textcolor{red}{[Brendan: #1]}}}
\newcommand{\hai}[1]{{\textcolor{orange}{[Hai: #1]}}}
% \newcommand{\roni}[1]{{\textcolor{green}{[Roni: #1]}}}
\newcommand{\roni}[1]{ }



\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
\newtheorem{definition}{Definition}
% \newtheorem{assumption}[theorem]{Assumption}

\newtheorem{remark}[theorem]{Remark}
% \theoremstyle{observation}
\newtheorem{observation}[theorem]{Observation}



\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunc, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Learning Safe Action Models with Partial Observability}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,\\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz\equalcontrib,
    Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    1900 Embarcadero Road, Suite 101\\
    Palo Alto, California 94303-3310 USA\\
    % email address must be in roman text type, not monospace or sans serif
    proceedings-questions@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{Learning Safe Action Models with Partial Observability}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{Learning Safe Action Models with Partial Observability}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
A common approach for solving planning problems is to model them in a formal language such as the Planning Domain Definition Language (PDDL), and then use an appropriate PDDL planner.
Several algorithms for learning PDDL models from observations have been proposed but plans created with these learned models may not be sound.
We propose two algorithms for learning PDDL models that are guaranteed to be safe to use even when given observations that include partially observable states.
We analyze these algorithms theoretically, characterizing the sample complexity each algorithm requires to guarantee probabilistic completeness.
We also show experimentally that our algorithms are often better than FAMA, a state-of-the-art PDDL learning algorithm.
\end{abstract}

















\section{Introduction}

% What is domain-independent planning
Classical planning, i.e., planning in a discrete, deterministic, and fully observable environment, is a useful abstraction for solving many planning problems.
%, and has been researched extensively in the Artificial Intelligence (AI) community. Powerful and well-documented classical planners such as Fast Downward~\cite{helmert2006fast} are available. % and have been developed and refined over the years.
% The challenge modeling % Current approaches: FAMA and SAM. One supports partial observability, the other supports safety.
In order to use these planners, however, one must first model the problem at hand in a formal language, such as the Planning Domain Definition Language (PDDL). This is not an easy task. Therefore, several approaches to learning a PDDL model from observations have been proposed~\citep{aineto2019learning,stern2017efficientAndSafe,juba2021safe,cresswell2013acquiring,wu2007arms}. A prominent example is FAMA~\citep{aineto2019learning}, which is a state-of-the-art algorithm for learning a PDDL model from observations. A major advantage of FAMA is that it is able to learn a PDDL model even if the given observations are incomplete, in the sense that only a subset of the actions and state variables are observed.
% Safety is cool
A major disadvantage of FAMA and most PDDL model learning algorithms is that they do not provide any guarantee on the performance of the learned model.
Plans generated with the learned model may not be executable or may fail to achieve their intended goals.
SAM Learning~\citep{stern2017efficientAndSafe,juba2021safe,juba2022learning,mordoch2022collaborative} is a recently introduced family of learning algorithms that provide \emph{safety} guarantees over the learned PDDL model: any plan generated with the model they return is guaranteed to be executable and achieve the intended goals.
%SAM Learning for classical planning domains runs in polynomial time, has a reasonable sample complexity, and, of course, is guaranteed out output a safe action model. However, SAM Learning is limited to learning from fully observed trajectories.
SAM Learning, however, is limited to learning from fully observed trajectories.

% Our contributions
% \roni{This needs to be revised to remove the relation between PI-SAM and bounded concealment}
%In this paper, we present the first work to address safe action model learning from partially observed trajectories.
In this paper, we propose two algorithms for learning safe PDDL models in partially observed domains.
The first algorithm, PI-SAM, extends SAM \citep{juba2021safe}
to support partially observable domains by only applying the SAM learning rules  when a literal is observed in the states immediately before and after an action is applied.
PI-SAM is easy to implement, has a polynomial running time, and outputs a classical planning PDDL model that provides the desired safety guarantee.
The second algorithm, EPI-SAM, utilizes observations that PI-SAM ignores to learn a stronger formulation. EPI-SAM compiles its knowledge and uncertainty about the underlying action model into a \emph{conformant planning problem}, whose solution is also a safe solution to the underlying classical planning problem.
We analyze the running time and prove that the conformant planning problem created by EPI-SAM is the strongest safe problem formulation.


In terms of sample complexity, we show that in general it is not possible to guarantee efficient learning of a safe action model when the observations are partially observable.
Nevertheless, we introduce a form of \textit{bounded concealment assumption}, adapted from prior work on learning from partial observations~\citep{michael2010partialObservability}, under which both PI-SAM and EPI-SAM are guaranteed probabilistic completeness with a tractable sample complexity.
Experimentally, we evaluated the performance of both algorithms and compared them with FAMA~\citep{aineto2019learning} on common domains from the International Planning Competition (IPC) \citep{ipc}.
Our results show that PI-SAM and EPI-SAM often outperform FAMA in terms of the number of samples they require to learn effective action models, while still preserving our safety guarantee.
% (which, as noted above, is not safe but can handle partial observability)

% through experiments



% plan that can be executed safely in the real underlying domain.

% formulation

% is not necessarily a classical planning action model, and in general is an

% can utilize , which can work on domains without the \textit{bounded concealment assumption}. We prove that EPI-SAM returns the strongest safe action model, which then can be mapped to a contingent planning problem. Finally, we empirically demonstrate the performance of each algorithm through experiments on common domains from the International Planning Competition (IPC) \cite{ipc}.


% satisfying a \textit{bounded concealment assumption}, adapted from prior work on learning from partial observations by Michael~\shortcite{michael2010partialObservability}.
% We theoretically show that PI-SAM is safe, has polynomial running time, and guarantees probabilistic completeness with a tractable sample complexity.
% The second algorithm, EPI-SAM, is a more general version, which can work on domains without the \textit{bounded concealment assumption}. We prove that EPI-SAM returns the strongest safe action model, which then can be mapped to a contingent planning problem. Finally, we empirically demonstrate the performance of each algorithm through experiments on common domains from the International Planning Competition (IPC) \cite{ipc}.





% Our contribution: the first work to address safe planning with partially observable trajectories.

% Negative results, and the bounded concealment property. PI-SAM

% EPI-SAM, link to contingent planning

% Experimental results are good


% The rest of the paper is structured as follows. We start with the background (related work) and definition of the problem. We then present our two algorithm for learning safe action models on domains with partial observability. Next, we report experiments that illustrate the performance of these two algorithms. Finally, we give our conclusions and sketch some possible directions for future work. \roni{We don't really need this for a conference paper}

\section{Background and Problem Definition}

% \subsection{Classical Planning}
% Planning domain and an action model
A classical planning \emph{domain} is defined by a tuple
$\tuple{F,A}$ where $F$ is a set of Boolean state variables, also known as fluents,
and $A$ is a set of actions.
A \emph{state} is a complete assignment of values to all fluents, i.e., $s:F\rightarrow\{\true, \false\}$.
A \emph{partial state} is an assignment of values to some (possibly all) of the fluents.
For a fluent $f$ and a partial state $p$, we denote by $p[f]$ the value assigned to $f$ according to $p$.
A partial state $p$ is consistent with a partial state $p'$
if for every fluent $f$ either $p[f]=p'[f]$, $f$ is not assigned in $p$, or $f$ is not assigned in $p'$.
A \emph{literal} in this context is either a fluent $f\in F$ or its negation $\neg f$.
For a literal $\ell=\neg f$, we denote by $p[\ell]=\true$, and $p[\ell]=\false$ the fact that $p[f]=\false$ and $p[f]=\true$, respectively.
We say that a literal $\ell$ is in a partial state $p$, denoted $\ell\in p$, if $p[\ell]=\true$.
Similarly, if $p[\ell]=\false$ we say that $\ell$ is not in $s$, denoted $\ell\notin s$.
% We denote by $p[f]=\unobserved$ that fact that $f$ is not assigned in $p$.
% A partial state $p$ is consistent with a partial state $p'$
% if every fluent $f$ either $p[f]=p'[f]$, $p[f]=\unobserved$, or $p'[f]=\unobserved$.
% A \emph{literal} in this context is either a fluent $f\in F$ or its negation $\neg f$.
% For literal $\ell=\neg f$, we denote by $p[\ell]=\true$, $p[\ell]=\true$, and $p[\ell]=\unobserved$ the fact that $p[f]=\false$, $p[f]=\true$, and $p[f]=\unobserved$.
% We say that a literal $\ell$ is in a partial state $p$, denoted $\ell\in p$, if $p[\ell]=\true$.
% Similarly, if $p[\ell]=\false$ we say that $\ell$ is not in $s$, denoted $\ell\notin s$.
An action $a$ is defined by a tuple $\tuple{\name(a), \pre(a), \eff(a)}$ where
$\name(a)$ is a unique identifier of the action
and $\pre(a)$ and $\eff(a)$ are partial states that specify the preconditions and effects of $a$, respectively.
An \emph{action model} of a planning domain is its set of actions including their names, preconditions, and effects.
% Basic planning axioms
An action $a$ is \emph{applicable} in a state $s$ if $\pre(a)$ is consistent with $s$.
\emph{Applying} $a$ in $s$ results in a state $a(s)$ where
for every fluent $f \in F$:
(1) if $f$ is assigned in $\eff(a)$ then $\eff(a)[f]=a(s)[f]$,
(2) otherwise, $s[f]=a(s)[f]$.
A sequence of actions $\pi=(a_1,\ldots a_n)$ is applicable in a state $s$ if
$a_1$ is applicable in $s$ and for every $i=2,\ldots,n$,
$a_i$ is applicable in $a_{i-1}(\cdots a_1(s)\cdots)$.
The result of applying such a sequence of actions in a state $s$, denoted $\pi(s)$, is the state $a_n(\cdots a_1(s)\cdots)$.

% Planning problem
A classical planning \emph{problem} is defined by a tuple $\tuple{F,A,I,G}$
where $\tuple{F,A}$ is a domain, $I$ is the initial state, and $G$ is a partial state representing  the goal we aim to achieve.
A state $s$ is called a goal state if $G$ is consistent with $s$.
A \emph{solution} to a planning problem is a \emph{plan}, which is a sequence of actions $\pi$ such that $\pi$ is applicable in $I$ and
$\pi(I)$ results in a goal state.
Classical planning domains and problems are often described in a \emph{lifted} manner, where fluents and actions are parameterized over objects.
For ease of presentation, we describe our work in a grounded manner, but our work fully supports a lifted domain representation directly following \citet{juba2021safe}.
%A \textbf{trajectory} is an alternating sequence of states and actions $(s_0,a_1,\ldots,a_n,s_n)$.
A \emph{trajectory} is an alternating sequence of states and actions. % $(s_0,a_1,\ldots,a_n,s_n)$.
For a trajectory $T=(s_0,a_1,\ldots,a_n,s_n)$,
let $T.s_i=s_i$ and $T.a_i=a_i$.
%let $T.s_i=s_i$ for every $i\in\{0,\ldots,n\}$ and $T.a_i=a_i$ for every $i\in\{1,\ldots,n\}$.
The last state and action in $T$ are denoted by $T.s_{-1}$ and $T.a_{-1}$, respectively,
and $T.s$ and $T.a$ denote the sequence of states and actions in $T$, respectively.
An action model $A$ is \emph{consistent} with a trajectory $T$
if according to $A$ the sequence of actions $T.a$ is applicable in $T.s_0$
and $T.s_i=T.a_i(\cdots T.a_1(T.s_0)\cdots)$ for every $i\in\{1,\ldots,|T|\}$.
%for every $i$ $$ \[ \forall i\in\{1,\ldots,|T|\}: T.s_i=T.a_i(\cdots T.a_1(T.s_0)\cdots) \]

% A trajectory $T$ is \emph{consistent} with an action model $A$ if according to $A$ the sequence of actions
% $T.a$ is applicable in $T.s_0$
% and
% \[ \forall i\in\{1,\ldots,|T|\}: T.s_i=T.a_i(\cdots T.a_1(T.s_0)\cdots) \]

% for every $i\in\{1,\ldots,|T|\}$
% $T.s_i=T.a_i(\cdots T.a_1(T.s_0)\cdots)$.
% A \emph{trajectory} is an alternating sequence of states and actions $s_0,a_1,\ldots,a_n,s_n$
% such that $a_i$ is applicable in $s_{i-1}$ and $s_i=a_i(s_{i-1})$, for every $i=1,\ldots,n$.


%\subsection{Contingent Planning and Imperfect Observations}
%\subsection{Planning Under Uncertainty}
%Multiple formalisms have been proposed to describe planning problems in worlds that are not fully observable and deterministic.
%Two particular previously studied types of planning problems that are related to our work are \emph{Contingent
 \emph{Conformant planning}~\citep{bonet2010conformant} and
 \emph{contingent planning}~\citep{majercik2003contingent,hoffmann2005contingent,albore2009translation,brafman2012multi}
  are previously studied types of planning under uncertainty that are directly related to our work.
 % Both types of planning are two generalization of classical planning in which action effects may be conditioned over some unknown state variable.
%There are two main differences between classical planning and contingent and conformant planning.
In both, the effects of some actions may be non-deterministic,
and the initial state $I$ is replaced by a formula $\varphi_I$ over the set of fluents that defines a set of possible initial states.
%. Also, the effects of some actions may be non-deterministic.
% The first difference is that instead of an initial state $I$, planning problems of these types accepts a formula $\varphi_I$ over the set of fluents that defines a set of possible initial states.
% The second difference is that the effects of some actions may be non-deterministic.
% The difference between contingent planning and conformant planning is in the agent's ability or inability to observe parts of the state and react during execution.
In conformant planning, the agent is assumed to be unable to collect observations during execution.
As such, conformant planning algorithms output a \emph{linear plan}, which is a sequence of actions, as in classical planning.
A (strong) solution to a conformant planning problem is a linear plan that is guaranteed to achieve the goal regardless of the inherent uncertainty due to the initial state and non-deterministic effects.
In contingent planning, some actions' effects may include observing the values of some fluents, and the agent is assumed to be able to collect these observations and adapt its behavior accordingly.
%Therefore, Contingent planning algorithms output a \emph{plan tree}, which is an annotated tree where nodes are labelled with actions and edges are labelled with observations. Nodes labelled with actions that do not provide any observation will have a single outgoing edge labelled with a null observation. A (strong) solution to a contingent planning problem is a plan tree that guarantees a goal state is eventually found in every branch.
% Conformant planning~\cite{bonet2010conformant} is similar to contingent planning, but collecting observations during execution is not allowed. Conformant planners therefore output a linear plan, i.e., a sequence of actions, that are guaranteed to achieve the goal despite possible uncertainty over initial state and action effects.


%\subsection{Learning Action Models}
Many algorithms have been proposed for learning action models from a given set of trajectories~\citep{cresswell2013acquiring,yang2007learning,aineto2019learning,juba2021safe}.
Algorithms from the LOCM family~\citep{cresswell2011generalised, cresswell2013acquiring} learn action models by analyzing observed action sequences and constructing finite state machines that capture how actions change the states of objects in the world.
The FAMA algorithm~\citep{aineto2019learning} translates the problem of learning an action model to a planning problem, where every solution to this planning problem is an action model consistent with the available observations. FAMA works even if the observations given to it are partially observable.
Algorithms from the \sam learning family~\citep{stern2017efficientAndSafe,juba2021safe,juba2022learning,mordoch2022collaborative}
are different from other action model learning algorithms in that they guarantee that the action model they return is \emph{safe}, in the sense that plans consistent with it are also consistent with the real, unknown action model.
Most algorithms from this family have a tractable running time and reasonable sample complexity to ensure a probabilistic form of completeness, but rely on perfect observability of the given observations.


% However, it relies on perfect observability of the given set of trajectories. We relax this requirement and propose the first safe action model learning algorithm that is able to learn from partially observed trajectories. \roni{removed this to avoid repetition from the introduction}
% [Roni: removed this to avoid repetition from the introduction]some of the actions, states, or state variables in the given set of trajectories are hidden.
%FAMA~\citep{aineto2019learning}, a state of the art action model learning algorithm, translate the problem of learning an action model to a planning problem. This planning problem searches the space of possible action models that are consistent with the available observations. Importantly, FAMA is specifically designed to be able to handle partial observability of the given set of trajectories. That is, FAMA can work even when some of the actions, states, or state variables in the given set of trajectories are hidden. However, FAMA does not provide any formal guarantee that the learned action model is \emph{safe}, in the sense that using it will produce actionable plans that achieve their intended goals.

% To fill this gap, the \sam learning family of algorithms~\cite{stern2017efficientAndSafe,juba2021safe,juba2022learning,mordoch2022collaborative}
% guarantee that the action model they return is \emph{safe}.
% \sam learning has a tractable running time and reasonable sample complexity to ensure a probabilistic form of completeness.
% However, it relies on perfect observability of the given set of trajectories. We relax this requirement and propose the first safe action model learning algorithm that is able to learn from partially observed trajectories. \roni{removed this to avoid repetition from the introduction}


% \subsection{Imperfect Observation Model}
%\subsection{Problem Definition}

% Observation function
% To capture the type of partially observability we consider in this work formally, we define an observation function $O$ that accepts a state $s$ in a fully observable trajectory and outputs a partial state. [[Roni: I think we need O to accept a trajectory, not a state]]
% The \emph{partial view} of a fully observable trajectory $T=(s_0,a_1,\ldots,a_n,s_n)$
% induced by an observation function $O$, denoted $O(T)$, is the trajectory created by applying $O$ to every state in $T$, i.e., $O(T)=\left(O(T,s_0),a_1,\ldots,a_n,O(T,s_n)\right)$. A partially observed trajectory $T'$ is consistent with a fully observable trajectory $T$ if for every literal $l$ at every state $s$, $s_{T'}(l) = s_{T}(l)$ or $s_{T'}(l)$ is not observed.
% We denote by $s(l)=\unobserved$ the fact that the literal $l$ is not observed at state $s$.

% In general, a \emph{partially observable trajectory} is the same as a regular trajectory except that the states may be partial states.
% is a sequence of partial states and actions
% created by performing



% that may include any number of states and actions, e.g., $s_0, s_1, s_2, a_3, a_4$.
% In this work, we limit our scope to partially observable trajectories that comprise include an alternating sequence

% alternating sequences of

% that can be defined by
% $T=(s_0,a_1,\ldots,a_n,s_n)$

The \emph{partially observed trajectories} we consider are created by \emph{masking} some fluent values in a trajectory, essentially changing some states into partial states.
A literal $\ell$ is said to be \emph{masked} in a partial state $p$, denoted by $p[\ell]=\unobserved$ if the corresponding fluent is not assigned in $p$.
% A trajectory created by assigning values to all masked literals in a partially observable trajectory $T$ is called a \emph{realization} of $T$.
We say that an action model $A$ is consistent with a partially observable trajectory $T$ if
it is consistent with at least one trajectory created by assigning values to all masked literals in $T$.
% A trajectory created by assigning values to all masked literals in a partially observable trajectory $T$ is called a \emph{realization} of $T$.
% We say that an action model $A$ is consistent with a partially observable trajectory $T$ if
% it is consistent with at least one realization of $T$.
% there exists an assignment of values to all masked literals in $T$ that results in a trajectory $T'$ such that $A$ is consistent with $T'$.
% Finally, we can define the type \emph{safe model-free planning} problem we address in this work.
\begin{definition}
A safe model-free planning problem is a tuple $\tuple{\Pi, \mathcal{T}}$
where $\Pi=\tuple{F,A,I,G}$ is a classical planning problem,
and $\mathcal{T}$ is a set of partially observable trajectories created by
executing plans that solve other problems in the same domain, % $\tuple{F,A}$,
% observing the resulting trajectories,
and masking some literals in the states of the resulting trajectories.
% observing the resulting trajectories,
% and masking some of the literals in some of the states in it.
A safe model-free planning algorithm accepts the tuple $\tuple{F,I,G,\mathcal{T}}$ and outputs a plan $\pi$ that is a solution to the underlying planning problem $\Pi$.
\label{def:theProblem}
\end{definition}
The key challenge in solving such problems is that the problem-solver is not given any prior knowledge about the action model or the values of the masked literals.
Nevertheless, the returned plan $\pi$ must be \emph{safe}, in the sense that $\pi$ is a sequence of actions that are applicable in $I$ according to the real action model $A$ and ends up in a goal state.
% A safe model-free planning problem is defined by a tuple $\tuple{\Pi, \mathcal{T}}$
% where $\Pi=\tuple{F,A,I,G}$ is a classical planning problem and $\mathcal{T}$ is a set of partially observable trajectories.
% Every partially observable trajectory in $T\in\mathcal{T}$ have been created by applying an observation function $O$ on a trajectory created by executing a plan that solves some other problem in the same domain $\tuple{F,A}$. The problem-solver is tasked with find a plan that solves $\Pi$, without any prior knowledge on the action model $A$ and the observation function $O$.
% Thus, a solution to our safe model-free planning problem is a plan for $\Pi$.
% Importantly, the returned plan $\pi$ must be \emph{safe}, in the sense that the problem solver must be sure that $\pi$ is a solution to $\Pi$, that is, that $\pi$ is a sequence of actions that are applicable in $I$ according to the real action model $A$ and ends up in a goal state.
% Finally, we can define the type \emph{safe model-free planning} problem we address in this work.
% A safe model-free planning problem is defined by a tuple $\tuple{\Pi, \mathcal{T}}$
% where $\Pi=\tuple{F,A,I,G}$ is a classical planning problem and $\mathcal{T}$ is a set of partially observable trajectories. Every partially observable trajectory in $T\in\mathcal{T}$ have been created by applying an observation function $O$ on a trajectory created by executing a plan that solves some other problem in the same domain $\tuple{F,A}$.
% The problem-solver is tasked with find a plan that solves $\Pi$,
% without any prior knowledge on the action model $A$ and the observation function $O$.
% Thus, a solution to our safe model-free planning problem is a plan for $\Pi$.
% Importantly, the returned plan $\pi$ must be \emph{safe}, in the sense that the problem solver must be sure that $\pi$ is a solution to $\Pi$, that is, that $\pi$ is a sequence of actions that are applicable in $I$ according to the real action model $A$ and ends up in a goal state.
% Finally, we can define the type \emph{safe model-free planning} problem we address in this work.
% The problem-solver is tasked with solving a classical planning problem $\Pi=\tuple{F,A,I,G}$.
% The action model $A$, however, is not given to the problem solver. Instead, it is given a set of partially observable trajectories $\mathcal{T}$ created by applying an observation function $O$ on
% a set of trajectories created by executing plans that solve other problems in the same domain.
% A solution to our safe model-free planning problem is a safe plan for $\Pi$.
% Safety here refers to the requirement that the returned plan must be a plan for the  underlying planning problem $\Pi$, that is, a sequence of actions that are applicable in $I$ according to the real action model $A$ and ends up in a goal state.
% Scope
%In this work,
We make the following simplifying assumptions.
Actions have deterministic effects.
The preconditions and effects of actions are conjunctions of literals, as opposed to more complex logical statements, such as conditional effects.
The form of partial observability defined above embodies the assumption that observations are noiseless: the value of a literal that is not masked is assumed to be correct.
These assumptions are reasonable when planning in digital/virtual environments, such as video games, or environments that have been instrumented with reliable sensors, such as warehouses designed to be navigated by robots~\citep{li2020lifelong}.






% \section{Partial Information Safe Action Model Learning}
\section{Partial Information SAM Learning}
Following prior work~\citep{stern2017efficientAndSafe,juba2021safe},
we first learn an action model from the given trajectories, and then use a planner to solve the given planning problem.
We aim to learn an action model that is \emph{safe}.
\begin{definition}[Safe Action Model]
An action model $\hat{A}$ is safe w.r.t an action model $A$
if (1) for every action $a\in\hat{A}$ and state $s$ if $a$ is applicable in $s$ according to $\hat{A}$ then it is also applicable in $s$ according to $A$,
% (1) for every action $a\in\hat{A}$ and every state $s$ it holds that if $a$ is applicable in $s$ according to $\hat{A}$ then it is also applicable in $s$ according to $A$,
and (2) for every goal $G$, if a plan achieves $G$ according to $\hat{A}$ then it also achieves $G$ according to $A$. Safety of/w.r.t is defined analogously for a fixed problem and its goal $G$.
\label{def:safe-action-model}
\end{definition}

% \begin{definition}[Safe Action Model]
% An action model $\hat{A}$ is safe with respect to an action model $A$
% if for every plan $\pi$ every plan
% for every action $a\in\hat{A}$ and every state $s$ it holds that
% if $a$ is application in $s$ according to $\hat{A}$ then it is also applicable according to $A$ and if for every goal $g$, if the plan achieves $g$ under $\hat{A}$, it also achieves $g$ in $A$.
% \label{def:safe-action-model}
% \end{definition}



% Under the \textit{bounded concealment assumption}, we are able to extend the SAM learning algorithm \cite{juba2021safe} to partially observed domains. The core of our algorithm is based on the following generalization of the basic SAM learning rules~\cite{stern2017efficientAndSafe,juba2021safe}:
% \paragraph{Learning Rules for Partially Observable Domains}

% The first algorithm we propose for learning a safe action model given a partially observable set of trajectories is called Partial Information SAM (PI-SAM).
% PI-SAM is based on the following observation.
%PI-SAM is based on the following generalization of the basic SAM Learning rules~\cite{stern2017efficientAndSafe,juba2021safe}:
%\roni{Maybe we should just say these are the original SAM rules}
The first learning algorithm we propose is called Partial Information SAM (PI-SAM).
PI-SAM is based on the following observation.
\begin{observation}[PI-SAM Rules]\label{obs:pi-sam-learning-rules}
For any action triplet $\tuple{s, a, s'}$ and literal $\ell$% it holds that
\begin{compactitem}
    \item[Rule 1][not a precondition]. If $\left(\ell\in s\right)\wedge
    \left(s[\ell]\neq\unobserved\right)$
    then $\neg \ell$ is not a precondition of $a$.
    \item[Rule 2][an effect]. If
    $\left(\ell\notin s\right)\wedge
     \left(\ell\in s'\right)\wedge
    \left(s[\ell]\neq\unobserved\right)\wedge
     \left(s'[\ell]\neq\unobserved\right)$ then $\ell$ is an effect of $a$.
    \item[Rule 3][not an effect]. If
    $\left(\ell\notin s'\right)\wedge
    \left(s'[\ell]\neq\unobserved\right)$ then $\ell$ is not an effect of $a$.


    % \item Rule 1 [not a precondition]. If we observe an unmasked literal $\neg l \in s$, $l$ is not a precondition of action $a$. %Remove $l$ from $\pre(a)$. [Roni: this is already part of the PI-SAM algorithm, not the rules]

    % \item Rule 1 [not a precondition]. If $\left(s(l)=\true\right)\wedge
    % \left(s(l)\neq\unobserved\right)$
    % then $\neg l$ is not a precondition of $a$.
    % %Remove $l$ from $\pre(a)$. [Roni: this is already part of the PI-SAM algorithm, not the rules]


    % % \item Rule 2 [not an effect]. If we observe a literal $\neg l \in s'$, $l$ is not an effect of action $a$.

    % % \item Rule 2 [not an effect]. If $\left(s'(l)=\false\right)\wedge\left(s'(l)\neq\unobserved\right)$
    % % then $l$ is not an effect of $a$.

    % \item Rule 2 [an effect]. If
    % $\left(s(l)=\false\right)\wedge
    %  \left(s'(l)=\true\right)\wedge
    % \left(s(l)\neq\unobserved\right)\wedge
    %  \left(s'(l)\neq\unobserved\right)$ then $l$ is an effect of $a$.
    %Add $l$ to $\eff(a).$ [Roni: {Sthis is already part of the PI-SAM algorithm, not the rules]
\end{compactitem}
\end{observation}


% \begin{algorithm}[t]
% \small
% \DontPrintSemicolon
% \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
% \Input{Partially Observed Trajectories $\mathcal{T}$}
% \Output{($\pre$, $\eff$) for a safe action model}
% \BlankLine
%     \ForEach{action $a$}{
%         $\eff(a)\gets\emptyset$; $\pre(a)\gets $ all literals \nllabel{line:init_pre_pi} \\
%         \ForEach{transition $\tuple{s, a, s'}$}{
%             \nllabel{pisam:start-loop}
%             \ForEach{literal $\ell$ that is unmasked in $s$ and $s'$}{
%                 \lIf{$\ell\notin s$}{
%                 Remove $\ell$ from $\pre(a)$
%                 }
%                 \lIf{$(\ell\in s')\wedge (\ell\notin s)$}{
%                 Add $\ell$ to $\eff(a)$
%                 }\nllabel{pisam:end-loop}
%             }

%         }
%     }
%     Return $\tuple{\pre, \eff}$

% \caption{Partial Information SAM Learning Algorithm (PI-SAM)}\label{alg:pisam}
% \end{algorithm}


PI-SAM applies rules 1 and 2 in almost the same way as SAM Learning. For every action $a$ observed in some trajectory, we first assume that it has no effects and its preconditions consist of all possible literals.
Then, for every transition $\tuple{s, a, s'}$ and each literal $\ell$ observed in both pre- and post-states, i.e.,  $(s[\ell]\neq ?)\land (s'[\ell]\neq ?)$, we apply Rule 1 to remove preconditions and apply Rule 2 to add effects.
%The complete psuedo-code for PI-SAM is given in Algorithm \ref{alg:pisam}.


PI-SAM runs in $\mathcal{O}\Big(\sum_{a\in \mathcal{A}}|\mathcal{T}(a)|\cdot |\mathcal{F}|\Big)$,
where $\mathcal{T}(a)$ is the set of transitions in $\mathcal{T}$ with action $a$.\footnote{Assuming one can access $\mathcal{T}(a)$ in $O(1)$.}
PI-SAM also returns a safe action model, following the same reasoning given for the fully observable case~\citep{stern2017efficientAndSafe}.
Note that PI-SAM essentially uses the SAM learning rules, except that they are only applied for literals observed in both pre- and post-states.
This may seem unintuitive, since Rule 1 does not require that a literal $l$ is observed in a post-state to infer that it cannot be a precondition.
To see why this modification is needed, consider running PI-SAM on a single trajectory with a single transition $\tuple{s,a,s'}$ where $\ell\notin s$ and $s'[\ell]=\unobserved$.
% An important aspect of PI-SAM is that we only apply
% the rules in Observation~\ref{obs:pi-sam-learning-rules} for literals that are observed in both pre- and post-states.
% This may seem unintuitive, since Rule 1 does not require that a literal $l$ is observed in a post-state to infer that it cannot be a precondition.
% We chose to implement PI-SAM in this way to
% ensure that the action model it returns only allows an action to be applicable in states where its effects are predictable.
% To see this, consider running PI-SAM on a single trajectory with a single transition $\tuple{s,a,s'}$ where $\ell\notin s$ and $s'[\ell]=\unobserved$.
Since the value of $l$ is masked in $s'$, we cannot apply Rule 3, and thus PI-SAM will assume $l$ is not an effect of $a$. However, we cannot know if $l$ is an effect of $a$ or not. Thus, even though we can infer that $l$ is not a precondition of $a$, returning an action model that allows $a$ in such states may yield an unsafe action model.




% \subsection{Theoretical Properties}
%PI-SAM is easy to implement and has several appealing theoretical properties: it returns a safe action model and it runs in time that is polynomial in the number of observed transitions, literals, and actions.

% \paragraph{Safety Property}
% \begin{theorem}\label{safe-pisam-thm}
% PI-SAM returns a safe action model.
% \label{thm:pi-sam-safe}
% \end{theorem}
% % The proof of Theorem~\ref{thm:pi-sam-safe} follows the same lines as the proof in the fully observable case~\cite{stern2017efficientAndSafe}.
% The proof of Theorem~\ref{thm:pi-sam-safe} follows the same lines as the proof  in the fully observable case~\cite{stern2017efficientAndSafe}.
% \begin{proof}
% PI-SAM initializes every action with no effects, and preconditions consisting of all possible literals. During the main loop (lines~\ref{pisam:start-loop}-\ref{pisam:end-loop} in Algorithm \ref{alg:pisam}), PI-SAM removes preconditions and adds effects based on Observation~\ref{obs:pi-sam-learning-rules}. Thus, it
% never removes a precondition that is a precondition in the real action model, and only adds effects that are effects in the real action model. Thus, for every action, the preconditions returned by PI-SAM is a superset of its true preconditions, and the effects returned by PI-SAM is a subset of its true effects.
% Finally, since a literal $\ell$ is removed from the set of preconditions
% Finally, since each effect $\ell$ is added when $\neg\ell$ is observed in the pre-state, causing $\ell$ to be removed from the preconditions, if $\ell$ is not added then $\ell$ must remain a precondition. Thus, PI-SAM returns an action model $M$ such that every action which is applicable to $M$ must also be applicable in the true action model $M^*$, and every effect of $a$ in $M^*$ is true in the post-state of $a$ in $M$ whenever $a$ satisfies $\pre_M(a)$.
% % PI-SAM (Algorithm~\ref{alg:pisam})  initializes every action with no effects, and preconditions consisting of all possible literals. During the main loop (lines 5-9 in Algorithm \ref{alg:pisam}), the algorithm never removes any literal of an action's preconditions that is an actual precondition (because they are observed violated), and only adds literals that are actual effects to its effects (because the literal is observed to switch to true). Thus, for every action, the preconditions returned by PI-SAM is a superset of its true preconditions, and the effects returned by PI-SAM is a subset of its true effects. Finally, since each effect $\ell$ is added when $\neg\ell$ is observed in the pre-state, causing $\ell$ to be removed from the preconditions, if $\ell$ is not added then $\ell$ must remain a precondition. Thus, PI-SAM returns an action model $M$ such that every action which is applicable to $M$ must also be applicable in the true action model $M^*$, and every effect of $a$ in $M^*$ is true in the post-state of $a$ in $M$ whenever $a$ satisfies $\pre_M(a)$.
% \end{proof}
% Let $\mathcal{T}(a)$ denotes the set of transitions in $\mathcal{T}$ with action $a$. If the available set of trajectories $\mathcal{T}$ are arranged such that one can access $\mathcal{T}(a)$ in $O(1)$, then the running time of PI-SAM is $\mathcal{O}\Big(\sum_{a\in \mathcal{A}}|\mathcal{T}(a)|\cdot |\mathcal{F}|\Big)$.% as follows.
% \begin{equation}
%  \mathcal{O}\Big(\sum_{a\in \mathcal{A}}|\mathcal{T}(a)|\cdot |\mathcal{F}|\Big)
% \end{equation}
% $\mathcal{O}\Big(\sum_{a\in \mathcal{A}}|\mathcal{T}(a)|\cdot |\mathcal{F}|\Big)$

% In terms of runtime, PI-SAM iterates for every action $a\in\mathcal{A}$ over all transitions in $\mathcal{T}$ that included it. If the transitions in $\mathcal{T}$ are indexed such that accessing the set of transitions with Let $\mathcal{T}$
% every action $a\in \mathcal{A}$
% \begin{theorem}
% Given a set of trajectories $\mathcal{T}$, PI-SAM runs in time $\mathcal{O}\Big(\sum_{a\in \mathcal{A}}|\mathcal{T}(a)|\cdot |\mathcal{F}|\Big)$.
% \end{theorem}
% \begin{proof}
% For every action $a\in \mathcal{A}$, PI-SAM iterates over all transitions in $\mathcal{T}(a)$. In each iteration, PI-SAM attempts to apply  and over all literals. The number of literals is exactly $2|\mathcal{F}|$, having a literal for every fluent and its negation.
% \end{proof}
% \begin{proof}
% For every action $a\in \mathcal{A}$, PI-SAM iterates over all transitions in $\mathcal{T}(a)$ and over all literals. The number of literals is exactly $2|\mathcal{F}|$, having a literal for every fluent and its negation.
% \end{proof}
% \begin{theorem}
% Given a set of trajectories $\mathcal{T}$, PI-SAM runs in time
% \begin{small}
% \[\mathcal{O}\Big(\sum_{a\in \mathcal{A}}|\mathcal{T}(a)|\sum_{f\in\mathcal{F}}\prod_{t\in T}arity(a,t)^{arity(f,t)}\Big)\]
% \end{small}
% where
% \end{theorem}

% \paragraph{Time Complexity}
% Let $arity(\liftf,t)$ and $arity(\lifta,t)$ be the number of type-$t$ parameters of the lifted fluent $\liftf$ and action $\lifta$, and $\mathcal{T}(a)$ denotes the set of triplets of trajectories in $\mathcal{T}$ with action $a$. %, respectively.
%Let $\eta$ be the the \textit{random masking} probability.

% \begin{theorem}
% Given a set of trajectories $\mathcal{T}$, PI-SAM learning (Algorithm~\ref{alg:pisam}) runs in time
% \begin{small}
% \[\mathcal{O}\Big(\sum_{a\in \mathcal{A}}|\mathcal{T}(a)|\sum_{f\in\mathcal{F}}\prod_{t\in T}arity(a,t)^{arity(f,t)}\Big)\]
% \end{small}
% \end{theorem}









% \subsection{OLD Theoretical Properties}
% PI-SAM has several appealing theoretical properties: it returns a safe action model, its running time is polynomial in the number of observed transitions, literals, and actions, and it only requires a tractable number of samples to guarantee that future solvable problems will be solvable with the learned action model.


% %% NOTE: Safety only holds whp because it depends on observing the results of the effects in addition to the preconditions. We could modify it slightly to obtain unconditional safety, but (a) these weren't the experiments we ran, (b) we are short on time, and (c) we are also short on space.
% \paragraph{Safety Property}
% \begin{theorem}\label{safe-pisam-thm} The PI-SAM Learning Algorithm (Algorithm ~\ref{alg:pisam}) creates a safe action model.
% \end{theorem}
% \begin{proof}
% PI-SAM (Algorithm~\ref{alg:pisam})  initializes every action with no effects, and preconditions consisting of all possible literals. During the main loop (lines 5-9 in Algorithm \ref{alg:pisam}), the algorithm never removes any literal of an action's preconditions that is an actual precondition (because they are observed violated), and only adds literals that are actual effects to its effects (because the literal is observed to switch to true). Thus, for every action, the preconditions returned by PI-SAM is a superset of its true preconditions, and the effects returned by PI-SAM is a subset of its true effects. Finally, since each effect $\ell$ is added when $\neg\ell$ is observed in the pre-state, causing $\ell$ to be removed from the preconditions, if $\ell$ is not added then $\ell$ must remain a precondition. Thus, PI-SAM returns an action model $M$ such that every action which is applicable to $M$ must also be applicable in the true action model $M^*$, and every effect of $a$ in $M^*$ is true in the post-state of $a$ in $M$ whenever $a$ satisfies $\pre_M(a)$.
% \end{proof}

% \paragraph{Time Complexity}
% Let $arity(\liftf,t)$ and $arity(\lifta,t)$ be the number of type-$t$ parameters of the lifted fluent $\liftf$ and action $\lifta$, and $\mathcal{T}(a)$ denotes the set of triplets of trajectories in $\mathcal{T}$ with action $a$. %, respectively.
% %Let $\eta$ be the the \textit{random masking} probability.

% \begin{theorem}
% Given a set of trajectories $\mathcal{T}$, PI-SAM learning (Algorithm~\ref{alg:pisam}) runs in time
% \begin{small}
% \[\mathcal{O}\Big(\sum_{a\in \mathcal{A}}|\mathcal{T}(a)|\sum_{f\in\mathcal{F}}\prod_{t\in T}arity(a,t)^{arity(f,t)}\Big)\]
% \end{small}
% \end{theorem}

% \begin{proof}
% For every action $a\in \mathcal{A}$, PI-SAM iterates over all state-action triplets in $\mathcal{T}(a)$. There are $arity(a,t)^{arity(f,t)}$ ways to bind the parameters.
% %Additionally, for each literal, PI-SAM needs to observe it unmasked in both the pre-state and post-state of a triplet to add it as an effect.
% %Since at each state, each literal has $\eta$ chances of being observed, it adds a factor of $\frac{1}{\eta^2}$ to the running time.
% %% Once the trajectories are fixed there is no probability involved.
% Thus, in total, the running time is $\mathcal{O}\Big(\sum_{a\in \mathcal{A}}|\mathcal{T}(a)|\sum_{f\in\mathcal{F}}\prod_{t\in T}arity(a,t)^{arity(f,t)}\Big)$.
% \end{proof}

\paragraph{Sample Complexity Analysis}
Learning a non-trivial safe action model without any restrictions on how the partially observable trajectories have been generated is impossible.
To see this, consider the case where the value of some fluent $f$ is always masked.
Since we never observe the value of $f$, then for every action $a$ we can never be certain if its preconditions include $f$, $\neg f$, or neither.
Thus, we can never have a safe action model that allows action $a$ to be applied.
This example highlights that some assumption about how the partially observable trajectories
were created is necessary in order to guarantee efficient learning of a safe action model.
We propose such an assumption, based on the definition of a \emph{masking function}.


% ah $O$ must be made in order to allow learning a safe action model.
% observation function $O$, ensuring that a non-trivial action model is safe is impossible.
% To see this, consider a type of observation function that always hides the value of some fluent $f$.
% Since we never observe the value of $f$, then for every action $a$ we can never be certain if its preconditions include $f$, $\neg f$, or neither.
% Thus, we can never have a safe action model that allows action $a$ to be applied.
% This example highlights that some assumption over $O$ must be made in order to allow learning a safe action model.

% A trajectory $T^*$ is called a \emph{realization} of a partially observable trajectory $T$
% if

\begin{definition}[Masking function]
A trajectory {masking function} $O$ maps a trajectory $T$ to a partially observable trajectory $O(T)$ where
(1) $T.a=O(T).a$,
(2) $|T|=|O(T)|$,
and (3) $\forall i: T.s_i$ is consistent with $O(T).s_i$.
% A trajectory {masking function} $O$ is a function that maps a trajectory $T$ to a partially observable trajectory $O(T)$ where
% (1) $T.a=O(T).a$,
% (2) $|T|=|O(T)|$,
% and (3) $\forall i: T.s_i$ is consistent with $O(T).s_i$.
\end{definition}
An example of a masking function is \emph{random masking}, which masks the value of each fluent with some fixed, independent probability.
Without loss of generality. we assume the set of trajectories $\mathcal{T}$ were created by applying some masking function $O$ on fully observable trajectories.
Next, we introduce the following assumption about masking functions, adapted from Michael's theory of learning from partial information~\cite{michael2010partialObservability}:
\begin{definition}[Bounded Concealment Assumption]
A masking function satisfies the {\em $\eta$-bounded concealment assumption} in an environment if for every literal that is not a precondition of an action, when that action is taken and the literal is false, then the corresponding fluent is observed in both the pre- and post-states with probability at least $\eta$.

% The {\em $\eta$-bounded concealment assumption} is the following property of a masking function and environment:
% For every literal that is not a precondition of an action, when that action is taken and the literal is false, then the corresponding fluent is observed in both the pre- and post-states with probability at least $\eta$.
%\roni{Is this sufficient also for learning effects?}
\end{definition}

% % \paragraph{Random Masking}
% \emph{Random masking} is an example of an observation function that satisfies the \textit{bounded concealment assumption}.
% \begin{definition}[Random Masking]
% % A random masking process $O_\eta$ is a function that maps a complete trajectory $T$ to a consistent partially observed trajectory $T' = O_{\eta}(T)$ such that for every state $s$ in the trajectory $T$, each literal $l$ is independently observed with probability $\sqrt{\eta}$.
% Random masking is a parametric observation function $O_\alpha$ that maps a complete trajectory $T$ to a consistent partially observed trajectory such that for every state $s$ in the trajectory $T$, the value of each fluent is independently observed with probability $\alpha$.
% \end{definition}
% \noindent
As an example of a masking function that satisfies a bounded concealment assumption, consider a random masking function, where every literal is masked with a fixed independent probability $\alpha$. Thus, each literal is observed in both the pre- and post-states with probability $\alpha^2$ on each transition, i.e., such cases feature $\alpha^2$-bounded concealment.
%Note that in partially observable trajectories created by a random masking function with a fixed masking probability $1-\alpha$, each literal is observed in both the pre- and post-states with probability $\alpha^2$ on each transition. So, such cases satisfy $\alpha^2$-bounded concealment.
Next, we analyze the relation between the number of trajectories given to PI-SAM and the ability of the action model it returns to solve new problems in the same domain, under the bounded concealment assumption.
Let $\mathcal{P}_D$ be a probability distribution over solvable planning problems in a domain $D$.
%That is, $\mathcal{P}_D$ is a probability distribution over pairs $\tuple{s_I, s_G}$ where $s_I$ is a state, $s_G$ is a goal condition that is achievable from $s_I$.
Let $\mathcal{T}_D$ be a probability distribution over pairs $\tuple{P, T}$
given by drawing a problem $P$ from $\mathcal{P}(D)$,
using a sound and complete planner to generate a plan for $P$,
and setting $T$ to be the trajectory from following this plan.\footnote{%
The planner need not be deterministic.}
% \roni{The theorem and proof below assume the domain is lifted. We intentionally avoided talking about lifted domains in the paper since it adds lots of notation and there isn't anything special in this paper about it. Better to re-write this theorem and proof in the grounded version.@Hai said he'll do it}

\begin{theorem}\label{complexity-pisam-thm}
Under $\eta$-bounded concealment, given
%\begin{equation*}
%    \small
$m \geq \frac{1}{\epsilon \cdot\eta} (2\ln 3 |A|\cdot {|\mathcal{F}|} + \ln \frac{1}{\delta})$
%\end{equation*}
trajectories sampled from $\mathcal{T}_D$,
PI-SAM returns a safe action model $M_\pisam$ such that with probability at least $1-\delta$, a problem drawn from $\mathcal{P}_D$ is not solvable with $M_\pisam$ with probability at most $\epsilon$.
\end{theorem}

\begin{definition}[Adequate]
% An action model $M$ is {\em $\epsilon$-adequate} if, with probability at most $\epsilon$, a trajectory $T$ sampled from $\mathcal{T}_D$ contains an action triplet $\tuple{s,a,s'}$ where
An action model $M$ is {\em $\epsilon$-adequate} if, w.p. at most $\epsilon$, a trajectory $T$ sampled from $\mathcal{T}_D$ contains an action triplet $\tuple{s,a,s'}$ where
\begin{inparaenum}
\item $s$ does not satisfy $\pre_M(a)$ or
\item there is a literal in $s'\setminus s$ but not in $\eff_M(a)$.
\end{inparaenum}
\end{definition}

\begin{lemma}\label{adequate-lem}
The action model returned by PI-SAM Learning
given $m$ trajectories (as specified in Theorem \ref{complexity-pisam-thm})
is $\epsilon$-adequate with probability at least $1-\delta$.
\end{lemma}
\noindent
A proof of Lemma \ref{adequate-lem} appears in the appendix.
We now prove Theorem~\ref{complexity-pisam-thm}.
\noindent
\proof{
When PI-SAM deletes a literal from $\pre(a)$, it observed a triplet $\tuple{s,a,s'}$ where $l$ is false in $s$. %, and hence cannot be a precondition of $a$ in $M^*$.
Thus, whenever action $a$ can be taken in some state under $M_\pisam$, it can also be taken in $M^*$. Conversely, since $M_\pisam$ is $\epsilon$-adequate, with probability at least $1-\epsilon$ the sequence of actions appearing in the trajectory associated with a draw from $\mathcal{T}_D$ is a valid plan in $M_\pisam$. The first condition ensures that the preconditions of $M_\pisam$ allow the action to be executed, and the second condition guarantees that $M_\pisam$ obtains the same states on each transition. Thus, with probability $1-\epsilon$, the goal is achievable under $M_\pisam$ using the plan. $\qed$
}
% Theorem \ref{complexity-pisam-thm}.

% \noindent
% % \emph{Proof of Theorem \ref{complexity-pisam-thm}.}
% % When PI-SAM deletes a literal from $\pre(a)$, it observed a triplet $\tuple{s,a,s'}$ where $l$ is false in $s$, and hence cannot be a precondition of $a$ in $M^*$. Thus, whenever action $a$ can be taken in some state under $M_\pisam$, it can also be taken in $M^*$. Conversely, since $M_\pisam$ is $\epsilon$-adequate, with probability at least $1-\epsilon$ the sequence of actions appearing in the trajectory associated with a draw from $\mathcal{T}_D$ is a valid plan in $M_\pisam$. The first condition ensures that the preconditions of $M_\pisam$ allow the action to be executed, and the second condition guarantees that $M_\pisam$ obtains the same states on each transition. Thus, with probability $1-\epsilon$, the goal is achievable under $M_\pisam$ using the plan. \qed
% \emph{Proof of Theorem \ref{complexity-pisam-thm}.}
% When PI-SAM deletes a literal from $\pre(a)$, it observed a triplet $\tuple{s,a,s'}$ where $l$ is false in $s$. %, and hence cannot be a precondition of $a$ in $M^*$.
% Thus, whenever action $a$ can be taken in some state under $M_\pisam$, it can also be taken in $M^*$. Conversely, since $M_\pisam$ is $\epsilon$-adequate, with probability at least $1-\epsilon$ the sequence of actions appearing in the trajectory associated with a draw from $\mathcal{T}_D$ is a valid plan in $M_\pisam$. The first condition ensures that the preconditions of $M_\pisam$ allow the action to be executed, and the second condition guarantees that $M_\pisam$ obtains the same states on each transition. Thus, with probability $1-\epsilon$, the goal is achievable under $M_\pisam$ using the plan. $\qed$

% }
% A proof of Lemma \ref{adequate-lem} appears in the appendix.
% \noindent
% % \emph{Proof of Theorem \ref{complexity-pisam-thm}.}
% % When PI-SAM deletes a literal from $\pre(a)$, it observed a triplet $\tuple{s,a,s'}$ where $l$ is false in $s$, and hence cannot be a precondition of $a$ in $M^*$. Thus, whenever action $a$ can be taken in some state under $M_\pisam$, it can also be taken in $M^*$. Conversely, since $M_\pisam$ is $\epsilon$-adequate, with probability at least $1-\epsilon$ the sequence of actions appearing in the trajectory associated with a draw from $\mathcal{T}_D$ is a valid plan in $M_\pisam$. The first condition ensures that the preconditions of $M_\pisam$ allow the action to be executed, and the second condition guarantees that $M_\pisam$ obtains the same states on each transition. Thus, with probability $1-\epsilon$, the goal is achievable under $M_\pisam$ using the plan. \qed
% \emph{Proof of Theorem \ref{complexity-pisam-thm}.}
% When PI-SAM deletes a literal from $\pre(a)$, it observed a triplet $\tuple{s,a,s'}$ where $l$ is false in $s$. %, and hence cannot be a precondition of $a$ in $M^*$.
% Thus, whenever action $a$ can be taken in some state under $M_\pisam$, it can also be taken in $M^*$. Conversely, since $M_\pisam$ is $\epsilon$-adequate, with probability at least $1-\epsilon$ the sequence of actions appearing in the trajectory associated with a draw from $\mathcal{T}_D$ is a valid plan in $M_\pisam$. The first condition ensures that the preconditions of $M_\pisam$ allow the action to be executed, and the second condition guarantees that $M_\pisam$ obtains the same states on each transition. Thus, with probability $1-\epsilon$, the goal is achievable under $M_\pisam$ using the plan. $\qed$




%\section{Extended Partial Observability SAM (EPI-SAM) Learning Algorithm}
\section{Extended PI-SAM (EPI-SAM)}


% \begin{observation}\label{obs:pi-sam-learning-rules}
% For any action triplet $\tuple{s, a, s'}$% it holds that
% \begin{itemize}
%     \item Rule 1 [not a precondition]. If we observe an unmasked literal $\neg l \in s$, $l$ is not a precondition of action $a$. %Remove $l$ from $\pre(a)$. [Roni: this is already part of the PI-SAM algorithm, not the rules]

%     \item Rule 2 [not an effect]. If we observe an umasked literal $\neg l \in s$, $l$ is not an effect of action $a$.

%     \item Rule 3 [an effect]. If we observe an unmasked literal $l \in s'/s$, $l$ is an effect of action $a$.
%     %Add $l$ to $\eff(a).$ [Roni: {Sthis is already part of the PI-SAM algorithm, not the rules]
% \end{itemize}

The PI-SAM algorithm is easy to implement and outputs an action model that can be used by any planner designed to solve classical planning problems.
Yet, it only uses transitions where there are literals that are observed in both pre- and post-states.
%It is possible, in some cases, to use other transitions to learn a stronger safe action model.
For example, consider an action $a$, a literal $\ell$, and three transitions $\tuple{s_1,a,s_1'}$, $\tuple{s_2,a,s_2'}$, and $\tuple{s_3,a,s_3'}$ where $\ell$ is not observed in any state except
$s_1$, $s_2'$, and $s_3'$ in which its values are $\false$, $\false$, and $\true$, respectively.
Since $\ell$ was observed to be false in $s_1$, we can deduce it is not a precondition of $a$ (Rule 1 in Observation~\ref{obs:pi-sam-learning-rules}).
Since $\ell$ is never observed in both pre- and post-states of the same transition, the PI-SAM algorithm still does not remove $\ell$ from $\pre(a)$.
However, considering the value of $\ell$ in $s'_2$ and $s'_3$,
we can deduce that neither $\ell$ nor $\neg\ell$ are effects of $a$ (Rule 2 and 3 in Observation~\ref{obs:pi-sam-learning-rules}).
Thus, it is possible to apply $a$ in states without $\ell$ and maintain our safety property.
Next, we propose the Extended PI-SAM (EPI-SAM) learning algorithm, which is able to make such inferences.
%and is guaranteed to learn the strongest safe action model possible given the available trajectories. \roni{revisit this claim. Since we defined a safe action model above differently, it is hard to say this nicely}


EPI-SAM relies on several key observations.
The first observation is that learning of the effects of actions and learning their preconditions can be done separately, because we can never be certain that a literal is a precondition of an action.
The second observation is that limiting the output to a classical planning action model limits the scope of safe model-free planning problems we can solve.
For example, if we observe a trajectory $(s_0,a_1,s_1,a_2,s_2)$, where $s_0[\ell]=\false$, $s_2[\ell]=\true$, and
 $\ell$ is masked in $s_1$, we cannot discern which action --- $a_1$ or $a_2$ --- achieved $\ell$, but we can learn that at least one of them has done so.
 While classical planning action models cannot capture this knowledge directly, such uncertainty can be compiled into a non-classical planning problem.
% by learning a non-classical action model, namely, a Conformant Planning action model, even though the underlying planning problem is a classical planning problem.
% For example, if we observe a trajectory $(s_0,a_1,s_1,a_2,s_2)$, where $s_0[\ell]=\false$, $s_2[\ell]=\false$, and
%  $\ell$ is masked in all other states. While we cannot discern which action --- $a_1$ or $a_2$ --- achieved $\ell$, we can learn that at least one of them has done so. While no classical planning action model can capture this knowledge directly,


% a potentially stronger algorithm by
% learning a non-classical action model, namely a Conformant Planning action model, and using an appropriate planner.

% find safe plans for potentially more problems by

% a potentially stronger algorithm by
% learning a non-classical action model, namely a Conformant Planning action model, and using an appropriate planner.



Based on these observations,  EPI-SAM has the following parts: learning effects, learning preconditions, and compilation to non-classical planning.
In the first part (learning effects), EPI-SAM creates a Conjunctive Normal Form (CNF) formula for each literal $\ell$, denoted by $\cnf_{\eff}(\ell)$, which describes conditions for
sequences of actions that achieve $\ell$ in the problems returned by EPI-SAM.
The literals of this CNF are of the form $\iseff(\ell,a)$, representing whether literal $\ell$ is an effect of action $a$.
In the second part (learning preconditions), EPI-SAM creates a set of literals $\pre(a)$ for each action $a$ that describes the preconditions of $a$ in the returned problems.
In the third part (compilation to non-classical planning), EPI-SAM creates a conformant planning problem using the output of the previous two parts. This conformant planning problem is constructed so that any (strong) solution to this problem is a safe solution to the actual planning problem.
We describe these in detail next.

% A key observation in EPI-SAM is that learning of the effects of actions and learning their preconditions can be done separately, because we can never be certain that a literal is a precondition of an action.
% Thus, the EPI-SAM algorithm has two separate parts: learning effects and learning preconditions.
% In the first part (learning effects), EPI-SAM creates a Conjunctive Normal Form (CNF) formula for each literal $l$, denoted $CNF_{\eff}(l)$, which describes conditions over
% which sequences of actions may and may not achieve $l$ in the action model returned by EPI-SAM. The literals of this CNF are of the form $\iseff(l,a)$, representing whether literal $l$ is an effect of action $a$.
% In the second part (learning preconditions), EPI-SAM creates a set of literals $\pre(a)$ for each action $a$ that describes the preconditions of $a$ in the returned action model.
% Next, we describe both EPI-SAM parts in details.







% FOR ALGORITHM2E
\begin{algorithm}[t]
\small
\DontPrintSemicolon
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetKwBlock{Main}{Main}{end}
\Input{Partially observed trajectories $\mathcal{T}$}
\Output{$CNF_{\eff}(\ell)$ for each literal $l$}
    \ForEach{literal $\ell$}{
        $\cnf_{\eff}(\ell) \gets \emptyset$\\
        \lForEach{action $a$}{
            Add to $\cnf_{\eff}(\ell)$: $\left\{\neg \iseff(\ell, a) \vee \neg \iseff(\neg \ell, a)\right\}$ \nllabel{epi:not-mutex}
        }
        % \ForEach{trajectory $T\in\mathcal{T}$ and index $i$ where $l\in T.s_i$}{
        \ForEach{trajectory $T\in\mathcal{T}$}{
            \ForEach{index $i\in\{1,\ldots,|T|\}$ where $\ell\in T.s_i$}{
            % \ForEach{$\tuple{s,a,s'}\in T\in\mathcal{T}$ where $l\in s'$}{
            $T'\gets$ max. prefix of $T.s_i$ where $\ell$ is masked \nllabel{epi:max-prefix}\\
            % EPI-SAM Rule 1: if l changed from false to true, it must be an effect of one of the actions
            \lIf{$\ell\notin T'.s_0$}{
                Add to $\cnf_{\eff}(\ell)$:  $\left\{\iseff(\ell, T'.a_1)  \vee   \cdots \vee \iseff(\ell, T'.a_{|T'|})\right\}$ \nllabel{epi:refute1}
            }
            % EPI-SAM Rule 2: \neg l cannot be an effect
            Add to $\cnf_{\eff}(\ell)$:  $\{\neg \iseff(\neg\ell, T'.a_{|T'|})\}$ \nllabel{epi:add-not-an-effect}\\
            % EPI-SAM Rule 3: Ensure if l is deleted it will be added afterwards
            \ForEach{$j=1$ to $|T'|-1$}{
                Add to $\cnf_{\eff}(\ell)$:
                $\{\neg\iseff(\neg\ell, T'.a_j) \vee \iseff(\ell, T'.a_{j+1}) \vee
                \cdots \vee \iseff(\ell, T'.a_{|T'|})\}$  \nllabel{epi:refute2}
            }
        }
        }
    }
    \Return $\{\cnf_\eff(\ell)\}_\ell$
\caption{EPI-SAM: Learning Effects}
\label{alg:episam-effects}
\end{algorithm}


\paragraph{Learning Effects}
To learn effects, EPI-SAM extends PI-SAM rules 2 and 3 (Observation~\ref{obs:pi-sam-learning-rules}) from rules over transitions to rules over \emph{sub-trajectories}.
A trajectory $T'$ is a \emph{sub-trajectory} of trajectory $T$, denoted $T'\subseteq T$, if it is a consecutive subsequence of $T$, i.e., there exists $i$ and $j$ where $i<j$ such that  $T'.s_0=T.s_i$
and for every $k\in \{1,\ldots,|T'|\}$ we have $T'.s_k=T.s_{i+k}$ and $T'.a_k=T.a_{i+k}$.

\begin{observation}[EPI-SAM Rules]
For any sub-trajectory $T'$ of a trajectory in $\mathcal{T}$ that ends in a state where literal $l$ is not masked, i.e., where $T'.s_{-1}[l]\neq\unobserved$, then
% $\left(T'[-1].s(l)\neq\unobserved\right)\wedge \left(T'[-1].s(l)=\true\right)$.
    \begin{compactitem}
        \item[Rule 1][an effect].
        If $l\in T'.s_{-1}$ and $l\notin T'.s_0$
        then $\exists a\in T'.a$ that has $l$ as an effect.
        \item[Rule 2][not an effect].
        If $l\in T.s_{-1}$ then $\neg l$ is not an effect of $T'.a_{-1}$
        \item[Rule 3][not deleted].%[must not delete].
        If $l\in T'.s_{-1}$ and $\neg l$ is an effect of an action $T'.a_i$ then $\exists i'>i$ that has $l$ as an effect.
    \end{compactitem}
\label{obs:epi-sam-learning-rules}
\end{observation}
Algorithm~\ref{alg:episam-effects} lists the pseudo-code for effects learning in EPI-SAM, which builds on the EPI-SAM rules in  Observation~\ref{obs:epi-sam-learning-rules}.
Initially, $\cnf_\eff(\ell)$ contains a single clause for every action $a$ that ensures the effects of $a$ are mutually exclusive (line~\ref{epi:not-mutex}).
Then, we implement the EPI-SAM rules by going over every trajectory $T$ and every state $T.s_i$ in which $\ell$ is not masked.
For each such pair of trajectory and state, we extract the longest sub-trajectory $T'\subseteq T$ that ends in $T.s_i$ and where $\ell$ is masked in all other states in $T'$ (line~\ref{epi:max-prefix}).
If a literal $\ell$ was false at the first state of $T'$, then we add to $\cnf_\eff(\ell)$ a clause to ensure that $\ell$ is an effect of some action $a_i$ (EPI-SAM Rule 1).
Then, we add a clause to ensure that $\neg\ell$ is not an effect of the last action in $T'$ (EPI-SAM Rule 2).
Finally, we add a clause to ensure that if $\neg \ell$ was an effect of any action $a\in T'.a$
then some action in $T'$ after that action must have had $\ell$ as an effect (EPI-SAM Rule 3).


% unmasked and
% which the fluent in literal $l$ is not observed and $l$ is true at the end of the sequence, there are two kinds of clauses we can write about $l$. First, if literal $l$ was $false$ at the beginning state $s_0$, $l$ must be an effect of some action $a_i$ (line 4-5). Second, if $\neg l$ was an effect of any action $a_j$ (which guarantee $l$ is $\false$ in state $s_j$, then some action $a_k$ for $k > j$ must have had $l$ as an effect (line 6-7). Finally, for each action $a$, we also add to the CNF a mutual exclusion clause for literal $l$ (line 8-9).
% \roni{I think we're missing in the explanation and the pseudo-code the PI-SAM rule that identifies literals that are not an effect.
% That is, we should start the creation of $CNF_\eff(l)$ by identifying the set of actions that we know do not have $l$ as an effect (PI-SAM rule 2).
% Then remove these actions from the trajectories considered when creating $CNF_\eff(s)$. This relates to the new preconditions algorithms, which I think can be viewed as re-running EPI-SAM effect learning on a modified set of trajectories that assumed $l$ is a precondition of $a$.}














% representing the

% all possible literals are preconditions. For each sequence of states in the trajectory that end at $a$, there are two ways that literal $l$ can be known to be true. Either $l$ was true at the beginning state $s_0$ and was not an effect of any other action in the sequence (which would switch it off), or  $l$ was an effect at some action $a_i$ and wasn't switched off by a later action (line 15- 23). The algorithm only removes $\neg l$ from the precondition $pre(a)$ if any of the above conditions are true. Since it is a Disjunctive Normal Form (DNF) formula, we first negate it then combine with the CNF effect clauses created from part 1 and use refutation to check if the combined formula is \textit{satisfiable}. If it is \textit{satisfiable}, we leave $\neg l$ as a precondition. Otherwise, we remove $\neg l$ from the precondition $\pre(a)$ (line 24-25).


% The literals of this

% which contains all possible effect clauses (i.e., atoms of the form $\iseff(l,a) $ that specify whether literal $l$ is an effect of action $a$) that we can write about literal $l$.
% Unlike PI-SAM, the action model returned by EPI-SAM is no longer a classical planning action model.
% Instead, the effects in the action model returned by EPI-SAM are specified by a set of CNF formulas.
% Since we currently do not support conditional effects, EPI-SAM separates learning effects and preconditions.





% A trajectory $T'$ is a \emph{sub-trajectory} of trajectory $T$, denoted $T\subseteq T'$, if it is a consecutive subsequence of $T$, i.e., there exists $i$ and $j$ where $i<j$ such that  $T'[0].s=T[i].s$
% and for every $k\in \{1,\ldots,|T'|\}$ we have $T'[k].s=T[i+k].s$ and $T'[k].a=T[i+k].a$.

% A trajectory $T'$ is a \emph{sub-trajectory} of trajectory $T$, denoted $T\subseteq T'$, if it is a consecutive subsequence of $T$, i.e., there exists $i$ and $j$ where $i<j$ such that  $T'[0].s=T[i].s$
% % and for every $k\in \{1,\ldots,|T'|\}$ we have $T'[k].s=T[i+k].s$ and $T'[k].a=T[i+k].a$.


% To describe EPI-SAM, we introduce the following notations.
% For a trajectory $T$ , we denote by $T[i].a$ and $T[i].s$ the $i^{th}$ action and state, respectively.
% The sequences of actions and states in $T$ are denoted $T.a$ and $T.s$, respectively, and the last state and action in $T$ are denoted $T[-1].s$ and $T[-1].a$, respectively.
% A trajectory $T'$ is a \emph{sub-trajectory} of trajectory $T$, denoted $T\subseteq T'$, if it is a consecutive subsequence of $T$, i.e., there exists $i$ and $j$ where $i<j$ such that  $T'[0].s=T[i].s$
% and for every $k\in \{1,\ldots,|T'|\}$ we have $T'[k].s=T[i+k].s$ and $T'[k].a=T[i+k].a$.






\begin{algorithm}[t]
\small
\DontPrintSemicolon
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetKwBlock{Irrelevant}{Irrelevant}{end}
\SetKwBlock{AssumePrecondition}{AssumePrecondition}{end}
\SetKwBlock{LearnPreconditions}{LearnPreconditions}{end}
\SetKwBlock{Main}{Main}{end}
\Input{Partially observed trajectories $\mathcal{T}$}
\Output{Precondition $\pre(a)$ for each action $a$}
% \Irrelevant{
%     % \Input{action $a$, literal $l$, set $\cnf_\eff(l)$}
%     \Input{action $a$, literal $\ell$, set of trajectories $\mathcal{T}$}
%     \Output{$\true$ ~iff $a$ does not affect the value of $l$}
%     \ForEach{$\tuple{s_1,a,s_1'}, \tuple{s_2,a,s_2'}$ in $T\in\mathcal{T}$}{
%         \If{$s'_1[\ell]\neq\unobserved \wedge s'_2[\ell]\neq\unobserved\wedge s_1'[\ell]\neq s_2'[\ell]$}{
%         % \If{$l$ is unmasked in $s'_1$ and $s'_2$ $\wedge$ $\left(s_1'[l]\neq s_2'[l]\right)$}{
%             \Return $\true$
%         }
%     }
%     \Return $\false$
% }
% \AssumePrecondition{
%     \Input{action $a$, literal $\ell$, set of trajectories $\mathcal{T}$}
%     \Output{a copy of $\mathcal{T}$ where $\ell$ is always true before $a$}
%     $\mathcal{T}_{a,\ell}\gets\emptyset$\\
%     \ForEach{trajectory $T\in\mathcal{T}$}{
%         Initialize $T_{a,\ell}\gets$ a copy of $T$\\
%         \ForEach{$\tuple{s,a,s'}\in T_{a,\ell}$}{
%             $s[\ell]\gets\true$
%         }
%         Add $T_{a,\ell}$ to $\mathcal{T}_{a,\ell}$
%     }
%     \Return $\mathcal{T}_{l,a}$
% }
% \LearnPreconditions{
    \lForEach{action $a$}{$\pre(a)\gets$ all literals}
    \ForEach{action $a$, literal $\ell$}{
        \uIf{$\exists \tuple{s,a,s'}\in T\in\mathcal{T}$ where $\neg \ell\in s$ \nllabel{line:pi-rule-start}}{
            % SAM Rule 1: if \neg l is in s then it cannot be a precondition of a
            Remove $\ell$ from $\pre(a)$ \nllabel{episam-easy-pre-delete}\\
            Continue to the next $(a, \ell)$ pair \nllabel{line:pi-rule-end}
        }
        % Assume l is a precondition of a.
        % Create a copy of all trajectories under this assumption
        % $\mathcal{T}_{l,a}\gets\emptyset$\\
        % \ForEach{trajectory $\mathcal{T}_i\in\mathcal{T}$}{
        %     Initialize $T_{l,a}\gets \emptyset$\\
        %     \ForEach{$\tuple{s,a,s'}\in \mathcal{T}_i\in\mathcal{T}$}{
        %         Set $s(l)$ to true in $T_{l,a}$
        %     }
        %     Add $T_{l,a}$ to $\mathcal{T}_{l,a}$
        % }
        $\mathcal{T}_{a,\ell}\gets$ \textbf{AssumePrecondition}($a$, $\ell$, $\mathcal{T}$) \nllabel{epi:assume}; $A_{irr}\gets\emptyset$ \nllabel{line:epi-propagate-start} \\
        % Remove actions that are known to not have any effect on the value of l
        % $A_{irr}\gets\emptyset$ \nllabel{line:epi-propagate-start}\\
        \While{$\exists a'\notin A_{irr}$ where \textbf{Irrelevant}($a'$,$\ell$,$\mathcal{T}_{a,\ell}$)}{
            \ForEach{$\tuple{s,a',s'}$ in $T\in\mathcal{T}_{a,\ell}$}{
                \eIf{$s[\ell]$ and $s'[\ell]$ are inconsistent \nllabel{epi:inconsistent}}{
                            Remove $\ell$ from $\pre(a)$ \nllabel{episam-hard-pre-delete}\\
                            Continue to the next $(a,\ell)$ pair\nllabel{line:propagate-end}
                }{
                    \lIf{$s[\ell]=\unobserved$}{
                        $s[\ell]\gets s'[\ell]$ \nllabel{epi:propagate}
                    }
                    Remove $\tuple{s,a',s'}$ from $T$ \nllabel{epi:remove}
                }
            }
        }
    }
    \Return $\{\pre(a)\}_a$
% }
\caption{EPI-SAM: Learning Preconditions}\label{alg:episam-preconditions}
\end{algorithm}


\paragraph{Learning Preconditions}
%Like PI-SAM,
EPI-SAM starts by assuming for every action $a$ that it has all literals as preconditions.
Then, it removes a literal $l$ from the set of preconditions of an action $a$ if and only if assuming $l$ is a precondition of $\pre(a)$ is inconsistent with $\mathcal{T}$. %the given set of partially observable trajectories $\mathcal{T}$.
There are two possible ways in which the assumption that $l$ is a precondition of $a$ can be inconsistent with the observations:
(1) there is a transition $\tuple{s,a,s'}$ in $\mathcal{T}$ where $s[l]=\false$,
and (2) no set of action effects is consistent with $\mathcal{T}$ when we additionally set $s[l]=\true$ for every transition $\tuple{s,a,s'}$ in $\mathcal{T}$.
The former corresponds to PI-SAM Rule 1, which can be easily verified in linear time.
The latter can be checked by setting $s[l]=\true$ in the relevant transitions,
running EPI-SAM's effect-learning part (Algorithm~\ref{alg:episam-effects}) on the resulting set of trajectories, and checking if the resulting CNF is satisfiable.
This check can be done by calling any SAT solver.
Fortunately, it is also possible to perform this satisfiability check in polynomial time. % tractable time without calling a SAT solver.
This is because assumptions about which action achieves literal $l$ are independent of any assumption about which actions achieve any other literal except $\neg l$.\footnote{This independence fails when conditional effects are allowed.}


Algorithm~\ref{alg:episam-preconditions} lists the pseudo-code s precondition learning part.
Like PI-SAM, EPI-SAM initially assumes that the preconditions of every action include all literals.
Then, EPI-SAM iterates over every pair of action $a$ and literal $\ell$ to check if $\ell$ can be removed from the set of preconditions assumed for $a$.
The first way EPI-SAM attempts to remove $\ell$ from $\pre(a)$ is by checking if it violates PI-SAM Rule 1 (lines~\ref{line:pi-rule-start}-\ref{line:pi-rule-end}).
The second way is by using a proof-by-contradiction approach, checking if assuming $\ell$ is a precondition of $a$ leads to a contradiction with the observations and every possible assumption about actions' effects.
EPI-SAM performs this check by performing the following steps.
First, it creates a copy of the set of trajectories $\mathcal{T}$ where
$\ell$ is set to be true in every state where $a$ is applied (the \textbf{AssumePrecondition} call in line~\ref{epi:assume}).
This set of modified trajectories is denoted by $\mathcal{T}_{a,\ell}$ in Algorithm~\ref{alg:episam-preconditions}.
Then, EPI-SAM iteratively searches for actions that are \emph{irrelevant} for the value of $\ell$.
An action $a$ is said to be irrelevant for the value of $\ell$ if we can infer that neither $\ell$ nor $\neg \ell$ are effects of $a$. We do this by invoking PI-SAM Rule 2 for both $\ell$ and $\neg\ell$.
That is, action $a'$ is identified as irrelevant to $\ell$ if there are two transitions $\tuple{s_1, a', s'_1}$ and $\tuple{s_2, a', s'_2}$ where $\ell$ is not masked in their post-states and it has different values, i.e.,
$(s'_1[\ell]\neq \unobserved) \wedge (s'_2[\ell]\neq\unobserved) \wedge (s'1[\ell]\neq s'_2[\ell])$.
A contradiction is identified if there exists a transition $\tuple{s,a',s'}$ where $a'$ is an irrelevant action but the value of $\ell$ in $s$ and in $s'$ is inconsistent, i.e., unmasked and different (line~\ref{epi:inconsistent}).
If $a'$ is irrelevant but the values of $s$ and $s'$ are consistent, then we propagate the value of $s'$ to $s$ and remove the transition $\tuple{s,a',s'}$ from $\mathcal{T}_{a,\ell}$ (lines~\ref{epi:propagate}-\ref{epi:remove}).
\footnote{If $\exists\tuple{s',a'',s''}\in T$, then removing $\tuple{s,a',s'}$ implicitly adds the transition $\tuple{s,a'',s''}$.}
% \footnote{If $s'$ isn't the last state, i.e.,  $\exists\tuple{s',a'',s''}\in T$, then removing $\tuple{s,a',s'}$ implicitly adds the transition $\tuple{s,a'',s''}$.}
% \footnote{If $s'$ isn't the last state, i.e., there exists  $\tuple{s',a'',s''}\in T$, then removing $\tuple{s,a',s'}$ implicitly adds the transition $\tuple{s,a'',s''}$.}


% It relies on two auxiliary functions: \textbf{AssumePrecondition}$(a,l, \mathcal{T})$ and \textbf{Irrelevant}$(a,l, \mathcal{T})$.
% AssumePrecondition$(a,l,\mathcal{T})$ returns a set of trajectories that are equivalent to $\mathcal{T}$ except that it sets $l$ to $\true$ in every state immediately before $a$ is applied.
% Irrelevant$(a,l, \mathcal{T})$ returns true if based on the trajectories in $\mathcal{T}$ we can infer that neither $l$ nor $\neg l$ are effects of $l$.
% Concretely, Irrelevant$(a,l, \mathcal{T})$ returns true if $\mathcal{T}$ includes two transitions $\tuple{s_1, a, s'_1}$ and $\tuple{s_2, a, s'_2}$ where $l$ is not masked in their post-states and it has different values, i.e.,
% \[ s'_1[l]\neq \unobserved \wedge s'_2[l]\neq\unobserved \wedge s'1[l]\neq s'_2[l] \]
% This is equivalent to applying PI-SAM Rule 2 for $l$ and $\neg l$.  % Maybe this is already obvious

% Like PI-SAM, EPI-SAM initially assumes that the preconditions of every action includes all literals.
% Then, EPI-SAM iterates over every pair of action $a$ and literal $\ell$ to check if $\ell$ can be removed from the set of preconditions assumed for $a$.
% Lines~\ref{line:pi-rule-start}-\ref{line:pi-rule-end} remove preconditions using PI-SAM Rule 1 (not a precondition).
% Lines~\ref{linex}-\ref{liney} remove preconditions that are inconsistent with every possible assumption about actions' effects by iteratively identifying actions that are irrelevant

% performing the following iterative process.

% We do this
% To do this, EPI-SAM iterates over every sub-trajectory that end at $a$, there are two ways that literal $l$ can be known to be true. Either $l$ was true at the beginning state $s_0$ and was not an effect of any other action in the sequence (which would switch it off), or  $l$ was an effect at some action $a_i$ and wasn't switched off by a later action (line 15- 23). The algorithm only removes $\neg l$ from the precondition $pre(a)$ if any of the above conditions are true. Since it \roni{who is ``it'' here} is a Disjunctive Normal Form (DNF) formula, we first negate it then combine with the CNF effect clauses created from part 1 and use refutation to check if the combined formula is \textit{satisfiable}. If it is \textit{satisfiable}, we leave $\neg l$ as a precondition. Otherwise, we remove $\neg l$ from the precondition $\pre(a)$ (line 24-25).



% in $\mathcal{T}$


% modifying the trajectories in $\mathcal{T}$ as sp

% setting $$

% running EPI-SAM's effect learning part over the set of trajectories


% leads to a contradiction with
% other knowledge inferred about

% every action model in which $l$ is a precondition of $a$ is inconsistent with the available set of trajectories $\mathcal{T}$.

% every action model that satisfies $\cnf_l$ is in


% $\{\cnf_\eff(\ell)\}_\ell$


% effects

% the literal $\iseff(l,a)\$

% re

% is a transition $\tuple{s,a,'s}$ where $s[l]=\unobserved$ and the literal

% $l$ is observed in a
% This is a generalization over the PI-SAM rules:
% The assumption that $l$
% In the EPI-SAM
% As in PI-SAM,
% To ensure that the learned action model is safe, EPI-SAM initially for every action $a$ that all literals are




% Regarding preconditions, we generalize over the PI-SAM rules as follows: if the assumption that a literal $l$ exists before any application of an action $a$ is inconsistent with the above rules, then $l$ cannot be precondition.







% PI-SAM algorithm (Algorithm \ref{alg:pisam}) can only work on domains that satisfy the \textit{bounded concealment assumption}.
% When the assumption does not hold, PI-SAM can't guarantee that it can learn any effect after a bounded number of state-action triplets since PI-SAM needs to see a literal unmasked in both the pre-state and post-state in a triplet in order to add it as an effect.
% To address this issue, we propose the Extended PI-SAM (EPI-SAM) learning algorithm, which describes the safe action model in form of a set of Conjunctive Normal Form (CNF) formulas $CNF_\eff(l)$ for each literal $l$,
% and a conjunction $Pre(a)$ for each action $a$.
% \roni{not important now, but I think we should replace the above with an example showing that PI-SAM is not utilizing all the information that it can use, and not go for the bounded concealment argument}

% \subsection{EPI-SAM Learning: Effects - Preconditions}
% The EPI-SAM learning algorithm, presented in Algorithm \ref{alg:episam}, can be split in two separate parts: learning the effects and learning the preconditions.
% The first part (lines 1-9 in Algorithm \ref{alg:episam}) creates a Conjunctive Normal Form (CNF) formula for each literal $l$, denoted $CNF_{\eff}(l)$, which contains all possible effect clauses (i.e., atoms of the form $\iseff(l,a) $ that specify whether literal $l$ is an effect of action $a$) that we can write about literal $l$.
% $CNF_{\eff}(l)$ is first initialized as empty. For each sequence of states in the trajectory during which the fluent in literal $l$ is not observed and $l$ is true at the end of the sequence, there are two kinds of clauses we can write about $l$. First, if literal $l$ was $false$ at the beginning state $s_0$, $l$ must be an effect of some action $a_i$ (line 4-5). Second, if $\neg l$ was an effect of any action $a_j$ (which guarantee $l$ is $\false$ in state $s_j$, then some action $a_k$ for $k > j$ must have had $l$ as an effect (line 6-7). Finally, for each action $a$, we also add to the CNF a mutual exclusion clause for literal $l$ (line 8-9).
% \roni{I think we're missing in the explanation and the pseudo-code the PI-SAM rule that identifies literals that are not an effect.
% That is, we should start the creation of $CNF_\eff(l)$ by identifying the set of actions that we know do not have $l$ as an effect (PI-SAM rule 2).
% Then remove these actions from the trajectories considered when creating $CNF_\eff(s)$. This relates to the new preconditions algorithms, which I think can be viewed as re-running EPI-SAM effect learning on a modified set of trajectories that assumed $l$ is a precondition of $a$.}


% The second part (lines 10-24 in Algorithm \ref{alg:episam}) learns the precondition $pre(a)$ (as a conjunction) for each action $a$. Initially, $pre(a)$ represents that all possible literals are preconditions. For each sequence of states in the trajectory that end at $a$, there are two ways that literal $l$ can be known to be true. Either $l$ was true at the beginning state $s_0$ and was not an effect of any other action in the sequence (which would switch it off), or  $l$ was an effect at some action $a_i$ and wasn't switched off by a later action (line 15- 23). The algorithm only removes $\neg l$ from the precondition $pre(a)$ if any of the above conditions are true. Since it is a Disjunctive Normal Form (DNF) formula, we first negate it then combine with the CNF effect clauses created from part 1 and use refutation to check if the combined formula is \textit{satisfiable}. If it is \textit{satisfiable}, we leave $\neg l$ as a precondition. Otherwise, we remove $\neg l$ from the precondition $\pre(a)$ (line 24-25).


\paragraph{Compilation to Non-Classical Planning}

% The output of the first two parts is the pair $\tuple{\{\cnf_\eff(\ell)\}_\ell,\{\pre_a\}_a}$, which we will refer to as the EPI-SAM action model.
% Classical planners are not designed to accept such an action model.
% Instead, we propose to solve our planning problem with the action model returned by EPI-SAM by mapping it to a Conformant Planning problem, and using an appropriate planner to solve it.


Next, EPI-SAM creates a \emph{conformant planning} problem $\Pi_\sam$ based on the outputs of the previous EPI-SAM parts, $\{\cnf_\eff(\ell)\}_\ell$ and $\{\pre_a\}_a$, and the available knowledge of the underlying planning problem $\Pi$.
A conformant planning problem is defined by a tuple $\tuple{F,O,A,I,G}$
where $F$, $A$, $I$, and $G$ are the set of fluents, actions, initial state, and goals, as in a classical planning problem,
except that
$A$ may include non-deterministic and conditional effects,
and $I$ is a set of possible initial states defined by a formula over $F$.
$O$ is the subset of fluents in $F$ that are observable.
% like a classical planning problem except that
% (1) only a subset of the fluents is observable,
% (2) instead of a single initial state we have a set of initial states, defined by a formula over the set of fluents, and (3) actions may have non-deterministic effects. The Conformant Planning problem we construct, denoted require only the first two differences. %, having only actions with deterministic effects.
% Let $\Pi$ be the original planning problem we aim to solve, and let $\Pi_\sam$ be the Conformant Planning problem we are constructing. % based on the output previous parts.
The set of fluents in $\Pi_\sam$ includes all fluents in $\Pi$ and an additional fluent $f_{\iseff(a,\ell)}$ for every action $a$ and literal $\ell$.
All fluents from $\Pi$ are observable in $\Pi_\sam$ and all others are not.
The initial state formula in $\Pi_\sam$ sets the values of all observable fluents according to their initial values in $\Pi$. In addition, it includes all the clauses in the CNFs returned by EPI-SAM ($\{\cnf_\eff(\ell)\}_\ell$), replacing every literal $\iseff(a,\ell)$ with the corresponding fluent $f_{\iseff(a,\ell)}$.
The action model of $\Pi_\sam$ includes all actions observed in $\mathcal{T}$.
For each action $a$, we set its preconditions to the set of preconditions learned for it by EPI-SAM's learning preconditions part, $\pre(a)$.
All the effects of $a$ are \emph{conditional effects}.
A conditional effect of an action is an effect (i.e., a partial state) that is only applied if a specified condition holds.
For each action $a$ and literal $\ell$,
we add a conditional effect such that if
$f_{\iseff(a,\ell)}$ is true then $\ell$ is an effect of $a$.
Note that conditional effects are supported by many classical and conformant planners~\citep{bonet2010conformant,grastien2017intelligent}.
If the agent executing the plan can observe the values of fluents during execution and react, then the above compilation can be used almost as-is to construct a contingent planning problem instead of a conformant planning problem. The output of a contingent planning algorithm is a plan tree, branching over the observed values during execution, which can be more efficient than the linear plan returned for the respective conformant planning problem. \roni{Maybe the above paragraph should go later, e.g., to the discussion section at this end.}




\paragraph{Theoretical Properties}

%Here we analyze the safety and runtime  showing that it is both safe and tractable. We also show that it is the strongest algorithm for solving safe model-free planning problems, in the sense that any algorithm able to solve a problem that cannot be solved by EPI-SAM cannot also be safe.

Next, we analyze EPI-SAM theoretically, showing that it is safe, runs in polynomial time, and it is the strongest algorithm for solving safe model-free planning problems, in the sense that any algorithm able to solve a problem that cannot be solved by EPI-SAM cannot also be safe.
Throughout this analysis, we denote by $A^*$ the action model of the underlying problem, and denote by $\pre_A(a)$ and $\eff_A(a)$ the set of preconditions and effects, respectively, of an action $a$ according to an action model $A$.
Observe that every classical action model $A$ corresponds to an assignment $\sigma_A$ to the formula $\Phi_\eff=\bigwedge_\ell \cnf_\eff(\ell)$, by setting $\iseff(\ell,a)$ to true if $\ell$ is an effect of $a$ for each literal $\ell$ and action $a$.
Similarly, every satisfying assignment of $\Phi_\eff$ describes the effects of a classical action model.
Proofs of the lemmas and theorems given below are in the appendix.
\begin{lemma}\label{lem:cnf-char}
If a classical action model $A$ is consistent with $\mathcal{T}$
then $\sigma_A$ is a satisfying assignment of $\Phi_\eff$.
Conversely, every satisfying assignment $\sigma$ to $\Phi_\eff$ describes the effects of at least one classical action model that is consistent with $\mathcal{T}$.
\end{lemma}
% \noindent
% {\em Sketch of proof.}
% % Consider the logical formula created by the STRIPS axioms, instantiated at each step of each trajectory in $\mathcal{T}$.
% % This formula is defined over variables of the form $\iseff(l,a)$, $\ispre(l,a)$, and $\state(l,i,T)$, representing that
% % $l$ is a precondition of $a$,
% % $l$ is an effect of $a$,
% % and $l=\true$ in the $i^{th}$ state of trajectory $T$, respectively.
% % This formula includes the following clauses for every transition $\tuple{s_{i-1},a_i,s_i}$ in every trajectory $T\in\mathcal{T}$:
% % \begin{enumerate}
% % \item $\ispre(l,a_i)\rightarrow \state(l,i-1,T)$
% % \item $\iseff(l,a_i)\rightarrow \state(l,i,T)$
% % \item $\neg\iseff(l,a_i)\rightarrow (\state(l,i-1,T)\rightarrow(\state(l,i,T)))$
% % \end{enumerate}
% % with $\state(l,i,T)$ replaced by true or false when $l$ is observed true or false, respectively, at step $i$ in $T$.
% % The clausal encoding of the above formula to a CNF, denoted $\cnf_{\mathcal{T}}$,
% % is as follows:
% % \begin{itemize}
% % \item (C1) $\neg \ispre(l,a_i)\vee \state(l,i-1,T)$
% % \item (C2) $\neg \iseff(l,a_i)\vee \state(l,i,T)$
% % \item (C3) $\iseff(l,a_i)\vee \neg \state(l,i-1,T) \vee \state(l,i,T)$
% % \end{itemize}
% Consider the clausal encoding of the STRIPS axioms, instantiated at each step of each trajectory in $\mathcal{T}$.
% This CNF, denoted $\cnf_\mathcal{T}$ is defined over variables of the form $\iseff(l,a)$, $\ispre(l,a)$, and $\state(l,i,T)$, representing that
% $l$ is a precondition of $a$,
% $l$ is an effect of $a$,
% and $l=\true$ in the $i^{th}$ state of trajectory $T$, respectively.
% This CNF includes the following clauses for every transition $\tuple{s_{i-1},a_i,s_i}$ in every trajectory $T\in\mathcal{T}$:
% \begin{compactitem}
% \item (C1) $\neg \ispre(l,a_i)\vee \state(l,i-1,T)$
% \item (C2) $\neg \iseff(l,a_i)\vee \state(l,i,T)$
% \item (C3) $\iseff(l,a_i)\vee \neg \state(l,i-1,T) \vee \state(l,i,T)$
% \end{compactitem}
% By construction, a satisfying assignment to $\cnf_{\mathcal{T}}$ corresponds to the effects of an action model and the complete trajectories for this action model, given the values observed in the trajectories of $\mathcal{T}$. Moreover, the action model with these effects and no preconditions is consistent with $\mathcal{T}$.

% Let $\cnf_\mathcal{T}(\ell)$ be the formula containing all the clauses in $\cnf_\mathcal{T}$ containing literals for a single fluent literal $\ell$.
% Note that the clauses of $\cnf_\mathcal{T}$ only contain literals for a single fluent literal, so
% $\cnf_\mathcal{T}$ is satisfiable iff for every $\ell$ the formula $\cnf_{\mathcal{T}}(\ell)$ is satisfiable.
% The final part of our proof will show that the CNF returned by EPI-SAM, $\cnf_\eff(\ell)$, is satisfiable iff $\cnf_\mathcal{T}(\ell)$ is satisfiable.
% To this end, we rely on the refutation-completeness of resolution and examine which clauses may appear in a refutation of $\cnf_{\mathcal{T}}(\ell)$.
% The $\ispre(a,\ell)$ literals, appearing only negatively, cannot appear in a refutation.
% Thus, any refutation will be based on clauses of types C2 and C3.
% Two types of proofs can be created from such clauses.
% The first requires observing the value of $\ell$ in enough states such that we have contradicting unit clauses with $\iseff$ literals for some action $a_i$. That is, we have transitions
% $\tuple{s_i,a_i,s'_i}$ and $\tuple{s_j,a_i,s'_j}$
% where $l$ is observable in states
% $s'_i$, $s_{j-1}$, and $s_j$
% with values $\false$, $\true$, and $\false$, respectively.
% This option is implemented in line~\ref{episam-easy-pre-delete} of Algorithm~\ref{alg:episam-preconditions}.
% The second type of proof requires using resolution to eliminate at least one $\state$ literal. Reordering the applications of the resolution rule on these literals to the beginning of the proof, we see that we must create clauses that correspond to consecutive runs of unobserved literals using the resolution rule on clauses of type C3 for each step, beginning with either an observed literal or with using clauses of type C2 to eliminate the first $\state(\ell, i, T)$ literal. These are, respectively, the clauses of $\cnf_{\eff}(l)$ created on lines~\ref{epi:refute1} and~\ref{epi:refute2} in Algorithm~\ref{alg:episam-effects}.




% \subsection{Roni Version}
% Thus, any refutation must eliminate at least one $\iseff(l,a_i)$ literal.
% This requires applying resolution between a C2 clause and a C3 clause in a pair of transitions $\tuple{s_{i-1},a_i,s_i}\in T$ and $\tuple{s_{j-1},a_j, s_j}\in T'$
% where $a_i=a_j$, resulting in the clause:
% \[
% \state(l,i,T)\vee
% \neg\state(l,j-1,T')\vee\state(l,j,T')
% \]
% There are two ways to proof a contradiction with this clause.
% The first is by observing that the value of $l$ in the states $s_i$, $s_{j-1}$, and $s_j$
% is $\false$, $\true$, and $\false$, respectively.
% This option is implemented in line~\ref{episam-easy-pre-delete} of Algorithm~\ref{alg:episam-preconditions}.
% The second option corresponds to where  $l$ is masked in at least one of these states, and requires a proof that eliminates at least one $\state$ literal.
% Every $\state(l,i,T)$ literal appears in exactly two clauses: a C2 and a C3 clause that correspond to consecutive states where the $\ell$ is masked. Therefore, any proof must create clauses that correspond to consecutive runs of state where $\ell$ is masked using the resolution rule on C3 clauses for each step, beginning with either an observed literal or by using a C2 clause to eliminate the first $\state(\ell,i,T)$ literal.
% These are, respectively, the clauses of $CNF_{\eff}(l)$ created on lines~\ref{epi:refute1} and~\ref{epi:refute2} in Algorithm~\ref{alg:episam-effects}.
% \roni{How is it now?}
% \roni{I am not super happy with this proof. Don't we need a claim about the preconditions?} \brendan{OK, properly the statement should only concern the effects part of the model. But remember that we can always find a consistent set of preconditions by just saying there are no preconditions, so the question about preconditions is not important for the existence of a consistent action model.}\roni{Ok, I think I edited this accordingly. Did I get it right and readable? @Brendan@Hai}



% \subsubsection{Original version of the proof}
% We then use the refutation-completeness of resolution to reduce the problem to identifying the clauses that may appear in resolution refutations. The $\ispre(a,l)$ literals, appearing only negatively, cannot appear in a refutation, and the literals $State(l,i,T)$ must be eliminated, \roni{not necessarily: there are clauses of type C2 and C3 where there is no $\state(l,i,T)$ literals, when the value of $l$ is observed in state $i$ at trajectory $T$} we can reach a contradiction if we observe the values of $\ell$ in states but these only appear in consecutive instances of the second and third type of clauses where the literal $l$ is unobserved; and only the third clause has the negative literal. Reordering the applications of the resolution rule on these literals to the beginning of the proof, \roni{Not obviousy to me why we can do this reordering} we see that we must create clauses that correspond to consecutive runs of unobserved literals using the resolution rule on the third type of clause for each step, beginning with either an observed literal or with using the second type of clause to eliminate the first $State(l,i,T)$ literal. These are, respectively, the clauses of $CNF_{\eff}(l)$ created on lines~\ref{epi:refute1} and~\ref{epi:refute2} in Algorithm~\ref{alg:episam-effects}.


% Any refutation must eliminate $\iseff(l,a_i)$ literals.
% This requires applying resolution between a C2 clause and a C3 clause in a pair of transitions $\tuple{s_{i-1},a_i,s_i}$ and $\tuple{s_{j-1},a_j, s_j}$
% where $a_i=a_j$.
% If $\ell$ is unmasked in all these states, then the corresponding clause exists in the CNF returned by EPI-SAM.
% Otherwise, i.e., if $\ell$ is masked in at least one of these states, we must eliminate a $\state(\ell,i,T)$ literal.
% Such a literal only exists in instances of clauses C2 and C3 that correspond to consecutive states where the $\ell$ is masked.
% Therefore, any proof must create clauses that correspond to consecutive runs of state where $\ell$ is masked using the resolution rule on C3 clauses for each step, beginning with either an observed literal or by using a C2 clause to eliminate the first $\state(\ell,i,T)$ literal.
% These are, respectively, the clauses of $CNF_{\eff}(l)$ created on lines~\ref{epi:refute1} and~\ref{epi:refute2} in Algorithm~\ref{alg:episam-effects}
% \roni{I am not super happy with this proof. Don't we need a claim about the preconditions?} \brendan{OK, properly the statement should only concern the effects part of the model. But remember that we can always find a consistent set of preconditions by just saying there are no preconditions, so the question about preconditions is not important for the existence of a consistent action model.}\roni{Ok, I think I edited this accordingly. Did I get it right and readable? @Brendan@Hai}


\begin{lemma}\label{lem:pre-strong}
For every action $a$ in $A_\sam$ and literal $\ell$, it holds that
$\ell\in\pre_{A_\sam}(a)$ if and only if there exists an action model $A$ consistent with $\mathcal{T}$ where $\ell\in\pre_A(a)$.
\end{lemma}
% {\em Sketch of proof.}
% EPI-SAM removes a literal $\ell$ from $\pre(a)$ only if there exists a transition $\tuple{s,a,s'}$ in $\mathcal{T}$ where $\ell$ is false, and hence cannot be in $\pre_{A*}(a)$.
% In addition, we can show that if a literal has been removed from $\pre(a)$ by EPI-SAM then there exists an action model $A$ consistent with $\mathcal{T}$ where $\ell\in\pre_A(a)$. See a complete proof in the supplementary material.


% \begin{proof}
% We first prove that if EPI-SAM removes a literal $\ell$ from $\pre(a)$, then there exists a transition $\tuple{s,a,s'}$ in $\mathcal{T}$ where $\ell$ is false, and hence cannot be in $\pre_{A*}(a)$.
% EPI-SAM removes $\ell$ from $\pre(a)$ in two places in Algorithm~\ref{alg:episam-preconditions}: line~\ref{episam-easy-pre-delete} and line~\ref{episam-hard-pre-delete}.
% The correctness of line~\ref{episam-easy-pre-delete} is immediate: if $\ell$ is observed to be false in a state where $a$ has been applied then it cannot be a precondition of $a$ (PI-SAM Rule 1).
% Before removing a precondition due to line~\ref{episam-hard-pre-delete},
% EPI-SAM creates a set of trajectories $\mathcal{T}_{\ell,a}$ that assumes $\ell$ was true whenever $a$ was taken,
% and detects the set of actions $A_{irr}$ that cannot affect the value of $\ell$ in any action model consistent with $\mathcal{T}_{\ell,a}$.
% Because of the frame axioms, the value of $\ell$ gets propagated in any transition that includes an action in $A_{irr}$. $\ell$ is only removed in line \ref{episam-hard-pre-delete} if this propagation results in a state where $\ell$ has contradicting values. As this occurs for any action model consistent with $\mathcal{T}_{\ell,a}$, this implies that $\ell$ cannot be true in every state where $a$ was applied, and thus cannot be a precondition of $a$ in any action model consistent with $\mathcal{T}$.



% Next, we prove that if $\ell$ has not been deleted from $\pre(a)$ by EPI-SAM, then there exists an action model $A$ consistent with $\mathcal{T}$ where $\ell\in\pre_A(a)$.
% % Since $\ell$ has not been deleted from $\pre_{A_\sam}$, then
% % for any action $a\notin A_{irr}$ we have that the value of $\ell$ after $a$ is either masked or takes at most one value (true of false).
% Consider the subset of $A_{irr}$ that includes only actions that have been in a transition where the value of $\ell$ is not masked.
% For each action $a'$ in this set, we are guaranteed that this value of $\ell$ is always the same, denoted $v(a',\ell)$. Otherwise $a'$ would have been added to $A_{irr}$.
% The action model created by assigning $v(a',\ell)$ as an effect of $a'$ for each of these actions is consistent with $\mathcal{T}_{l,a}$.
% Therefore, there exists an action model where $\ell$ is a precondition of $a$ that is consistent with $\mathcal{T}$.
% \end{proof}

% \begin{proof}
% We first prove that if EPI-SAM removes a literal $\ell$ from $\pre(a)$, then there exists a transition $\tuple{s,a,s'}$ in $\mathcal{T}$ where $\ell$ is false, and hence cannot be in $\pre_{A*}(a)$.
% EPI-SAM removes $\ell$ from $\pre(a)$ in two places in Algorithm~\ref{alg:episam-preconditions}: line~\ref{episam-easy-pre-delete} and line~\ref{episam-hard-pre-delete}.
% The correctness of line~\ref{episam-easy-pre-delete} is immediate: if $\ell$ is observed to be false in a state where $a$ has been applied then it cannot be a precondition of $a$ (PI-SAM Rule 1).
% Before removing a precondition due to line~\ref{episam-hard-pre-delete},
% EPI-SAM creates a set of trajectories $\mathcal{T}_{\ell,a}$ that assumes $\ell$ was true whenever $a$ was taken,
% and detects the set of actions $A_{irr}$ that cannot affect the value of $\ell$ in any action model consistent with $\mathcal{T}_{\ell,a}$.
% Because of the frame axioms, the value of $\ell$ gets propagated in any transition that includes an action in $A_{irr}$. $\ell$ is only removed in line \ref{episam-hard-pre-delete} if this propagation results in a state where $\ell$ has contradicting values. As this occurs for any action model consistent with $\mathcal{T}_{\ell,a}$, this implies that $\ell$ cannot be true in every state where $a$ was applied, and thus cannot be a precondition of $a$ in any action model consistent with $\mathcal{T}$.



% Next, we prove that if $\ell$ has not been deleted from $\pre(a)$ by EPI-SAM, then there exists an action model $A$ consistent with $\mathcal{T}$ where $\ell\in\pre_A(a)$.
% % Since $\ell$ has not been deleted from $\pre_{A_\sam}$, then
% % for any action $a\notin A_{irr}$ we have that the value of $\ell$ after $a$ is either masked or takes at most one value (true of false).
% Consider the subset of $A_{irr}$ that includes only actions that have been in a transition where the value of $\ell$ is not masked.
% For each action $a'$ in this set, we are guaranteed that this value of $\ell$ is always the same, denoted $v(a',\ell)$. Otherwise $a'$ would have been added to $A_{irr}$.
% The action model created by assigning $v(a',\ell)$ as an effect of $a'$ for each of these actions is consistent with $\mathcal{T}_{l,a}$.
% Therefore, there exists an action model where $\ell$ is a precondition of $a$ that is consistent with $\mathcal{T}$.
% \end{proof}
% The algorithm continues to the next iteration of the loop and does not consider this literal again, thus it remains in $\pre(a)$ upon termination.
% any action at the end of a run of states where the fluent is unobserved must be followed by states where the fluent takes at most one value. Then we could assign the effect to that action that sets the fluent to that value, and thus obtain a consistent action model with $\mathcal{T}_{l,a}$. Thus, in this case, $l$ could be a precondition of $a$. The algorithm continues to the next iteration of the loop and does not consider this literal again, thus it remains in $\pre(a)$ upon termination.

% Finally, we note that the contingent plan must achieve the goal with any setting of the $\iseff$ variables that is consistent with $CNF_{\eff}$. By Lemma~\ref{lem:cnf-char}, we see that this means that in particular the goal is achieved with the assignment corresponding to the real action model. Thus, the EPI-SAM action model is indeed safe.
%%
%% OLD SAT-BASED ANALYSIS
%%
%%Consider the formula $CNF_{\mathcal{T}}(l)$ created in Lemma~\ref{lem:cnf-char}: $\ispre(l,a)$ can be set to true in a satisfying assignment iff for every step $t$ where $a=a_t$, we have $State(l,t-1,h)$ true and $State(\neg l,t-1,h)$ false (equiv., $\neg State(\neg l,t-1,h)$ true).  (Equivalently, these are the clauses that can be derived using the unit clause $\ispre(l,a)$.) Thus, by the refutation completeness of resolution, we can refute this collection of unit clauses with $CNF_{\mathcal{T}}(l)$ iff $l$ cannot be a precondition of $a$.
%%
%%Again, the $State$ variables must be eliminated, and the (other) $\ispre$ variables cannot be used in a refutation because they only appear negatively. Again, recall that the $State$ variables only appear negatively in clauses of the third type in $CNF_{\mathcal{T}}$. Thus, in addition to the clauses constructed in $CNF_{\eff}(l)$, these new literals (only) allow us to obtain clauses for each run of steps in the trajectory where $l$ is not observed, either starting with a clause of the third type where $l$ is observed true (created in lines 16--17), or using a clause of the second type to eliminate the initial state literal (in line 22). By Lemma~\ref{lem:cnf-char}, this resulting formula can be refuted with $CNF_{\eff}(l)$ iff $l$ cannot be a precondition of $a$.
%%
%%Because we only remove literals from the preconditions when this formula is refuted, in any plan constructed with the EPI-SAM action model, whenever an action is taken it must satisfy the preconditions of the real action. Finally, we note that the contingent plan must achieve the goal with any setting of the $\iseff$ variables that is consistent with $CNF_{\eff}$. Again, by Lemma~\ref{lem:cnf-char}, we see that this means that in particular the goal is achieved with the assignment corresponding to the real action model. Thus, the EPI-SAM action model is indeed safe.
% \end{proof}
% (line~\ref{} in Algorithm~\ref{X}), or when assuming $\ell$ is precondition

% in a transition
% This immediate that this holds for any $\ell$ deleted in line \ref{episam-easy-pre-delete}.

% % some trajectory in $\mathcal{T}$ must have had $\ell$ false in the pre-state of $a$, and hence $l$ could not have been in $\pre_{M^*}(a)$ for the true action model; it therefore will follow that whenever a state $s$ satisfies $\pre(a)$, it satisfies $\pre_{A^*}(a)$ and is therefore applicable.

% % It is immediate that this holds for any transition where
% % $\ell$ observed to be false just before $a$ (line \ref{epi:line:easy-remove} in Algorithm~\ref{alg:episam-preconditions}).
% Otherwise, Algorithm \ref{alg:episam} creates a set of trajectories $\mathcal{T}_{\ell,a}$ in which we assume that $\ell$ was true whenever $a$ was taken;

%\roni{I don't fully follow this condition. Do you mean that $l$ can be deleted iff there is no action model consistent with $\mathcal{T}_{l,a}$?} \brendan{Roughly so -- except that only the portions of the action model that have effects on l's fluent are relevant/considered, and the question is then whether or not there is an action model (restricted to this fluent) for which l could be the precondition. We consider each literal (hence, fluent) separately.}

%\emph{Sketch of proof (incomplete)}
%In the first part (line 1-9 in Algorithm \ref{alg:episam}), EPI-SAM creates for every literal a CNF that captures all possible clauses that we can write about literal $l$. If literal $l$ was an effect at some action in the trajectories, it must be represented by a clause in the CNF. In the second part (line 10-25 in Algorithm \ref{alg:episam}), the algorithm initializes that for every action, its preconditions contain all possible literals. For each action, the algorithm removes from its preconditions only literal such that there is no possible assignment that satisfies the CNF that the algorithm can infer about that literal. Thus, it won't remove any literal that is an actual precondition of an action.
% \begin{definition}[Strength of Action Models]
% If there exists a trajectory that is consistent with $M'$ but not with $M$, then we say that $M$ is weaker than $M'$.
% If no such trajectory exists then we say that $M$ is at least as strong as $M'$.
% \label{def:weakness}
% \end{definition}

% \begin{theorem}[The Strength of SGAM Learning]
% Let $M_{SGAM}$ be the action model created by SGAM learning given the set of trajectories $\mathcal{T}$.
% $M_{SGAM}$ is at least as strong as any action model $M'$ that is safe and consistent with $\mathcal{T}$.
% \label{thm:sam-learning-complete-grounded}
% \end{theorem}
% \begin{proof}
% Consider an action model $M'$, which is safe and consistent with $\mathcal{T}$. % and safe w.r.t.\ \realm.
% Let $a$ be an action and $s$ be a state such that $a$ is applicable in $s$ according to $M'$, i.e., $\pre_{M'}(a)\subseteq s$.

% Since $M'$ is safe w.r.t.\ \realm, then
% $\pre_{\realm}(a)\subseteq s$

% and $a_{M'}(s)=a_{\realm}(s)$.
% By construction of $M_\sgam$, if a literal $l$ is a precondition of $a$ according to $M_\sgam$,
% then it has appeared in the pre-state of all action triplets in $\mathcal{T}(a)$.
% Thus, there exists a consistent action model in which $l$ is a precondition of $a$
% and this action model may be the real model.
% Therefore, since $M'$ is safe it follows that $\pre_{M_\sgam}(a) \subseteq \pre_{M'}(a)$,
% and thus $a$ is applicable in $s$ according to $M_\sgam$,
% i.e., $\pre_{M_\sgam}(a)\subseteq s$.
% Since $M_\sgam$ is safe, %it follows that
% $a_{M_\sgam}(s)=a_{\realm}(s)=a_{M'}(s)$. %, as required.
% Thus, every trajectory consistent with  $M'$ will also be consistent with $M_\sgam$.
% \end{proof}


% We now prove the learned action model is safe.
\begin{theorem}\label{thm:episam-safe}
EPI-SAM returns a safe plan.
\end{theorem}
% \begin{proof}
% Let $A^*$ denote the action model of the underlying planning problem.
% Due to Lemma~\ref{lem:pre-strong}, every action applicable according to $\Pi_\sam$ is also applicable according to $A^*$.
% Consider a goal $G$ and a (strong) plan to achieve it $\pi_\sam$ created by a conformant planner given $\Pi_\sam$.
% This means $\pi_\sam$ achieves $G$ for any action model that satisfies the $\{\cnf_\ell\}_\ell$.
% Due to Lemma~\ref{lem:cnf-char}, we know that this means $\pi_\sam$ achieves $G$ according to any action model consistent with $\mathcal{T}$.
% Thus, $\pi_\sam$ also achieves $G$ according to $A^*$, as required.
% \end{proof}
% \begin{theorem}[Safety]\label{thm:episam-safe}
% Let $\pi_\sam$ be a plan generated by a conformant planner given the
% action model returned by EPI-SAM. The contingent planning formulation created by EPI-SAM (Algorithm \ref{alg:episam}) is safe.
% \end{theorem}
% \begin{proof}
% We first claim that if Algorithm \ref{alg:episam} omits $l$ from $\pre(a)$, then some trajectory in $\mathcal{T}$ must have had $l$ false in the pre-state of $a$, and hence $l$ could not have been in $\pre_{M^*}(a)$ for the true action model; it therefore will follow that whenever a state $s$ satisfies $\pre(a)$, it satisfies $\pre_{M^*}(a)$ and is therefore applicable.
% It is immediate that this holds for any $l$ that is deleted in line \ref{episam-easy-pre-delete}.
% Otherwise, Algorithm \ref{alg:episam} creates a set of trajectories $\mathcal{T}_{l,a}$ in which we assume that $l$ was true whenever $a$ was taken;
% observe that $l$ can be deleted iff there does not exist a set of effects on the fluent for $l$ consistent with $\mathcal{T}_{l,a}$.
% %\roni{I don't fully follow this condition. Do you mean that $l$ can be deleted iff there is no action model consistent with $\mathcal{T}_{l,a}$?} \brendan{Roughly so -- except that only the portions of the action model that have effects on l's fluent are relevant/considered, and the question is then whether or not there is an action model (restricted to this fluent) for which l could be the precondition. We consider each literal (hence, fluent) separately.}
% Now, note that line \ref{episam-action-delete} only deletes actions that cannot have an effect on this fluent, as any subsequent (deleted) actions also could not have had an effect on the fluent, and the fluent then occurs with two different values at the next observation.
% %\roni{This is where I get lost a bit. It seems there's an invariant you're claiming here but I don't get it}\brendan{Yes, I omitted a bit here. Strictly speaking what you have is inductively that for all of the deleted actions in the trajectory, none of those actions could have had an effect on the literal. Supposing that this is true, then any action that appears immediately prior to such sequences of actions, if it has an effect, must set the fluent equal to the next observed value in the trajectory since no subsequent action can change it. But now, if an action would need to set the fluent to two different values (in different portions of the trajectories), then since any action effect would only set the fluent to one value, the action could not had an effect on the fluent, so we can add this action to that sequence, i.e., delete/ignore all occurrences of the action in the trajectories, since the action cannot change the fluent we are examining.}\roni{This seems to say that if we run PI-SAM on the resulting trajectories (projected over that single literal), then we know we get a CNF for the effects that is trivially satisfiable  (without running a SAT solver). I can almost see it but not fully. I understand that the satisfying assignment would set the effect of the remaining action to be the constant value we see for $l$ after them. But, can't that conflict with something we observed for $\neg l$? e.g., some sequence of actions? maybe if I look at it from the fluent prespective I'll get it. Let me think about this more} \brendan{I can try to help: As long as the last possible action in the unobserved sequence sets the literal $l$ true, the action model will be consistent with the observed trajectory. At that point, it doesn't matter whether some earlier action in the sequence had $\neg l$ as an effect. (If that's your concern.) The other thing you could be concerned about is that the same action can't have both $l$ and $\neg l$ as an effect, but this is precisely the condition that causes us to omit the occurrences of the action from the trajectory---so by the contrapositive, if you are setting the action's effect to $l$, this didn't occur (it doesn't appear immediately prior to an observation of $\neg l$), and so the action only appears prior to an observation of $l$ being true.}
% Therefore, in particular, if we encounter an action for which this fluent had a different value in the pre-state that cannot have an effect on the fluent, no such action model can exist and so we can delete the literal on line \ref{episam-hard-pre-delete}.
% %\roni{I got lost here also. How in the pseudo-code do you ensure that the action (I guess $a'$) cannot have an effect on the fluent?}\brendan{See above. We only delete actions that can't have an effect on the fluent. Hopefully it's clearer now.}
% Now, on the other hand, if the loop on line \ref{episam-action-delete-loop} terminates, any action at the end of a run of states where the fluent is unobserved must be followed by states where the fluent takes at most one value. Then we could assign the effect to that action that sets the fluent to that value, and thus obtain a consistent action model with $\mathcal{T}_{l,a}$. Thus, in this case, $l$ could be a precondition of $a$. The algorithm continues to the next iteration of the loop and does not consider this literal again, thus it remains in $\pre(a)$ upon termination.
% Finally, we note that the contingent plan must achieve the goal with any setting of the $\iseff$ variables that is consistent with $CNF_{\eff}$. By Lemma~\ref{lem:cnf-char}, we see that this means that in particular the goal is achieved with the assignment corresponding to the real action model. Thus, the EPI-SAM action model is indeed safe.
% %%
% %% OLD SAT-BASED ANALYSIS
% %%
% %%Consider the formula $CNF_{\mathcal{T}}(l)$ created in Lemma~\ref{lem:cnf-char}: $\ispre(l,a)$ can be set to true in a satisfying assignment iff for every step $t$ where $a=a_t$, we have $State(l,t-1,h)$ true and $State(\neg l,t-1,h)$ false (equiv., $\neg State(\neg l,t-1,h)$ true).  (Equivalently, these are the clauses that can be derived using the unit clause $\ispre(l,a)$.) Thus, by the refutation completeness of resolution, we can refute this collection of unit clauses with $CNF_{\mathcal{T}}(l)$ iff $l$ cannot be a precondition of $a$.
% %%
% %%Again, the $State$ variables must be eliminated, and the (other) $\ispre$ variables cannot be used in a refutation because they only appear negatively. Again, recall that the $State$ variables only appear negatively in clauses of the third type in $CNF_{\mathcal{T}}$. Thus, in addition to the clauses constructed in $CNF_{\eff}(l)$, these new literals (only) allow us to obtain clauses for each run of steps in the trajectory where $l$ is not observed, either starting with a clause of the third type where $l$ is observed true (created in lines 16--17), or using a clause of the second type to eliminate the initial state literal (in line 22). By Lemma~\ref{lem:cnf-char}, this resulting formula can be refuted with $CNF_{\eff}(l)$ iff $l$ cannot be a precondition of $a$.
% %%
% %%Because we only remove literals from the preconditions when this formula is refuted, in any plan constructed with the EPI-SAM action model, whenever an action is taken it must satisfy the preconditions of the real action. Finally, we note that the contingent plan must achieve the goal with any setting of the $\iseff$ variables that is consistent with $CNF_{\eff}$. Again, by Lemma~\ref{lem:cnf-char}, we see that this means that in particular the goal is achieved with the assignment corresponding to the real action model. Thus, the EPI-SAM action model is indeed safe.
% \end{proof}
% %\emph{Sketch of proof (incomplete)}
% %In the first part (line 1-9 in Algorithm \ref{alg:episam}), EPI-SAM creates for every literal a CNF that captures all possible clauses that we can write about literal $l$. If literal $l$ was an effect at some action in the trajectories, it must be represented by a clause in the CNF. In the second part (line 10-25 in Algorithm \ref{alg:episam}), the algorithm initializes that for every action, its preconditions contain all possible literals. For each action, the algorithm removes from its preconditions only literal such that there is no possible assignment that satisfies the CNF that the algorithm can infer about that literal. Thus, it won't remove any literal that is an actual precondition of an action.
% % \begin{definition}[Strength of Action Models]
% % If there exists a trajectory that is consistent with $M'$ but not with $M$, then we say that $M$ is weaker than $M'$.
% % If no such trajectory exists then we say that $M$ is at least as strong as $M'$.
% % \label{def:weakness}
% % \end{definition}

% % \begin{theorem}[The Strength of SGAM Learning]
% % Let $M_{SGAM}$ be the action model created by SGAM learning given the set of trajectories $\mathcal{T}$.
% % $M_{SGAM}$ is at least as strong as any action model $M'$ that is safe and consistent with $\mathcal{T}$.
% % \label{thm:sam-learning-complete-grounded}
% % \end{theorem}
% % \begin{proof}
% % Consider an action model $M'$, which is safe and consistent with $\mathcal{T}$. % and safe w.r.t.\ \realm.
% % Let $a$ be an action and $s$ be a state such that $a$ is applicable in $s$ according to $M'$, i.e., $\pre_{M'}(a)\subseteq s$.
% % Since $M'$ is safe w.r.t.\ \realm, then
% % $\pre_{\realm}(a)\subseteq s$
% % and $a_{M'}(s)=a_{\realm}(s)$.
% % By construction of $M_\sgam$, if a literal $l$ is a precondition of $a$ according to $M_\sgam$,
% % then it has appeared in the pre-state of all action triplets in $\mathcal{T}(a)$.
% % Thus, there exists a consistent action model in which $l$ is a precondition of $a$
% % and this action model may be the real model.
% % Therefore, since $M'$ is safe it follows that $\pre_{M_\sgam}(a) \subseteq \pre_{M'}(a)$,
% % and thus $a$ is applicable in $s$ according to $M_\sgam$,
% % i.e., $\pre_{M_\sgam}(a)\subseteq s$.
% % Since $M_\sgam$ is safe, %it follows that
% % $a_{M_\sgam}(s)=a_{\realm}(s)=a_{M'}(s)$. %, as required.
% % Thus, every trajectory consistent with  $M'$ will also be consistent with $M_\sgam$.
% % \end{proof}

% \begin{theorem}[Strength]
% The contingent planning formulation returned by the EPI-SAM learning algorithm (Algorithm \ref{alg:episam}) is the strongest safe action model.
% \end{theorem}
% \begin{proof}
% In the proof of Theorem~\ref{thm:episam-safe}, we noted that literals are deleted from $\pre(a)$
% %the formula we created can be refuted iff $\ispre(l,a)$ can be refuted with $CNF_{\mathcal{T}}$, i.e.,
% iff $l$ cannot be a precondition of $a$ in any action model consistent with the trajectories in $\mathcal{T}$. Thus, suppose that some action model allows action $a$ to be taken in a state $s$ that does not satisfy the preconditions constructed by EPI-SAM. Then there is some $l$ that is a precondition of $a$ for EPI-SAM that is false in $s$. But by the above,
% %$\ispre(l,a)$ may be set to true in some satisfying assignment of $CNF_{\mathcal{T}}$ -- i.e.,
% $l$ may be a precondition of $a$ in the true action model, so the other action model is not safe.

% Similarly, suppose that there is a plan under the other action model that is allowed by the EPI-SAM action model, but for which EPI-SAM does not achieve the goal. This means (by Lemma~\ref{lem:cnf-char}) that there was some action model consistent with $\mathcal{T}$ under which the goal was not achieved. Again, the other action model is therefore not safe.
% \end{proof}

\begin{theorem}[Strength]
The problem $\Pi_\sam$ returned by EPI-SAM is the strongest safe problem formulation,
in the sense that if an action model $A$ is not safe with respect to $\Pi_\sam$,
then there exists an action model $A'$ consistent with $\mathcal{T}$
such that $A$ is not safe with respect to $A'$.
\end{theorem}
% \begin{proof}
% By contradiction, assume that $A_{bad}$ is an action model that is not safe with respect to $\Pi_{\sam}$, but it is safe w.r.t. any action model consistent with $\mathcal{T}$.
% This means that either a literal $\ell$ exists that is in
% %$\pre_{A_\sam}$ but not in $\pre_{A_{bad}}$
% $\pre_{A_\sam}\setminus \pre_{A_{bad}}$
% or a plan $\pi_{bad}$ that
% achieves the goal $G$ according to $A_{bad}$ but not according to $\Pi_\sam$.
% The first condition cannot hold due to Lemma~\ref{lem:pre-strong}: for any precondition assumed by $\Pi_\sam$ there exists an action model consistent with $\mathcal{T}$ that requires it.
% For the second condition, suppose that there is a plan under $A_{bad}$ that is allowed by the EPI-SAM problem, but for which EPI-SAM does not achieve the goal. This means (by Lemma~\ref{lem:cnf-char}) that there was some action model consistent with $\mathcal{T}$ under which the goal was not achieved. The other action model is therefore not safe.
% \end{proof}
% but not  its preconditions
% Due to Lemma~\ref{lem:pre-strong}, removing any precondition assumed by $A_\sam$ is not safe, in the sense that there are action models consistent with $\mathcal{T}$
% from any action
% In the proof of Theorem~\ref{thm:episam-safe}, we noted that literals are deleted from $\pre(a)$
% %the formula we created can be refuted iff $\ispre(l,a)$ can be refuted with $CNF_{\mathcal{T}}$, i.e.,
% iff $l$ cannot be a precondition of $a$ in any action model consistent with the trajectories in $\mathcal{T}$. Thus, suppose that some action model allows action $a$ to be taken in a state $s$ that does not satisfy the preconditions constructed by EPI-SAM. Then there is some $l$ that is a precondition of $a$ for EPI-SAM that is false in $s$. But by the above,
% %$\ispre(l,a)$ may be set to true in some satisfying assignment of $CNF_{\mathcal{T}}$ -- i.e.,
% $l$ may be a precondition of $a$ in the true action model, so the other action model is not safe.
%
% Similarly, suppose that there is a plan under the other action model that is allowed by the EPI-SAM action model, but for which EPI-SAM does not achieve the goal. This means (by Lemma~\ref{lem:cnf-char}) that there was some action model consistent with $\mathcal{T}$ under which the goal was not achieved. Again, the other action model is therefore not safe.
% \end{proof}


%\emph{Sketch of proof (incomplete)}
%Let $CNF_\eff=\bigwedge_l CNF_\eff(l)$.
%Observation 1: The effects part of every action model that is consistent with the observations is represented by a satisfying assignment to $CNF_\eff$.
%Observation 2: Every satisfying assignment to $CNF_\eff$ represents the effects part of an action model that is consistent with the observations.
%Proof of observation 1+2:
%Here we need to say something about how it is Ok to learn the effects before the preconditions.
%Observation 3: If EPI-SAM removed a literal $l$ from $pre(a)$ then $l$ isn't a precondition in any consistent action model.
%Observation 4: If EPI-SAM did not remove a literal $l$ from $pre(a)$ then there exists a consistent action model in which $l$ is in $pre(a)$.


% \paragraph{Time Complexity}
%\brendan{Note: while it's linear time for propositional domains so long as we can create a clause in unit time, it might actually be quadratic in the lifted domains since the same parameter bound literal could correspond to different ground literals at different points in the trajectory. We could suppress the difference by just saying ``polynomial time'' and leaving the exact exponent to the reader, and this would also save us from needing to talk about data structures.}

% \begin{theorem}
% Given a set of trajectories $\mathcal{T}$, the EPI-SAM learning runs in time
% \begin{small}
% \[\mathcal{O}\Big(\sum_{a\in \mathcal{A}}|\mathcal{T}(a)|\sum_{f\in\mathcal{F}}\prod_{t\in T}arity(a,t)^{arity(f,t)}\Big)\]
% \end{small}
% \end{theorem}

\begin{theorem}
% Given a set of trajectories $\mathcal{T}$, the EPI-SAM algorithm runs in time
Given a set of trajectories $\mathcal{T}$, EPI-SAM runs in time
%\begin{small}
%\[
$\mathcal{O}\Big( |A|\cdot|\mathcal{F}|\cdot \sum_{a\in \mathcal{A}}|\mathcal{T}(a)|    \Big).
$ %\]
%\end{small}
% where $A$ is the set of actions, $\mathcal{F}$ is the set of literals.
%removedVspace
\end{theorem}
% A complete proof is given in the appendix. [roni: since we said already proofs are given in the appendix]


% \begin{proof}
% EPI-SAM algorithm consists of two parts: learning the effects and learning the preconditions. For learning the effects (algorithm \ref{alg:episam-effects}), the algorithm has to iterate over all literals to create a CNF formula for each literal. The first inner loop (line 3-4) iterates through all actions to add mutually exclusive clauses for each action to the CNF, while the second inner loop (line 5-12) goes through every transition in each trajectory to add clauses to the CNF based on Rule 1, 3 of Observation \ref{obs:epi-sam-learning-rules}. Thus, the total time complexity of Algorithm \ref{alg:episam-effects} is $\mathcal{O}\Big(|\mathcal{F}|(|A| + \sum_{a\in \mathcal{A}}|\mathcal{T}(a))|\Big)$.

% The second part, learning the preconditions (Algorithm \ref{alg:episam-preconditions}), iterates over all actions and literals to build the precondition (in form of conjunction) for each action. In each inner loop, each literal in each transition of each trajectory is examined $O(1)$ times---in the first loop we create one clause for each step, and it is set to true or deleted at most once in the second loop.
% We can perform the bookkeeping in the second loop in linear time overall by suitable data structures: we maintain a linked list over the occurrences of a given action, all with a reference to a common structure for the action that records which settings of $l$ appear in the post-state, and we record each of the unobserved runs of a literal with a linked list. Then checking if an action should be deleted takes $O(1)$ time and deleting the occurrences of an action takes $O(1)$ time per occurrence. The data structures likewise take $O(1)$ time per each occurrence of a literal to initialize. The algorithm has to go through every transition in each trajectory to build the data structures and perform the check/delete operations. Thus, the total time complexity of Algorithm \ref{alg:episam-preconditions} is $\mathcal{O}\Big(|A|\cdot|\mathcal{F}|\cdot \sum_{a\in \mathcal{A}}|\mathcal{T}(a)|\Big)$.
% \end{proof}
%The data structures described above are reminiscent of those used by SAT solvers for unit propagation.
% in SAT solvers. Indeed, a SAT encoding of the problem would be solved by standard SAT solvers in linear time.
%The data structures described above are similar to the data structures enabling unit propagation to be performed in linear time; indeed, a SAT encoding of the problem would be solved by standard SAT solvers in linear time.
%EPI-SAM (Algorithm \ref{alg:episam}) consists of two parts: extracting all possible effect clauses for each literal as a CNF, and running the SAT solver to determine the preconditions for each action. The first part runs in polynomial time in the number of literals and total number of state-action triplets. The second part, however, depends on the running time of the SAT solver, which cannot be polynomial time if it is complete (unless P=NP). This is inherent:
%\begin{theorem}
%The following problem is NP-complete: given a set of partially observed trajectories $\mathcal{T}$, action $a$, and literal $l$, decide whether $l$ is a precondition of $a$ in some action model consistent with $\mathcal{T}$.
%\end{theorem}
%\noindent
%{\em Sketch of proof.}
%The problem is clearly in NP. We reduce 3SAT to the problem by creating an action for each literal and a trajectory for each clause in a domain with a single fluent $f$, ending in an additional action. We also create a trajectory for each literal ensuring that one literal in each pair has $f$ as an effect. The additional action does not have $f$ as a precondition iff $f$ is false in one of these trajectories, which occurs only if the formula is unsatisfiable.

\section{Experiments}

% \begin{table}[t]
% \centering
% \scriptsize
% \begin{tabular}{l|c|c|c|c}
% \hline
%             &               \#&             \# &        max &       max  \\
%         Domain    & lifted  & lifted & arity & arity \\
%             &fluents&actions&fluents&actions  \\
% \hline

% Blocks      & 5                 & 4                 &2                  &2                 \\
% Depot      & 6                 & 5                 &4                  &2                    \\
% Ferry       & 5                 & 3                 &2                  &2              \\
% Floortile   & 10                & 7                 &2                  &4               \\
% Gripper     & 4                 & 3                 &2                  &3              \\
% Hanoi       & 3                 & 1                 &2                  &3                \\
% Npuzzle     & 3                 & 1                 &2                  &3               \\
% Parking     & 5                 & 4                 &2                  &3                  \\
% Sokoban     & 4                 & 2                 &3                  &5                 \\
% Transport      & 5                 & 3                 &2                  &5                 \\
% \hline
% \end{tabular}
% \caption{Description of the domains used in our experiments.}
% %removedVspace
% \label{tab:domains}
% \end{table}


% \begin{table}[t]
% \centering
% % \small
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{|l|l|c|c|c|c|c|c|c|c|c|c|}
% \hline
% \multirow{2}{*}{Domain} & \multirow{2}{*}{A} & \multicolumn{5}{c|}{$\eta = 0.3$}                                   & \multicolumn{5}{c|}{$\eta = 0.1$}                                     \\
% \cline{3-12}
%                           &                         & $|\mathcal{T}|$ & P(\pre) & R(\pre) & P(\eff) & R(\eff) & $|\mathcal{T}|$ & P(\pre)
%               & R(\pre)
%               & P(\eff)
%               & R(\eff)  \\
% \hline
% \multirow{2}{*}{Blocks} & S                     & 2               & \textbf{1.00}           & 0.90           & 1.00          & \textbf{0.95}          & 3               & \textbf{1.00}           & 0.80          & \textbf{1.00}           & 0.85            \\
%                         & F                       & 2               & 0.90           & 0.90          &  1.00         & 0.89          & 3               & 0.90           &  \textbf{0.83}         &  0.90         &   0.85         \\
% \hline

% \multirow{2}{*}{Depot} & S                     & 3               & \textbf{1.00}           & 0.85          & \textbf{1.00}          & 1.00        & 4               & \textbf{1.00}            &  0.80         &  \textbf{1.00}          & 1.00           \\
%                         & F                       & 3               &  0.80          & 0.85          &0.90           &1.00           & 4               & 0.80           & 0.80          & 0.90           & 1.00           \\
% \hline

% \multirow{2}{*}{Ferry} & S                     & 2               & \textbf{1.00}           &    1.00       & 1.00          & 1.00          & 2               & \textbf{1.00}           &  0.90         &  \textbf{1.00}          &   0.90         \\
%                         & F                       & 2               & 0.85           & 1.00          & 1.00          & 1.00          & 2               &  0.75          &  \textbf{1.00}         &  0.85          & \textbf{1.00}           \\
% \hline

% \multirow{2}{*}{Floortile} & S                     & 5              &  \textbf{1.00}          &  \textbf{0.87}         & \textbf{1.00}          & \textbf{0.85}          & 7               & \textbf{1.00}           & 0.85          & \textbf{1.00}           & 0.85           \\
%                         & F                       & 5               & 0.83           & 0.80          & 0.77          & 0.80          & 7               & 0.87           & 0.85          &  0.80          &  0.85          \\
% \hline

% \multirow{2}{*}{Gripper} & S                     & 4               & 1.00            &1.00           & 1.00          &1.00           & 4               &   1.00         &   1.00        &   1.00         &   1.00         \\
%                         & F                       & 4               & 1.00           & 1.00          & 1.00          &1.00           & 4               &  1.00          & 1.00          & 1.00           &  1.00          \\
% \hline

% \multirow{2}{*}{Hanoi} & S                     & 1               & \textbf{1.00}           & 1.00          &  1.00         & 1.00          & 1               & \textbf{ 1.00}          & 1.00          &  1.00          &  1.00          \\
%                         & F                       & 1               & 0.85           & 1.00          & 1.00          & 1.00          & 1               &  0.80          & 1.00          &  1.00          & 1.00           \\
% \hline

% \multirow{2}{*}{Npuzzle} & S                     & 1               & 1.00           & 1.00          &1.00           & 1.00          & 1               &  \textbf{1.00}          &   1.00        &   1.00         &  1.00          \\
%                         & F                       & 1               & 1.00           & 1.00          & 1.00          & 1.00          & 1               & 0.80           &1.00           & 1.00           &  1.00          \\
% \hline

% \multirow{2}{*}{Parking} & S                     & 5               & \textbf{1.00}           & 0.85          & 1.00          & 1.00          & 6               & \textbf{1.00}           & 0.80          & \textbf{1.00}           &  1.00          \\
%                         & F                       & 5               & 0.83           & 0.85          & 1.00          &  1.00         & 6               & 0.80           & \textbf{0.85}          &  0.89          &  1.00          \\
% \hline

% \multirow{2}{*}{Sokoban} & S                     & 2               & 1.00           &1.00           &1.00           &1.00           & 2               &1.00            & 1.00          & 1.00           & 1.00           \\
%                         & F                       & 2               &1.00            & 1.00          & 1.00          &1.00           & 2               &  1.00          &1.00          & 1.00           &  1.00          \\
% \hline

% \multirow{2}{*}{Transport} & S                     & 3               & \textbf{1.00}           & \textbf{0.83}          &\textbf{1.00}           & 0.90          & 4               &  \textbf{1.00}          & 0.80          &  \textbf{1.00}          & 0.90           \\
%                         & F                       & 3               &  0.75          &0.80           & 0.80          &0.90           & 4               & 0.80           & 0.80          &  0.83          &  0.90          \\
% \hline
% \end{tabular}
% }
% \caption{Empirical precision and recall of PI-SAM and FAMA under random masking probability $\eta = 0.1$ and $\eta = 0.3$}
% \label{tab:prec-rec}

% \end{table}

% \begin{table}[t]
% \centering
% % \small
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{|l|c|p{0.01\textwidth}|c|c|c|c|c|c|c|c|c|c|c|}
% \hline
% \multirow{2}{*}{Domain} & \multirow{2}{*}{Alg.} & \multicolumn{6}{c|}{$\eta = 0.3$}                                   & \multicolumn{6}{c|}{$\eta = 0.1$}                                     \\
% \cline{3-14}
%                           &                         & $|\mathcal{T}|$ & P(p) & R(p) & P(e) & R(e) & T & $|\mathcal{T}|$ & P(p)
%               & R(p)
%               & P(e)
%               & R(e) & T\\
% \hline
% \multirow{2}{*}{Blocks} & P                     & 3               & \textbf{1.00}           & 0.90           & 1.00          & \textbf{0.95}          &9 &6               & \textbf{1.00}           & 0.83          & \textbf{1.00}           & 0.85       & 43    \\
%                          & E                       & 3               & \textbf{1.00}           & \textbf{0.92}          &  1.00         & 0.95          & - &6               & \textbf{1.00}           &  \textbf{0.85}         &  \textbf{1.00}         &   \textbf{0.88}       & - \\
%                         & F                       & 3               & 0.90           & 0.90          &  1.00         & 0.89          &13 & 6               & 0.90           &  \textbf{0.85}         &  0.90         &   0.85     & 60    \\

% \hline

% \multirow{2}{*}{Depot} & P                     & 5               & \textbf{1.00}           & 0.85          & \textbf{1.00}          & 1.00        & 12& 8               & \textbf{1.00}            &  0.82         &  \textbf{1.00}          & 1.00          &53 \\
%                         & E                     & 5               & \textbf{1.00}           & 0.85          & \textbf{1.00}          & 1.00        & - &8               & \textbf{1.00}            &  \textbf{0.83  }       &  \textbf{1.00}          & 1.00         & - \\
%                         & F                       & 5               &  0.80          & 0.85          &0.90           &1.00           &17 &8               & 0.80           & 0.80          & 0.90           & 1.00        &60   \\

% \hline

% \multirow{2}{*}{Ferry} & P                     & 3               & \textbf{1.00}           &    1.00       & 1.00          & 1.00          &5 &6               & \textbf{1.00}           &  0.94         &  \textbf{1.00}          &   0.90         &27\\
%                         & E                     & 3               & \textbf{1.00}           &    1.00       & 1.00          & 1.00          & - &6               & \textbf{1.00}           &  0.95         &  \textbf{1.00}          &   0.90       & -  \\
%                         & F                       & 3               & 0.85           & 1.00          & 1.00          & 1.00          &9 &6               &  0.80          &  \textbf{1.00}         &  0.85          & \textbf{1.00}          &35 \\

% \hline

% \multirow{2}{*}{Floortile} & P                     & 5              &  \textbf{1.00}          &  0.87         & \textbf{1.00}          & 0.87          & 15&9               & \textbf{1.00}           & 0.85          & \textbf{1.00}           & 0.85           &50\\
%                         & E                     & 5              &  \textbf{1.00}          &  \textbf{0.89}         & \textbf{1.00}          & \textbf{0.90}          &- &9               & \textbf{1.00}           & \textbf{0.87 }         & \textbf{1.00}           & \textbf{0.87}          & -\\

%                         & F                       & 5               & 0.84           & 0.80          & 0.79          & 0.80          &18 &9               & 0.87           & 0.82          &  0.80          &  0.83        & 60 \\

% \hline

% \multirow{2}{*}{Gripper} & P                     & 5               & 1.00            &1.00           & 1.00          &1.00           &5 &10               &   1.00         &   1.00        &   1.00         &   1.00     &24    \\
%                         & E & 5               & 1.00            &1.00           & 1.00          &1.00           &- &10               &   1.00         &   1.00        &   1.00         &   1.00        &- \\

%                         & F                       & 5               & 1.00           & 1.00          & 1.00          &1.00           &8 &10               &  1.00          & 1.00          & 1.00           &  1.00         &30 \\

% \hline

% \multirow{2}{*}{Hanoi} & P                     & 1               & \textbf{1.00}           & 1.00          &  1.00         & 1.00          &1 &1               & \textbf{ 1.00}          & 1.00          &  1.00          &  1.00       & 15  \\
%                         & E                     & 1               & \textbf{1.00}           & 1.00          &  1.00         & 1.00          &- &1               & \textbf{ 1.00}          & 1.00          &  1.00          &  1.00        &-  \\

%                         & F                       & 1               & 0.85           & 1.00          & 1.00          & 1.00          &1 &1               &  0.81          & 1.00          &  1.00          & 1.00          & 60\\

% \hline

% \multirow{2}{*}{Npuzzle} & P                     & 1               & 1.00           & 1.00          &1.00           & 1.00          &1 &1               &  \textbf{1.00}          &   1.00        &   1.00         &  1.00        & 17 \\
%                         & E                     & 1               & 1.00           & 1.00          &1.00           & 1.00          &- &1               &  \textbf{1.00}          &   1.00        &   1.00         &  1.00         &- \\
%                         & F                       & 1               & 1.00           & 1.00          & 1.00          & 1.00          &1 &1               & 0.83           &1.00           & 1.00           &  1.00          &23\\

% \hline

% \multirow{2}{*}{Parking} & P                     & 6               & \textbf{1.00}           & \textbf{0.88}          & 1.00          & 1.00          &8 &8               & \textbf{1.00}           & 0.83          & \textbf{1.00}           &  1.00         &49 \\
%                         & E                     & 6               & \textbf{1.00}           & \textbf{0.88}          & 1.00          & 1.00          &- &8               & \textbf{1.00}           & \textbf{0.85 }         & \textbf{1.00}           &  1.00        & - \\
%                         & F                       & 6               & 0.85           & 0.85          & 1.00          &  1.00         &13 &8               & 0.83           & \textbf{0.85}          &  0.90          &  1.00          &60\\

% \hline

% \multirow{2}{*}{Sokoban} & P                     & 2               & 1.00           &1.00           &1.00           &1.00           &6 &5               &1.00            & 1.00          & 1.00           & 1.00     & 33     \\
%                         & E                     & 2               & 1.00           &1.00           &1.00           &1.00           &- &5               &1.00            & 1.00          & 1.00           & 1.00     &-      \\
%                         & F                       & 2               &1.00            & 1.00          & 1.00          &1.00           &8 &5               &  1.00          &1.00          & 1.00           &  1.00         &40 \\

% \hline

% \multirow{2}{*}{Transport} & P                     & 5               & \textbf{1.00}           & 0.83          &\textbf{1.00}           & 0.90          &9 &9               &  \textbf{1.00}          & 0.80          &  \textbf{1.00}          & 0.90         &48  \\
%                         & E                     & 5               & \textbf{1.00}           & \textbf{0.85}          &\textbf{1.00}           & \textbf{0.92}          &- &9               &  \textbf{1.00}          & \textbf{0.83}          &  \textbf{1.00}          & \textbf{0.92}         &-  \\
%                         & F                       & 5               &  0.77          &0.80           & 0.80          &0.90           &14 &9               & 0.80           & 0.80          &  0.84          &  0.90        &60  \\

% \hline
% \end{tabular}
% }
% % \caption{Empirical precision and recall of PI-SAM, FAMA and EPI-SAM* (a simplified version  under random masking probability $\eta = 0.1$ and $\eta = 0.3$}
% \caption{Empirical precision and recall of PI-SAM, FAMA and EPI-SAM* under random masking with $\eta = 0.1$ and $\eta = 0.3$}
% \label{tab:prec-rec}
% \end{table}




\begin{table*}[t]
\centering
% \small
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{|l|l|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Domain} & \multirow{2}{*}{Algorithm} & \multicolumn{6}{c|}{$\eta = 0.3$}                                   & \multicolumn{6}{c|}{$\eta = 0.1$}                                     \\
\cline{3-14}
                          &                         & $|\mathcal{T}|$ & P(\pre) & R(\pre) & P(\eff) & R(\eff) & T(sec) & $|\mathcal{T}|$ & P(\pre)
              & R(\pre)
              & P(\eff)
              & R(\eff)  & T(sec)\\
\hline
Blocks & FAMA                       & 3               & 0.90           & 0.90          &  1.00         & 0.89          &13 & 6               & 0.90           &  \textbf{0.85}         &  0.90         &   0.85     & 60    \\
(5,4,2,2)                         & PI-SAM                     & 3               & \textbf{1.00}           & 0.90           & 1.00          & \textbf{0.95}          &9 &6               & \textbf{1.00}           & 0.83          & \textbf{1.00}           & 0.85       & 43    \\
                        & EPI-SAM*                       & 3               & \textbf{1.00}           & \textbf{0.92}          &  1.00         & 0.95          & - &6               & \textbf{1.00}           &  \textbf{0.85}         &  \textbf{1.00}         &   \textbf{0.88}       & - \\


\hline

Depot & FAMA                       & 5               &  0.80          & 0.85          &0.90           &1.00           &17 &8               & 0.80           & 0.80          & 0.90           & 1.00        &60   \\
(6,5,4,2)                        & PI-SAM                     & 5               & \textbf{1.00}           & 0.85          & \textbf{1.00}          & 1.00        & 12& 8               & \textbf{1.00}            &  0.82         &  \textbf{1.00}          & 1.00          &53 \\
                        & EPI-SAM*                     & 5               & \textbf{1.00}           & 0.85          & \textbf{1.00}          & 1.00        & - &8               & \textbf{1.00}            &  \textbf{0.83  }       &  \textbf{1.00}          & 1.00         & - \\


\hline

Ferry & FAMA                       & 3               & 0.85           & 1.00          & 1.00          & 1.00          &9 &6               &  0.80          &  \textbf{1.00}         &  0.85          & \textbf{1.00}          &35 \\
(5,3,2,2)                        & PI-SAM                     & 3               & \textbf{1.00}           &    1.00       & 1.00          & 1.00          &5 &6               & \textbf{1.00}           &  0.94         &  \textbf{1.00}          &   0.90         &27\\
                        & EPI-SAM*                     & 3               & \textbf{1.00}           &    1.00       & 1.00          & 1.00          & - &6               & \textbf{1.00}           &  0.95         &  \textbf{1.00}          &   0.90       & -  \\


\hline

Floortile & FAMA                       & 5               & 0.84           & 0.80          & 0.79          & 0.80          &18 &9               & 0.87           & 0.82          &  0.80          &  0.83        & 60 \\
       (10,7,2,4)                 & PI-SAM                     & 5              &  \textbf{1.00}          &  0.87         & \textbf{1.00}          & 0.87          & 15&9               & \textbf{1.00}           & 0.85          & \textbf{1.00}           & 0.85           &50\\
                        & EPI-SAM*                     & 5              &  \textbf{1.00}          &  \textbf{0.89}         & \textbf{1.00}          & \textbf{0.90}          &- &9               & \textbf{1.00}           & \textbf{0.87 }         & \textbf{1.00}           & \textbf{0.87}          & -\\



\hline

Gripper & FAMA                       & 5               & 1.00           & 1.00          & 1.00          &1.00           &8 &10               &  1.00          & 1.00          & 1.00           &  1.00         &30 \\
        (4,3,2,3)                & PI-SAM                     & 5               & 1.00            &1.00           & 1.00          &1.00           &5 &10               &   1.00         &   1.00        &   1.00         &   1.00     &24    \\
                        & EPI-SAM*                     & 5               & 1.00            &1.00           & 1.00          &1.00           &- &10               &   1.00         &   1.00        &   1.00         &   1.00        &- \\



\hline

Hanoi & FAMA                       & 1               & 0.85           & 1.00          & 1.00          & 1.00          &1 &1               &  0.81          & 1.00          &  1.00          & 1.00          & 60\\
       (3,1,2,3)                 & PI-SAM                     & 1               & \textbf{1.00}           & 1.00          &  1.00         & 1.00          &1 &1               & \textbf{ 1.00}          & 1.00          &  1.00          &  1.00       & 15  \\
                        & EPI-SAM*                     & 1               & \textbf{1.00}           & 1.00          &  1.00         & 1.00          &- &1               & \textbf{ 1.00}          & 1.00          &  1.00          &  1.00        &-  \\



\hline

Npuzzle & FAMA                       & 1               & 1.00           & 1.00          & 1.00          & 1.00          &1 &1               & 0.83           &1.00           & 1.00           &  1.00          &23\\
     (3,1,2,3)                   & PI-SAM                     & 1               & 1.00           & 1.00          &1.00           & 1.00          &1 &1               &  \textbf{1.00}          &   1.00        &   1.00         &  1.00        & 17 \\
                        & EPI-SAM*                     & 1               & 1.00           & 1.00          &1.00           & 1.00          &- &1               &  \textbf{1.00}          &   1.00        &   1.00         &  1.00         &- \\


\hline

Parking& FAMA                       & 6               & 0.85           & 0.85          & 1.00          &  1.00         &13 &8               & 0.83           & \textbf{0.85}          &  0.90          &  1.00          &60\\
    (5,4,2,3)                    & PI-SAM                     & 6               & \textbf{1.00}           & \textbf{0.88}          & 1.00          & 1.00          &8 &8               & \textbf{1.00}           & 0.83          & \textbf{1.00}           &  1.00         &49 \\
                        & EPI-SAM*                     & 6               & \textbf{1.00}           & \textbf{0.88}          & 1.00          & 1.00          &- &8               & \textbf{1.00}           & \textbf{0.85 }         & \textbf{1.00}           &  1.00        & - \\


\hline

Sokoban & FAMA                       & 2               &1.00            & 1.00          & 1.00          &1.00           &8 &5               &  1.00          &1.00          & 1.00           &  1.00         &40 \\
    (4,2,3,5)                        & PI-SAM                     & 2               & 1.00           &1.00           &1.00           &1.00           &6 &5               &1.00            & 1.00          & 1.00           & 1.00     & 33     \\
                        & EPI-SAM*                     & 2               & 1.00           &1.00           &1.00           &1.00           &- &5               &1.00            & 1.00          & 1.00           & 1.00     &-      \\


\hline

Transport & FAMA                       & 5               &  0.77          &0.80           & 0.80          &0.90           &14 &9               & 0.80           & 0.80          &  0.84          &  0.90        &60  \\
     (5,3,2,5)                   & PI-SAM                     & 5               & \textbf{1.00}           & 0.83          &\textbf{1.00}           & 0.90          &9 &9               &  \textbf{1.00}          & 0.80          &  \textbf{1.00}          & 0.90         &48  \\
                        & EPI-SAM*                     & 5               & \textbf{1.00}           & \textbf{0.85}          &\textbf{1.00}           & \textbf{0.92}          &- &9               &  \textbf{1.00}          & \textbf{0.83}          &  \textbf{1.00}          & \textbf{0.92}         &-  \\


\hline
\end{tabular}
}
% \caption{Empirical precision and recall of PI-SAM, FAMA and EPI-SAM* (a simplified version  under random masking probability $\eta = 0.1$ and $\eta = 0.3$}
% \caption{Empirical precision and recall of PI-SAM, FAMA and EPI-SAM* under random masking with $\eta = 0.1$ and $\eta = 0.3$.}
\caption{Empirical precision and recall results under random masking with $\eta = 0.1$ and $\eta = 0.3$.}
%removedVspace
\label{tab:prec-rec}
\end{table*}


% \begin{table*}[t]
% \centering
% % \small
% \resizebox{0.8\textwidth}{!}{
% \begin{tabular}{|l|l|c|c|c|c|c|c|c|c|c|c|c|c|}
% \hline
% \multirow{2}{*}{Domain} & \multirow{2}{*}{Algorithm} & \multicolumn{6}{c|}{$\eta = 0.3$}                                   & \multicolumn{6}{c|}{$\eta = 0.1$}                                     \\
% \cline{3-14}
%                           &                         & $|\mathcal{T}|$ & P(\pre) & R(\pre) & P(\eff) & R(\eff) & T(sec) & $|\mathcal{T}|$ & P(\pre)
%               & R(\pre)
%               & P(\eff)
%               & R(\eff)  & T(sec)\\
% \hline
% \multirow{2}{*}{Blocks} & FAMA                       & 3               & 0.90           & 0.90          &  1.00         & 0.89          &13 & 6               & 0.90           &  \textbf{0.85}         &  0.90         &   0.85     & 60    \\
%                         & PI-SAM                     & 3               & \textbf{1.00}           & 0.90           & 1.00          & \textbf{0.95}          &9 &6               & \textbf{1.00}           & 0.83          & \textbf{1.00}           & 0.85       & 43    \\
%                          & EPI-SAM*                       & 3               & \textbf{1.00}           & \textbf{0.92}          &  1.00         & 0.95          & - &6               & \textbf{1.00}           &  \textbf{0.85}         &  \textbf{1.00}         &   \textbf{0.88}       & - \\


% \hline

% \multirow{2}{*}{Depot} & FAMA                       & 5               &  0.80          & 0.85          &0.90           &1.00           &17 &8               & 0.80           & 0.80          & 0.90           & 1.00        &60   \\
%                         & PI-SAM                     & 5               & \textbf{1.00}           & 0.85          & \textbf{1.00}          & 1.00        & 12& 8               & \textbf{1.00}            &  0.82         &  \textbf{1.00}          & 1.00          &53 \\
%                         & EPI-SAM*                     & 5               & \textbf{1.00}           & 0.85          & \textbf{1.00}          & 1.00        & - &8               & \textbf{1.00}            &  \textbf{0.83  }       &  \textbf{1.00}          & 1.00         & - \\


% \hline

% \multirow{2}{*}{Ferry} & FAMA                       & 3               & 0.85           & 1.00          & 1.00          & 1.00          &9 &6               &  0.80          &  \textbf{1.00}         &  0.85          & \textbf{1.00}          &35 \\
%                         & PI-SAM                     & 3               & \textbf{1.00}           &    1.00       & 1.00          & 1.00          &5 &6               & \textbf{1.00}           &  0.94         &  \textbf{1.00}          &   0.90         &27\\
%                         & EPI-SAM*                     & 3               & \textbf{1.00}           &    1.00       & 1.00          & 1.00          & - &6               & \textbf{1.00}           &  0.95         &  \textbf{1.00}          &   0.90       & -  \\


% \hline

% \multirow{2}{*}{Floortile} & FAMA                       & 5               & 0.84           & 0.80          & 0.79          & 0.80          &18 &9               & 0.87           & 0.82          &  0.80          &  0.83        & 60 \\
%                         & PI-SAM                     & 5              &  \textbf{1.00}          &  0.87         & \textbf{1.00}          & 0.87          & 15&9               & \textbf{1.00}           & 0.85          & \textbf{1.00}           & 0.85           &50\\
%                         & EPI-SAM*                     & 5              &  \textbf{1.00}          &  \textbf{0.89}         & \textbf{1.00}          & \textbf{0.90}          &- &9               & \textbf{1.00}           & \textbf{0.87 }         & \textbf{1.00}           & \textbf{0.87}          & -\\



% \hline

% \multirow{2}{*}{Gripper} & FAMA                       & 5               & 1.00           & 1.00          & 1.00          &1.00           &8 &10               &  1.00          & 1.00          & 1.00           &  1.00         &30 \\
%                         & PI-SAM                     & 5               & 1.00            &1.00           & 1.00          &1.00           &5 &10               &   1.00         &   1.00        &   1.00         &   1.00     &24    \\
%                         & EPI-SAM*                     & 5               & 1.00            &1.00           & 1.00          &1.00           &- &10               &   1.00         &   1.00        &   1.00         &   1.00        &- \\



% \hline

% \multirow{2}{*}{Hanoi} & FAMA                       & 1               & 0.85           & 1.00          & 1.00          & 1.00          &1 &1               &  0.81          & 1.00          &  1.00          & 1.00          & 60\\
%                         & PI-SAM                     & 1               & \textbf{1.00}           & 1.00          &  1.00         & 1.00          &1 &1               & \textbf{ 1.00}          & 1.00          &  1.00          &  1.00       & 15  \\
%                         & EPI-SAM*                     & 1               & \textbf{1.00}           & 1.00          &  1.00         & 1.00          &- &1               & \textbf{ 1.00}          & 1.00          &  1.00          &  1.00        &-  \\



% \hline

% \multirow{2}{*}{Npuzzle} & FAMA                       & 1               & 1.00           & 1.00          & 1.00          & 1.00          &1 &1               & 0.83           &1.00           & 1.00           &  1.00          &23\\
%                         & PI-SAM                     & 1               & 1.00           & 1.00          &1.00           & 1.00          &1 &1               &  \textbf{1.00}          &   1.00        &   1.00         &  1.00        & 17 \\
%                         & EPI-SAM*                     & 1               & 1.00           & 1.00          &1.00           & 1.00          &- &1               &  \textbf{1.00}          &   1.00        &   1.00         &  1.00         &- \\


% \hline

% \multirow{2}{*}{Parking}& FAMA                       & 6               & 0.85           & 0.85          & 1.00          &  1.00         &13 &8               & 0.83           & \textbf{0.85}          &  0.90          &  1.00          &60\\
%                         & PI-SAM                     & 6               & \textbf{1.00}           & \textbf{0.88}          & 1.00          & 1.00          &8 &8               & \textbf{1.00}           & 0.83          & \textbf{1.00}           &  1.00         &49 \\
%                         & EPI-SAM*                     & 6               & \textbf{1.00}           & \textbf{0.88}          & 1.00          & 1.00          &- &8               & \textbf{1.00}           & \textbf{0.85 }         & \textbf{1.00}           &  1.00        & - \\


% \hline

% \multirow{2}{*}{Sokoban} & FAMA                       & 2               &1.00            & 1.00          & 1.00          &1.00           &8 &5               &  1.00          &1.00          & 1.00           &  1.00         &40 \\
%                             & PI-SAM                     & 2               & 1.00           &1.00           &1.00           &1.00           &6 &5               &1.00            & 1.00          & 1.00           & 1.00     & 33     \\
%                         & EPI-SAM*                     & 2               & 1.00           &1.00           &1.00           &1.00           &- &5               &1.00            & 1.00          & 1.00           & 1.00     &-      \\


% \hline

% \multirow{2}{*}{Transport} & FAMA                       & 5               &  0.77          &0.80           & 0.80          &0.90           &14 &9               & 0.80           & 0.80          &  0.84          &  0.90        &60  \\
%                         & PI-SAM                     & 5               & \textbf{1.00}           & 0.83          &\textbf{1.00}           & 0.90          &9 &9               &  \textbf{1.00}          & 0.80          &  \textbf{1.00}          & 0.90         &48  \\
%                         & EPI-SAM*                     & 5               & \textbf{1.00}           & \textbf{0.85}          &\textbf{1.00}           & \textbf{0.92}          &- &9               &  \textbf{1.00}          & \textbf{0.83}          &  \textbf{1.00}          & \textbf{0.92}         &-  \\


% \hline
% \end{tabular}
% }
% % \caption{Empirical precision and recall of PI-SAM, FAMA and EPI-SAM* (a simplified version  under random masking probability $\eta = 0.1$ and $\eta = 0.3$}
% \caption{Empirical precision and recall of PI-SAM, FAMA and EPI-SAM* under random masking with $\eta = 0.1$ and $\eta = 0.3$}
% %removedVspace
% \label{tab:prec-rec}
% \end{table*}

% \begin{table*}
% \resizebox{\textwidth}{!}{
% \centering
% \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|}
% \hline
%       & \multicolumn{4}{c|}{$|\mathcal{T}| = 3$} & \multicolumn{4}{c|}{$|\mathcal{T}| = 5$} & \multicolumn{4}{c|}{$|\mathcal{T}| = 7$}  \\
% \cline{2-13}
%       & P(\pre) & R(\pre) & P(\eff) & R(\eff)             & P(\pre) & R(\pre) & P(\eff) & R(\eff)             & P(\pre) & R(\pre) & P(\eff) & R(\eff)              \\
% \hline
% PI-SAM &   \textbf{1.00}     &  0.90      &   1.00     &   \textbf{0.95}                 &  \textbf{1.00}      &  \textbf{0.92}      &   \textbf{1.00}     &      \textbf{0.96}              &   \textbf{1.00}     &  \textbf{0.93}      &  \textbf{1.00}      &     \textbf{0.96}                \\
% \hline
% FAMA   &   0.90     &  0.90      &   1.00     &   0.89                 &   0.87     &  0.88      &  0.95      & 0.87                   &    0.90    & 0.90       &  1.00      &  0.90      \\
% \hline

% \end{tabular}
% }
% \caption{Empirical precision and recall of PI-SAM and FAMA on Blocks domain given increasing number of input trajectories (random masking probability $\eta = 0.3$)}
% \label{tab:blocks}
% \end{table*}


% \begin{table}[b]
% \centering
% %removedVspace
% \resizebox{0.6\textwidth}{!}{
% \begin{tabular}{@{}l|ll|ll|ll@{}}
% \toprule
%        & \multicolumn{2}{c|}{$\mathcal|{T}|=3$} & \multicolumn{2}{c|}{$\mathcal|{T}|=5$} & \multicolumn{2}{c}{$\mathcal|{T}|=7$} \\
%        % & PI-SAM     & FAMA      & PI-SAM     & FAMA     & PI-SAM     & FAMA     \\ \midrule
%        & PI-SAM     & FAMA      & PI-SAM     & FAMA     & PI-SAM     & FAMA     \\ \midrule
% P(pre) & \textbf{1.00}       & 0.90      & \textbf{1.00}       & 0.87     & \textbf{1.00}       & 0.90      \\
% R(pre) & 0.90               & 0.90      & \textbf{0.92}       & 0.88     & \textbf{0.93}       & 0.90      \\
% P(eff) & 1.00               & 1.00      & \textbf{1.00}       & 0.95     & 1.00       & 1.00        \\
% R(eff) & \textbf{0.95}       & 0.89      & \textbf{0.96}       & 0.87     & \textbf{0.96}       & 0.90      \\ \bottomrule
% \end{tabular}
% }
% % \caption{\small Empirical precision and recall of PI-SAM and FAMA on Blocks domain given increasing number of input trajectories (random masking probability $\eta = 0.3$).}
% % \caption{\small Empirical precision and recall of PI-SAM and FAMA on Blocks domain with random masking probability $\eta = 0.3$.}
% \caption{\small Empirical precision and recall results on Blocks domain with random masking probability $\eta = 0.3$.}
% \label{tab:blocks}
% \end{table}


%%%%%%%%%%%% RONI1
\begin{table}[b]
\centering
\resizebox{\columnwidth}{!}{
    % \setlength{\tabcolsep}{1.5pt}
    % \begin{small}
        \begin{tabular}{@{}l|ll|ll|ll@{}}
    % \multicolumn{7}{c}{ }\\
    \toprule
           & \multicolumn{2}{c|}{$\mathcal|{T}|=3$} & \multicolumn{2}{c|}{$\mathcal|{T}|=5$} & \multicolumn{2}{c}{$\mathcal|{T}|=7$} \\
           % & PI-SAM     & FAMA      & PI-SAM     & FAMA     & PI-SAM     & FAMA     \\ \midrule
           & PI-SAM     & FAMA      & PI-SAM     & FAMA     & PI-SAM     & FAMA     \\ \midrule
    P(pre) & \textbf{1.00}       & 0.90      & \textbf{1.00}       & 0.87     & \textbf{1.00}       & 0.90      \\
    R(pre) & 0.90               & 0.90      & \textbf{0.92}       & 0.88     & \textbf{0.93}       & 0.90      \\
    P(eff) & 1.00               & 1.00      & \textbf{1.00}       & 0.95     & 1.00       & 1.00        \\
    R(eff) & \textbf{0.95}       & 0.89      & \textbf{0.96}       & 0.87     & \textbf{0.96}       & 0.90      \\ \bottomrule
    \end{tabular}
}
\caption{Results on Blocks with $\eta = 0.3$.}
\label{tab:results1}
\end{table}

%%%%%%%%%%%% RONI2
\begin{table}
\centering
\begin{tabular}{@{}l|r|rr@{}}
\toprule
% Alg.    & SAM ($\eta=1$) & PI-SAM ($\eta=0.3$) & PI-SAM ($\eta=0.1$)\\ \midrule
    & \multicolumn{1}{c}{SAM} & \multicolumn{2}{c}{PI-SAM} \\
Alg.     & ($\eta=1.0$) & ($\eta=0.3$)    & ($\eta=0.1$)    \\ \midrule
Hanoi   & 1   & 10     & 95     \\
Npuzzle & 1   & 9      & 92     \\
Ferry   & 4   & 42     & 355    \\
Gripper & 5   & 51     & 476    \\
Sokoban & 6   & 55     & 563    \\ \bottomrule
\end{tabular}
% }
\caption{\# of transitions needed to learn the preconditions}
\label{tab:results2}
\end{table}


% \begin{table}[b]
% \centering
% % \setlength{\tabcolsep}{1.5pt}
% \begin{small}
%     \begin{tabular}{@{}l|ll|ll|ll@{}}
% % \multicolumn{7}{c}{ }\\
% \toprule
%        & \multicolumn{2}{c|}{$\mathcal|{T}|=3$} & \multicolumn{2}{c|}{$\mathcal|{T}|=5$} & \multicolumn{2}{c}{$\mathcal|{T}|=7$} \\
%        % & PI-SAM     & FAMA      & PI-SAM     & FAMA     & PI-SAM     & FAMA     \\ \midrule
%        & PI-SAM     & FAMA      & PI-SAM     & FAMA     & PI-SAM     & FAMA     \\ \midrule
% P(pre) & \textbf{1.00}       & 0.90      & \textbf{1.00}       & 0.87     & \textbf{1.00}       & 0.90      \\
% R(pre) & 0.90               & 0.90      & \textbf{0.92}       & 0.88     & \textbf{0.93}       & 0.90      \\
% P(eff) & 1.00               & 1.00      & \textbf{1.00}       & 0.95     & 1.00       & 1.00        \\
% R(eff) & \textbf{0.95}       & 0.89      & \textbf{0.96}       & 0.87     & \textbf{0.96}       & 0.90      \\ \bottomrule
% \end{tabular}
% \end{small}
% \qquad
% % \resizebox{0.33\columnwidth}{!}{
% \begin{tabular}{@{}l|r|rr@{}}
% \toprule
% % Alg.    & SAM ($\eta=1$) & PI-SAM ($\eta=0.3$) & PI-SAM ($\eta=0.1$)\\ \midrule
%     & \multicolumn{1}{c}{SAM} & \multicolumn{2}{c}{PI-SAM} \\
% Alg.     & ($\eta=1.0$) & ($\eta=0.3$)    & ($\eta=0.1$)    \\ \midrule
% Hanoi   & 1   & 10     & 95     \\
% Npuzzle & 1   & 9      & 92     \\
% Ferry   & 4   & 42     & 355    \\
% Gripper & 5   & 51     & 476    \\
% Sokoban & 6   & 55     & 563    \\ \bottomrule
% \end{tabular}
% % }
% \caption{\small (Left) Results on Blocks with $\eta = 0.3$.
% (Right) \# of transitions needed to learn the preconditions}
% \label{tab:joint-results}
% \end{table}

% \begin{table}[b]
% \centering
%     \begin{tabular}{@{}llrrrrr@{}}
%     \toprule
%      % Alg. & \multicolumn{1}{c}{$\eta$} & \multicolumn{1}{l}{Hanoi} & \multicolumn{1}{l}{Npuzzle} & \multicolumn{1}{l}{Ferry} & \multicolumn{1}{l}{Gripper} & \multicolumn{1}{l}{Sokoban} \\ \midrule
%     % Alg. & \multicolumn{1}{c}{$\eta$} & \multicolumn{1}{l}{Hano.} & \multicolumn{1}{l}{Npuz.} & \multicolumn{1}{l}{Ferr.} & \multicolumn{1}{l}{Grip.} & \multicolumn{1}{l}{Soko.} \\ \midrule
%     SAM    & 1.0                   & 1                         & 1                           & 4                         & 5                           & 6                           \\
%     PI-SAM & 0.3                   & 10                        & 9                           & 42                        & 51                          & 55                          \\
%     PI-SAM & 0.1                   & 95                        & 92                          & 355                       & 476                         & 563                         \\ \bottomrule
%     \end{tabular}
%     \caption{\small \# of transitions needed to learn the preconditions.}
%     \label{tab:pisam_sam}
% \end{table}


% \begin{table}[t]
%     \centering
%     \resizebox{\columnwidth}{!}{
%     \begin{tabular}{l|c|c|c}
%     \hline
%     \multirow{2}{*}{Domain} & \multicolumn{3}{c}{\# triplets}  \\
%     \cline{2-4}
%                         & SAM ($\eta = 1.0$) & PI-SAM ($\eta = 0.3$)    & PI-SAM ($\eta = 0.1$)                 \\
%     \hline
%     Hanoi                    & 1    &      10          &       95     \\
%     Npuzzle                    & 1    &     9         &       92       \\
%     Ferry                    & 4    &     42          &     355         \\
%     Gripper                    & 5    &   51          &      476         \\
%     Sokoban                    & 6    &   55          &    563          \\
%     \hline
%     \end{tabular}
%     }
%     \caption{\# of transitions needed to learn the preconditions}
%     \label{tab:pisam_sam}
% \end{table}


% \begin{table}[b]
% \centering
% %removedVspace
% \resizebox{0.6\columnwidth}{!}{
%     \begin{tabular}{@{}llrrrrr@{}}
%     \toprule
%     Alg. & \multicolumn{1}{c}{$\eta$} & \multicolumn{1}{l}{Hanoi} & \multicolumn{1}{l}{Npuzzle} & \multicolumn{1}{l}{Ferry} & \multicolumn{1}{l}{Gripper} & \multicolumn{1}{l}{Sokoban} \\ \midrule
%     SAM    & 1.0                   & 1                         & 1                           & 4                         & 5                           & 6                           \\
%     PI-SAM & 0.3                   & 10                        & 9                           & 42                        & 51                          & 55                          \\
%     PI-SAM & 0.1                   & 95                        & 92                          & 355                       & 476                         & 563                         \\ \bottomrule
%     \end{tabular}
% }
%     \caption{\small \# of transitions needed to learn the preconditions.}
%     \label{tab:pisam_sam}
% \end{table}



% \begin{table}[t]
%     \centering
%     \resizebox{\columnwidth}{!}{
%     \begin{tabular}{l|l|c|c|c|c}
%     \hline
%     Domain & Algorithm & P (\pre) & R (\pre) & P (\eff) & R (\eff) \\
%     \hline
%     \multirow{2}{*}{Blocks}    & EPI-SAM   &1.00 &0.90 & 1.00 & 0.95           \\
%                           & PI-SAM    &1.00 &0.90 & 1.00 & 0.95          \\
%     \hline
%     \multirow{2}{*}{Depot}     & EPI-SAM   & 1.00 &0.85 & 1.00 & 1.00          \\
%                           & PI-SAM    & 1.00 &0.85 & 1.00 &1.00           \\
%     \hline
%     \multirow{2}{*}{Floortile} & EPI-SAM   &1.00 & \textbf{0.89} & 1.00 & \textbf{0.90  }         \\
%                           & PI-SAM    &1.00 & 0.87 & 1.00 & 0.85           \\
%     \hline
%     \multirow{2}{*}{Parking}   & EPI-SAM   &1.00 &0.85 & 1.00 &1.00           \\
%                           & PI-SAM    &1.00 &0.85 & 1.00 &1.00           \\
%     \hline
%     \multirow{2}{*}{Transport} & EPI-SAM   &1.00 & \textbf{0.85} & 1.00  & \textbf{0.92 }          \\
%                           & PI-SAM    &1.00 & 0.83 & 1.00  & 0.90                \\
%     \hline


%     \end{tabular}
%     }
%     \caption{Empirical precision and recall under random masking probability $\eta = 0.3$}
%     \label{tab:episam}
% \end{table}


% To evaluate our algorithms' performance in practice, we perform experiments on common domains from the IPC\cite{ipc}, which the feature descriptions are shown in Table \ref{tab:domains}. For each domain, we generated the problems using the generator provided by the IPC learning tracks and solved them using their true model in an off-the-shelf planner to obtain the trajectories. We then produced partially observable trajectories using \textit{random masking}. At each state, each literal is randomly masked with probability $1-\eta$ (i.e. each literal has $\eta$ chances of being observed). By definition, these partially observable trajectories satisfy the \textit{$\eta^2$-bounded concealment assumption}. They were then split into state-action triplets and passed to our algorithms to obtain the safe action model.

%To evaluate our algorithms' performance in practice, we perform experiments on domains from the IPC~\cite{ipc}, as described in Table \ref{tab:domains}.
% We evaluate our algorithms' performance experimentally on the IPC~\cite{ipc} domains listed in Table \ref{tab:domains}.
We evaluate our algorithms' performance experimentally on the IPC~\citep{ipc} domains listed in Table \ref{tab:prec-rec}.
The tuple listed under each domain details the number of lifted fluents, lifted actions, maximal arity of fluents, and maximal arity of actions in that domain.  For each domain, we generated problems using the generators provided by the IPC learning tracks and solved them using the true action model and an off-the-shelf planner. In the resulting trajectories, we masked some states using \textit{random masking} with masking probability $\eta=0.1$ and $\eta=0.3$.
%These trajectories were given to our algorithms as input.
% for each the trajectories were generated using two random masking probabilities $\eta$, $0.1$ and $0.3$.
%:   At each state, each literal is randomly masked with probability $1-\eta$ (i.e. each literal has $\eta$ chances of being observed). By definition, these partially observable trajectories satisfy the \textit{$\eta^2$-bounded concealment assumption}.
%The trajectories were then split into state-action triplets and passed to our algorithms to obtain the safe action model.


% \begin{table}[t]
% \centering
% \small
% \begin{tabular}{l|c|c|c|c}
% \hline
%             &               \#&             \# &        max &       max  \\
%         Domain    & lifted  & lifted & arity & arity \\
%             &fluents&actions&fluents&actions  \\
% \hline

% Blocks      & 5                 & 4                 &2                  &2                 \\
% Depot      & 6                 & 5                 &4                  &2                    \\
% Ferry       & 5                 & 3                 &2                  &2              \\
% Floortile   & 10                & 7                 &2                  &4               \\
% Gripper     & 4                 & 3                 &2                  &3              \\
% Hanoi       & 3                 & 1                 &2                  &3                \\
% Npuzzle     & 3                 & 1                 &2                  &3               \\
% Parking     & 5                 & 4                 &2                  &3                  \\
% Sokoban     & 4                 & 2                 &3                  &5                 \\
% Transport      & 5                 & 3                 &2                  &5                 \\
% \hline
% \end{tabular}

% \caption{Description of the domains used in our experiments.}
% \label{tab:domains}
% \end{table}


% \paragraph{Metrics} To measure the performance of each algorithm, we computed the \textit{empirically-based precision-recall} for \textit{preconditions} and \textit{effects} defined as follows:
% \begin{equation}\small
% precision = \frac{TP}{TP + FP},~
% recall = \frac{TP}{TP + FN}
% \end{equation}
% where, for \textit{preconditions}:
% \begin{compactitem}
%     \item TP (true positives):  the obtained action model and the true action model.
%     \item FP (false positives): number of transitions that are valid according to  the obtained action model but not the true action model.
%     \item TN (true negatives): number of transitions that are invalid according to both the obtained action model and the true action model.
%     \item FN (false negatives:): number of transitions that are valid according to   the true action model but not the obtained action model.
% \end{compactitem}
% and for \textit{effects}:
% \begin{compactitem}
%     \item TP (true positives): number of transitions in which similar literals switch values in both the obtained action model and the true action model.
%     \item FP (false positives): number of transitions in which exists a literal that switches value in the obtained action model but not the true action model.
%     \item TN (true negatives): number of transitions in which similar literals don't switch values in both the obtained action model and the true action model.
%     \item FN (false negatives:): number of transitions in which exists a literal that switches value in the true action model but not the obtained action model.
% \end{compactitem}
% Unlike the \textit{syntactic-based precision-recall}, which is computed based on the number of overlapping literals between the obtained action model and the true model, our \textit{empirically-based precision-recall} is computed using the number of actual valid transitions according to both the obtained action model and the true model. Thus, it can clearly indicate how safe the learned action model is (the higher the precision, the safer the learned model; at $precision = 1$, the obtained model is as safe as the true model).

\paragraph{Metrics}
A common approach to comparing action models is by computing the precision and recall of the learned action model with respect to which literals appear in the real action model. However, this syntactic measure has three limitations. First, it requires the evaluated action models to use the same fluents and action names. Second, it gives the same ``penalty'' for every mistake in the learned model. Third, domains may have distinct but semantically-equivalent action models. For example, in Npuzzle, we could have a precondition that the tile we are sliding into the empty position is not an empty position. This precondition is not necessary, as there is only ever one empty position in any puzzle. Thus, either formulation of the domain is adequate for planning purposes, but a syntactic measure of correctness will penalize one of the two formulations. Instead, we introduce and use
\textit{empirically-based precision and recall} measures, which are based on comparing the number of transitions that are valid or invalid according to the learned action model ($\hat{A}$) and the true action model ($A$).
% Similar to the standard precision and recall measures, our empirical precision and recall measures tives/negatives (TP,FP,TN,FN), i.e., precision is $\frac{TP}{TP + FP}$ and recall is $\frac{TP}{TP + FN}$.
% \begin{equation}\small
% precision = \frac{TP}{TP + FP},~
% recall = \frac{TP}{TP + FN}
% \end{equation}
% But, TP, FP, TN, and FN are computed here differently as follows.
% Similar to the standard precision and recall measures, our
The empirical precision and recall measures tives/negatives (TP,FP,TN,FN)
% , i.e., precision is $\frac{TP}{TP + FP}$ and recall is $\frac{TP}{TP + FN}$,
but compute TP, FP, TN, and FN differently.
For preconditions, TP  both $\hat{A}$ and $A$,
FP is the number of transitions that are valid according to $\hat{A}$ but not $A$,
TN is the number of transitions that are invalid according to $\hat{A}$ and $A$, and
FN is the number of transitions that are valid according to $A$ and but not $\hat{A}$.
TP, FP, TN, and FN for effects are 
%For effects, TP is the number of transitions in which similar literals switch values in both $\hat{A}$ and $A$, FP is the number of transitions in which there exists a literal that switches value in $\hat{A}$ but not in $A$, TN is the number of transitions in which similar literals do not switch values in $\hat{A}$ and in $A$, and FN is the number of transitions in which exists a literal that switches value in $A$ but not in $\hat{A}$.


%differently, based on analyzing a set of transitions sampled from observed trajectories.
% \paragraph{For preconditions}
% TP are the number of transitions that are valid according to both $\hat{A}$ and $A$.
% FP are the number of transitions that are valid according to $\hat{A}$ but not $A$.
% TN are the number of transitions that are invalid according to $\hat{A}$ and $A$.
% FN are the number of transitions that are valid according to $A$ and but not $\hat{A}$.
% \paragraph{For effects}
% TP are the number of transitions in which similar literals switch values in both $\hat{A}$ and $A$.
% FP are the number of transitions in which exists a literal that switches value in $\hat{A}$ but not in $A$.
% TN are the number of transitions in which similar literals do not switch values in $\hat{A}$ and in $A$.
% FN are the number of transitions in which exists a literal that switches value in $A$ but not in $\hat{A}$.
% Let $a_X(s)$ denote the state reached by perfoming $a$ at $s$ according to action model $X$.
% TP are the number of transitions in which $a_{\hat{A}}(s)=a_A(s)$.
% FP are the number of transitions in which $\exists l\in s$ where $s[l]\neq \hat{A}}(s)$=a_A(s)$.
% TP are the number of transitions in which $a_{\hat{A}}(s)=a_A(s)$.
% TP are the number of transitions in which $a_{\hat{A}}(s)=a_A(s)$.
% FP are the number of transitions that are valid according to $\hat{A}$ but not $A$.
% TN are the number of transitions that are invalid accord. %ing to $\hat{A}$ and $A$.
% FN are the number of transitions that are valid according to $A$ and but not $\hat{A}$.

% \begin{compactitem}
%     \item TP (true positives): number of transitions in which similar literals switch values in both the obtained action model and the true action model.
%     \item FP (false positives): number of transitions in which exists a literal that switches value in the obtained action model but not the true action model.
%     \item TN (true negatives): number of transitions in which similar literals don't switch values in both the obtained action model and the true action model.
%     \item FN (false negatives:): number of transitions in which exists a literal that switches value in the true action model but not the obtained action model.
% \end{compactitem}
% Unlike the \textit{syntactic-based precision-recall}, which is computed based on the number of overlapping literals between the obtained action model and the true model, our \textit{empirically-based precision-recall} is computed using the number of actual valid transitions according to both the obtained action model and the true model. Thus, it can clearly indicate how safe the learned action model is (the higher the precision, the safer the learned model; at $precision = 1$, the obtained model is as safe as the true model).

\paragraph{Results and Discussion}
We performed experiments using PI-SAM and EPI-SAM*, a simplified (unsafe) ver in   precision and recall directly to EPI-SAM.
EPI-SAM* ery action in the CNF returned by Algorithm \ref{alg:episam-effects}, by checking if assuming literal $l$ is an effect of action $a$ if the CNF formula extended by $\neg\iseff(a,\ell)$ is satisfiable.\roni{This sentence is not clear.}
%by checking if assuming literal $l$ is an effect of action $a$ if the CNF formula extended by $\neg\iseff(a,\ell)$ is satisfiable.
EPI-SAM* outputs a classical action model instead of a conformant plan. %requiring to construct a Contingent Planning problem.
Nevertheless, observe that the infer a subset of those obtainable in EPI-SAM's formulation. Thus, since EPI-SAM is safe, the precision and recall for EPI-SAM* provide a lower bound on the performance  %\hai{Can we proof that EPI-SAM* give a lower bound on the performance of EPI-SAM?} Hopefully the above elaboration is enough -- see what you think. --BJ

As a baseline, we compared our algorithms to FAMA \citep{aineto2019learning}, a modern algomark domains. %, for each the trajectories were generated using two random masking probabilities $\eta$, $0.1$ and $0.3$.
For each domain, we computed the \textit{empirical precision} (P) and \textit{recall} (R) separately for the preconditions (\pre) and effects (\eff).
Table \ref{tab:prec-rec} lists the pendent runs.
Columns ``$P(\pre)$'', ``$R(\pre)$'', ``$P(\eff)$'', and ``$R(\eff)$'' show the empirical precision and recall for preconditions and effects for every evaluated algorithm.
%In Table \ref{tab:prec-rec}, we compare the precision and recall of the action model returned by each algorithm: PI-SAM, EPI-SAM*, and FAMA using the same number of trajectories $|\mathcal{T}|$ as input.
$|\mathcal{T}|$ is determined as the point that FAMA started decreasing performance (i.e. precision-recall) or reaching  algorithm to 60 seconds. Column ``T'' is the runtime of each algorithm in seconds. %n Table \ref{tab:prec-rec}, round to the nearest second.
Since EPI-SAM* is unsafe, we do not report its runtime. % to compare in Table \ref{tab:prec-rec}.
Since PI-SAM and EPI-SAM*, by definition, never remove a literal that is   
 lower masking probability (high $\eta$), while FAMA tends to obtain higher recall under higher masking probability (low $\eta$). EPI-SAM* generally outperforms both.
Note that FAMA's performance may decrease as more input is given, while PI-SAM cannot.
%, in theory, can recover the true action model if given enough data.
To demonstrate this, we picked a domain (Blocks) and recorded their perfor input. The results are shown in Table \ref{tab:results1}.
We also com learn the preconditions (i.e., $P(\pre)$ and $R(\pre) = 1.0$)
when using PI-SAM with $\eta\in\{0.1, 0.3\}$ and when having full observability and using SAM.
The results are shown in Table \ref{tab:results2}. As expected, the number of transitions required scales inversely with the random masking probability $\eta^2$, which verifies the tightness of the bound in Theorem \ref{complexity-pisam-thm}.
The source code of the experiments will 


%Nevertheless, the differences are not significant.
% <<- Significant in a statistical sense? omitting for now unless we can confirm.
% \begin{table*}[t]
% \centering
% \small
% %\resizebox{\textwidth}{!}{
% \begin{tabular}{|l|l|c|c|c|c|c|c|c|c|c|c|}
% \hline
% \multirow{2}{*}{Domain} & \multirow{2}{*}{Algorithm} & \multicolumn{5}{c|}{$\eta = 0.3$}                                   & \multicolumn{5}{c|}{$\eta = 0.1$}                                     \\
% \cline{3-12}
%                           &                         & $|\mathcal{T}|$ & P(\pre) & R(\pre) & P(\eff) & R(\eff) & $|\mathcal{T}|$ & P(\pre)
%               & R(\pre)
%               & P(\eff)
%               & R(\eff)  \\
% \hline
% \multirow{2}{*}{Blocks} & PI-SAM                     & 2               & \textbf{1.00}           & 0.90           & 1.00          & \textbf{0.95}          & 3               & \textbf{1.00}           & 0.80          & \textbf{1.00}           & 0.85            \\
%                         & FAMA                       & 2               & 0.90           & 0.90          &  1.00         & 0.89          & 3               & 0.90           &  \textbf{0.83}         &  0.90         &   0.85         \\
% \hline

% \multirow{2}{*}{Depot} & PI-SAM                     & 3               & \textbf{1.00}           & 0.85          & \textbf{1.00}          & 1.00        & 4               & \textbf{1.00}            &  0.80         &  \textbf{1.00}          & 1.00           \\
%                         & FAMA                       & 3               &  0.80          & 0.85          &0.90           &1.00           & 4               & 0.80           & 0.80          & 0.90           & 1.00           \\
% \hline

% \multirow{2}{*}{Ferry} & PI-SAM                     & 2               & \textbf{1.00}           &    1.00       & 1.00          & 1.00          & 2               & \textbf{1.00}           &  0.90         &  \textbf{1.00}          &   0.90         \\
%                         & FAMA                       & 2               & 0.85           & 1.00          & 1.00          & 1.00          & 2               &  0.75          &  \textbf{1.00}         &  0.85          & \textbf{1.00}           \\
% \hline

% \multirow{2}{*}{Floortile} & PI-SAM                     & 5              &  \textbf{1.00}          &  \textbf{0.87}         & \textbf{1.00}          & \textbf{0.85}          & 7               & \textbf{1.00}           & 0.85          & \textbf{1.00}           & 0.85           \\
%                         & FAMA                       & 5               & 0.83           & 0.80          & 0.77          & 0.80          & 7               & 0.87           & 0.85          &  0.80          &  0.85          \\
% \hline

% \multirow{2}{*}{Gripper} & PI-SAM                     & 4               & 1.00            &1.00           & 1.00          &1.00           & 4               &   1.00         &   1.00        &   1.00         &   1.00         \\
%                         & FAMA                       & 4               & 1.00           & 1.00          & 1.00          &1.00           & 4               &  1.00          & 1.00          & 1.00           &  1.00          \\
% \hline

% \multirow{2}{*}{Hanoi} & PI-SAM                     & 1               & \textbf{1.00}           & 1.00          &  1.00         & 1.00          & 1               & \textbf{ 1.00}          & 1.00          &  1.00          &  1.00          \\
%                         & FAMA                       & 1               & 0.85           & 1.00          & 1.00          & 1.00          & 1               &  0.80          & 1.00          &  1.00          & 1.00           \\
% \hline

% \multirow{2}{*}{Npuzzle} & PI-SAM                     & 1               & 1.00           & 1.00          &1.00           & 1.00          & 1               &  \textbf{1.00}          &   1.00        &   1.00         &  1.00          \\
%                         & FAMA                       & 1               & 1.00           & 1.00          & 1.00          & 1.00          & 1               & 0.80           &1.00           & 1.00           &  1.00          \\
% \hline

% \multirow{2}{*}{Parking} & PI-SAM                     & 5               & \textbf{1.00}           & 0.85          & 1.00          & 1.00          & 6               & \textbf{1.00}           & 0.80          & \textbf{1.00}           &  1.00          \\
%                         & FAMA                       & 5               & 0.83           & 0.85          & 1.00          &  1.00         & 6               & 0.80           & \textbf{0.85}          &  0.89          &  1.00          \\
% \hline

% \multirow{2}{*}{Sokoban} & PI-SAM                     & 2               & 1.00           &1.00           &1.00           &1.00           & 2               &1.00            & 1.00          & 1.00           & 1.00           \\
%                         & FAMA                       & 2               &1.00            & 1.00          & 1.00          &1.00           & 2               &  1.00          &1.00          & 1.00           &  1.00          \\
% \hline

% \multirow{2}{*}{Transport} & PI-SAM                     & 3               & \textbf{1.00}           & \textbf{0.83}          &\textbf{1.00}           & 0.90          & 4               &  \textbf{1.00}          & 0.80          &  \textbf{1.00}          & 0.90           \\
%                         & FAMA                       & 3               &  0.75          &0.80           & 0.80          &0.90           & 4               & 0.80           & 0.80          &  0.83          &  0.90          \\
% \hline
% \end{tabular}
% %}
% \caption{Empirical precision and recall of PI-SAM and FAMA under random masking probability $\eta = 0.1$ and $\eta = 0.3$}
% \label{tab:prec-rec}
% \end{table*}
% We also compared the number of transitions required to correctly learn the preconditions (i.e., $P(\pre)$ and $R(\pre) = 1.0$)
% when using PI-SAM with $\eta\in\{0.1, 0.3\}$ and when having full observability and using SAM.
% % the original SAM algorithm \cite{juba2021safe}
% % used by PI-SAM with $\eta\in\{0.1, 0.3\}$ to correctly learn the preconditions (i.e., $P(\pre)$ and $R(\pre) = 1.0$). Here, we ran the SAM algorithm \cite{juba2021safe} on the same trajectories without random masking on a subset of the domains above in which PI-SAM achieved \textit{precision} and \textit{recall} $=1.0$ for the preconditions ($\eta = 0.3$).
% The results are shown in Table \ref{tab:joint-results} (right). As expected, the number of transitions required scales inversely with the random masking probability $\eta^2$, which verifies the tightness of the bound in Theorem \ref{complexity-pisam-thm}.
% The source code of the experiments will be made available upon acceptance.
% \begin{table}[t]
%     \centering
%     \resizebox{\columnwidth}{!}{
%     \begin{tabular}{l|c|c|c}
%     \hline
%     \multirow{2}{*}{Domain} & \multicolumn{3}{c}{\# triplets}  \\
%     \cline{2-4}
%                         & SAM ($\eta = 1.0$) & PI-SAM ($\eta = 0.3$)    & PI-SAM ($\eta = 0.1$)                 \\
%     \hline
%     Ferry                    & 4    & 15              &    42          \\
%     Gripper                    & 5    & 18            &    55           \\
%     Hanoi                    & 1    & 4               &     38       \\
%     Npuzzle                    & 1    & 4             &     36         \\
%     Sokoban                    & 6    & 22            &     64         \\

%     \hline
%     \end{tabular}
%     }
%     \caption{Number of state-action triplets needed to learn the preconditions}
%     \label{tab:pisam_sam}
% \end{table}

% We also investigated the advantage of EPI-SAM on the other subset of the domains (\{Blocks, Depot, Floortile, Parking, Transport\}). These are the more complicated domains (more fluents and actions) in which neither PI-SAM nor FAMA learned the preconditions exactly. EPI-SAM does not necessarily determine the state of fluents, so we used unit propagation to compute a subset of effects that must hold, and used the resulting action model instead of a contingent planner. The results are recorded in Table \ref{tab:episam}. EPI-SAM performed as well as PI-SAM  on the Blocks, Depot, and Parking domains and achieved slightly better results on the Floortile and Transport domains.  Note that EPI-SAM also has the advantage of providing guarantees on domains that don't satisfy the \textit{bounded concealment assumption}, which is not exploited here.

% \begin{table}[t]
%     \centering
%     \resizebox{\columnwidth}{!}{
%     \begin{tabular}{l|l|c|c|c|c}
%     \hline
%     Domain & Algorithm & P (\pre) & R (\pre) & P (\eff) & R (\eff) \\
%     \hline
%     \multirow{2}{*}{Blocks}    & EPI-SAM   &1.00 &0.90 & 1.00 & 0.95           \\
%     \cline{2-6}
%                           & PI-SAM    &1.00 &0.90 & 1.00 & 0.95          \\
%     \hline
%     \multirow{2}{*}{Depot}     & EPI-SAM   & 1.00 &0.85 & 1.00 & 1.00          \\
%     \cline{2-6}
%                           & PI-SAM    & 1.00 &0.85 & 1.00 &1.00           \\
%     \hline
%     \multirow{2}{*}{Floortile} & EPI-SAM   &1.00 & \textbf{0.89} & 1.00 & \textbf{0.90  }         \\
%     \cline{2-6}
%                           & PI-SAM    &1.00 & 0.85 & 1.00 & 0.87           \\
%     \hline
%     \multirow{2}{*}{Parking}   & EPI-SAM   &1.00 &0.85 & 1.00 &1.00           \\
%     \cline{2-6}
%                           & PI-SAM    &1.00 &0.85 & 1.00 &1.00           \\
%     \hline
%     \multirow{2}{*}{Transport} & EPI-SAM   &1.00 & \textbf{0.85} & 1.00  & \textbf{0.92 }          \\
%     \cline{2-6}
%                           & PI-SAM    &1.00 & 0.83 & 1.00  & 0.90                \\
%     \hline


%     \end{tabular}
%     }
%     \caption{Empirical precision and recall of EPI-SAM under random masking probability $\eta = 0.3$}
%     \label{tab:episam}
% \end{table}

\section{}
  rithm \citep{juba2021safe} to partially observable    work on general observations. In practice,  we can choose  observation sets (e.g., whether they satisfy the bounded concealment assumption or not). For future work, we 



% \pagebreak
\clearpage
\bibliography{aaai24}

\end{document}