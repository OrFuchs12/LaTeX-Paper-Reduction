


The trajectory of a plan starts with $s_I$ and ends with a goal state $s_G$ (where $G\subseteq s_G$). 
The \emph{safe model-free planning} problem~\cite{stern2017efficientAndSafe} is defined as follows. 
\begin{definition}[Safe model-free planning]
Let $\Pi=\tuple{\tuple{T, \mathcal{F}, \mathcal{A}, \realm}, O, s_I, G}$ be a classical planning problem and let $\mathcal{T}=\{\mathcal{T}_1,\ldots, \mathcal{T}_m\}$ be a set of trajectories %of plans 
for other planning problems in the same domain. 
The input to a safe model-free planning algorithm is the tuple $\tuple{T,O, s_I, G, \mathcal{T}}$ and the desired output is a plan $\pi$ that is a solution to $\Pi$. We denote this safe model-free planning problem as $\Pi_{\mathcal{T}}$. 
\label{def:safe-model-free-planning}
\end{definition}
We refer to the action model $\realm$ as the real action model. 
The trajectories in $\mathcal{T}$ share the same domain as $\Pi$, 
and thus they have been generated by applying actions from $\mathcal{A}$ 
and following the action model specified in $\realm$. 
However, these trajectories may start in states that are not from $s_I$, 
may end in states that do not satisfy $G$, 
and may consider a set of objects that is different from $O$.  
% $O4. have objects that do not exist in $O$. 
% created for pla
% were created for problems that are different from $\Pi$. 
% This includes having potentially different start states, goals, and set of objects. different from the problem at hand, 
Safety is captured in Definition~\ref{def:safe-model-free-planning} by requiring that the output plan $\pi$ is a \textbf{sound plan} for $\Pi$. That is, $\pi$ is applicable and ends up reaching a state that satisfies the goal.  
The main challenge is that the problem-solver -- the agent -- needs to find a sound plan to $\Pi$ but it is not given the set of fluents, actions, and action model of the domain ($\mathcal{F}$, $\mathcal{A}$, and \realm, respectively). 


% Scope
In this work, we make the following simplifying assumptions. 
Actions have deterministic effects. 
While the agent may not have complete observability, when it does observe a grounded action $a=\tuple{\lifta, b_a}$, it is able to discern that $a$ is the result of grounding $\lifta$ with $b_a$. 
Similarly, if it observes a state with a grounded fluent $f=\tuple{\liftf, b_f}$, it is able to discern that $f$ is the result of grounding $\liftf$ with $b_f$. Also, we assume that actions' preconditions and effects are conjunctions of literals, as opposed to more complex logical statements, and we do not currently consider conditional effects of actions. 
These assumptions are reasonable when planning in digital/virtual environments, such as video games, or environments that have been instrumented with reliable sensors, such as warehouses designed to be navigated by robots~\cite{li2020lifelong}. 
Later, we will discuss approaches to relax these assumptions and apply our work to a broader range of environments. 
% Here we limit the scope

%We discuss how these assumptions can be relaxed later in this paper. TODO: DO we?










\subsection{Contigent Planning}
\subsection{Learning Action Models}
\subsection{Imperfect Observation Model}
\subsection{Problem Definition}
% Objects and types
Let $O$ be a set of objects and let $T$ be a set of types. 
Every object $o\in O$ is associated with a type $t\in T$ denoted $\type(o)$. 
For example, in the logistics domain from the International Planning Competition (IPC)~\cite{ipc} there are types \emph{truck} and \emph{location} and there may be objects $t_1$ and $t_2$ that represent two different trucks and two objects $l_1$ and $l_2$ that represent two different locations. 

\subsection{Lifted and Grounded Literals}
% Lifted and grounded fluent
A \emph{lifted fluent} $\liftf$ is a pair $\tuple{\name, \params}$ representing a relation over typed objects, where  $\name$ is a symbol and $\params$ is a list of types. 
We denote the name of $\liftf$ and its parameters by $\name(\liftf)$ and $\params(\liftf)$ respectively, and $arity(\liftf,t)$ denotes the number of type-$t$ parameters. 
% relation over a list of  \emph{types}.  
% These types are called the \emph{parameters} of $\liftf$ and denoted by $\params(\liftf)$. 
For example, in the logistics domain $at(?truck, ?location)$ is a lifted fluent that represents which trucks ($?truck$) are at which locations ($?location$). 
A \emph{binding} of a lifted fluent $\liftf$ is a function $b: \params(\liftf)\rightarrow O$ 
mapping every parameter of $\liftf$ to an object in $O$ of the indicated type. 
A \emph{grounded fluent} $f$ is a pair $\tuple{\liftf, b}$ where $\liftf$ is a lifted fluent 
and $b$ is a binding for \liftf. 
To \emph{ground} a lifted fluent $\liftf$ with a binding $b$ means to 
create a (Boolean-valued) fluent with a value determined by whether or not the objects in the image of $b$ satisfy the relation associated with the lifted fluent. 
In our logistics example, for $\liftf=at(?truck, ?location)$ and $b=\{?truck: truck1, ?location: loc1\}$ 
the corresponding grounded fluent $f$ is $at(truck1, loc1)$, indicating whether $truck1$ is at $loc1$.
The term \emph{literal} refers to either a fluent or its negation. 
The definitions of binding, lifted, and grounded fluents transfer naturally to literals. 
A \emph{state} of the world is a set of grounded literals that, for every grounded fluent, either includes that fluent or its negation. 


% Actions, parameter binding, grounded action
\subsection{Lifted and Grounded Actions}
A lifted action $\lifta\in \mathcal{A}$ is a pair $\tuple{\name, \params}$ 
where $\name$ is a symbol and $\params$ is a list of types, 
denoted $\name(\lifta)$ and $\params(\lifta)$, respectively, and $arity(\lifta,t)$ denotes the number of type-$t$ parameters. 
The action model $M$ for a set of actions $\mathcal{A}$ 
is a pair of functions $\pre_M$ and $\eff_M$ that map every action in $\mathcal{A}$ to its preconditions and effects. 
To define the preconditions and effects of a lifted action, 
we first define the notion of a \emph{parameter-bound literal}. 
A \emph{parameter binding} of a lifted literal $\liftl$ and an action $\lifta$ is a function $b_{\liftl,\lifta}: \params(\liftl)\rightarrow \params(\lifta)$ that maps every parameter of $\liftl$ to a parameter in $\lifta$. 
A \emph{parameter-bound literal} $l$ for the lifted action $\lifta$ is a 
pair of the form $\tuple{\liftl,b_{\liftl,\lifta}}$ where $b_{\liftl,\lifta}$ is a parameter binding of $\liftl$ and $\lifta$. 
$\pre_M(\lifta)$ and $\eff_M(\lifta)$ are sets of parameter-bound literals for $\lifta$. 
% We assume here that all parameter bindings are injective, i.e., 
% every parameter of every parameter-bound literal is bound to one of the corresponding action's parameters. 




A \emph{binding} of a lifted action $\lifta$ is defined like a binding of a lifted fluent, i.e., a function $b:\params(\lifta)\rightarrow O$. 
A \emph{grounded action} $a$ is a tuple $\tuple{\lifta, b_\lifta}$ where $\lifta$ is a lifted action and $b_\lifta$ is a binding of $\lifta$. 
The preconditions of a grounded action $a$ according to the action model $M$, denoted $\pre_M(a)$, is the set of grounded literals created by taking every parameter-bound literal $\tuple{\liftl, b_{\liftl,\lifta}}\in \pre_M(\lifta)$ and grounding $\liftl$ with the binding $b_\lifta\circ b_{\liftl,\lifta}$. 
The effects of a grounded action $a$, denoted $\eff_M(a)$, are defined in a similar manner. 
The grounded action $a$ can be applied in a state $s$ iff $\pre_M(a)\subseteq s$. 
The outcome of applying $a$ to a state $s$ according to action model $M$, denoted $a_M(s)$, is a new state that contains all literals in $\eff_M(a)$ and all the literals in $s$ such that their negation is not in $\eff_M(a)$. 
Formally:
\begin{equation}\small
    a_M(s)=\{ l | (l\in s \wedge \neg l\notin \eff_M(a)) \vee l\in \eff_M(a) \} 
\end{equation}
We omit $M$ from $a_M(s)$ when it is clear from the context.
The outcome of applying a sequence of grounded actions $\pi=(a_1,\ldots a_n)$ to a state $s$ is the state $s'=a_n(\cdots a_1(s)\cdots)$. 
A sequence of actions $a_1,\ldots, a_n$ can be applied to a state $s$ 
if for every $i\in 1,\ldots,n$ the action $a_i$ is applicable in the state 
$a_{i-1}(\cdots a_1(s)\cdots)$. 

\begin{definition}[Trajectory]
A trajectory $T=\tuple{s_0, a_1, s_1, \ldots a_n, s_n}$ is an alternating sequence of states $(s_0,\ldots,s_n)$ and actions $(a_1,\ldots,a_n)$ that starts and ends with a state.
\end{definition}
The trajectory created by applying $\pi$ to a state $s$ is 
the sequence $\tuple{s_0, a_1, \ldots, a_{|\pi|}, s_{|\pi|}}$ such that 
$s_0=s$ and for all $0<i\leq |\pi|$, $s_i=a_i(s_{i-1})$. 
In the literature on learning action models~\cite{wang1994learning,wang1995learning,walsh2008efficient,stern2017efficientAndSafe,arora2018review}, 
it is common to represent a trajectory 
$\tuple{s_0, a_1, \ldots, a_{|\pi|}, s_{|\pi|}}$
as a set of triples 
$\big\{\tuple{s_{i-1},a_i,s_i}\big\}_{i=1}^{|\pi|}$.
Each triple $\tuple{s_{i-1},a_i,s_i}$ is called an \emph{action triplet},  and the states $s_{i-1}$ and $s_i$ are referred to as the pre- and post- state of action $a_i$. 
We denote by $\mathcal{T}(a)$ the set of all action triplets in the trajectories in $\mathcal{T}$ that include the grounded action $a$. $\mathcal{T}(\lifta)$ is defined for all action triplets that contain actions that are groundings of the lifted action $\lifta$.  

\subsection{Domains and Problems}

% Planning domain
A classical planning \textbf{domain} is defined by a tuple 
$\tuple{T, \mathcal{F}, \mathcal{A}, M}$
where $T$ is a set of types, 
$\mathcal{F}$ is a set of lifted fluents, 
$\mathcal{A}$ is a set of lifted actions, 
and $M$ is an action model for $\mathcal{A}$.
% Objective
A classical planning \textbf{problem} is defined by a tuple $\tuple{D, O,  s_I, G}$ where $D$ is a classical planning domain;  
$O$ is a set of objects; 
$s_I$ is the start state, i.e., the state of the world before planning;  
and $G$ is a set of grounded literals that define when the goal has been found. 
A \textbf{solution} to a planning problem is a sequence of grounded actions that can be applied to $s_I$ and if applied to $s_I$ results in a state $s_G$ that contains all the grounded literals in $G$. 
Such a sequence of grounded actions is called a \emph{plan}. 
The trajectory of a plan starts with $s_I$ and ends with a goal state $s_G$ (where $G\subseteq s_G$). 









% Later, we will discuss approaches to relax these assumptions and apply our work to a broader range of environments. 
% Here we limit the scope

% While the agent may not have complete observability, when it does observe a grounded action $a=\tuple{\lifta, b_a}$, it is able to discern that $a$ is the result of grounding $\lifta$ with $b_a$. 
% Similarly, if it observes a state with a grounded fluent $f=\tuple{\liftf, b_f}$, it is able to discern that $f$ is the result of grounding $\liftf$ with $b_f$. Also, we assume that actions' preconditions and effects are conjunctions of literals, as opposed to more complex logical statements, and we do not currently consider conditional effects of actions. 
% These assumptions are reasonable when planning in digital/virtual environments, such as video games, or environments that have been instrumented with reliable sensors, such as warehouses designed to be navigated by robots~\cite{li2020lifelong}. 
% Later, we will discuss approaches to relax these assumptions and apply our work to a broader range of environments. 
% % Here we limit the scope
% We assume that the observation function is noise-free, that is, the partial state returned by applying the observation function on a state $s$ is consistent with $s$. 

%We discuss how these assumptions can be relaxed later in this paper. TODO: DO we?



% In this work, we extend Stern and Juba's~\shortcite{stern2017efficientAndSafe} definition of the safe model-free planning problem to explicitly consider lifted representation of actions and literals, which is a common way to represent planning problems. 
% % For the majority of this paper, we will assume the following set of assumptions:
% % \begin{enumerate}
% %     \item \textbf{Observe action binding.} When a the agent observes a grounded action $a=\tuple{\lifta, b_a}$, 
% % it is able to discern that $a$ is the results of grounding $\lifta$ with $b_a$. 
% %     \item  \textbf{Observe literal binding.} When a the agent observes a state in which a literal $l=\tuple{\liftl, b_\liftl}$ is true, it is able to discern that $l$ is the results of grounding $\liftl$ with $b_\liftl$.
% %     \item \textbf{Fully bound preconditions and effects.} 
% %     every precondition and effect of every action
% %     In the domain,  
% % \end{enumerate}
% We assume that when the agent observes a grounded action $a=\tuple{\lifta, b_a}$, 
% it is able to discern that $a$ is the results of grounding $\lifta$ with $b_a$. 
% Similarly, if it observed a state with a grounded fluent $f=\tuple{\liftf, b_f}$, it is able to discern that $f$ is the results of grounding $\liftf$ with $b_f$. We discuss how these assumptions can be relaxed later in this paper. 

% % and $f$ is the result of grounding $\liftf$ with $b_f$. 
% % As we show later, this allows to significant reduce the sample complexity needed to learn a useful action model. 



% This extension raises the question of whether the agent can identify that two grounded actions are groundings of the same lifted action?
% Similarly, when observing two states, 

% and fluents are groundings of the same lifted action and fluent, respectively?
% % Consider a grounded action $a=\tuple{\lifta, b_a}$ and a grounded fluent $f=\tuple{\liftf, b_f}$ that are observed in some given trajectory $\mathcal{T}^i\in\mathcal{T}$. 
% % That is, $a\in \mathcal{T}^i$ and $\exists s\in \mathcal{T}^i$ such that $f\in s$.  
% Without the ability to identify the lifted part, it is difficult, if not impossible to learn anything about the lifted, especially in the context of safe planning, where we do not allow plans to be unsound. 
% Thus, we will focus on setting in which when the agent observes $a=\tuple{\lifta, b_a}$ and $f=\tuple{\liftf, b_f}$ it is able to discern that $a$ is the results of grounding $\lifta$ with $b_a$
% and $f$ is the result of grounding $\liftf$ with $b_f$. 
% As we show later, this allows to significant reduce the sample complexity needed to learn a useful action model. 


% \subsection{Lifted Planning and Logical Subsumption}

% \section{Related Work}  TODO Roni




% ly observable trajectories $\mathcal{T}$, each of which is a partial view of a set of fully observable trajectories induced by an observation function $O$. 


% =\{\mathcal{T}_1,\ldots,\mathcal{T}_n\}$ created by  
% partial view of a set of trajectories $\mathcal{T}$, created by 


% We assume the world in which the planning agent operates in can be defined as a classical planning domain $\tuple{F,A}$, and the problem the agent is tasked to solve is a classical planning problem $\tuple{F,A,I,G}$. 
% The 
% The main challenge in our setting is that the action model $A$ is not given to the planning agent. 
% Instead, it is given a \emph{partially observable} set of trajectories $\mathcal{T}$. 
% These , which are the partial view of trajectories created by executing of plans that solve other problems in the same domain 





% % The \emph{partially observable} set of trajectories we consider in this work are created 
% To capture this partially observability formally, we define an observation function $O$ that accepts a state $s$ in a fully observable trajectory and outputs a partial state. 
n% The partially observable set of toiserajectories $\mathcal{T}$ the 

% % represents the part of the $j^{th}$ state in $\mathcal{T}_i$ that is observable by the agent. 
% % We assume that the observation is noise-free, that is, for every fluent $f$ assigned in $O(\mathcal{T}_i,j)$, 
% % its assigned value is the same as in the corresponding state. 


% Finally, we can define the \emph{safe model-free planning} problem we address in this work. 
% \begin{definition}[Safe model-free planning]
% Let $\Pi=\tuple{F,A,I,G}$ 
% \tuple{T, \mathcal{F}, \mathcal{A}, \realm}, O, s_I, G}$ be a classical planning problem and let $\mathcal{T}=\{\mathcal{T}_1,\ldots, \mathcal{T}_m\}$ be a set of trajectories %of plans 
% for other planning problems in the same domain. 
% The input to a safe model-free planning algorithm is the tuple $\tuple{T,O, s_I, G, \mathcal{T}}$ and the desired output is a plan $\pi$ that is a solution to $\Pi$. We denote this safe model-free planning problem as $\Pi_{\mathcal{T}}$. 
% \label{def:safe-model-free-planning}
% \end{definition}
% We refer to the action model $\realm$ as the real action model. 
% The trajectories in $\mathcal{T}$ share the same domain as $\Pi$, 
% and thus they have been generated by applying actions from $\mathcal{A}$ 
% and following the action model specified in $\realm$. 
% However, these trajectories may start in states that are not from $s_I$, 
% may end in states that do not satisfy $G$, 
% and may consider a set of objects that is different from $O$.  
% % $O4. have objects that do not exist in $O$. 
% % created for pla
% % were created for problems that are different from $\Pi$. 
% % This includes having potentially different start states, goals, and set of objects. different from the problem at hand, 
% Safety is captured in Definition~\ref{def:safe-model-free-planning} by requiring that the output plan $\pi$ is a \textbf{sound plan} for $\Pi$. That is, $\pi$ is applicable and ends up reaching a state that satisfies the goal.  
% The main challenge is that the problem-solver -- the agent -- needs to find a sound plan to $\Pi$ but it is not given the set of fluents, actions, and action model of the domain ($\mathcal{F}$, $\mathcal{A}$, and \realm, respectively). 




% Let $F$, $I$, and $G$ be the set of fluents, initial state, and goal, of the original underlying planning problem we aim to solve. 
% The set of fluents in the Conformant Planning problem we construct includes all fluents in $F$, and an additional fluent $f_{\iseff(a,\ell)}$ for every action $a$ and literal $\ell$. 
% All fluents in $F$ are observable, and all other fluents are not. 
% The initial state formula sets the values of all fluents in $F$ according to $I$, 
% and includes all the clauses in the CNFs returned by EPI-SAM ($\{\cnf_\eff(\ell)\}_\ell$), mapping every literal $\iseff(a,\ell)$ to the fluent $f_{\iseff(a,\ell)}$. 
% The action model includes all actions observed in $\mathcal{T}$. 
% For each action $a$, we set its preconditions to the set of preconditions learned for it by EPI-SAM's learning preconditions part, $\pre(a)$. 
% The effects of $a$ are a set of \emph{conditional effects}. A conditional effect of an action is an effect (i.e., a partial state) that is only applied if a specified condition holds. 
% Conditional effects are supported by many planning languages and standard Conformant Planning formulations~\cite{bonet2010conformant}. 
% For each action $a$ and literal $\ell$, 
% we add a conditional effect such that if 
% $f_{\iseff(a,\ell)}$ is true then $\ell$ is an effect of $a$. 


% The value of this fluent is not set in the initial state, which instead include all the clauses in the CNFs returned by EPI-SAM. 


% % \Pi=\tuple{F,A,I,G}$ be the original underlying planning problem we aim to solve.

% It contains all fluents in our original 


% This mapping is done by adding a fluent $\iseff(a,\ell)$ for every action $a$ and literal $\ell$ to the conformant planning problem. 
% The value of this fluent is not set in the initial state, which instead include all the clauses in the CNFs returned by EPI-SAM. 
% The conformant planner's output is then a sequence of actions guaranteed to reach the goal, subject to any action model consistent with these CNFs. 




% by a tuple $\tuple{F,I,G,A,Z}$ 
% where $F$ is the set of fluents, 
% $I$ is a formula over $F$ that defines the set of possible initial states, 
% $G$ is a formula over $G$ that defines the set of goal states, 
% $A$ is the action model, 
% and $Z\subseteq F$ is the set of \emph{observable} fluents. 


% similarly to a class  
% This mapping is done by adding a fluent $\iseff(a,\ell)$ for every action $a$ and literal $\ell$ to the conformant planning problem. 



% A Conformant Planning problem 
% \paragraph{Safe planning with an impercise action model}
% The EPI-SAM algorithm (Algorithms~\ref{alg:episam-effects} and~\ref{alg:episam-preconditions}) returns an action model in which the effects are represented by CNF formulas. 
% Classical planners are not designed to accept such an action model.
% Instead, we propose to solve our planning problem with the action model returned by EPI-SAM by mapping it to a conformant planning problem. This mapping is done by adding a fluent $\iseff(a,\ell)$ for every action $a$ and literal $\ell$ to the conformant planning problem. 
% The value of this fluent is not set in the initial state, which instead include all the clauses in the CNFs returned by EPI-SAM. 
% The conformant planner's output is then a sequence of actions guaranteed to reach the goal, subject to any action model consistent with these CNFs. 



% \paragraph{Safe planning with an impercise action model}
% The EPI-SAM algorithm (Algorithms~\ref{alg:episam-effects} and~\ref{alg:episam-preconditions}) returns an action model in which the effects are represented by CNF formulas. 
% Classical planners are not designed to accept such an action model.
% Instead, we propose to solve our planning problem with the action model returned by EPI-SAM by mapping it to a conformant planning problem. This mapping is done by adding a fluent $\iseff(a,\ell)$ for every action $a$ and literal $\ell$ to the conformant planning problem. 
% The value of this fluent is not set in the initial state, which instead include all the clauses in the CNFs returned by EPI-SAM. 
% The conformant planner's output is then a sequence of actions guaranteed to reach the goal, subject to any action model consistent with these CNFs. 
% \roni{Important edits above, see if you agree}


% set to be either true of false, 

% we solve our planning problem by mapping it a \emph{contingent planning problem}, and solving it with any off-the-shelf contingent planner. 
% In our work, we use the Contingent Planner \cite{albore2009translation}. The procedure to convert EPI-SAM's output to an appropriate input to the Contingent Planner is presented in the supplementary material. 

% Algorithm \ref{alg:tocontingent} describes how to do this mapping, i.e., how to convert a classical planning problem (without its action model), and the output from EPI-SAM, to an appropriate input to the Contingent Planner \cite{albore2009translantion}. 



% \begin{algorithm}[t]
% \small
% \DontPrintSemicolon
% \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
% \Input{Partially Observed Trajectories $\mathcal{T}$}
% \Output{CNF representation of effects $CNF_{\eff}(l)$ for each literal $l$ and precondition $\pre(a)$ for each action $a$}
% \BlankLine
%     \ForEach{ literal $l$}{
%         $CNF_{\eff}(l) \gets \emptyset$\\
%         % \ForEach{trajectory $\{s_0, a_1, s_1, ..., a_k, s_k\}\in\mathcal{T}$ where $s_0(l) = false$ and $s_k(l) = true$}{
%         \ForEach{\roni{maximal} sequence $\{s_0, a_1, s_1, \ldots, a_k, s_k\}\subseteq \mathcal{T}_i\in\mathcal{T}$ 
%         where $s_k(l) = true$, $s_1(l),\ldots,s_{k-1}(l)$ unobserved}{
%             \uIf{$s_0(l) = false $}{
%                 $CNF_{\eff}(l) \gets CNF_{\eff}(l) \wedge (\iseff(l, a_1)  \vee   \iseff(l, a_2) \vee .. \vee \iseff(l, a_k))$
%             }
        
%             \ForEach{$j=1$ to $k-1$}{
%                 $CNF_{\eff}(l) \gets CNF_{\eff}(l) \wedge (\neg\iseff(\neg l, a_j) \vee \iseff(l, a_{j+1}) \vee\ldots\vee \iseff(l, a_k)) $
%             }
%         }
%         \ForEach{action $a$}{
%             $CNF_{\eff}(l) \gets CNF_{\eff}(l) \wedge (\neg \iseff(l, a) \vee \neg \iseff(\neg l, a))$
%         }
%     } 
%     \lForEach{ action $a$}{$\pre(a)\gets$ all literals}
%     \ForEach{ literal $l$, action $a$}{
%         $\mathcal{T}_{l,a}\gets\emptyset$\\
%         \ForEach{trajectory     $T\in\mathcal{T}$}{
%             Initialize $T_{l,a}\gets T$\\
%             \ForEach{step $t$ with $a_t=a$}{
%                 \lIf{$s_{t-1}(l)$ is false}{
%                 Delete $l$ from $\pre(a)$ and skip to the next $(l,a)$ pair.\nllabel{episam-easy-pre-delete}
%                 }
%                 \lElse{Set $s_{t-1}(l)$ true in $T_{l,a}$.}
%             }
%             Put $T_{l,a}$ in $\mathcal{T}_{l,a}$
%         }
%         \While{Some $a'$ occurs in $\tuple{s_1,a',s'_1}$ and $\tuple{s_2,a',s'_2}$ in trajectories in $\mathcal{T}_{l,a}$ with observed $s'_1(l)\neq s'_2(l)$}{
%         \nllabel{episam-action-delete-loop}
%             \lIf{For some $\tuple{s,a',s'}$ in $\mathcal{T}_{l,a}$ $s(l)$ with observed $s(l)\neq s'(l)$}{
%                 Delete $l$ from $\pre(a)$ and skip to the next $(l,a)$.\nllabel{episam-hard-pre-delete}
%             }
%             \lElse{Delete all occurrences of $a'$ from $\mathcal{T}_{l,a}$\nllabel{episam-action-delete}}
%         }
% \iffalse{
% %%
% %% OLD SAT-BASED ALGORITHM BELOW
% %%
%         $\pre(a)\gets $ all literals \nllabel{line:init_pre_epi} \\
%         $\varphi_{l,a}\gets \emptyset$\\
%         \ForEach{trajectory $T=\{s_0, a_1, s_1, ..., a_k, s_k\}\in\mathcal{T}$ where $a_k = a$}{
%             $i \gets k$\\
%             $\varphi_{l,a,T}\gets \emptyset$ \roni{Should we initialize $\varphi$ here or only once per action and literal}\\
%             \While{$i>0$}{
%                 \uIf{$l$ is observed in $s_{i-1}$}{
%                     $\varphi_{l,a,T} \gets \varphi_{l,a,T} \lor (\neg \iseff(\neg l,a_i) \wedge \neg \iseff(\neg l, a_{i+1}) \wedge\cdots\wedge \neg \iseff(\neg l, a_{k-1}))$\\
%                     break\\
%                 }
%                 \uElseIf{$\neg l$ is observed in $s_{i-1}$}{
%                     break
%                 }
%                 \Else{
%                     $\varphi_{l,a,T} \gets \varphi_{l,a,T} \lor (\iseff( l,a_i) \wedge \neg \iseff(\neg l, a_{i+1}) \wedge\cdots\wedge \neg \iseff(\neg l, a_{k-1}))$\\
%                     $i \gets i-1 $\\
%                 }
%             }
%             $\varphi_{l,a}\gets ??$\\
%         }
%         \uIf{ $\neg \varphi_{l,a,T} \wedge CNF_{\eff}(l) $ is not satisfiable}{
%             Remove $\neg l$ from $\pre(a)$
%         }
% %%%
% %%% END OLD SAT-BASED ALGORITHM
% %%%

%         % If $\exist T$ such that $\neg \varphi_{l,a,T} \wedge CNF_{\eff}(l)$ is not SAT, 
%         % then $\neg l$ cannot be a precondition of $a$. 
%         % CNF_\eff = M1 or M2
%         % T1 l was not true before a if M1 is true
%         % T2 l was not true before a if M2 is true
%         % \uIf{ $\neg \varphi_{l,a,T} \wedge CNF_{\eff}(l) $ is not satisfiable}{
%         %     Remove $\neg l$ from $\pre(a)$
%         % }
% }\fi
%     }
%     % \ForEach{ literal $l$, action $a$}{
%     %     $\pre(a)\gets $ all literals \nllabel{line:init_pre_epi} \\
%     %     \ForEach{trajectory $\{s_0, a_1, s_1, ..., a_k, s_k\}\in\mathcal{T}$ where $a_k = a$}{
%     %         $i \gets k$\\
%     %         $\varphi\gets \emptyset$ \roni{Should we initialize $\varphi$ here or only once per action and literal}\\
%     %         \While{$i>0$}{
%     %             \uIf{$l$ is observed in $s_{i-1}$}{
%     %                 $\varphi \gets \varphi \lor (\neg \iseff(\neg l,a_i) \wedge \neg \iseff(\neg l, a_{i+1}) \wedge\cdots\wedge \neg \iseff(\neg l, a_{k-1}))$\\
%     %                 break\\
%     %             }
%     %             \uElseIf{$\neg l$ is observed in $s_{i-1}$}{
%     %                 break
%     %             }
%     %             \Else{
%     %                 $\varphi \gets \varphi \lor (\iseff( l,a_i) \wedge \neg \iseff(\neg l, a_{i+1}) \wedge\cdots\wedge \neg \iseff(\neg l, a_{k-1}))$\\
%     %                 $i \gets i-1 $\\
%     %             }
%     %         }
%     %     }
%     %     \uIf{ $\neg \varphi \wedge CNF_{\eff}(l) $ is not satisfiable}{
%     %         Remove $\neg l$ from $\pre(a)$
%     %     }
%     % }

% \caption{Extended PI-SAM Learning Algorithm (EPI-SAM)}\label{alg:episam}
% \end{algorithm}





% \begin{algorithm}[t]
% \small
% \DontPrintSemicolon
% \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
% \SetKwBlock{LearnEffects}{LearnEffects}{end}
% \SetKwBlock{Irrelevant}{Irrelevant}{end}
% \SetKwBlock{LearnPreconditions}{LearnPreconditions}{end}
% \SetKwBlock{Main}{Main}{end}
% \Input{Partially observed trajectories $\mathcal{T}$}
% \Output{CNF representation of effects $CNF_{\eff}(l)$ for each literal $l$ and precondition $\pre(a)$ for each action $a$}
% \LearnEffects{
%     \ForEach{ literal $l$}{
%         $\cnf_{\eff}(l) \gets \emptyset$\\
%         \ForEach{action $a$}{
%             Add to $\cnf_{\eff}(l)$: $\left\{\neg \iseff(l, a) \vee \neg \iseff(\neg l, a)\right\}$
%         }
%         % \ForEach{trajectory $\{s_0, a_1, s_1, ..., a_k, s_k\}\in\mathcal{T}$ where $s_0(l) = false$ and $s_k(l) = true$}{
%         \ForEach{$\tuple{s,a,s'}\in \mathcal{T}_i\in\mathcal{T}$ where $s'(l)=\true$}{
%             $T\gets$ max. prefix of $\tuple{s,a,s'}$ where $l$ is masked\\
%             % SAM Rule 3: if l changed from false to true, it must be an effect of one of the actions
%             \uIf{$T[1].s(l) = \false $}{
%                 Add to $\cnf_{\eff}(l)$:  $\left\{\iseff(l, T[1].a)  \vee   \cdots \vee \iseff(l, T[|T|].a)\right\}$
%             }
            
%             % SAM Rule 2: \neg l cannot be an effect
%             Add to $\cnf_{\eff}(l)$:  $\{\neg \iseff(\neg l, a)\}$ \\
%             % Ensure if l is deleted it will be added afterwards
%             \ForEach{$j=1$ to $|T|-1$}{
%                 Add to $\cnf_{\eff}(l)$: $\left\{\neg\iseff(\neg l, T[j].a) \vee \iseff(l, T[j+1].a) \vee \cdots \vee \iseff(l, T[|T|].a)\right\}$ 
%             }
%         }
%     }
%     \Return $\{\cnf_\eff(l)\}_l$
% }
% \Irrelevant{
%     % \Input{action $a$, literal $l$, set $\cnf_\eff(l)$}
%     \Input{action $a$, literal $l$, set $\mathcal{T}$}
%     \Output{$\true$ ~iff $a$ does not affect the value of $l$}
%     \ForEach{$\tuple{s_1,a,s_1'}, \tuple{s_2,a,s_2'}$ in $\mathcal{T}$}{
%         \If{$s_1\neq s_2$ and $s_1'(l)\neq s_2'(l)$}{
%             \Return $\true$
%         }
%     }
%     \Return $\false$
%     % \uIf{$\{\neg\iseff(l)\}\notin \cnf_\eff(l) \vee \{\neg\iseff(\neg l)\}\notin \cnf_\eff(l)$}{
%     %     \Return \false
%     % }
%     % \uIf{$\{\neg\iseff(\neg l)\}\notin \cnf_\eff(l)$}{
%     %     \Return \false
%     % }
%     % \uIf{$\left\{\{\neg\iseff(l)\}, \{\neg\iseff(\neg l)\}\right\}\nsubseteq \cnf_\eff(l)$}{
%     %     \Return \false
%     % }
%     % \Return \true
% }
% \LearnPreconditions{
%     \lForEach{action $a$}{$\pre(a)\gets$ all literals}
%     \ForEach{literal $l$, action $a$}{
%         \uIf{$\exists \tuple{s,a,s'}\in \mathcal{T}_i\in\mathcal{T}$ where $\neg l\in s$}{
%             % SAM Rule 1: if \neg l is in s then it cannot be a precondition of a
%             Delete $l$ from $\pre(a)$\\
%             Continue to the next $(l,a)$ pair\\
%         }
        
%         % Assume l is a precondition of a. 
%         % Create a copy of all trajectories under this assumption
%         $\mathcal{T}_{l,a}\gets\emptyset$\\
%         \ForEach{trajectory $\mathcal{T}_i\in\mathcal{T}$}{
%             Initialize $T_{l,a}\gets \emptyset$\\
%             \ForEach{$\tuple{s,a,s'}\in \mathcal{T}_i\in\mathcal{T}$}{
%                 Set $s(l)$ to true in $T_{l,a}$
%             }
%             Add $T_{l,a}$ to $\mathcal{T}_{l,a}$
%         }
%         % Remove actions that are known to not have any effect on the value of l
%         $A_{irr}\gets\emptyset$\\
%         \While{$\exists a'\notin A_{irr}$ where \textbf{Irrelevant}($a'$,$l$,$\mathcal{T}_{l,a}$)}{
%             \nllabel{episam-action-delete-loop}
%             % Implement the assumption that $a'$ is irrelevant to l\\
%             \ForEach{$\tuple{s,a',s'}$ in $\mathcal{T}_{l,a}$}{
%                 \lIf{$s(l)=\unobserved$}{
%                     $s(l)\gets s'(l)$
%                 }
%                 \lIf{$s'(l)=\unobserved$}{
%                     $s'(l)\gets s(l)$
%                 }
%                 \uIf{$s(l)\neq s'(l)$}{
%                     Remove $l$ from $\pre(a)$\\
%                     Continue to the next $(l,a)$ pair\nllabel{episam-hard-pre-delete}
%                 }
%             }
%             Add $a'$ to $A_{irr}$\\
%         }
%     }
%         % \roni{If we get to here all actions not in $A_{irr}$ always have after them the same value or $\unobserved$}
%         % \roni{Question: is this sufficient to infer that $l$ may be a precondition of $a$?}\brendan{It's sufficient to ensure that there exists a consistent action model, which in turn implies that $l$ could be a precondition of $a$: this could actually be the action model, and in that case, $l$ could be a precondition since it is always true in $a$'s pre-state under this action model.}
%         % \roni{There seem to be some assumption about how the output of the learning effects algorithm be on the set of trajectories after all the pruning. But what is that assumption? is it that it is satisfiable?}\brendan{I guess yes, you could express it as that the EPI-SAM formula encoding of the possible effects we would get (after setting $l$ true in the pre-states of $a$) is satisfiable. We guarantee that there exists a consistent action model, by setting those actions' literals consistent with the observed values. Note that this may not be the action model we would learn using PI-SAM (say) since it may be that no effect is necessary for some action, if the literal isn't changing value since its last observation.}
%         % \roni{This is perfect. I finally got it during the weekend and your explanation above "closed" it for me. I'll work on the proof today.}
%     \Return $\{\pre(a)\}_a$
% }
% \caption{EPI-SAM with Roni's Edits}\label{alg:episam-roni}
% \end{algorithm}





% \section{Attempt}
% A trajectory $T'$ is called a subtrajectory of a trajectory $T=(s_0,a_1,\ldots, a_n, s_n)$  
% if there exists $i$ and $j$ such that $T'=(s_{i-1},a_{i},\ldots,a_j, s_j)$. 
% We denote this subtrajectory by $T[i,j]$. 
% A trajectory is said to be \emph{unobservable} with respect to literal $l$ 
% if $l$ is masked in all the states in $T$.  
% For a trajectory $T$, we say that $T[i,j]$ is a a \emph{maximaly unobservable subtrajectory with respect to literal $l$} 
% if $l$ is masked in $T[i,j]$ and if $1<i$ then $l$ is not masked in $i-1$ 
% and if $j<|T|-1$ then $l$ is not masked in $j+1$. 




% \begin{algorithm}[t]
% \small
% \DontPrintSemicolon
% \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
% \Input{Partially-observed trajectories $\mathcal{T}$}
% \Output{CNF representation of effects $CNF_{\eff}(l)$ for each literal $l$, or $\false$ if no action model is consistent with $\mathcal{T}$}
% \BlankLine
%     \tcc{Init $CNF_\eff$ with the PI-SAM rules}
%     \ForEach{fluent $f$}{
%         $CNF_{\eff}(f) \gets \emptyset$\\
%     }
%     \ForEach{transition $\tuple{s,a,s'}$}{
%         \ForEach{fluent $f$}{
%             \lIf{$s'(f)=\true$}{
%                 Add $\neg \iseff(a, \neg f)$ to $CNF_{\eff}(f)$\\
%                 \lIf{$s(f)=\false$}{
%                     Add $\iseff(a,f)$ to $CNF_{\eff}(f)$
%                 }
%             }
%             \lIf{$s'(f)=\false$}{
%                 Add $\neg \iseff(a, f)$ to $CNF_{\eff}(f)$\\
%                 \lIf{$s(f)=\true$}{
%                     Add $\iseff(a,\neg f)$ to $CNF_{\eff}(f)$
%                 }
%             }
%         }
%     }
% \caption{Generalized EPI-SAM Effects Learning)}\label{alg:episam-effects}
% \end{algorithm}


% Input: Pre(.), CNF$_{eff}$, 
% Output: Contingent PDDL domain
% Input: CNF for literal
% Output: relevant oneof statements in the problem definition


% Link to contignent planner stuff:
% https://www.upf.edu/web/ai-ml/clg-contingent-planner

% \begin{algorithm}[t]
% \small
% \DontPrintSemicolon
% \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
% \Input{Original problem $\Pi=\tuple{F,A,I,G}$, 
% $Pre(a)$ for every action $a$,
% $CNF_{\eff}(l)$ for every literal $l$}
% \Output{Contingent planning problem $\Pi'$}

% \ForEach{action $a$}{Create $obs_a$}
% \ForEach{literal $l$}{Create $obs_l$}
% \ForEach{pair of action and literal}{Create a fluent \texttt{(is-eff $obj_a$, $obj_l$)}} 

% % Stuff to add to the domain file
% \ForEach{observed action $a$}{
%     Add an action to the domain with $Pre(a)$ as its preconditions\\
%     \ForEach{literal $l$}{
%         Add a conditional effect of the form:
%         \texttt{(when (is-eff $obj_a$ $obj_l$) $l$)}
%     }
% }

% % Stuff to add to the problem file
% %The problem contains all fluents from the original problem and the following fluents:
% \ForEach{literal $l$}{
%     \ForEach{clause $ C=(\sigma_1 \iseff(a_1,l) \vee \sigma_2 \iseff(a_2,l) \vee ... \vee \sigma_k \iseff(a_k,l)) \in \varphi$ ($\sigma_i$ is identity or $\neg$)}{
%     %-- i.e., if it is identity, no symbol appears for $\sigma_i$; if it's negation, ``not'' appears for $\sigma_i$ below
%         Add to problem file:\\
%         $\left(\textit{oneof}~~ \sigma_1 \iseff(a_1,l) 
%         ~\sigma_2 \iseff(a_2,l)
%         \cdots
%         ~\sigma_k \iseff(a_k,l) \right)$
%         }
%     }
% \caption{Action Model to Contingent PDDL}\label{alg:tocontingent}
% \end{algorithm}



% For a literal $l$, action $a$, and trajectory T=$\tuple{s_0,\ldots a_k}$ where $a_k=$, the corresponding $\varphi$ formula 
% describes all cases where $l$ was true before $a_k$ 
% So, it should be something like this: \varphi_{l,a,T}
% varph_{l,a}=\vee_T \varphi_{l,a,T}
