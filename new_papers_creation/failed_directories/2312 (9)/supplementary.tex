\section{Implementation Details}
We employ pre-trained I3D~\cite{qian_locate_2022} and C3D~\cite{c3d} models to extract video frame features for Charades-STA and ActivityNet-Captions, respectively. We uniformly sample $T\!=
\!128$ features per video to ensure a fixed length. 
During the pseudo-query generation phase, we employ a Faster R-CNN object detector that is trained on objects enumerated in VisualGenome~\cite{krishna_visual_2017}. We employ a top-$k$ strategy to sample the most probable nouns found in the video segment. We choose a \(k\) value of $5$ based on the experimental analysis by \citet{nam_zero-shot_2021}. 
As for the CEM module, we rely on ConceptNet \cite{speer_conceptnet_2017} for commonsense information and extract the English sub-graph for our experiments. Moreover, we prune the commonsense graph \(G_{C}\) by preserving edge types that convey relevant contextual information, as detailed in Table 1 in the main paper.
We randomly initialize the weights for the GCN-based concept encoder.
Experiments for the balancing hyperparameter $\lambda$, spanning a range of $\lambda \in \{0.75, 0.7, 0.3, 0.25\}$ show that performance is consistently high across all metrics for $\lambda=0.7$, indicating the relative importance of temporal attention-guided loss over the overall localization regression loss.

\section{Ablation Studies}
\label{sec:ablations}
We further present ablation studies that focus on the Commonsense Enhancement Module (CEM) design and the overall efficacy of commonsense integration. Unless specified otherwise, we perform ablations on Charades-STA~\cite{Gao_2017_ICCV} and \modelname with 300 seed concepts.

\input{supp_sections/qcc}
\input{supp_sections/cem}
\input{supp_sections/prepost}
\input{supp_sections/relational}
\input{supp_sections/hops}
\input{supp_sections/encoder}
\input{supp_sections/lfvl}

