\input{supp_sections/tables/encoders}
\subsection{How to Best Encode Inputs?}
\label{ablation:encoder}
We also investigate the impact of adopting a recurrent architecture (GRU/LSTM) \vs Transformers~\cite{vaswani_attention_2017} for generating the video $V$ and pseudo-query $Q$ encodings. Table \ref{tab:encoderAblation} quantitatively compares model performance under such encoding variants for \modelname.
While Transformer-based methods are more than twice as fast as recurrent methods, they surprisingly impede model performance by large margins across most metrics.
