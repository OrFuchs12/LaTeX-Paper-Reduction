\subsection{Does Auxiliary Commonsense Information Boost Performance?}
\label{ablation:hops}
\input{supp_sections/tables/hops}
We analyze the impact of including auxiliary contextual information provided through \(G_{C}\). We examine the performance of \modelname by replacing the proposed seed concept graph \(G_{C}\) with a bigger $1$-hop neighborhood graph. Since including a 1-hop neighborhood leads to an exponential increase in the graph size, we limit $G_C$ to include 1-hop neighborhood only with edge types that add valuable information to the localization setup. Specifically, we include edge types that may involve action information (\ie, verbs) in relation to the objects observed in our video corpus (\eg, $UsedFor$, $CapableOf$, $Causes$, \etc). 
Figure~\ref{fig:ablationHops}
shows the relative performance of this model variant in comparison to the original seed concept (0-hop) graph. Performance consistently worsens across all metrics for both 300 and 250 seed concept sizes, with more drastic drops for 300 concept sizes. We hypothesize that, despite the increased context via a larger graph, the additional information may prove to be noisy, thereby affecting localization accuracy as well as generalization capabilities.