\subsection{Which Modality to Enhance with Commonsense?}
\label{ablation:cem}
To analyze the importance of commonsense enhancement across modalities, we train \modelname with the following configurations: (1) Only query features $Q$ are enhanced (\textbf{Q}), (2) Only video features $V$ are enhanced (\textbf{V}). In addition, we employ two configurations for which both video and query are enhanced. \modelname's CEM makes use of the same concept vectors, but employs separate enhancement steps for video and text query, \ie, we rely on two separate sub-modules $\phi_{C_{\text{vid}}}(V)$ and $\phi_{C_{\text{pq}}}(Q)$. This design choice stems from the hypothesis that the gap between video and query modalities is exacerbated when dealing with less sophisticated queries. Essentially, less sophisticated (or more general) queries may lack specificity or fail to capture the nuances of the desired information accurately. As a result, the gap between the information contained in the video and the intended query widens, making it more challenging to match the two modalities effectively. 
Having separate projection matrices for video and query allows differently enhancing $V$ and $Q$  with the same commonsense information (through the same concept vectors). 
To test this rationale, (3) we train \modelname with shared weights for $V$ and $Q$, \ie, $\phi_{C_{\text{vid}}}(V)$ and $\phi_{C_{\text{pq}}}(Q)$ are identical (\textbf{VQ}). Finally, (4) we represent the original setting of separate enhancement mechanisms for $V$ and $Q$ as \textbf{V+Q}.

\input{supp_sections/tables/cemWeights}
Table \ref{tab:weightsCEM} presents results for the aforementioned configurations. We observe a significant drop in performance with \textbf{Q} across all metrics, which shows that the localization abilities are negatively impacted by omitting the video feature enhancement. To further support this observation, we see a consistent increase across all metrics for \textbf{V}, where only video features are enhanced and query feature enhancement is omitted. This highlights the positive impact of incorporating important commonsense information in the visual context for boosting model performance. 
Furthermore, we observe a consistent deterioration in model performance across all metrics in \textbf{VQ} except for $R@0.7$ when compared to \textbf{V+Q}. This could be attributed to the fact that a common enhancement flow for $V$ and $Q$ may potentially collapse diverging sources of information into one latent representation. Separating the enhancement for the two modalities allows disentangling the learned latent representations for video and pseudo-query, thereby capturing different relationships, but with the same underlying commonsense knowledge. Finally, \textbf{V+Q} performing the best across all the aforementioned configurations validates our hypothesis of maintaining separate enhancement flows for video and text query features.
