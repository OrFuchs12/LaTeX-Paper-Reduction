\relax 
\bibstyle{aaai24}
\citation{ho2020denoising,song2020score}
\citation{nichol2021improved,song2020denoising}
\citation{choi2021ilvr}
\citation{dhariwal2021diffusion,avrahami2022blended}
\citation{kwon2022diffusion}
\citation{rombach2022high,zhang2023adding}
\citation{lugmayr2022repaint}
\citation{meng2021sdedit}
\citation{ho2022cascaded}
\citation{nichol2021glide,kwon2022diffusion,hertz2022prompt}
\citation{nichol2021glide,yang2023paint}
\citation{avrahami2022blended}
\citation{couairon2022diffedit}
\citation{matsunaga2022fine}
\citation{kwon2022diffusion,preechakul2022diffusion}
\citation{kim2022diffusionclip,kawar2023imagic}
\citation{hertz2022prompt,kawar2023imagic}
\citation{kwon2022diffusion}
\citation{mokady2023null}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig1}{{1}{1}{Reconstruction and editing results under various levels of inversion and denoising steps. While increasing steps makes reconstruction nearly perfect, the outcomes of editing still remain far from satisfactory (attribute: smiling).}{}{}}
\citation{david2016hypernetworks}
\citation{kim2022diffusionclip,kwon2022diffusion}
\citation{song2020score}
\citation{ho2020denoising,song2020score,lipman2022flow}
\citation{choi2021ilvr}
\citation{meng2021sdedit}
\citation{lugmayr2022repaint}
\citation{nichol2021glide,yang2023paint,avrahami2022blended}
\citation{kwon2022diffusion}
\citation{kim2022diffusionclip,hertz2022prompt,kawar2023imagic}
\citation{mao2023guided}
\citation{gal2022image,mokady2023null}
\citation{song2020denoising}
\citation{goodfellow2020generative}
\citation{richardson2021encoding}
\citation{abdal2020image2stylegan++}
\citation{zhu2020domain}
\citation{tov2021designing}
\citation{alaluf2021restyle}
\citation{pehlivan2023styleres}
\citation{wang2022high}
\citation{li2023reganie}
\citation{alaluf2022hyperstyle}
\citation{zhang2022unsupervised}
\citation{mokady2023null}
\citation{ho2022classifier}
\citation{zhang2022unsupervised}
\citation{dhariwal2021diffusion}
\citation{zhang2022unsupervised}
\citation{david2016hypernetworks}
\citation{ronneberger2015u}
\citation{alaluf2022hyperstyle}
\citation{song2020denoising}
\citation{ho2020denoising}
\citation{alaluf2022hyperstyle}
\citation{alaluf2022hyperstyle,wang2022high}
\newlabel{fig2}{{2}{3}{Overview of our proposed rectifier framework. The rectifier is a hypernetwork consisting of a global encoder and multiple subnet branches. It takes as input the original image $\bm  {x}_0$ and the estimation at each step ($\mathbb  {P}_t[\bm  {\epsilon }_t^\theta (\bm  {x}_t)]$), targets to modulate the degraded residual features into offset weights, providing compensated information for high-fidelity reconstruction. We select the middle and up-sampling blocks of U-Net for modulate, considering that these blocks contain both high-level semantic information and low-level details. We also employ separable convolution to reduce the amount of generated parameters.}{}{}}
\citation{kim2022diffusionclip,kwon2022diffusion}
\citation{song2020score}
\citation{ho2020denoising}
\citation{song2020score}
\citation{kwon2022diffusion}
\citation{kim2022diffusionclip}
\citation{gal2022stylegan}
\citation{kim2022diffusionclip}
\citation{ho2020denoising}
\citation{karras2019style}
\citation{karras2017progressive}
\citation{choi2020stargan}
\citation{karras2020training}
\citation{yu2015lsun}
\newlabel{fig3}{{3}{4}{Editing training strategy. Instead of shifting from previous edited results in a Markovian style used in DiffusionCLIP (a), which may lead to error propagation, we start from the original trajectory at each step to find editing direction (b), further alleviating error accumulation caused in editing process.}{}{}}
\newlabel{alg:algorithm}{{1}{4}{Editing Training Strategy}{}{}}
\citation{nichol2021improved}
\citation{song2020score}
\citation{yu2015lsun}
\citation{kwon2022diffusion}
\citation{kim2022diffusionclip}
\citation{nichol2021glide}
\citation{huang2020curricularface}
\newlabel{tab:1}{{1}{5}{Quantitative results of image reconstruction.}{}{}}
\newlabel{tab:2}{{2}{5}{Quantitative comparisons of editing. We compare different methods with the identity similarity between original and edited images.}{}{}}
\newlabel{fig4}{{4}{5}{Comparison of reconstruction quality under 50 steps. Our method is more robust to occlusions (1st column), illuminations (2nd column), viewpoints (3rd and 4th columns), and performs better at restoring coarse shapes (5th column) and preserving fine details (6th column).}{}{}}
\newlabel{fig5}{{5}{6}{Editing qualitative comparisons. Our method delivers realistic edits while maintaining low distortion and high fidelity.}{}{}}
\newlabel{fig6}{{6}{6}{Effects of different editing training strategies. Different methods are evaluated across various ranges of editing intervals, "original" denotes default configuration, while "full" refers to editing through the entire denoising process. View with better clarity when zoomed-in.}{}{}}
\newlabel{fig7}{{7}{6}{The influence of incorporating rectifier into SDEdit. The rectifier makes translation results more lifelike and realistic, as well as exhibiting richer texture and details. No extra domain specific training are employed.}{}{}}
\citation{meng2021sdedit}
\newlabel{fig8}{{8}{7}{Generalize our method to out-of-domain images. Our model trained only on FFHQ successfully adapts to images from METFACES, performing well on oil paintings and sculptures which possess intricate and unique textures unseen in FFHQ.}{}{}}
\bibdata{aaai24}
\citation{he2016deep}
\citation{alaluf2022hyperstyle,pehlivan2023styleres,alaluf2021restyle}
\citation{ho2020denoising}
\citation{karras2017progressive}
\citation{kwon2022diffusion}
\citation{kim2022diffusionclip}
\newlabel{fig9}{{9}{30}{Detailed model architecture of rectifier (a) and its subnet (b).}{}{}}
\citation{choi2020stargan}
\citation{nichol2021glide}
\citation{meng2021sdedit}
\citation{meng2021sdedit}
\citation{karras2020training}
\citation{karras2020training}
\newlabel{fig10}{{10}{31}{Reconstruction under different loss functions. $e \nobreakspace  {}loss$ accomplishes both restoring overall shapes and preserving details compared to other loss functions.}{}{}}
\newlabel{fig11}{{11}{31}{Exploration on hyper-parameters.}{}{}}
\newlabel{fig12}{{12}{32}{Visual comparisons of reconstructions and editings, under 50 inversion and denoising steps.}{}{}}
\newlabel{fig13}{{13}{33}{Visual comparisons of reconstructions and editings, under 200 inversion and denoising steps. As the reconstruction quality for most images are quite well over 200 steps, this could also be seen as an ablation study toward the effectiveness of our editing training strategy alone.}{}{}}
\newlabel{fig14}{{14}{33}{Visual comparisons of reconstructions and editings under 1000 steps.}{}{}}
\newlabel{fig15}{{15}{34}{Visual comparisons of editings of AFHQ-Dogs. GLIDE achieves great detail preservation due to its mask mechanism, but sometimes causes inconsistency and unsatisfying results in semantic editing.}{}{}}
\newlabel{fig16}{{16}{35}{Image samples for calculating identity similarity.}{}{}}
\newlabel{fig17}{{17}{36}{Applications on image-to-image translation, compared to SDEdit\citep  {meng2021sdedit}. Every two columns of translation results share common random seed. }{}{}}
\newlabel{fig18}{{18}{36}{Applications on out-of-domain images editing on various attributes performed on METFACES \citep  {karras2020training}.}{}{}}
\gdef \@abspage@last{36}
