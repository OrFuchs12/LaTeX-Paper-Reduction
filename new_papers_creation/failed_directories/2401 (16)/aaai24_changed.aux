\relax 
\bibstyle{aaai24}
\citation{yi2018neuralSymbolic}
\citation{Mao2019TheNC}
\citation{Johnson2016CLEVRAD}
\citation{wu2017neural,yi2018neuralSymbolic}
\citation{Andreas2015NeuralMN}
\citation{Agrawal2015VQAVQ}
\citation{GrundeMcLaughlin2022AGQA2}
\citation{GrundeMcLaughlin2021AGQA,GrundeMcLaughlin2022AGQA2}
\citation{Wu2021STAR}
\citation{Gao2018MotionAppearanceCN}
\citation{Zhang2019OpenEndedLV,Li2019BeyondRP,Kumar2019LeveragingTA}
\citation{Xu2017VideoQA,Gao2018MotionAppearanceCN,Fan2019HeterogeneousME,Kim2019ProgressiveAM}
\citation{Jin2021AdaptiveSG,seo2021attend,Xiao2021VideoAC,Cherian2022251D,Park2021BridgeTA,Zhao2022Collaborative}
\citation{Lei2021LessIM,Fu2021VIOLETE,Zellers2021MERLOTMN,Zellers2022MERLOTRN,Wang2023Vstar}
\citation{Alayrac2022FlamingoAV,Li2023VideoChatCV,Zhang2023VideoLLaMAAI,Lyu2023MacawLLMML}
\citation{Kim2020ModalitySA,Gao2022MISTMI,Li2022InvariantGF}
\citation{yi2019clevrer}
\citation{Qian2022DynamicSM}
\citation{Ding2021DynamicVR,chen2021grounding}
\citation{Andreas2015NeuralMN,Hu2017LearningTR,Johnson2017InferringAE,Mascharka2018TransparencyBD,Hu2018ExplainableNC}
\citation{Andreas2015NeuralMN}
\citation{Johnson2016CLEVRAD}
\citation{Gupta2023VisualProgramming}
\citation{Radford2021LearningTV}
\citation{Brown2020LanguageMA}
\citation{rombach2021highresolution}
\citation{Le2022VGNMNVN,Qian2022DynamicSM}
\citation{Qian2022DynamicSM}
\citation{sigurdsson2016hollywood,Ji2019ActionGA}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:overview}{{1}{3}{Overview of STAIR.}{}{}}
\citation{wei2021finetunedLM}
\citation{Hu2017LearningTR}
\citation{GrundeMcLaughlin2021AGQA}
\citation{sigurdsson2016hollywood}
\citation{Ji2019ActionGA}
\citation{GrundeMcLaughlin2022AGQA2}
\citation{Le2020HierarchicalCR}
\citation{He2015DeepRL}
\citation{Xie2016AggregatedRT}
\citation{Carreira2017QuoVA}
\newlabel{fig:intermediate_supervision}{{2}{4}{A Diagram of Intermediate Supervision.}{}{}}
\citation{Fan2019HeterogeneousME}
\citation{Le2020HierarchicalCR}
\citation{Li2019BeyondRP}
\citation{Qian2022DynamicSM}
\citation{hudson2018compositional}
\citation{Qian2022DynamicSM}
\citation{Qian2022DynamicSM}
\citation{Qian2022DynamicSM}
\newlabel{tab:agqa1}{{1}{5}{Results of AGQA. \dag  : Results from \citep  {Qian2022DynamicSM}. \#Prm denotes number of parameters. \#Prm of MAC varies slightly with its number of steps, here we show \#Prm of a 12-step model.}{}{}}
\newlabel{tab:agqa2}{{2}{5}{Results of AGQA2.}{}{}}
\newlabel{tab:interpret}{{3}{5}{Performances of Filter, Localize and Temopral modules.}{}{}}
\citation{Radford2019LanguageMA}
\citation{Fu2021VIOLETE}
\citation{Li2020BridgingTA}
\citation{Maaz2023VideoChatGPTTD}
\citation{Maaz2023VideoChatGPTTD}
\citation{Wu2021STAR}
\citation{Wu2021STAR}
\citation{Gao2018MotionAppearanceCN}
\citation{Fan2019HeterogeneousME}
\citation{Le2020HierarchicalCR}
\citation{Lei2021LessIM}
\citation{Wu2021STAR}
\citation{Xu2017VideoQA}
\citation{wei2021finetunedLM}
\citation{Lin2022TowardsFA}
\citation{Mao2019TheNC}
\newlabel{sec:other_task}{{}{6}{}{}{}}
\newlabel{tab:pretrain}{{4}{7}{Results of AGQA and AGQA2, comparing with pre-trained models.}{}{}}
\newlabel{tab:zs_agqa2}{{5}{7}{Results of Video-ChatGPT on AGQA2.}{}{}}
\newlabel{tab:star}{{6}{7}{Accuracy on STAR test set, categorized by question type. \dag  : Results from \citep  {Wu2021STAR}}{}{}}
\newlabel{tab:msrvttqa}{{7}{7}{Accuracy on MSRVTT-QA test set.}{}{}}
\bibdata{aaai24}
\citation{Wu2018UnsupervisedFL}
\citation{Le2020HierarchicalCR}
\citation{He2015DeepRL}
\newlabel{fig:case}{{3}{13}{Examples of a successful case (left) and a failing case (right). In the first case, the person is touching things throughout the video, so the \texttt  {ExistsFrame} module returns a uniform distribution on all the frames. The last 2 things the person touches are phone and tissue, though \texttt  {Filter} module only finds one correct answer ``phone'', but as it is not equal to the choice ``table'', so \texttt  {Equals} module returns the correct final answer ``No''. In the second case, \texttt  {Localize} module successfully finds when the person is taking some clothes and \texttt  {ExistsFrame} module successfully finds when the person is on the side of something, but \texttt  {Filter} module fails to recognize the exact thing that is on the side of the person (probably due to low video quality and the pillow is blocked by the body). Outputs of \texttt  {FilterFrame} modules are too complex to be visualized.}{}{}}
\newlabel{tab:modules}{{8}{14}{Intentions and implementation details of all modules. There are 4 types of variables: $v \in \mathbb  {R}^{T \times H}$ denotes a video feature, which contains a feature vector of size $\mathbb  {R}^H$ for each of the $T$ frames; $t \in \mathbb  {R}^H$ denotes a text feature, which is usually an action/object or a set of actions/objects; $a \in \mathbb  {R}^T$ denotes an attention map over the frames; and $s$ denotes a keyword that switches between the branches in a module. $sum(\cdot , n)$ denotes summing a tensor on dimension $n$, $conv(\cdot )$ denotes a 1-D convolutional network, and $\{\dotsc  \}$ denotes a list of items. Activation functions and dropouts are omitted to avoid cluttering. Freq. denotes the average number of occurrences of the module in one program in train and valid set of AGQA.}{}{}}
\gdef \@abspage@last{14}
