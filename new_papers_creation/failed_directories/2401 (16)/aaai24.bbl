\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Agrawal et~al.(2015)Agrawal, Lu, Antol, Mitchell, Zitnick, Parikh, and Batra}]{Agrawal2015VQAVQ}
Agrawal, A.; Lu, J.; Antol, S.; Mitchell, M.; Zitnick, C.~L.; Parikh, D.; and Batra, D. 2015.
\newblock VQA: Visual Question Answering.
\newblock \emph{International Journal of Computer Vision}, 123: 4--31.

\bibitem[{Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, Ring, Rutherford, Cabi, Han, Gong, Samangooei, Monteiro, Menick, Borgeaud, Brock, Nematzadeh, Sharifzadeh, Binkowski, Barreira, Vinyals, Zisserman, and Simonyan}]{Alayrac2022FlamingoAV}
Alayrac, J.-B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Hasson, Y.; Lenc, K.; Mensch, A.; Millican, K.; Reynolds, M.; Ring, R.; Rutherford, E.; Cabi, S.; Han, T.; Gong, Z.; Samangooei, S.; Monteiro, M.; Menick, J.; Borgeaud, S.; Brock, A.; Nematzadeh, A.; Sharifzadeh, S.; Binkowski, M.; Barreira, R.; Vinyals, O.; Zisserman, A.; and Simonyan, K. 2022.
\newblock Flamingo: a Visual Language Model for Few-Shot Learning.
\newblock \emph{ArXiv}, abs/2204.14198.

\bibitem[{Andreas et~al.(2015)Andreas, Rohrbach, Darrell, and Klein}]{Andreas2015NeuralMN}
Andreas, J.; Rohrbach, M.; Darrell, T.; and Klein, D. 2015.
\newblock Neural Module Networks.
\newblock \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 39--48.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}]{Brown2020LanguageMA}
Brown, T.~B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.~J.; Child, R.; Ramesh, A.; Ziegler, D.~M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020.
\newblock Language Models are Few-Shot Learners.
\newblock \emph{ArXiv}, abs/2005.14165.

\bibitem[{Carreira and Zisserman(2017)}]{Carreira2017QuoVA}
Carreira, J.; and Zisserman, A. 2017.
\newblock Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 4724--4733.

\bibitem[{Chen et~al.(2021)Chen, Mao, Wu, Wong, Tenenbaum, and Gan}]{chen2021grounding}
Chen, Z.; Mao, J.; Wu, J.; Wong, K.-Y.~K.; Tenenbaum, J.~B.; and Gan, C. 2021.
\newblock Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Cherian et~al.(2022)Cherian, Hori, Marks, and Roux}]{Cherian2022251D}
Cherian, A.; Hori, C.; Marks, T.~K.; and Roux, J.~L. 2022.
\newblock (2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 444--453.

\bibitem[{Ding et~al.(2021)Ding, Chen, Du, Luo, Tenenbaum, and Gan}]{Ding2021DynamicVR}
Ding, M.; Chen, Z.; Du, T.; Luo, P.; Tenenbaum, J.~B.; and Gan, C. 2021.
\newblock Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language.
\newblock In \emph{Neural Information Processing Systems}.

\bibitem[{Fan et~al.(2019)Fan, Zhang, Zhang, Wang, Zhang, and Huang}]{Fan2019HeterogeneousME}
Fan, C.; Zhang, X.; Zhang, S.; Wang, W.; Zhang, C.; and Huang, H. 2019.
\newblock Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering.
\newblock \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 1999--2007.

\bibitem[{Fu et~al.(2021)Fu, Li, Gan, Lin, Wang, Wang, and Liu}]{Fu2021VIOLETE}
Fu, T.-J.; Li, L.; Gan, Z.; Lin, K.; Wang, W.~Y.; Wang, L.; and Liu, Z. 2021.
\newblock VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling.
\newblock \emph{ArXiv}, abs/2111.12681.

\bibitem[{Gao et~al.(2022)Gao, Zhou, Ji, Zhu, Yang, and Shou}]{Gao2022MISTMI}
Gao, D.; Zhou, L.; Ji, L.; Zhu, L.; Yang, Y.; and Shou, M.~Z. 2022.
\newblock MIST: Multi-modal Iterative Spatial-Temporal Transformer for Long-form Video Question Answering.
\newblock \emph{ArXiv}, abs/2212.09522.

\bibitem[{Gao et~al.(2018)Gao, Ge, Chen, and Nevatia}]{Gao2018MotionAppearanceCN}
Gao, J.; Ge, R.; Chen, K.; and Nevatia, R. 2018.
\newblock Motion-Appearance Co-memory Networks for Video Question Answering.
\newblock \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 6576--6585.

\bibitem[{Grunde-McLaughlin, Krishna, and Agrawala(2021)}]{GrundeMcLaughlin2021AGQA}
Grunde-McLaughlin, M.; Krishna, R.; and Agrawala, M. 2021.
\newblock AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning.
\newblock \emph{2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 11282--11292.

\bibitem[{Grunde-McLaughlin, Krishna, and Agrawala(2022)}]{GrundeMcLaughlin2022AGQA2}
Grunde-McLaughlin, M.; Krishna, R.; and Agrawala, M. 2022.
\newblock AGQA 2.0: An Updated Benchmark for Compositional Spatio-Temporal Reasoning.
\newblock \emph{ArXiv}, abs/2204.06105.

\bibitem[{Gupta and Kembhavi(2023)}]{Gupta2023VisualProgramming}
Gupta, T.; and Kembhavi, A. 2023.
\newblock Visual programming: Compositional visual reasoning without training.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 14953--14962.

\bibitem[{He et~al.(2015)He, Zhang, Ren, and Sun}]{He2015DeepRL}
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2015.
\newblock Deep Residual Learning for Image Recognition.
\newblock \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 770--778.

\bibitem[{Hu et~al.(2018)Hu, Andreas, Darrell, and Saenko}]{Hu2018ExplainableNC}
Hu, R.; Andreas, J.; Darrell, T.; and Saenko, K. 2018.
\newblock Explainable Neural Computation via Stack Neural Module Networks.
\newblock In \emph{European Conference on Computer Vision}.

\bibitem[{Hu et~al.(2017)Hu, Andreas, Rohrbach, Darrell, and Saenko}]{Hu2017LearningTR}
Hu, R.; Andreas, J.; Rohrbach, M.; Darrell, T.; and Saenko, K. 2017.
\newblock Learning to Reason: End-to-End Module Networks for Visual Question Answering.
\newblock \emph{2017 IEEE International Conference on Computer Vision (ICCV)}, 804--813.

\bibitem[{Hudson and Manning(2018)}]{hudson2018compositional}
Hudson, D.~A.; and Manning, C.~D. 2018.
\newblock Compositional Attention Networks for Machine Reasoning.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Ji et~al.(2019)Ji, Krishna, Fei-Fei, and Niebles}]{Ji2019ActionGA}
Ji, J.; Krishna, R.; Fei-Fei, L.; and Niebles, J.~C. 2019.
\newblock \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 10233--10244.

\bibitem[{Jin et~al.(2021)Jin, Zhao, Cao, Zhu, He, and Zhuang}]{Jin2021AdaptiveSG}
Jin, W.; Zhao, Z.; Cao, X.; Zhu, J.; He, X.; and Zhuang, Y. 2021.
\newblock Adaptive Spatio-Temporal Graph Enhanced Vision-Language Representation for Video QA.
\newblock \emph{IEEE Transactions on Image Processing}, 30: 5477--5489.

\bibitem[{Johnson et~al.(2016)Johnson, Hariharan, van~der Maaten, Fei-Fei, Zitnick, and Girshick}]{Johnson2016CLEVRAD}
Johnson, J.; Hariharan, B.; van~der Maaten, L.; Fei-Fei, L.; Zitnick, C.~L.; and Girshick, R.~B. 2016.
\newblock CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 1988--1997.

\bibitem[{Johnson et~al.(2017)Johnson, Hariharan, van~der Maaten, Hoffman, Fei-Fei, Zitnick, and Girshick}]{Johnson2017InferringAE}
Johnson, J.; Hariharan, B.; van~der Maaten, L.; Hoffman, J.; Fei-Fei, L.; Zitnick, C.~L.; and Girshick, R.~B. 2017.
\newblock Inferring and Executing Programs for Visual Reasoning.
\newblock \emph{2017 IEEE International Conference on Computer Vision (ICCV)}, 3008--3017.

\bibitem[{Kim et~al.(2019)Kim, Ma, Kim, Kim, and Yoo}]{Kim2019ProgressiveAM}
Kim, J.; Ma, M.; Kim, K.; Kim, S.; and Yoo, C.~D. 2019.
\newblock Progressive Attention Memory Network for Movie Story Question Answering.
\newblock \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 8329--8338.

\bibitem[{Kim et~al.(2020)Kim, Ma, Pham, Kim, and Yoo}]{Kim2020ModalitySA}
Kim, J.; Ma, M.; Pham, T.~X.; Kim, K.; and Yoo, C.~D. 2020.
\newblock Modality Shifting Attention Network for Multi-Modal Video Question Answering.
\newblock \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 10103--10112.

\bibitem[{Kumar et~al.(2019)Kumar, Okur, Sahay, Huang, and Nachman}]{Kumar2019LeveragingTA}
Kumar, S.~H.; Okur, E.; Sahay, S.; Huang, J.; and Nachman, L. 2019.
\newblock Leveraging Topics and Audio Features with Multimodal Attention for Audio Visual Scene-Aware Dialog.
\newblock \emph{ArXiv}, abs/1912.10131.

\bibitem[{Le, Chen, and Hoi(2022)}]{Le2022VGNMNVN}
Le, H.; Chen, N.~F.; and Hoi, S. C.~H. 2022.
\newblock VGNMN: Video-grounded Neural Module Networks for Video-Grounded Dialogue Systems.
\newblock In \emph{North American Chapter of the Association for Computational Linguistics}.

\bibitem[{Le et~al.(2020)Le, Le, Venkatesh, and Tran}]{Le2020HierarchicalCR}
Le, T.~M.; Le, V.; Venkatesh, S.; and Tran, T. 2020.
\newblock Hierarchical Conditional Relation Networks for Video Question Answering.
\newblock \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 9969--9978.

\bibitem[{Lei et~al.(2021)Lei, Li, Zhou, Gan, Berg, Bansal, and Liu}]{Lei2021LessIM}
Lei, J.; Li, L.; Zhou, L.; Gan, Z.; Berg, T.~L.; Bansal, M.; and Liu, J. 2021.
\newblock Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling.
\newblock \emph{2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 7327--7337.

\bibitem[{Li et~al.(2023)Li, He, Wang, Li, Wang, Luo, Wang, Wang, and Qiao}]{Li2023VideoChatCV}
Li, K.; He, Y.; Wang, Y.; Li, Y.; Wang, W.; Luo, P.; Wang, Y.; Wang, L.; and Qiao, Y. 2023.
\newblock VideoChat: Chat-Centric Video Understanding.
\newblock \emph{ArXiv}, abs/2305.06355.

\bibitem[{Li et~al.(2019)Li, Song, Gao, Liu, Huang, He, and Gan}]{Li2019BeyondRP}
Li, X.; Song, J.; Gao, L.; Liu, X.; Huang, W.; He, X.; and Gan, C. 2019.
\newblock Beyond RNNs: Positional Self-Attention with Co-Attention for Video Question Answering.
\newblock In \emph{AAAI Conference on Artificial Intelligence}.

\bibitem[{Li et~al.(2022)Li, Wang, Xiao, Ji, and Chua}]{Li2022InvariantGF}
Li, Y.; Wang, X.; Xiao, J.; Ji, W.; and Chua, T.-S. 2022.
\newblock Invariant Grounding for Video Question Answering.
\newblock \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2918--2927.

\bibitem[{Li et~al.(2020)Li, Li, Zhang, Feng, Niu, and Zhou}]{Li2020BridgingTA}
Li, Z.; Li, Z.; Zhang, J.; Feng, Y.; Niu, C.; and Zhou, J. 2020.
\newblock Bridging Text and Video: A Universal Multimodal Transformer for Audio-Visual Scene-Aware Dialog.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 29: 2476--2483.

\bibitem[{Lin et~al.(2022)Lin, Tiwari, Huang, Li, Shou, Ji, and Chang}]{Lin2022TowardsFA}
Lin, X.; Tiwari, S.; Huang, S.; Li, M.; Shou, M.~Z.; Ji, H.; and Chang, S.-F. 2022.
\newblock Towards Fast Adaptation of Pretrained Contrastive Models for Multi-channel Video-Language Retrieval.
\newblock \emph{ArXiv}, abs/2206.02082.

\bibitem[{Lyu et~al.(2023)Lyu, Wu, Wang, Huang, Liu, Du, Shi, and Tu}]{Lyu2023MacawLLMML}
Lyu, C.; Wu, M.; Wang, L.; Huang, X.; Liu, B.; Du, Z.; Shi, S.; and Tu, Z. 2023.
\newblock Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration.
\newblock \emph{ArXiv}, abs/2306.09093.

\bibitem[{Maaz et~al.(2023)Maaz, Rasheed, Khan, and Khan}]{Maaz2023VideoChatGPTTD}
Maaz, M.; Rasheed, H.; Khan, S.; and Khan, F.~S. 2023.
\newblock Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models.
\newblock \emph{ArXiv}, abs/2306.05424.

\bibitem[{Mao et~al.(2019)Mao, Gan, Kohli, Tenenbaum, and Wu}]{Mao2019TheNC}
Mao, J.; Gan, C.; Kohli, P.; Tenenbaum, J.~B.; and Wu, J. 2019.
\newblock The Neuro-Symbolic Concept Learner: Interpreting Scenes Words and Sentences from Natural Supervision.
\newblock \emph{ArXiv}, abs/1904.12584.

\bibitem[{Mascharka et~al.(2018)Mascharka, Tran, Soklaski, and Majumdar}]{Mascharka2018TransparencyBD}
Mascharka, D.; Tran, P.; Soklaski, R.; and Majumdar, A. 2018.
\newblock Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning.
\newblock \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 4942--4950.

\bibitem[{Park, Lee, and Sohn(2021)}]{Park2021BridgeTA}
Park, J.; Lee, J.; and Sohn, K. 2021.
\newblock Bridge to Answer: Structure-aware Graph Interaction Network for Video Question Answering.
\newblock \emph{2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 15521--15530.

\bibitem[{Qian et~al.(2022)Qian, Wang, Duan, Chen, and Zhu}]{Qian2022DynamicSM}
Qian, Z.; Wang, X.; Duan, X.; Chen, H.; and Zhu, W. 2022.
\newblock Dynamic Spatio-Temporal Modular Network for Video Question Answering.
\newblock \emph{Proceedings of the 30th ACM International Conference on Multimedia}.

\bibitem[{Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever}]{Radford2021LearningTV}
Radford, A.; Kim, J.~W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; Krueger, G.; and Sutskever, I. 2021.
\newblock Learning Transferable Visual Models From Natural Language Supervision.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever}]{Radford2019LanguageMA}
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019.
\newblock Language Models are Unsupervised Multitask Learners.

\bibitem[{Rombach et~al.(2021)Rombach, Blattmann, Lorenz, Esser, and Ommer}]{rombach2021highresolution}
Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2021.
\newblock High-Resolution Image Synthesis with Latent Diffusion Models.
\newblock arXiv:2112.10752.

\bibitem[{Seo et~al.(2021)Seo, Kang, Park, and Zhang}]{seo2021attend}
Seo, A.; Kang, G.-C.; Park, J.; and Zhang, B.-T. 2021.
\newblock Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, 6167--6177.

\bibitem[{Sigurdsson et~al.(2016)Sigurdsson, Varol, Wang, Farhadi, Laptev, and Gupta}]{sigurdsson2016hollywood}
Sigurdsson, G.~A.; Varol, G.; Wang, X.; Farhadi, A.; Laptev, I.; and Gupta, A. 2016.
\newblock Hollywood in homes: Crowdsourcing data collection for activity understanding.
\newblock In \emph{European Conference on Computer Vision}, 510--526. Springer.

\bibitem[{Wang et~al.(2023)Wang, Zheng, Zhao, Li, Wang, and Zhao}]{Wang2023Vstar}
Wang, Y.; Zheng, Z.; Zhao, X.; Li, J.; Wang, Y.; and Zhao, D. 2023.
\newblock {VSTAR}: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, 5036--5048.

\bibitem[{Wei et~al.(2022)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le}]{wei2021finetunedLM}
Wei, J.; Bosma, M.; Zhao, V.; Guu, K.; Yu, A.~W.; Lester, B.; Du, N.; Dai, A.~M.; and Le, Q.~V. 2022.
\newblock Finetuned Language Models are Zero-Shot Learners.

\bibitem[{Wu et~al.(2021)Wu, Yu, Chen, Tenenbaum, and Gan}]{Wu2021STAR}
Wu, B.; Yu, S.; Chen, Z.; Tenenbaum, J.; and Gan, C. 2021.
\newblock STAR: A Benchmark for Situated Reasoning in Real-World Videos.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)}.

\bibitem[{Wu, Tenenbaum, and Kohli(2017)}]{wu2017neural}
Wu, J.; Tenenbaum, J.~B.; and Kohli, P. 2017.
\newblock Neural scene de-rendering.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 699--707.

\bibitem[{Wu et~al.(2018)Wu, Xiong, Yu, and Lin}]{Wu2018UnsupervisedFL}
Wu, Z.; Xiong, Y.; Yu, S.~X.; and Lin, D. 2018.
\newblock Unsupervised Feature Learning via Non-parametric Instance Discrimination.
\newblock \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 3733--3742.

\bibitem[{Xiao et~al.(2021)Xiao, Yao, Liu, Li, Ji, and Chua}]{Xiao2021VideoAC}
Xiao, J.; Yao, A.; Liu, Z.; Li, Y.; Ji, W.; and Chua, T.-S. 2021.
\newblock Video as Conditional Graph Hierarchy for Multi-Granular Question Answering.
\newblock In \emph{AAAI Conference on Artificial Intelligence}.

\bibitem[{Xie et~al.(2016)Xie, Girshick, Doll{\'a}r, Tu, and He}]{Xie2016AggregatedRT}
Xie, S.; Girshick, R.~B.; Doll{\'a}r, P.; Tu, Z.; and He, K. 2016.
\newblock Aggregated Residual Transformations for Deep Neural Networks.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 5987--5995.

\bibitem[{Xu et~al.(2017)Xu, Zhao, Xiao, Wu, Zhang, He, and Zhuang}]{Xu2017VideoQA}
Xu, D.; Zhao, Z.; Xiao, J.; Wu, F.; Zhang, H.; He, X.; and Zhuang, Y. 2017.
\newblock Video Question Answering via Gradually Refined Attention over Appearance and Motion.
\newblock \emph{Proceedings of the 25th ACM international conference on Multimedia}.

\bibitem[{Yi et~al.(2019)Yi, Gan, Li, Kohli, Wu, Torralba, and Tenenbaum}]{yi2019clevrer}
Yi, K.; Gan, C.; Li, Y.; Kohli, P.; Wu, J.; Torralba, A.; and Tenenbaum, J.~B. 2019.
\newblock CLEVRER: Collision Events for Video Representation and Reasoning.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Yi et~al.(2018)Yi, Wu, Gan, Torralba, Kohli, and Tenenbaum}]{yi2018neuralSymbolic}
Yi, K.; Wu, J.; Gan, C.; Torralba, A.; Kohli, P.; and Tenenbaum, J. 2018.
\newblock Neural-symbolic vqa: Disentangling reasoning from vision and language understanding.
\newblock \emph{Advances in Neural Information Processing Systems}, 31.

\bibitem[{Zellers et~al.(2022)Zellers, Lu, Lu, Yu, Zhao, Salehi, Kusupati, Hessel, Farhadi, and Choi}]{Zellers2022MERLOTRN}
Zellers, R.; Lu, J.; Lu, X.; Yu, Y.; Zhao, Y.; Salehi, M.; Kusupati, A.; Hessel, J.; Farhadi, A.; and Choi, Y. 2022.
\newblock MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound.
\newblock \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 16354--16366.

\bibitem[{Zellers et~al.(2021)Zellers, Lu, Hessel, Yu, Park, Cao, Farhadi, and Choi}]{Zellers2021MERLOTMN}
Zellers, R.; Lu, X.; Hessel, J.; Yu, Y.; Park, J.~S.; Cao, J.; Farhadi, A.; and Choi, Y. 2021.
\newblock MERLOT: Multimodal Neural Script Knowledge Models.
\newblock In \emph{Neural Information Processing Systems}.

\bibitem[{Zhang, Li, and Bing(2023)}]{Zhang2023VideoLLaMAAI}
Zhang, H.; Li, X.; and Bing, L. 2023.
\newblock Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding.
\newblock \emph{ArXiv}, abs/2306.02858.

\bibitem[{Zhang et~al.(2019)Zhang, Zhao, Lin, Song, and He}]{Zhang2019OpenEndedLV}
Zhang, Z.; Zhao, Z.; Lin, Z.; Song, J.; and He, X. 2019.
\newblock Open-Ended Long-Form Video Question Answering via Hierarchical Convolutional Self-Attention Networks.
\newblock In \emph{International Joint Conference on Artificial Intelligence}.

\bibitem[{Zhao et~al.(2022)Zhao, Wang, Tao, Wang, and Zhao}]{Zhao2022Collaborative}
Zhao, X.; Wang, Y.; Tao, C.; Wang, C.; and Zhao, D. 2022.
\newblock Collaborative Reasoning on Multi-Modal Semantic Graphs for Video-Grounded Dialogue Generation.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2022}, 5988--5998.

\end{thebibliography}
