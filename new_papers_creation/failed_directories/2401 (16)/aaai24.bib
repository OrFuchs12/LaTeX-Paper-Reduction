@article{GrundeMcLaughlin2021AGQA,
  title={AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning},
  author={Madeleine Grunde-McLaughlin and Ranjay Krishna and Maneesh Agrawala},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={11282-11292}
}

@inproceedings{sigurdsson2016hollywood,
  title={Hollywood in homes: Crowdsourcing data collection for activity understanding},
  author={Sigurdsson, Gunnar A and Varol, G{\"u}l and Wang, Xiaolong and Farhadi, Ali and Laptev, Ivan and Gupta, Abhinav},
  booktitle={European Conference on Computer Vision},
  pages={510--526},
  year={2016},
  organization={Springer}
}

@article{Ji2019ActionGA,
e={Action Genome: Actions As Compositions of Spatio-Temporal Scene Graphs},
  author={Jingwei Ji and Ranjay Krishna and Li Fei-Fei and Juan Carlos Niebles},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={10233-10244}
}

@article{GrundeMcLaughlin2022AGQA2,
  title={AGQA 2.0: An Updated Benchmark for Compositional Spatio-Temporal Reasoning},
  author={Madeleine Grunde-McLaughlin and Ranjay Krishna and Maneesh Agrawala},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.06105}
}


@article{Le2020HierarchicalCR,
  title={Hierarchical Conditional Relation Networks for Video Question Answering},
  author={Thao Minh Le and Vuong Le and Svetha Venkatesh and T. Tran},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={9969-9978}
}

@article{He2015DeepRL,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={770-778}
}

@article{Xie2016AggregatedRT,
  title={Aggregated Residual Transformations for Deep Neural Networks},
  author={Saining Xie and Ross B. Girshick and Piotr Doll{\'a}r and Zhuowen Tu and Kaiming He},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={5987-5995}
}

@article{Carreira2017QuoVA,
  title={Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset},
  author={Jo{\~a}o Carreira and Andrew Zisserman},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={4724-4733}
}

@article{Qian2022DynamicSM,
  title={Dynamic Spatio-Temporal Modular Network for Video Question Answering},
  author={Zi Qian and Xin Wang and Xuguang Duan and Hong Chen and Wenwu Zhu},
  journal={Proceedings of the 30th ACM International Conference on Multimedia},
  year={2022}
}

@inproceedings{hudson2018compositional,
  title={Compositional Attention Networks for Machine Reasoning},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}

@article{Li2020BridgingTA,
  title={Bridging Text and Video: A Universal Multimodal Transformer for Audio-Visual Scene-Aware Dialog},
  author={Zekang Li and Zongjia Li and Jinchao Zhang and Yang Feng and Cheng Niu and Jie Zhou},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2020},
  volume={29},
  pages={2476-2483}
}

@article{Wu2018UnsupervisedFL,
  title={Unsupervised Feature Learning via Non-parametric Instance Discrimination},
  author={Zhirong Wu and Yuanjun Xiong and Stella X. Yu and Dahua Lin},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={3733-3742}
}

@article{yi2018neuralSymbolic,
  title={Neural-symbolic vqa: Disentangling reasoning from vision and language understanding},
  author={Yi, Kexin and Wu, Jiajun and Gan, Chuang and Torralba, Antonio and Kohli, Pushmeet and Tenenbaum, Josh},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{Mao2019TheNC,
  title={The Neuro-Symbolic Concept Learner: Interpreting Scenes Words and Sentences from Natural Supervision},
  author={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},
  journal={ArXiv},
  year={2019},
  volume={abs/1904.12584}
}

@article{Johnson2016CLEVRAD,
  title={CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},
  author={Justin Johnson and Bharath Hariharan and Laurens van der Maaten and Li Fei-Fei and C. Lawrence Zitnick and Ross B. Girshick},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={1988-1997}
}

@inproceedings{wu2017neural,
  title={Neural scene de-rendering},
  author={Wu, Jiajun and Tenenbaum, Joshua B and Kohli, Pushmeet},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={699--707},
  year={2017}
}

@article{Agrawal2015VQAVQ,
  title={VQA: Visual Question Answering},
  author={Aishwarya Agrawal and Jiasen Lu and Stanislaw Antol and Margaret Mitchell and C. Lawrence Zitnick and Devi Parikh and Dhruv Batra},
  journal={International Journal of Computer Vision},
  year={2015},
  volume={123},
  pages={4-31}
}

% 从这里开始一大段image NMN
@article{Andreas2015NeuralMN,
  title={Neural Module Networks},
  author={Jacob Andreas and Marcus Rohrbach and Trevor Darrell and Dan Klein},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={39-48}
}

@article{Hu2017LearningTR,
  title={Learning to Reason: End-to-End Module Networks for Visual Question Answering},
  author={Ronghang Hu and Jacob Andreas and Marcus Rohrbach and Trevor Darrell and Kate Saenko},
  journal={2017 IEEE International Conference on Computer Vision (ICCV)},
  year={2017},
  pages={804-813}
}

@article{Johnson2017InferringAE,
  title={Inferring and Executing Programs for Visual Reasoning},
  author={Justin Johnson and Bharath Hariharan and Laurens van der Maaten and Judy Hoffman and Li Fei-Fei and C. Lawrence Zitnick and Ross B. Girshick},
  journal={2017 IEEE International Conference on Computer Vision (ICCV)},
  year={2017},
  pages={3008-3017}
}

@article{Mascharka2018TransparencyBD,
  title={Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning},
  author={David Mascharka and Philip Tran and Ryan Soklaski and Arjun Majumdar},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={4942-4950}
}

@inproceedings{Hu2018ExplainableNC,
  title={Explainable Neural Computation via Stack Neural Module Networks},
  author={Ronghang Hu and Jacob Andreas and Trevor Darrell and Kate Saenko},
  booktitle={European Conference on Computer Vision},
  year={2018}
}

@inproceedings{Jiang2019SelfAssemblingMN,
  title={Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning},
  author={Yichen Jiang and Mohit Bansal},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2019}
}

@inproceedings{gupta2019neural,
  title={Neural Module Networks for Reasoning over Text},
  author={Gupta, Nitish and Lin, Kevin and Roth, Dan and Singh, Sameer and Gardner, Matt},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{Le2022VGNMNVN,
  title={VGNMN: Video-grounded Neural Module Networks for Video-Grounded Dialogue Systems},
  author={Hung Le and Nancy F. Chen and Steven C. H. Hoi},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2022}
}

@inproceedings{yi2019clevrer,
  title={CLEVRER: Collision Events for Video Representation and Reasoning},
  author={Yi, Kexin and Gan, Chuang and Li, Yunzhu and Kohli, Pushmeet and Wu, Jiajun and Torralba, Antonio and Tenenbaum, Joshua B},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{Ding2021DynamicVR,
  title={Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language},
  author={Mingyu Ding and Zhenfang Chen and Tao Du and Ping Luo and Joshua B. Tenenbaum and Chuang Gan},
  booktitle={Neural Information Processing Systems},
  year={2021}
}

@inproceedings{chen2021grounding,
  title={Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning},
  author={Chen, Zhenfang and Mao, Jiayuan and Wu, Jiajun and Wong, Kwan-Yee Kenneth and Tenenbaum, Joshua B and Gan, Chuang},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

% Video QA预训练模型
@inproceedings{li2020hero,
  title={HERO: Hierarchical Encoder for Video+ Language Omni-representation Pre-training},
  author={Li, Linjie and Chen, Yen-Chun and Cheng, Yu and Gan, Zhe and Yu, Licheng and Liu, Jingjing},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={2046--2065},
  year={2020}
}

@article{Lei2021LessIM,
  title={Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling},
  author={Jie Lei and Linjie Li and Luowei Zhou and Zhe Gan and Tamara L. Berg and Mohit Bansal and Jingjing Liu},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={7327-7337}
}

@article{Fu2021VIOLETE,
  title={VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling},
  author={Tsu-Jui Fu and Linjie Li and Zhe Gan and Kevin Lin and William Yang Wang and Lijuan Wang and Zicheng Liu},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.12681}
}

@inproceedings{Zellers2021MERLOTMN,
  title={MERLOT: Multimodal Neural Script Knowledge Models},
  author={Rowan Zellers and Ximing Lu and Jack Hessel and Youngjae Yu and Jae Sung Park and Jize Cao and Ali Farhadi and Yejin Choi},
  booktitle={Neural Information Processing Systems},
  year={2021}
}

@article{Zellers2022MERLOTRN,
  title={MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound},
  author={Rowan Zellers and Jiasen Lu and Ximing Lu and Youngjae Yu and Yanpeng Zhao and Mohammadreza Salehi and Aditya Kusupati and Jack Hessel and Ali Farhadi and Yejin Choi},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  pages={16354-16366}
}

@article{Li2022LAVENDERUV,
  title={LAVENDER: Unifying Video-Language Understanding as Masked Language Modeling},
  author={Linjie Li and Zhe Gan and Kevin Lin and Chung-Ching Lin and Zicheng Liu and Ce Liu and Lijuan Wang},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.07160}
}

% Video QA 基于graph
@inproceedings{Jiang2020ReasoningWH,
  title={Reasoning with Heterogeneous Graph Alignment for Video Question Answering},
  author={Pin Jiang and Yahong Han},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020}
}

@article{Jin2021AdaptiveSG,
  title={Adaptive Spatio-Temporal Graph Enhanced Vision-Language Representation for Video QA},
  author={Weike Jin and Zhou Zhao and Xiaochun Cao and Jieming Zhu and Xiuqiang He and Yueting Zhuang},
  journal={IEEE Transactions on Image Processing},
  year={2021},
  volume={30},
  pages={5477-5489}
}

@article{Liu2021HAIRHV,
  title={HAIR: Hierarchical Visual-Semantic Relational Reasoning for Video Question Answering},
  author={Fei Liu and Jing Liu and Weining Wang and Hanqing Lu},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={1678-1687}
}

@inproceedings{seo2021attend,
  title={Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering},
  author={Seo, Ahjeong and Kang, Gi-Cheon and Park, Joonhan and Zhang, Byoung-Tak},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={6167--6177},
  year={2021}
}

@inproceedings{Xiao2021VideoAC,
  title={Video as Conditional Graph Hierarchy for Multi-Granular Question Answering},
  author={Junbin Xiao and Angela Yao and Zhiyuan Liu and Yicong Li and Wei Ji and Tat-Seng Chua},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2021}
}

% Video QA 基于Memory Network
@article{Xu2017VideoQA,
  title={Video Question Answering via Gradually Refined Attention over Appearance and Motion},
  author={D. Xu and Zhou Zhao and Jun Xiao and Fei Wu and Hanwang Zhang and Xiangnan He and Yueting Zhuang},
  journal={Proceedings of the 25th ACM international conference on Multimedia},
  year={2017}
}

@article{Gao2018MotionAppearanceCN,
  title={Motion-Appearance Co-memory Networks for Video Question Answering},
  author={J. Gao and Runzhou Ge and Kan Chen and Ramakant Nevatia},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={6576-6585}
}

@article{Fan2019HeterogeneousME,
  title={Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering},
  author={Chenyou Fan and Xiaofan Zhang and Shu Zhang and Wensheng Wang and Chi Zhang and Heng Huang},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={1999-2007}
}

@article{Kim2019ProgressiveAM,
  title={Progressive Attention Memory Network for Movie Story Question Answering},
  author={Junyeong Kim and Minuk Ma and Kyungsu Kim and Sungjin Kim and Chang Dong Yoo},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={8329-8338}
}

% 花式attention based
@inproceedings{Zhang2019OpenEndedLV,
  title={Open-Ended Long-Form Video Question Answering via Hierarchical Convolutional Self-Attention Networks},
  author={Zhu Zhang and Zhou Zhao and Zhijie Lin and Jingkuan Song and Xiaofei He},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2019}
}

@inproceedings{Li2019BeyondRP,
  title={Beyond RNNs: Positional Self-Attention with Co-Attention for Video Question Answering},
  author={Xiangpeng Li and Jingkuan Song and Lianli Gao and Xianglong Liu and Wenbing Huang and Xiangnan He and Chuang Gan},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2019}
}

@article{Kumar2019LeveragingTA,
  title={Leveraging Topics and Audio Features with Multimodal Attention for Audio Visual Scene-Aware Dialog},
  author={Shachi H. Kumar and Eda Okur and Saurav Sahay and Jonathan Huang and Lama Nachman},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.10131}
}


@inproceedings{Wu2021STAR,
  author       = {Bo Wu and
                  Shoubin Yu and
                  Zhenfang Chen and
                  Josh Tenenbaum and
                  Chuang Gan},
  title = {STAR: A Benchmark for Situated Reasoning in Real-World Videos},
  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2021}
}

@article{Li2019VisualBERTAS,
  title={VisualBERT: A Simple and Performant Baseline for Vision and Language},
  author={Liunian Harold Li and Mark Yatskar and Da Yin and Cho-Jui Hsieh and Kai-Wei Chang},
  journal={ArXiv},
  year={2019},
  volume={abs/1908.03557}
}

@article{Hu2019LanguageConditionedGN,
  title={Language-Conditioned Graph Networks for Relational Reasoning},
  author={Ronghang Hu and Anna Rohrbach and Trevor Darrell and Kate Saenko},
  journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2019},
  pages={10293-10302}
}

@inproceedings{wei2021finetunedLM,
  title={Finetuned Language Models are Zero-Shot Learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={International Conference on Learning Representations},
  year={2022},
}

@inproceedings{Cherian2022251D,
    title={(2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering.},
    author={Anoop Cherian and Chiori Hori and Tim K. Marks and Jonathan Le Roux},
    booktitle={AAAI Conference on Artificial Intelligence},
    pages={444-453},
    year={2022}
}

@article{Park2021BridgeTA,
  title={Bridge to Answer: Structure-aware Graph Interaction Network for Video Question Answering},
  author={Jungin Park and Jiyoung Lee and Kwanghoon Sohn},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={15521-15530}
}

@article{Kim2020ModalitySA,
  title={Modality Shifting Attention Network for Multi-Modal Video Question Answering},
  author={Junyeong Kim and Minuk Ma and Trung Xuan Pham and Kyungsu Kim and Chang Dong Yoo},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={10103-10112}
}

@article{Gao2022MISTMI,
  title={MIST: Multi-modal Iterative Spatial-Temporal Transformer for Long-form Video Question Answering},
  author={Difei Gao and Luowei Zhou and Lei Ji and Linchao Zhu and Yezhou Yang and Mike Zheng Shou},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.09522}
}

@article{Li2022InvariantGF,
  title={Invariant Grounding for Video Question Answering},
  author={Yicong Li and Xiang Wang and Junbin Xiao and Wei Ji and Tat-Seng Chua},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  pages={2918-2927}
}

@article{Lin2022TowardsFA,
  title={Towards Fast Adaptation of Pretrained Contrastive Models for Multi-channel Video-Language Retrieval},
  author={Xudong Lin and Simran Tiwari and Shiyuan Huang and Manling Li and Mike Zheng Shou and Heng Ji and Shih-Fu Chang},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.02082},
  url={https://api.semanticscholar.org/CorpusID:249394660}
}

% 大模型
@inproceedings{Radford2021LearningTV,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:231591445}
}

@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Brown2020LanguageMA,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and T. J. Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165},
  url={https://api.semanticscholar.org/CorpusID:218971783}
}

@article{Alayrac2022FlamingoAV,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andy Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.14198},
  url={https://api.semanticscholar.org/CorpusID:248476411}
}

@article{Li2023VideoChatCV,
  title={VideoChat: Chat-Centric Video Understanding},
  author={Kunchang Li and Yinan He and Yi Wang and Yizhuo Li and Wen Wang and Ping Luo and Yali Wang and Limin Wang and Yu Qiao},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.06355},
  url={https://api.semanticscholar.org/CorpusID:258588306}
}

@article{Zhang2023VideoLLaMAAI,
  title={Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding},
  author={Han Zhang and Xin Li and Lidong Bing},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.02858},
  url={https://api.semanticscholar.org/CorpusID:259075356}
}

@article{Lyu2023MacawLLMML,
  title={Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration},
  author={Chenyang Lyu and Minghao Wu and Longyue Wang and Xinting Huang and Bingshuai Liu and Zefeng Du and Shuming Shi and Zhaopeng Tu},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.09093},
  url={https://api.semanticscholar.org/CorpusID:259165461}
}

@article{Su2023PandaGPTOM,
  title={PandaGPT: One Model To Instruction-Follow Them All},
  author={Yixuan Su and Tian Lan and Huayang Li and Jialu Xu and Yan Wang and Deng Cai},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.16355},
  url={https://api.semanticscholar.org/CorpusID:258947721}
}

@inproceedings{Gupta2023VisualProgramming,
  title={Visual programming: Compositional visual reasoning without training},
  author={Gupta, Tanmay and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14953--14962},
  year={2023}
}

@article{Maaz2023VideoChatGPTTD,
  title={Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models},
  author={Muhammad Maaz and Hanoona Rasheed and Salman Khan and Fahad Shahbaz Khan},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.05424},
  url={https://api.semanticscholar.org/CorpusID:259108333}
}

@inproceedings{Zhao2022Collaborative,
    title = "Collaborative Reasoning on Multi-Modal Semantic Graphs for Video-Grounded Dialogue Generation",
    author = "Zhao, Xueliang  and
      Wang, Yuxuan  and
      Tao, Chongyang  and
      Wang, Chenshuo  and
      Zhao, Dongyan",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    url = "https://aclanthology.org/2022.findings-emnlp.442",
    doi = "10.18653/v1/2022.findings-emnlp.442",
    pages = "5988--5998",
}

@inproceedings{Wang2023Vstar,
    title = "{VSTAR}: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions",
    author = "Wang, Yuxuan  and
      Zheng, Zilong  and
      Zhao, Xueliang  and
      Li, Jinpeng  and
      Wang, Yueqian  and
      Zhao, Dongyan",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    url = "https://aclanthology.org/2023.acl-long.276",
    doi = "10.18653/v1/2023.acl-long.276",
    pages = "5036--5048",
}

@article{liu2023cross,
  title={Cross-modal causal relational reasoning for event-level visual question answering},
  author={Liu, Yang and Li, Guanbin and Lin, Liang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}

