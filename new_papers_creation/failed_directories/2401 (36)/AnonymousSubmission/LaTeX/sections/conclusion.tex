\section{Conclusion}
% We present an in-depth analysis of the \textit{modality gap} phenomenon in CLIP latent space and we uncovered two key phenomena. Firstly, the  tighter proximity of the CLIP visual feature within image subregions to paired captions was observed. Furthermore, empirical evidence supported the notion that the modality gap between paired image and text adhered to a zero-mean Gaussian distribution. In response to these insights, we introduced a novel approach region noise injected text reconstruction training to effectively mitigate the \textit{modality gap}. Leveraging the concept of subregion feature aggregation in zero-shot caption generation, we harnessed subregion information to create a closer visual representation with paired caption representation. Additionally, we propose a inference time noise and CLIP reranking to further boost performance. Experimental results demonstrate that the MacCap outperform state-of-arts methods and verify our insights of CLIP embedding space. 

We present an in-depth analysis of the \textit{modality gap} phenomenon in the CLIP latent space, uncovering two key phenomena: tighter proximity of CLIP visual features within image subregions to paired captions and a modality gap adhering to a zero-mean Gaussian distribution. In response to these insights, we introduced a novel approachâ€”region noise-injected text reconstruction training. Leveraging subregion feature aggregation in zero-shot caption generation, we harnessed subregion information to create a closer visual representation with paired caption representation. Additionally, we propose an inference-time noise and CLIP reranking to further boost performance. Experimental results demonstrate that MacCap outperforms state-of-the-art methods.
% introduce \textit{concept region} in CLIP embedding space. We observe that the embeddings of image subregions and their corresponding captions can have higher similarity scores. Motivated by the observation, we propose a zero-shot image captioning framework MicroCap. We introduce region noise injection in text reconstruction training and adopt subregion information to refine image global feature. Additionally, we propose a multiple sampling and filtering strategy for better caption generation quality.
% Experimental results demonstrate that the MicroCap outperform state-of-arts methods and verify our insights of CLIP embedding space.