\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1.定义image captioning任务 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Image captioning is a fundamental task in vision-language understanding that involves generating natural language descriptions for a given image. It plays a critical role in facilitating more complex vision-language tasks, such as visual question answering \cite{Agrawal2015VQAVQ,gqa,okvqa} and visual dialog \cite{Das2016VisualD,Niu2018RecursiveVA,llava}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% text-only training 的介绍
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The mainstream image captioning methods \cite{conimgcap4,conimgcap1,conimgcap3,conimgcap2} require expensive human annotation of image-text pairs for training neural network models in an end-to-end manner. Recent developments in Contrastive Image Language Pre-training (CLIP) \cite{clip} have enabled researchers to explore a new paradigm, zero-shot image captioning, through text-only training. In particular, CLIP learns a multi-modal embedding space where semantically related images and text are encoded into features with close proximity. As such, if a model learns to map the CLIP text features to their corresponding texts, it is feasible to generate image captions from the CLIP image features without needing supervision from caption annotations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% text-only training 的优势
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

One main advantage of this zero-shot captioning paradigm is that it enables a Large Language Model (LLM) \cite{gpt3, Zhang2022OPTOP} with image captioning capabilities using only text data and affordable computational resources. Despite the impressive performance achieved by recent powerful multimodal models \cite{miniGPT4,llava}, they typically require large-scale, high-quality human-annotated data and expensive computational resources for fine-tuning an LLM. Zero-shot captioning methods can significantly reduce such costs, which is particularly important in situations of data scarcity and limited resources. Moreover, recent work \cite{Guo2022FromIT, Changpinyo2022AllYM,Tiong2022PlugandPlayVZ} demonstrates that other vision-language tasks, such as VQA, can be addressed by LLMs and image captions. Consequently, the paradigm of zero-shot captioning has the potential to pave the way to solving complex vision-language tasks with LLMs through efficient text-only training. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% zero-shot image captioning via text-only training 的challenge
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
A critical challenge in zero-shot image captioning through text-only training is to mitigate a widely observed phenomenon known as the \textit{modality gap}. While the features of paired texts and images are close in the CLIP embedding space, there remains a gap between them \cite{MindGap}. This gap often results in inaccurate mappings from the image embeddings to the text ones. Consequently, without fine-tuning with paired data, it significantly impairs the performance of zero-shot image captioning.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% current works intro
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Several works have attempted to address the modality gap in zero-shot image captioning, relying mainly on two strategies: (1) The first strategy leverages a memory bank from training text data to project visual embeddings into the text embedding space \cite{DeCap}. However, this projection prevents it from representing any semantic content outside the distribution of the memory bank features and introduces extra inference costs; (2) The second approach injects noise during training to encourage the visual embeddings to be included inside the semantic neighborhood of the corresponding text embeddings \cite{CapDec}. Nonetheless, the noise injection tends to diffuse the distribution of visual inputs at the cost of weakening the semantic correlation between paired images and text embeddings. 

%However, in the first strategy, the projection of visual embeddings prevents them from  For the second strategy, noise injection during training diffuses the input distribution at the cost of degrading the semantic correlation between paired images and text embeddings.

%Previous attempts \cite{CapDec,DeCap} to reduce the modality gap in zero-shot image captioning can be summarized into two aspects: (1) Decap\cite{DeCap} leverages a memory bank from training text data to project visual embeddings into text embedding space. However, the projection of visual embeddings prevents it from representing any semantic content outside the distribution of the memory bank and introduce extra inference cost. (2) CapDec\cite{CapDec}proposes to inject noise during training to encourage the visual embedding to be included inside the text embedding space. 
% current work weakness
%Nevertheless, noise injection during training diffuses the input distribution at the cost of degrading the semantic correlation between paired images and text embeddings.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 我们工作的流程
% 分析得到两个结论：1.subregion带来更好的匹配2.image text gap符合高斯分布
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To tackle these challenges, we first conduct a thorough analysis of the CLIP feature space, leading to two key observations. First, most text descriptions are unable to fully capture the content of their paired images. However, we empirically find that the visual embedding of certain local regions of an image, named image subregions, have closer proximity to the text embedding of the paired caption. Integrating such image subregions with the global image representation generates a tighter alignment between image and text. Additionally, we analyze the distribution of the gap between the CLIP features of image or subregion-text pairs and find that it closely resembles a zero-mean Gaussian distribution.
%initiate our investigation by conducting a thorough analysis of the CLIP latent space. Building upon the insights from the work \cite{MindGap}, we identify a key factor contributing to the existence of a modality gap. Due to the inherent disparities between textual and visual modalities, text is incapable of comprehensively describing the information within an image. However, we empirically demonstrate that the CLIP embedding of some part of image, named image subregions, exhibit closer proximity to the CLIP embedding of the paired caption. The integration between image subregion information and global image feature leads to more compact image text alignment. Besides, we collect the statistics of the gap between CLIP image and text feature. The results demonstrate the gap is close to gaussian distribution. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 我们的方法简略介绍
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on our findings, we propose a novel zero-shot image captioning framework, named \textit{\textbf{M}ining Fine-Grained Image-Text \textbf{A}lignment in \textbf{C}LIP for \textbf{Cap}tioning} (MacCap), to address the aforementioned challenges. In this framework, we introduce a region-aware cross-modal representation based on CLIP and an effective unimodal training strategy for an LLM-based caption generator. Our cross-modal representation maps an input image into the language space of LLMs and consists of two main components. First, we design a \textit{sub-region feature aggregation} module to fuse both global and subregion-level CLIP image features, resulting in a smaller gap between the corresponding CLIP text embedding. Next, we introduce a learnable adaptor-decoder to transform the CLIP representation into the LLM's language space.
To train our model with text-only data, we develop a robust procedure to learn a projection from the CLIP embedding space to a language representation, enabling the LLM to reconstruct captions. Specifically, our learning procedure first injects noise into our region-aware CLIP-text representation, mimicking the modality gap between image and text features. This is followed by a multiple sampling and filtering step that leverages the CLIP knowledge to improve the quality of the captioning.
%tackles the problem from three key perspectives. Firstly, we focus on learning a robust projection from CLIP embedding space to language model space by text reconstruction training, which enable model to generate text based on both CLIP image and text feature. The region noise injection in training alleviate the \textit{modality gap} between image and text feature, which makes the projection works for both image and text features. Secondly, we design \textit{sub-region feature aggregation} to obtain a more compact CLIP image feature, which is based on the observation that CLIP subregion feature exhibit closer disntance with corresponding text feature. Third, we propose multiple sampling and filtering to mitigate the drawbacks of noise injection, which leverage CLIP knowledge to further boost caption performance. Finally, we design a pipeline for zero-shot VQA to demonstrate the extensibility of ouir methods to more intricate vision-language tasks.
In addition to the image captioning task, we further extend our framework to build a zero-shot VQA pipeline, demonstrating the generality of our cross-modal representation for more complex vision-language tasks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 我们的方法简略介绍
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We evaluate our framework on several widely-adopted image captioning benchmarks, such as MSCOCO \cite{mscoco} and Flickr30k \cite{Flickr30k}, as well as a standard VQA benchmark, VQAV2 \cite{vqav2}. Our extensive experiments cover multiple vision-language tasks, including zero-shot in-domain image captioning, zero-shot cross-domain image, and zero-shot VQA. The results not only demonstrate the superiority of our methods but also validate our findings on the CLIP embedding space.

% demonstrate through experiments that our proposed methods outperform previous approaches on popular captioning benchmarks, such as MSCOCO, Flickr30k, which further verify our understanding of \textit{concept region}



% Specifically, we evaluate the distribution of the image and text embedding space under hyperspherical coordinates and observe a geometric phenomenon \textit{concept region} 
% where semantically correlated image and text embedding tend to clustering despite the \textit{modality gap}.
% 我们基于concept region的观察提出的方法：concept region和modality gap的cause里面有mismatch pair data导致的semantic ambiguity，总体思路是在train的时候模拟在concept region。在training的时候，我们给text embedding加上region noise，具体而言就是以原本text embedding为中心，一定范围内的多个随机sample的related text embedding，这样的获得的text embedding全都是在输入text对应的concept region内部。在zs captioning的inference时，部分image sub-region inforamtion 会比global image 对text匹配度更高,因此我们基于部分image sub-region inforamtion
% Motivated by the semantic ambiguity of mismatched data observed in \textit{concept region}, we propose two 
% an image sub-region information aggregation strategy for .In detail

% result summary
