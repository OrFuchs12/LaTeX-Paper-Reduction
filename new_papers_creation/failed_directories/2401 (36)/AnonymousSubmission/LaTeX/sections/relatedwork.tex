\section{Related Work}
% \subsection{Large Language Models}
% Recently, the impressive performance of large language model has gain general attention in research field. 


%\subsection{Zero-shot Image Captioning}
\paragraph{Zero-shot Image Captioning}
Zero-shot image captioning is an emerging task setting of image captioning, where captions are generated without relying on training with annotated image data. While some approaches \cite{Changpinyo,Wang_Yu_Yu_Dai_Tsvetkov_Cao_2021,flamingo} exploit large noisy image-text datasets, demanding high data and computational resources, an alternative is to leverage pre-trained large models, which is more suitable for low-data scenarios.
% Zero-shot Image Captioning is a notable subfield within image captioning, where captions are generated without relying on human-annotated data. One approach to enable zero-shot captioning involves utilizing large-scale noisy paired image-text datasets, as demonstrated in previous works \cite{Changpinyo, Wang_Yu_Yu_Dai_Tsvetkov_Cao_2021, flamingo}. However, these methods suffer from the drawback of high data and computational requirements. Alternatively, leveraging pre-trained large models allows for impressive performance even in low-data scenarios, with a reduced number of training parameters.

% The use of pretrained multi-modality models has enabled advancements in text-only training for image captioning. Text-only training focuses on generating captions using only text modality data during training. Prior research has demonstrated promising results. For instance, CapDec \cite{CapDec} utilizes CLIP embeddings and employs a noise injection training strategy for text-only training. Similarly, DeCap \cite{DeCap} employs a memory bank to project visual features into the text modality. Furthermore, there exist methods that do not require typical training stage to accomplish this task. MAGIC \cite{MAGIC} introduces a CLIP-based score to guide language model generation while incorporating image information. ZeroCap \cite{ZeroCap} generates image captions through iterative optimization during inference.
The use of pre-trained multi-modality models has enabled progress in text-only training for image captioning, which has demonstrated promising results. CapDec \cite{CapDec} utilizes CLIP embeddings and employs a noise injection training strategy for text-only training. Similarly, DeCap \cite{DeCap} employs a memory bank to project visual features into the text modality. Furthermore, methods like MAGIC \cite{MAGIC} and ZeroCap \cite{ZeroCap} achieve zero-shot captioning without a typical training stage, with MAGIC introducing a CLIP-based score to guide language model generation and ZeroCap employing iterative optimization during inference.


%\subsection{Vision-language Models}
\paragraph{Vision-language Models}

% Recent advancements \cite{clip,align,blip,glip,glip2} in Vision-Language Models (VLM) have led to significant progress in various downstream tasks \cite{styleclip,maskclip,hoiclip,calip,pointclip,denclip}. In order to gain a deeper understanding of VLM, extensive research efforts \cite{clipbagofwords, Wang2020UnderstandingCR, MindGap,Wang2020UnderstandingTB} have been dedicated to analyze the multi-modal embedding space learned through contrastive training. Recently, a geometric phenomenon known as the \textit{modality gap} has been identified in \cite{MindGap}. This phenomenon arises from the misalignment between the text and image embeddings generated by CLIP, resulting in a separation within their shared representation. The presence of the \textit{modality gap} can be attributed to two factors: first, the optimization process of contrastive learning, which preserve \textit{modality gap} when the CLIP's prediction contains mismatched image-text pairs; and second, the random initialization of different encoders. Addressing this \textit{modality gap} becomes crucial when aiming to enhance the zero-shot capabilities of downstream tasks, particularly in scenarios where extensive fine-tuning is restricted.
Recent advancements \cite{clip,align,glip,blip,glip2} in Vision-Language Models (VLM) have led to significant progress in various downstream tasks \cite{styleclip,maskclip,pointclip,denclip,calip,hoiclip}. Extensive research efforts \cite{Wang2020UnderstandingCR,Wang2020UnderstandingTB,clipbagofwords,MindGap} have analyzed the multi-modal embedding space learned through contrastive training. Recently, a geometric phenomenon known as the \textit{modality gap} has been identified in \cite{MindGap}. This gap arises from misalignment between text and image embeddings of CLIP, impacting their shared representation. The \textit{modality gap} is attributed to the optimization process of contrastive learning and random initialization of different encoders. Addressing this \textit{modality gap} is crucial for enhancing zero-shot capabilities, especially in scenarios with limited fine-tuning opportunities.
