\begin{table*}
\tabcolsep=2pt
\small
\centering
\begin{tabular}{ccccccccccccccccccc}
% \hline
\toprule
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{6}{c|}{CC3M to MS-COCO}                                                                   & \multicolumn{6}{c|}{MS-COCO to Flickr30k}                          & \multicolumn{6}{c}{Flickr30k to MS-COCO}      \\
\multicolumn{1}{c|}{}                        & B@1 & B@4                        & M     & R\_L                   & C     & \multicolumn{1}{c|}{S}     & B@1   & B@4   & M     & R\_L  & C     & \multicolumn{1}{c|}{S}     & B@1   & B@4   & M     & R\_L  & C     & S     \\ \hline
\multicolumn{1}{c|}{CLIPRe}                  & -     & 0.046 & 0.133 & -     & 0.256 & \multicolumn{1}{c|}{0.092} & 0.387 & 0.044 & 0.096 & 0.272 & 0.059 & \multicolumn{1}{c|}{0.042} & 0.414 & 0.052 & 0.125 & 0.307 & 0.183 & 0.057 \\
\multicolumn{1}{c|}{ZeroCap}                 & -     & 0.026 & 0.115 & -     & 0.146 & \multicolumn{1}{c|}{0.055} & -     & -     & -     & -     & -     & \multicolumn{1}{c|}{-}     & -     & -     & -     & -     & -     & -\\
\multicolumn{1}{c|}{MAGIC}                   & -     & -     & -     & -     & -     & \multicolumn{1}{c|}{-}     & 0.464 & 0.062 & 0.122 & 0.313 & 0.175 & \multicolumn{1}{c|}{-}     & 0.414 & 0.052 & 0.125 & 0.307 & 0.183 & -     \\
% \multicolumn{1}{c|}{SMs 0-shot\tnote{1}}     & -     & -     & -     & -     & -     & \multicolumn{1}{c|}{-}     & -     & 0.069 & 0.150 & 0.341 & 0.445 & \multicolumn{1}{c|}{0.101} & -     & 0.069 & 0.150 & 0.341 & 0.445 & 0.101 \\
\multicolumn{1}{c|}{CapDec}                  & -     & -     & -     & -     & -     & \multicolumn{1}{c|}{-}     & 0.602 & 0.173 & 0.186 & 0.427 & 0.357 & \multicolumn{1}{c|}{-}     & 0.433 & 0.092 & 0.163 & 0.367 & 0.273 & -     \\
\multicolumn{1}{c|}{DeCap}                   & -     & 0.088 & 0.160 & -     & 0.421 & \multicolumn{1}{c|}{0.109} & -     & 0.163 & 0.179 & -     & 0.357 & \multicolumn{1}{c|}{0.111} & -     & 0.121 & 0.180 & -     & 0.444 & 0.109 \\ \hline
\multicolumn{19}{c}{Frozen Language Model}                                                                                                                        \\ \hline
\multicolumn{1}{c|}{Baseline}                  & 0.318 & 0.034 & 0.094 & 0.221 & 0.101 & \multicolumn{1}{c|}{0.046} & 0.429 & 0.072 & 0.126 & 0.305 & 0.140 & \multicolumn{1}{c|}{0.067} & 0.375 & 0.049 & 0.118 & 0.296 & 0.112 & 0.055 \\
\multicolumn{1}{c|}{$\mathrm{MAGIC}^\dagger$}  & 0.188 & 0.004 & 0.054 & 0.142 & 0.021 & \multicolumn{1}{c|}{0.011} & 0.188 & 0.006 & 0.051 & 0.134 & 0.021 & \multicolumn{1}{c|}{0.013} & 0.188 & 0.004 & 0.054 & 0.142 & 0.021 & 0.011 \\
\multicolumn{1}{c|}{$\mathrm{CapDec}^\dagger$} & 0.372 & 0.046 & 0.116 & 0.288 & 0.093 &\multicolumn{1}{c|}{0.052}  & 0.490 & 0.105 & 0.153 & 0.373 & 0.183 & \multicolumn{1}{c|}{0.090} & 0.453 & 0.087 & 0.151 & 0.353 & 0.178 & 0.064 \\
\multicolumn{1}{c|}{$\mathrm{DeCap}^\dagger$}  & 0.462 & 0.089 & 0.152 & 0.341 & 0.292 & \multicolumn{1}{c|}{0.093}& 0.511 & 0.099 & 0.153 & 0.362 & 0.247 & \multicolumn{1}{c|}{0.087} & 0.455 & 0.088 & 0.162 & 0.360 & 0.273 & \textbf{0.101} \\
\rowcolor{blue!6}\multicolumn{1}{c|}{MacCap}                  & \textbf{0.591} & \textbf{0.176} & \textbf{0.200} & \textbf{0.443} & \textbf{0.525} & \multicolumn{1}{c|}{\textbf{0.120}}  & \textbf{0.595} & \textbf{0.154} & \textbf{0.179} & \textbf{0.413} & \textbf{0.303} & \multicolumn{1}{c|}{\textbf{0.114}} & \textbf{0.473} & \textbf{0.092} & \textbf{0.166} & \textbf{0.362} & \textbf{0.278} & 0.092 \\ \toprule
\end{tabular}
% \vspace{0.1cm}
\caption{\textbf{Zero-shot Cross Domain Captioning}: We conduct experiments on cross-domain image captioning tasks. X to Y means source to target domain. We reproduce Magic, CapDec, and DeCap under the frozen LLM setting and mark them with \textbf{$\dagger$}.}
\label{tab:cross_domain_results}
\end{table*} 




\section{Experiments}

\subsection{Experimental setting}





\paragraph{\textbf{Datasets and Evaluation}} 
Our experimental evaluations are performed on two common benchmark datasets for image captioning: MSCOCO \cite{mscoco} and Flickr30k \cite{Flickr30k}. The MSCOCO dataset contains over 11,000 images, each associated with five captions. We follow previous works \cite{CapDec} using the widely-used Karpathy et al. split, which partitions the dataset into 5000 images for validation and 5,000 for testing. The Flickr30k dataset consists of 29,000 images for training and 1,000 images for testing. For training, we use the texts from MSCOCO, Flickr30K, and CC3M \cite{cc3m} datasets. Following previous works \cite{DeCap}, we remove the sentences with more than fifteen words. The resulting training text corpus has 566,747 sentences for MSCOCO, 144,998 sentences for Flickr30K, and 3,302,783 sentences for CC3M. To evaluate the performance of our models, we report the results on four standard evaluation metrics for captioning: BLEU \cite{Papineni2002BleuAM}, METEOR \cite{Banerjee2005METEORAA}, CIDEr \cite{Vedantam2014CIDErCI}, and SPICE \cite{Anderson2016SPICESP}. Additionally, we use the popular visual question answering benchmark VQAV2 \cite{vqav2} to evaluate the model's ability on complex visual tasks. The number of answer candidates for VQAV2 is 3,128. We randomly chose 20,000 samples from VQAV2 validation set for testing. 

% \textbf{Language Model} We make a modification on language model setting compared with previous work\cite{DeCap,calip}, we adopt a pre-trained large language model instead of training from sketch. This choice is motivated by the fact that achieving impressive performance with large language models often entails significant computational requirements for fine-tuning. By adopting this setting, we maintain the generalization capabilities of large language models and enable our framework to serve as a seamless module for expanding the capabilities of these models to the visual domain through text-only training.

\paragraph{\textbf{Implementation Details}} 
For a fair comparison with previous works \cite{CapDec,MAGIC,ZeroCap,DeCap}, we employ a frozen Vit-B/32 CLIP model. The adaptor decoder consists of one layer Transformer Decoder \cite{Vaswani2017AttentionIA} with 8 attention heads. For text reconstruction training, we set the noise variance $\sigma$ to 0.016 as suggested in \cite{CapDec}, and the region concept feature length $N_{cr}$ is set to 10. In caption generation, the sampling number $S$ in inference is set to 20. The text generation strategy is beam search with 4 beams. For the language model, we adopt a frozen pre-trained OPT \cite{Zhang2022OPTOP} 1.3b model. Our model and the reproduced baseline are trained with a batch size of 128 and a learning rate of 4e-4.
% The codes and logs for reproduced baselines will be provided in supplementary material,



% baseline介绍
\paragraph{\textbf{Baselines}} 
The following zero-shot captioning methods are compared in this study. \textbf{ZeroCap} leverages CLIP and GPT-2 to solve an optimization problem iteratively during inference. \textbf{DeCap} \cite{DeCap} utilizes a memory bank to project image embeddings into the text embedding space. We sample 50M texts in CC3M \cite{cc3m} datasets to generate the memory bank for DeCap. Additionally, we define \textbf{Baseline} as the model trained in the same way as DeCap but using image embedding directly in inference.
\textbf{MAGIC} \cite{MAGIC} incorporates a CLIP-induced score during inference to influence the language model's caption generation process. We show the performance of MAGIC with a fine-tuned language model as reported in \cite{MAGIC} and the performance of MAGIC with a frozen pre-trained language model. \textbf{CLIPRe} is a retrieval-based baseline mentioned in \cite{MAGIC}. \textbf{CapDec} \cite{CapDec} applies noise injection strategy in text reconstruction training, enabling the direct use of visual embedding in inference. We use the MLP variant of CapDec model. \textbf{SM} \cite{Zeng2022SocraticMC} is a modular framework in which multiple pre-trained models may be composed zero-shot. It uses the GPT-3 \cite{gpt3} API from OpenAI and achieves favorable performance.


\begin{table*}
\tabcolsep=2pt
\small
\centering
\begin{threeparttable}
\begin{tabular}{cccccccccccccccc}
\toprule
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{3}{c|}{Data}                                 & \multicolumn{6}{c|}{MSCOCO}                                        & \multicolumn{6}{c}{Flickr30k}                 \\
\multicolumn{1}{c|}{}                        & P.         & I.         & \multicolumn{1}{c|}{T.}         & $B\texttt{@}1$   & $B\texttt{@}4$   & M     & $R_L$   & C     & \multicolumn{1}{c|}{S}     & $B\texttt{@}1$   & $B\texttt{@}4$   & M     & $R_L$   & C     & S     \\ \hline
\multicolumn{1}{c|}{CLIPCap \cite{clipcap}}                 & \checkmark &            & \multicolumn{1}{c|}{}           & -     & 0.335 & 0.275 & -     & 1.131 & \multicolumn{1}{c|}{0.211} & -     & -     & -     & -     & -     & -     \\
\multicolumn{1}{c|}{CLIP-VL \cite{Shen2021HowMC}}                 & \checkmark &            & \multicolumn{1}{c|}{}           & -     & 0.375 & 0.281 & -     & 1.231 & \multicolumn{1}{c|}{0.219} & -     & -     & -     & -     & -     & -     \\
\multicolumn{1}{c|}{UVC-VI \cite{Liu2021AligningSV}}                  & \checkmark &            & \multicolumn{1}{c|}{}           & -     & 0.220 & 0.214 & -    & 0.723 & \multicolumn{1}{c|}{}      & -     & -     & -     & -     & -     & -     \\
\multicolumn{1}{c|}{Barraco et al. \cite{Barraco2022TheUE}}          & \checkmark &            & \multicolumn{1}{c|}{}           & -     & 0.360 & 0.278 & -     & 1.149 & \multicolumn{1}{c|}{0.208} & -     & -     & -     & -     & -     & -     \\
\multicolumn{1}{c|}{ESPER-Style \cite{Yu2022MultimodalKA}}             &            & \checkmark & \multicolumn{1}{c|}{\checkmark} & -     & 0.219 & 0.219 & -     & 0.782 & \multicolumn{1}{c|}{}      & -     & -     & -     & -     & -     & -     \\
\multicolumn{1}{c|}{ESPER-Free \cite{Yu2022MultimodalKA}}              &            & \checkmark & \multicolumn{1}{c|}{}           & -     & 0.063 & 0.133 & -     & 0.291 & \multicolumn{1}{c|}{}      & -     & -     & -     & -     & -     & -     \\
\multicolumn{1}{c|}{ZeroCap$^{\star} $\cite{ZeroCap}}        &            &            & \multicolumn{1}{c|}{\checkmark} & 0.498 & 0.007 & 0.154 & 0.318 & 0.345 & \multicolumn{1}{c|}{0.092} & 0.447 & 0.054 & 0.118 & 0.273 & 0.168 & 0.062 \\
\multicolumn{1}{c|}{CLIPRe \cite{MAGIC}}                  &            &            & \multicolumn{1}{c|}{\checkmark} & 0.395 & 0.049 & 0.114 & 0.290 & 0.136 & \multicolumn{1}{c|}{0.053} & 0.385 & 0.052 & 0.116 & 0.276 & 0.100 & 0.057 \\
\multicolumn{1}{c|}{MAGIC \cite{MAGIC}}                   &            &            & \multicolumn{1}{c|}{\checkmark} & 0.568 & 0.129 & 0.174 & 0.399 & 0.493 & \multicolumn{1}{c|}{0.113} & 0.445 & 0.064 & 0.131 & 0.316 & 0.204 & 0.071 \\
\multicolumn{1}{c|}{CapDec \cite{CapDec}}                   &            &            & \multicolumn{1}{c|}{\checkmark} & 0.692& 0.264 & 0.251 & 0.518 & 0.918 & \multicolumn{1}{c|}{-} & 0.555     & 0.177 & 0.200 & 0.439 & 0.391 & - \\
\multicolumn{1}{c|}{DeCap \cite{DeCap}}                   &            &            & \multicolumn{1}{c|}{\checkmark} & -     & 0.247 & 0.250 & -     & 0.912 & \multicolumn{1}{c|}{0.187} & -     & 0.212 & 0.218 & -     & 0.567 & 0.152 \\
\hline
\multicolumn{16}{c}{Frozen Language Model}                                                                                                                                                                                    \\ \hline
\multicolumn{1}{c|}{Baseline}                &            &            & \multicolumn{1}{c|}{\checkmark} & 0.414 & 0.069 & 0.141 & 0.317 & 0.221 & \multicolumn{1}{c|}{0.079} & 0.418 & 0.069 & 0.127 & 0.308 & 0.136 & 0.070 \\
\multicolumn{1}{c|}{$\mathrm{MAGIC}^\dagger $\cite{MAGIC}}                   &            &            & \multicolumn{1}{c|}{\checkmark} & 0.188 & 0.004 & 0.054 & 0.042 & 0.021 & \multicolumn{1}{c|}{0.011} & 0.188 & 0.006 & 0.051 & 0.134 & 0.021 & 0.013 \\
\multicolumn{1}{c|}{$\mathrm{CapDec}^\dagger $\cite{CapDec}}                  &            &            & \multicolumn{1}{c|}{\checkmark} & 0.537 & 0.156 & 0.206 & 0.435 & 0.465 & \multicolumn{1}{c|}{0.134} & 0.429 & 0.072 & 0.136 & 0.336 & 0.127 & 0.054 \\
\multicolumn{1}{c|}{$\mathrm{DeCap}^\dagger $\cite{DeCap}}                   &            &            & \multicolumn{1}{c|}{\checkmark} & 0.531 & 0.125 & 0.188 & 0.403 & 0.427 & \multicolumn{1}{c|}{0.126} & 0.485 & 0.096 & 0.143 & 0.351 & 0.213 & 0.079 \\
\rowcolor{blue!6}\multicolumn{1}{c|}{MacCap}                    &            &            & \multicolumn{1}{c|}{\checkmark} & \textbf{0.614} & \textbf{0.174} & \textbf{0.223} & \textbf{0.459} & \textbf{0.697} & \multicolumn{1}{c|}{\textbf{0.157}} & \textbf{0.564} & \textbf{0.153} & \textbf{0.189} & \textbf{0.414} & \textbf{0.358} & \textbf{0.124} \\ \toprule
\end{tabular}
% \begin{tablenotes}
%     \footnotesize
%     \item[1] The ZeroCap results are from \cite{MAGIC}
% \end{tablenotes}
\end{threeparttable}
% \vspace{0.05cm}
\caption{\textbf{Zero-shot In Domain Captioning}: The notation "P.", "I.", and "T." are used to represent paired data, unpaired image data, and unpaired text data, respectively. We reproduce Magic, CapDec, and DeCap under frozen language model setting and mark them with \textbf{$\dagger$}. Results tagged  $\star$ are from \cite{MAGIC}}
\label{tab:in_domain_results}
\end{table*}


\begin{table}{}{}
\small
\centering
\tabcolsep=4pt
\begin{tabular}{clllllll}
\toprule
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{Method}}            &  & \multicolumn{3}{c|}{VQAV2 Val (\%)} & \multicolumn{3}{c}{MSCOCO}                    \\
\multicolumn{1}{c|}{}                                   &              & Top1  & Top5   & \multicolumn{1}{c|}{Top10} & $R_L$  & C     & S     \\ \hline
\multicolumn{8}{c}{\textit{Finetuned Language Model}}                                                                                                      \\ \hline
\multicolumn{1}{c|}{{$\mathrm{CapDec}^\dagger$}}        &              & 0.86 & 3.13 & \multicolumn{1}{c|}{3.71} & \textbf{0.518} & \textbf{0.918} & - \\
\multicolumn{1}{c|}{{$\mathrm{DeCap}^\dagger$}}         &              & 4.09 & 10.89 & \multicolumn{1}{c|}{14.33} & - & 0.912 & \textbf{0.187}  \\ \hline
\multicolumn{8}{c}{\textit{Frozen Language Model}}                                                                                                     \\ \hline
\multicolumn{1}{c|}{Baseline}                           &              & 3.26 & 7.24 & \multicolumn{1}{c|}{11.21} & - & - & - \\
\multicolumn{1}{c|}{CapDec}                             &              & 6.53 & 11.06 & \multicolumn{1}{c|}{15.00} & 0.435 & 0.465 & 0.134 \\
\multicolumn{1}{c|}{DeCap}                              &              & 6.00 & 11.81 & \multicolumn{1}{c|}{15.57} & 0.403 & 0.427 & 0.126 \\
\rowcolor{blue!6}\multicolumn{1}{c|}{MacCap}                             &              & \textbf{7.96} & \textbf{14.00} & \multicolumn{1}{c|}{\textbf{18.72}} & 0.459 & 0.697 & 0.157 \\
\toprule
\end{tabular}
\caption{\textbf{Zero-shot VQA results} on VQAV2 validation set. $\dagger$ means the models come from the official release. Our method achieves superior performance under a frozen language model setting.}
\label{table:VQAV2}
\end{table}


% \subsection{Zero-Shot Image Captioning}
% % setting
% \textbf{Setting} In this section, we conduct zero-shot image captioning using webly-collected text corpus on MSCOCO\cite{mscoco} dataset and Flickr30K\cite{Flickr30k} dataset. Conventional image captioning methods requires manually annotated caption data which is expensive to collect. As suggested in previous works, we consider using an easily collected text corpus for our text-only training. In detail, we choose the 3302783 texts from CC3M\cite{cc3m} datasets, which contains three million image-description pairs collected from the web. 


% 下面的baseline介绍从4.1 decap里面抄过来，区别是说明我们使用了decap + ft language model版本，我们自己复现的decap + frozen language model，我们还提供了capdec的结果

% % results analysis
% \textbf{Results and Analysis} We show our zero-shot captioning results in Table~\ref{tab:zs_results}. 
% % Our approach demonstrates superior performance across all metrics on the MSCOCO and Flickr3K datasets. 
% We observe that the MacCap attains state-of-art results in all evaluation metrics. The results demonstrate that MacCap achieve impressive performance in captioning while preserve the generalization ability of large language model. We notice the performance gap of CIDEr between in DeCap\cite{DeCap} and $\mathrm{DeCap}^\dagger$. The results indicate that the CIDEr score tends to decrease when the language model is frozen. However, MacCap still achieves superior performance compared to the previous state-of-the-art method, DeCap\cite{DeCap}, with a 24.7\% improvement in CIDEr score on the MSCOCO dataset. Moreover, MacCap outperforms DeCap by 24.7\% in CIDEr score on MSCOCO under the same training settings.





% 暂定
\subsection{Zero-Shot Cross Domain Captioning}

In this section, we present a comprehensive evaluation of our method in the zero-shot cross-domain captioning setting. The zero-shot means our model is trained with only text data and the cross-domain means the training caption texts are different from captions in the test dataset. The CC3M is web-scale noisy captions data while MSCOCO and Flickr30K are human-annotated high-quality caption data. 
We compare our approach with previous methods on three different cross-domain settings to assess its performance and generalizability. We show our cross-domain image captioning results in Table~\ref{tab:cross_domain_results}. We have observed a positive correlation between the size of the training text corpus and the performance gain of MacCap. Our method achieves superiority in both domains in most metrics under our frozen language model setting. 

% We present a comprehensive evaluation of our method in the zero-shot cross-domain captioning setting, trained solely on text data, where the training caption texts are different from captions in testing dataset. We evaluate on CC3M (web-scale noisy captions) and MSCOCO/Flickr30K (human-annotated high-quality captions) to assess its performance and generalizability. Our cross domain image captioning results are shown in Table~\ref{tab:cross_domain_results}. Notably, our method outperforms others in most metrics on both domains under the frozen language model setting, showing a positive correlation between training text corpus size and performance gain.


% In this section, we evaluate our zero-shot cross-domain captioning method, trained solely on text data. Using CC3M (web-scale noisy captions) and MSCOCO/Flickr30K (human-annotated high-quality captions), we compare our approach with previous methods across three cross-domain settings in Table~\ref{tab:cross_domain_results}. Notably, our method, MacCap, outperforms others in most metrics on both domains under the frozen language model setting, showing a positive correlation between training text corpus size and performance gain.




% 暂定
\subsection{Zero-Shot In Domain Image Captioning}
\paragraph{\textbf{Setting}} In this section, we conduct zero-shot in-domain image captioning experiments, where the models are trained and tested on the same domain. We compare our method with other supervised methods, unpaired image captioning methods, and text-only training methods. 

\paragraph{\textbf{Results}} We show our results on MSCOCO \cite{mscoco} and Flickr30K \cite{Flickr30k} in Table~\ref{tab:in_domain_results}. Our method outperforms other methods on both domains in all metrics under our frozen language model setting and shows a gain of +27 in CIDEr compared with DeCap \cite{DeCap}. We also achieve higher performance compared with fully supervised methods such as Laina et al. \cite{Laina2019TowardsUI} and Feng et al \cite{Feng2018UnsupervisedIC} in terms of CIDEr on MSCOCO.



\subsection{Zero-Shot Visual Question Answering}
% In this section, we conduct zero-shot visual question answering experiments. Due to the redundancy of the large language model generation results, we don't use the VQAV2 traditional evaluation metric where the predicted answer should be exactly the same as ground truth answer. We adopt an image-text retrieval task to evaluate the VQAV2 performance where the answer candidates are provided. We report the top 1,5 and 10 accuracy in $\%$ for 20,000 VQAV2 validation set samples.
% We compared our results with previous zero-shot captioning methods \cite{DeCap,CapDec}. To verify the effectiveness of our method, we propose a baseline methods where the language model generated answer only based on the question without caption. We show our results in Table~\ref{table:VQAV2}. Our method outperform other methods with and without frozen language model. We observe a performance degradation between the frozen language model and finetuned language model, which indicate finetune language model on zero-shot captioning model harms model performance on other tasks.
In this section, we conduct zero-shot visual question-answering experiments. Due to the redundancy of the large language model generation results, we do not use the VQAV2 traditional evaluation metric where the predicted answer should be the same as the ground truth answer but instead use an image-text retrieval task where the answer candidates are provided. We report the top 1,5 and 10 accuracies in $\%$ for 20,000 VQAV2 validation set samples.
Compared with previous zero-shot captioning methods \cite{DeCap,CapDec}, including a baseline where the language model generates answers solely based on the question without a caption, our results are presented in Table~\ref{table:VQAV2}. Our method outperforms other methods with and without a frozen language model. We observe a performance degradation between the frozen language model and the finetuned language model, which indicates that the finetune language model on zero-shot captioning model harms model performance on other tasks.
% Comparing with previous zero-shot captioning methods \cite{DeCap,CapDec}, including a baseline where the language model generates answers solely based on the question without a caption, our results are presented in Table~\ref{table:VQAV2}. Our method outperforms others, both with and without a frozen language model. We observe a performance degradation between the frozen and finetuned language models, indicating that finetuning on zero-shot captioning harms performance on 

\subsection{Ablation Study}
In this section, we conduct an ablation study to provide a comprehensive interpretation of our proposed methods. The experiments are conducted under zero-shot cross-domain image captioning settings where the model is trained on CC3M text corpus and evaluated on Flickr30K dataset.


\paragraph{\textbf{Text Reconstruction Training}}
To validate the effectiveness of the region noise design in our framework, we conducted experiments to determine whether the observed improvements were due to the noise injection or the sequential representation of the text. we modify the input text feature of the adaptor decoder module, where we define two modes: \textit{single token} and \textit{multiple token}. In the \textit{single token} mode, a single text embedding $T_c$ is provided as input to the adaptor decoder. In contrast, in the \textit{Multiple tokens} mode, multiple text embeddings are used as input for the adaptor decoder.
The \textit{w/ noise} or \textit{w/o noise} indicate whether adding noise to the embedding. We present the results in Table~\ref{table:ablation}. Based on the results, we can conclude that the strong performance of our approach can be attributed to the effective combination of noise injection and sequential representation.
\begin{table}{}{}
\small
\centering
\tabcolsep=2pt
\begin{tabular}{clllllll}
\toprule
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{Method}} &  & \multicolumn{6}{c}{Flickr30K}                    \\
\multicolumn{1}{c|}{}                        &                           & $B\texttt{@}1$   & $B\texttt{@}4$   & M     & $R_L$   & C     & S     \\ \hline
\multicolumn{8}{c}{\textit{Text Reconstruction Training}}                                                                                                      \\ \hline
\multicolumn{1}{c|}{\textit{single token w/o noise}}                &              &    0.396    & 0.055 & 0.123 &   0.277    & 0.147 & 0.070 \\
\multicolumn{1}{c|}{\textit{single token w/ noise}}                 &              &    0.445   &  0.082 & 0.110 &  0.282     & 0.143 & 0.065 \\ 
\multicolumn{1}{c|}{\textit{multiple token w/o noise}}              &              &     0.388   & 0.049 & 0.122 &  0.277     & 0.135 &  0.060 \\
\multicolumn{1}{c|}{\textit{multiple token w/ noise}}                 &              &     0.520 & 0.107 &   0.153    & 0.369 & 0.197 & 0.084  \\ \hline
\multicolumn{8}{c}{\textit{Zero-shot Caption Generation}}                                                                                                     \\ \hline
\multicolumn{1}{c|}{\textit{CLS token}}                   &                 & 0.493 & 0.096 & 0.134 & 0.350 & 0.140 & 0.064 \\
\multicolumn{1}{c|}{\textit{subregion aggregation}}                  &                 & 0.520 & 0.107 & 0.153 & 0.369 & 0.197 & 0.084 \\
\multicolumn{1}{c|}{\textit{sampling and filtering}}                   &                 & \textbf{0.542} & \textbf{0.130} & \textbf{0.152} & \textbf{0.378} &  \textbf{0.199} & \textbf{0.078} \\
\toprule
\end{tabular}
\caption{\textbf{Ablation Results} on Flickr30K datasets. We evaluate the effectiveness of our training and inference paradigms.}
\label{table:ablation}
\end{table}

\paragraph{\textbf{Zero-shot Caption Generation}}
We conduct an ablation study based on the model trained with region noise. We modified the input image feature of the adaptor decoder module. We defined two modes: \textit{CLS token} indicate $I_{cr}$ doesn't contain subregion features $I_s$ and \textit{subregion aggregation} indicate $I_{cr}$ is the sum of $I_s$ and $I_c$. Furthermore, we incorporated the \textit{sampling and filtering} strategy. The obtained outcomes are illustrated in Table~\ref{table:ablation}. Our observations reveal that the adoption of the \textit{sampling and filtering} approach led to a noteworthy improvement in the BLEU metric, signifying the rectification of erroneous instances within the generated captions. However, its impact on semantic comprehension and contextual coherence was relatively modest in comparison.


% \subsection{Visualization}
%  In particular, we extract the attention weights from the language model and converter decoder, which are utilized to assign weights to the selected patches mentioned in Section \ref{method:inference}. Through our observation, we find that the converter decoder effectively learns to identify and select informative patches within the image, while the CLIP model filters out predictions that contain incorrectly predicted instances.



