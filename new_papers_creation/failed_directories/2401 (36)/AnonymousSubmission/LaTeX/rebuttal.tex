%File: anonymous-submission-latex-2024.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{graphicx}
\usepackage[dvipsnames,table,xcdraw]{xcolor}

\usepackage{booktabs}
\usepackage{amsmath}
\usepackage[table]{xcolor}
\usepackage{amssymb}

% added by ns
\usepackage{multirow}
\usepackage{threeparttable}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
%\title{Mining Fine-Grained Image-Text Alignment in CLIP for Zero-Shot Captioning via Text-Only Training}
\title{Author Response for Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training}
% \author{
%     %Authors
%     % All authors must be in the same font size and format.
%     Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
%     AAAI Style Contributions by Pater Patel Schneider,
%     Sunil Issar,\\ 
%     J. Scott Penberthy,
%     George Ferguson,
%     Hans Guesgen,
%     Francisco Cruz\equalcontrib,
%     Marc Pujol-Gonzalez\equalcontrib
% }
% \affiliations{
%     %Afiliations
%     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
%     % If you have multiple authors and multiple affiliations
%     % use superscripts in text and roman font to identify them.
%     % For example,

%     % Sunil Issar\textsuperscript{\rm 2},
%     % J. Scott Penberthy\textsuperscript{\rm 3},
%     % George Ferguson\textsuperscript{\rm 4},
%     % Hans Guesgen\textsuperscript{\rm 5}
%     % Note that the comma should be placed after the superscript

%     1900 Embarcadero Road, Suite 101\\
%     Palo Alto, California 94303-3310 USA\\
%     % email address must be in roman text type, not monospace or sans serif
%     proceedings-questions@aaai.org
% %
% % See more examples next
% }

% %Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \iffalse
% \title{My Publication Title --- Single Author}
% \author {
%     Author Name
% }
% \affiliations{
%     Affiliation\\
%     Affiliation Line 2\\
%     name@example.com
% }
% \fi

% \iffalse
% %Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \title{My Publication Title --- Multiple Authors}
% \author {
%     % Authors
%     First Author Name\textsuperscript{\rm 1},
%     Second Author Name\textsuperscript{\rm 2},
%     Third Author Name\textsuperscript{\rm 1}
% }
% \affiliations {
%     % Affiliations
%     \textsuperscript{\rm 1}Affiliation 1\\
%     \textsuperscript{\rm 2}Affiliation 2\\
%     firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
% }
% \fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry


\begin{document}


% \textcolor{Blue}{R3}
% \textcolor{Orange}{R4}
% \textcolor{Green}{R6}


% \maketitle

\noindent We sincerely thank the reviewers for their feedback. We are encouraged that the reviewers find our method \textit{novel and effective} (\textcolor{Blue}{R3},\textcolor{Green}{R6}), \textit{paper well written} (\textcolor{Blue}{R3}), \textit{In-depth analysis} (\textcolor{Green}{R6}) and \textit{experiments extensive} (\textcolor{Orange}{R4}). We address the reviewers' concerns below.


\noindent \textbf{\textcolor{blue}{@R3}:}  Thanks to the reviewer for acknowledging our work. We're glad that our efficient use of pretrained vision language models for zero-shot captioning was recognized. The positive feedback on the clear writing style and promising experimental results is appreciated.

\noindent \textbf{\textcolor{Orange}{@R4-Q1}: Modality Gap's Adverse Effect.}
The zero-shot captioning framework proposed in this paper capitalizes on CLIP's ability to learn a multi-modal embedding space, wherein semantically related images and text are encoded into features with close proximity. The performance is adversely affected when a significant modality gap exists, leading to substantial differences between training and inference. Our findings underscore the importance of minimizing this modality gap, as demonstrated in Main Paper Table 2. The \textit{Baseline} is trained to generate caption based on CLIP text features and ignore the modality gap between text features and image features, resulting in poor performance.

% ns 最棒啦zzzzzz qlt快去睡觉 略略略
\noindent \textbf{\textcolor{Orange}{@R4-Q2}: Verification of performance gains.}

\begin{table}[t]
\small
\centering
\tabcolsep=2pt
\begin{tabular}{l|llllll}
\hline
                            & B@1   & B@4   & M     & R\_L  & C     & S     \\ \hline
baseline                    & 0.414 & 0.069 & 0.141 & 0.317 & 0.221 & 0.079 \\ \hline
+ noise injection& 0.416 & 0.097 & 0.185 & 0.372 & 0.286 & 0.122 \\
+ subregion aggregation& 0.577 & 0.153 & 0.212 & 0.440 & 0.599 & 0.146 \\
+ sampling and filtering& 0.614 & 0.174 & 0.223 & 0.459 & 0.697 & 0.157 \\ \hline
\end{tabular}
\caption{}
\label{table:r2_abliation}
\end{table}
%略略略你仔细看这不是你的表n你w往下看看
As demonstrated in \textbf{\textcolor{Orange}{@R4-Q1}}, the baseline is directly trained on CLIP text embedding space then used to inference on CLIP image embedding space, where the gap is not processed. As discussed in paper Figure 1, we observed that the misalignment in paired image and text embedding appears as a Gaussian distribution with a mean approximately zero, noise injection is able to mimic image embedding during inference thus mitigate the misalignment. As shown in paper Table 1, with sub-region information aggregated, the similarity between CLIP text embedding and image embedding is also higher. Sampling and filtering strategies are introduced to relieve the additional uncertainty introduced by noise injection. We evaluate the performance of both the baseline and our proposed method on MSCOCO using the aforementioned techniques. The results, presented in Table~\ref{table:r2_abliation}, demonstrate performance improvement across all methods.

\begin{table}[b]
\small
\centering
\tabcolsep=2pt
\begin{tabular}{l|llllll}
\hline
                       & B@1   & B@4   & M     & R\_L  & C     & S     \\ \hline
informative subregions & 0.577 & 0.153 & 0.212 & 0.440 & 0.599 & 0.146 \\
half subregions        & 0.457 & 0.111 & 0.197 & 0.395 & 0.340 & 0.130 \\
random subregions      & 0.556 & 0.140 & 0.205 & 0.425 & 0.520 & 0.136 \\ \hline
\end{tabular}
\caption{}
\label{table:subregions}
\end{table}

\noindent \textbf{\textcolor{Orange}{@R4-Q3}: Representative of Subregions.}
In order to show the effectiveness of subregion information, we conduct an empirical analysis on MSCOCO and show the results in Table~\ref{table:subregions}. The first row is our method with only informative sub-region selected according to CLIP attention (without sampling and filtering) during inference.  Then we drop half of informative sub-region and show the performance at the second row. The performance drops significantly, which shows the information contained in sub-regions is not redundant. At the third row, we maintain the numbers of sub-regions the same, but we randomly select sub-regions instead of selecting according to CLIP attention. The performance also drops, which shows that the information contained in different sub-regions is actually different.
The reference paper demonstrate that the final layer in ViT appears to behave as a learned global pooling operation that aggregates information from all patches. The paper also find ViT trained with language model supervision like CLIP behave differently with ordinary ViTs. For example, CLIP is able to capture the components of images corresponding to caption text. Thus it is possible that CLIP maintain representative of subregions in images.



\noindent \textbf{\textcolor{Green}{@R6-Q1,Q2}: Selection of Sub-region.}
% We need to clarify that when computing the similarity of image and corresponding captions, we select top 10 subregion image feature and use the mean of these feature to compute similarity with caption feature. The operation is in line with the \textbf{Sub-region Feature Aggregation} mentioned in main paper \textit{4.3 Zero-shot Caption Generation}. To provide a comprehensive understanding of the subregion selection, we conduct more statistical analysis on the MSCOCO val set in Table\ref{table:subregion_abliation}. The \textit{Random 10 subregion} means the 10 image subregions are randomly selected and \textit{Random 1 subregion} means we only select one random image subregion. We observe that the \textit{Random 1 subregion} achieve the least similarity, which demonstrate the correct choice of subregion is important. Besides, the random select 10 image subregions achieve slightly worse performance than \textit{Mix representation}, which means the caption feature matches the mean of a subset of image regions and the subregions selected by the global CLS token are the best subset.

We would like to clarify our approach to computing the similarity between images and their corresponding captions. Specifically, we select the top 10 subregion image features and use their mean to calculate the similarity with the caption feature. This operation aligns with the \textbf{Sub-region Feature Aggregation} discussed in Section 4.3 of the main paper, titled \textit{Zero-shot Caption Generation}. In order to offer a comprehensive insight into subregion selection, additional statistical analysis was conducted on the MSCOCO validation set, presented in Table\ref{table:subregion_abliation}. In this table, \textit{Random 10 subregion} refers to the scenario where 10 image subregions are randomly selected, and \textit{Random 1 subregion} indicates the selection of a single random image subregion. Our observations reveal that \textit{Random 1 subregion} exhibits the least similarity, highlighting the importance of correct subregion choice. Additionally, the random selection of 10 image subregions yields slightly inferior performance compared to \textit{Mix representation}. This suggests that the caption feature aligns well with the mean of a subset of image regions, and the subregions chosen by the global CLS token represent the optimal subset.

\begin{table}[b]
\small
\centering
\tabcolsep=2pt
\begin{tabular}{c|ccc}
\hline
\multirow{2}{*}{} & \multicolumn{3}{l}{Pair Cosine Similarity} \\
                         & Mean        & Max        & Min        \\ \hline
 Global representation           & 0.330       & 0.446      & 0.228      \\ \hline
Random 1 subregion             & 0.327       & 0.421      & 0.224      \\ \hline
Random 10 subregion             & 0.350       & 0.428      & 0.241      \\ \hline
Mix representation             & 0.352       & 0.422      & 0.242      \\ \hline
\end{tabular}
\caption{}
\label{table:subregion_abliation}
\end{table}


\noindent \textbf{\textcolor{Green}{@R6-Q3}: Tables and Typo.}
Thanks for pointing out our mistakes. We will revise the typo and rearrange our tables in released version.


% \bibliography{aaai24}

\end{document}
