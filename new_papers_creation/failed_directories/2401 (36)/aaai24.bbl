\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Agrawal et~al.(2015)Agrawal, Lu, Antol, Mitchell, Zitnick, Parikh,
  and Batra}]{Agrawal2015VQAVQ}
Agrawal, A.; Lu, J.; Antol, S.; Mitchell, M.; Zitnick, C.~L.; Parikh, D.; and
  Batra, D. 2015.
\newblock VQA: Visual Question Answering.
\newblock \emph{International Journal of Computer Vision}, 123: 4--31.

\bibitem[{Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds et~al.}]{flamingo}
Alayrac, J.-B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Hasson, Y.; Lenc,
  K.; Mensch, A.; Millican, K.; Reynolds, M.; et~al. 2022.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:
  23716--23736.

\bibitem[{Anderson et~al.(2016)Anderson, Fernando, Johnson, and
  Gould}]{Anderson2016SPICESP}
Anderson, P.; Fernando, B.; Johnson, M.; and Gould, S. 2016.
\newblock SPICE: Semantic Propositional Image Caption Evaluation.
\newblock In \emph{European Conference on Computer Vision}.

\bibitem[{Anderson et~al.(2017)Anderson, He, Buehler, Teney, Johnson, Gould,
  and Zhang}]{conimgcap1}
Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; and
  Zhang, L. 2017.
\newblock Bottom-Up and Top-Down Attention for Image Captioning and Visual
  Question Answering.
\newblock \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 6077--6086.

\bibitem[{Antol et~al.(2015)Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, and
  Parikh}]{vqav2}
Antol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.; Zitnick, C.~L.; and
  Parikh, D. 2015.
\newblock Vqa: Visual question answering.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, 2425--2433.

\bibitem[{Banerjee and Lavie(2005)}]{Banerjee2005METEORAA}
Banerjee, S.; and Lavie, A. 2005.
\newblock METEOR: An Automatic Metric for MT Evaluation with Improved
  Correlation with Human Judgments.
\newblock In \emph{IEEvaluation@ACL}.

\bibitem[{Barraco et~al.(2022)Barraco, Cornia, Cascianelli, Baraldi, and
  Cucchiara}]{Barraco2022TheUE}
Barraco, M.; Cornia, M.; Cascianelli, S.; Baraldi, L.; and Cucchiara, R. 2022.
\newblock The Unreasonable Effectiveness of CLIP Features for Image Captioning:
  An Experimental Analysis.
\newblock \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW)}, 4661--4669.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}]{gpt3}
Brown, T.~B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.;
  Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.;
  Herbert-Voss, A.; Krueger, G.; Henighan, T.~J.; Child, R.; Ramesh, A.;
  Ziegler, D.~M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin,
  M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.;
  Sutskever, I.; and Amodei, D. 2020.
\newblock Language Models are Few-Shot Learners.
\newblock \emph{ArXiv}, abs/2005.14165.

\bibitem[{Changpinyo et~al.(2022)Changpinyo, Kukliansky, Szpektor, Chen, Ding,
  and Soricut}]{Changpinyo2022AllYM}
Changpinyo, S.; Kukliansky, D.; Szpektor, I.; Chen, X.; Ding, N.; and Soricut,
  R. 2022.
\newblock All You May Need for VQA are Image Captions.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics}.

\bibitem[{Changpinyo et~al.(2021{\natexlab{a}})Changpinyo, Sharma, Ding, and
  Soricut}]{Changpinyo}
Changpinyo, S.; Sharma, P.; Ding, N.; and Soricut, R. 2021{\natexlab{a}}.
\newblock Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To
  Recognize Long-Tail Visual Concepts.
\newblock In \emph{2021 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}.

\bibitem[{Changpinyo et~al.(2021{\natexlab{b}})Changpinyo, Sharma, Ding, and
  Soricut}]{cc3m}
Changpinyo, S.; Sharma, P.~K.; Ding, N.; and Soricut, R. 2021{\natexlab{b}}.
\newblock Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To
  Recognize Long-Tail Visual Concepts.
\newblock \emph{2021 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 3557--3567.

\bibitem[{Das et~al.(2016)Das, Kottur, Gupta, Singh, Yadav, Moura, Parikh, and
  Batra}]{Das2016VisualD}
Das, A.; Kottur, S.; Gupta, K.; Singh, A.; Yadav, D.; Moura, J. M.~F.; Parikh,
  D.; and Batra, D. 2016.
\newblock Visual Dialog.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 1080--1089.

\bibitem[{Feng et~al.(2018)Feng, Ma, Liu, and Luo}]{Feng2018UnsupervisedIC}
Feng, Y.; Ma, L.; Liu, W.; and Luo, J. 2018.
\newblock Unsupervised Image Captioning.
\newblock \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 4120--4129.

\bibitem[{Guo et~al.(2022{\natexlab{a}})Guo, Li, Li, Tiong, Li, Tao, and
  Hoi}]{Guo2022FromIT}
Guo, J.; Li, J.; Li, D.; Tiong, A. M.~H.; Li, B.; Tao, D.; and Hoi, S.
  2022{\natexlab{a}}.
\newblock From Images to Textual Prompts: Zero-shot VQA with Frozen Large
  Language Models.
\newblock \emph{ArXiv}, abs/2212.10846.

\bibitem[{Guo et~al.(2022{\natexlab{b}})Guo, Zhang, Qiu, Ma, Miao, He, and
  Cui}]{calip}
Guo, Z.; Zhang, R.; Qiu, L.; Ma, X.; Miao, X.; He, X.; and Cui, B.
  2022{\natexlab{b}}.
\newblock CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention.
\newblock \emph{ArXiv}, abs/2209.14169.

\bibitem[{Hudson and Manning(2019)}]{gqa}
Hudson, D.~A.; and Manning, C.~D. 2019.
\newblock GQA: A New Dataset for Real-World Visual Reasoning and Compositional
  Question Answering.
\newblock \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 6693--6702.

\bibitem[{Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and
  Duerig}]{align}
Jia, C.; Yang, Y.; Xia, Y.; Chen, Y.-T.; Parekh, Z.; Pham, H.; Le, Q.; Sung,
  Y.-H.; Li, Z.; and Duerig, T. 2021.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In \emph{International Conference on Machine Learning}, 4904--4916.
  PMLR.

\bibitem[{Laina, Rupprecht, and Navab(2019)}]{Laina2019TowardsUI}
Laina, I.; Rupprecht, C.; and Navab, N. 2019.
\newblock Towards Unsupervised Image Captioning With Shared Multimodal
  Embeddings.
\newblock \emph{2019 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, 7413--7423.

\bibitem[{Li et~al.(2022)Li, Li, Xiong, and Hoi}]{blip}
Li, J.; Li, D.; Xiong, C.; and Hoi, S. C.~H. 2022.
\newblock BLIP: Bootstrapping Language-Image Pre-training for Unified
  Vision-Language Understanding and Generation.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Li et~al.(2021)Li, Zhang, Zhang, Yang, Li, Zhong, Wang, Yuan, Zhang,
  Hwang, Chang, and Gao}]{glip}
Li, L.~H.; Zhang, P.; Zhang, H.; Yang, J.; Li, C.; Zhong, Y.; Wang, L.; Yuan,
  L.; Zhang, L.; Hwang, J.-N.; Chang, K.-W.; and Gao, J. 2021.
\newblock Grounded Language-Image Pre-training.
\newblock \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 10955--10965.

\bibitem[{Li et~al.(2023)Li, Zhu, Wen, and Yang}]{DeCap}
Li, W.; Zhu, L.; Wen, L.; and Yang, Y. 2023.
\newblock DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only
  Training.
\newblock \emph{arXiv preprint arXiv:2303.03032}.

\bibitem[{Liang et~al.(2022)Liang, Zhang, Kwon, Yeung, and Zou}]{MindGap}
Liang, V.~W.; Zhang, Y.; Kwon, Y.; Yeung, S.; and Zou, J.~Y. 2022.
\newblock Mind the gap: Understanding the modality gap in multi-modal
  contrastive representation learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:
  17612--17625.

\bibitem[{Liu et~al.(2021)Liu, Wu, You, Ge, Zou, and Sun}]{Liu2021AligningSV}
Liu, F.; Wu, X.; You, C.; Ge, S.; Zou, Y.; and Sun, X. 2021.
\newblock Aligning Source Visual and Target Language Domains for Unpaired Video
  Captioning.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 44: 9255--9268.

\bibitem[{Liu et~al.(2023)Liu, Li, Wu, and Lee}]{llava}
Liu, H.; Li, C.; Wu, Q.; and Lee, Y.~J. 2023.
\newblock Visual Instruction Tuning.
\newblock \emph{ArXiv}, abs/2304.08485.

\bibitem[{Marino et~al.(2019)Marino, Rastegari, Farhadi, and Mottaghi}]{okvqa}
Marino, K.; Rastegari, M.; Farhadi, A.; and Mottaghi, R. 2019.
\newblock OK-VQA: A Visual Question Answering Benchmark Requiring External
  Knowledge.
\newblock \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 3190--3199.

\bibitem[{Mokady(2021)}]{clipcap}
Mokady, R. 2021.
\newblock ClipCap: CLIP Prefix for Image Captioning.
\newblock \emph{ArXiv}, abs/2111.09734.

\bibitem[{Ning et~al.(2023)Ning, Qiu, Liu, and He}]{hoiclip}
Ning, S.; Qiu, L.; Liu, Y.; and He, X. 2023.
\newblock HOICLIP: Efficient Knowledge Transfer for HOI Detection with
  Vision-Language Models.
\newblock \emph{ArXiv}, abs/2303.15786.

\bibitem[{Niu et~al.(2018)Niu, Zhang, Zhang, Zhang, Lu, and
  Wen}]{Niu2018RecursiveVA}
Niu, Y.; Zhang, H.; Zhang, M.; Zhang, J.; Lu, Z.; and Wen, J.-R. 2018.
\newblock Recursive Visual Attention in Visual Dialog.
\newblock \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 6672--6681.

\bibitem[{Nukrai, Mokady, and Globerson(2022)}]{CapDec}
Nukrai, D.; Mokady, R.; and Globerson, A. 2022.
\newblock Text-Only Training for Image Captioning using Noise-Injected CLIP.
\newblock \emph{arXiv preprint arXiv:2211.00575}.

\bibitem[{Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu}]{Papineni2002BleuAM}
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
\newblock Bleu: a Method for Automatic Evaluation of Machine Translation.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}.

\bibitem[{Patashnik et~al.(2021)Patashnik, Wu, Shechtman, Cohen-Or, and
  Lischinski}]{styleclip}
Patashnik, O.; Wu, Z.; Shechtman, E.; Cohen-Or, D.; and Lischinski, D. 2021.
\newblock StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery.
\newblock \emph{2021 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, 2065--2074.

\bibitem[{Plummer et~al.(2015)Plummer, Wang, Cervantes, Caicedo, Hockenmaier,
  and Lazebnik}]{Flickr30k}
Plummer, B.~A.; Wang, L.; Cervantes, C.~M.; Caicedo, J.~C.; Hockenmaier, J.;
  and Lazebnik, S. 2015.
\newblock Flickr30k Entities: Collecting Region-to-Phrase Correspondences for
  Richer Image-to-Sentence Models.
\newblock \emph{International Journal of Computer Vision}, 123: 74--93.

\bibitem[{Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark et~al.}]{clip}
Radford, A.; Kim, J.~W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry,
  G.; Askell, A.; Mishkin, P.; Clark, J.; et~al. 2021.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International conference on machine learning}, 8748--8763.
  PMLR.

\bibitem[{Rao et~al.(2021)Rao, Zhao, Chen, Tang, Zhu, Huang, Zhou, and
  Lu}]{denclip}
Rao, Y.; Zhao, W.; Chen, G.; Tang, Y.; Zhu, Z.; Huang, G.; Zhou, J.; and Lu, J.
  2021.
\newblock DenseCLIP: Language-Guided Dense Prediction with Context-Aware
  Prompting.
\newblock \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 18061--18070.

\bibitem[{Shen et~al.(2021)Shen, Li, Tan, Bansal, Rohrbach, Chang, Yao, and
  Keutzer}]{Shen2021HowMC}
Shen, S.; Li, L.~H.; Tan, H.; Bansal, M.; Rohrbach, A.; Chang, K.-W.; Yao, Z.;
  and Keutzer, K. 2021.
\newblock How Much Can CLIP Benefit Vision-and-Language Tasks?
\newblock \emph{ArXiv}, abs/2107.06383.

\bibitem[{Su et~al.(2022)Su, Lan, Liu, Liu, Yogatama, Wang, Kong, and
  Collier}]{MAGIC}
Su, Y.; Lan, T.; Liu, Y.; Liu, F.; Yogatama, D.; Wang, Y.; Kong, L.; and
  Collier, N. 2022.
\newblock Language models can see: plugging visual controls in text generation.
\newblock \emph{arXiv preprint arXiv:2205.02655}.

\bibitem[{Tewel et~al.(2022)Tewel, Shalev, Schwartz, and Wolf}]{ZeroCap}
Tewel, Y.; Shalev, Y.; Schwartz, I.; and Wolf, L. 2022.
\newblock ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic
  Arithmetic.
\newblock In \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}.

\bibitem[{Tiong et~al.(2022)Tiong, Li, Li, Savarese, and
  Hoi}]{Tiong2022PlugandPlayVZ}
Tiong, A. M.~H.; Li, J.; Li, B.; Savarese, S.; and Hoi, S. C.~H. 2022.
\newblock Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained
  Models with Zero Training.
\newblock \emph{ArXiv}, abs/2210.08773.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{Vaswani2017AttentionIA}
Vaswani, A.; Shazeer, N.~M.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez,
  A.~N.; Kaiser, L.; and Polosukhin, I. 2017.
\newblock Attention is All you Need.
\newblock In \emph{NIPS}.

\bibitem[{Vedantam, Zitnick, and Parikh(2014)}]{Vedantam2014CIDErCI}
Vedantam, R.; Zitnick, C.~L.; and Parikh, D. 2014.
\newblock CIDEr: Consensus-based image description evaluation.
\newblock \emph{2015 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 4566--4575.

\bibitem[{Vinyals et~al.(2016)Vinyals, Toshev, Bengio, and Erhan}]{mscoco}
Vinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2016.
\newblock Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning
  Challenge.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 39: 652--663.

\bibitem[{Wang and Liu(2020)}]{Wang2020UnderstandingTB}
Wang, F.; and Liu, H. 2020.
\newblock Understanding the Behaviour of Contrastive Loss.
\newblock \emph{2021 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2495--2504.

\bibitem[{Wang and Isola(2020)}]{Wang2020UnderstandingCR}
Wang, T.; and Isola, P. 2020.
\newblock Understanding Contrastive Representation Learning through Alignment
  and Uniformity on the Hypersphere.
\newblock \emph{ArXiv}, abs/2005.10242.

\bibitem[{Wang et~al.(2021)Wang, Yu, Yu, Dai, Tsvetkov, and
  Cao}]{Wang_Yu_Yu_Dai_Tsvetkov_Cao_2021}
Wang, Z.; Yu, J.; Yu, A.; Dai, Z.; Tsvetkov, Y.; and Cao, Y. 2021.
\newblock SimVLM: Simple Visual Language Model Pretraining with Weak
  Supervision.

\bibitem[{Xu et~al.(2015)Xu, Ba, Kiros, Cho, Courville, Salakhutdinov, Zemel,
  and Bengio}]{conimgcap4}
Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.~C.; Salakhutdinov, R.; Zemel,
  R.~S.; and Bengio, Y. 2015.
\newblock Show, Attend and Tell: Neural Image Caption Generation with Visual
  Attention.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Yao et~al.(2018)Yao, Pan, Li, and Mei}]{conimgcap3}
Yao, T.; Pan, Y.; Li, Y.; and Mei, T. 2018.
\newblock Exploring Visual Relationship for Image Captioning.
\newblock In \emph{European Conference on Computer Vision}.

\bibitem[{Yu et~al.(2022)Yu, Chung, Yun, Hessel, Park, Lu, Ammanabrolu,
  Zellers, Bras, Kim, and Choi}]{Yu2022MultimodalKA}
Yu, Y.; Chung, J.; Yun, H.; Hessel, J.; Park, J.~S.; Lu, X.; Ammanabrolu, P.;
  Zellers, R.; Bras, R.~L.; Kim, G.; and Choi, Y. 2022.
\newblock Multimodal Knowledge Alignment with Reinforcement Learning.
\newblock \emph{ArXiv}, abs/2205.12630.

\bibitem[{Yuksekgonul et~al.(2022)Yuksekgonul, Bianchi, Kalluri, Jurafsky, and
  Zou}]{clipbagofwords}
Yuksekgonul, M.; Bianchi, F.; Kalluri, P.; Jurafsky, D.; and Zou, J.~Y. 2022.
\newblock When and why vision-language models behave like bags-of-words, and
  what to do about it?
\newblock \emph{ArXiv}, abs/2210.01936.

\bibitem[{Zeng et~al.(2022)Zeng, Wong, Welker, Choromanski, Tombari, Purohit,
  Ryoo, Sindhwani, Lee, Vanhoucke, and Florence}]{Zeng2022SocraticMC}
Zeng, A.; Wong, A.~S.; Welker, S.; Choromanski, K.; Tombari, F.; Purohit, A.;
  Ryoo, M.~S.; Sindhwani, V.; Lee, J.; Vanhoucke, V.; and Florence, P.~R. 2022.
\newblock Socratic Models: Composing Zero-Shot Multimodal Reasoning with
  Language.
\newblock \emph{ArXiv}, abs/2204.00598.

\bibitem[{Zhang et~al.(2022{\natexlab{a}})Zhang, Zhang, Hu, Chen, Li, Dai,
  Wang, Yuan, Hwang, and Gao}]{glip2}
Zhang, H.; Zhang, P.; Hu, X.; Chen, Y.-C.; Li, L.~H.; Dai, X.; Wang, L.; Yuan,
  L.; Hwang, J.-N.; and Gao, J. 2022{\natexlab{a}}.
\newblock GLIPv2: Unifying Localization and Vision-Language Understanding.
\newblock \emph{ArXiv}, abs/2206.05836.

\bibitem[{Zhang et~al.(2021)Zhang, Guo, Zhang, Li, Miao, Cui, Qiao, Gao, and
  Li}]{pointclip}
Zhang, R.; Guo, Z.; Zhang, W.; Li, K.; Miao, X.; Cui, B.; Qiao, Y.~J.; Gao, P.;
  and Li, H. 2021.
\newblock PointCLIP: Point Cloud Understanding by CLIP.
\newblock \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 8542--8552.

\bibitem[{Zhang et~al.(2022{\natexlab{b}})Zhang, Roller, Goyal, Artetxe, Chen,
  Chen, Dewan, Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura,
  Sridhar, Wang, and Zettlemoyer}]{Zhang2022OPTOP}
Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen, S.; Dewan, C.;
  Diab, M.; Li, X.; Lin, X.~V.; Mihaylov, T.; Ott, M.; Shleifer, S.; Shuster,
  K.; Simig, D.; Koura, P.~S.; Sridhar, A.; Wang, T.; and Zettlemoyer, L.
  2022{\natexlab{b}}.
\newblock OPT: Open Pre-trained Transformer Language Models.
\newblock \emph{ArXiv}, abs/2205.01068.

\bibitem[{Zhou, Loy, and Dai(2021)}]{maskclip}
Zhou, C.; Loy, C.~C.; and Dai, B. 2021.
\newblock Extract Free Dense Labels from CLIP.
\newblock In \emph{European Conference on Computer Vision}.

\bibitem[{Zhou et~al.(2019)Zhou, Palangi, Zhang, Hu, Corso, and
  Gao}]{conimgcap2}
Zhou, L.; Palangi, H.; Zhang, L.; Hu, H.; Corso, J.~J.; and Gao, J. 2019.
\newblock Unified Vision-Language Pre-Training for Image Captioning and VQA.
\newblock \emph{ArXiv}, abs/1909.11059.

\bibitem[{Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny}]{miniGPT4}
Zhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M. 2023.
\newblock MiniGPT-4: Enhancing Vision-Language Understanding with Advanced
  Large Language Models.
\newblock \emph{ArXiv}, abs/2304.10592.

\end{thebibliography}
