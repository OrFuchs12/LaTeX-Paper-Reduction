\relax 
\bibstyle{aaai24}
\citation{Agrawal2015VQAVQ,gqa,okvqa}
\citation{Das2016VisualD,Niu2018RecursiveVA,llava}
\citation{conimgcap4,conimgcap1,conimgcap3,conimgcap2}
\citation{clip}
\citation{gpt3,Zhang2022OPTOP}
\citation{miniGPT4,llava}
\citation{Guo2022FromIT,Changpinyo2022AllYM,Tiong2022PlugandPlayVZ}
\citation{MindGap}
\citation{DeCap}
\citation{CapDec}
\citation{mscoco}
\citation{Flickr30k}
\citation{vqav2}
\citation{Changpinyo,Wang_Yu_Yu_Dai_Tsvetkov_Cao_2021,flamingo}
\citation{CapDec}
\citation{DeCap}
\citation{MAGIC}
\citation{ZeroCap}
\citation{clip,align,glip,blip,glip2}
\citation{styleclip,maskclip,pointclip,denclip,calip,hoiclip}
\citation{Wang2020UnderstandingCR,Wang2020UnderstandingTB,clipbagofwords,MindGap}
\citation{MindGap}
\citation{MindGap}
\citation{MindGap}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{figure:ex}{{1}{3}{The upper half of this figure is an example of the misalignment in paired image and text description. The lower half of this figure is the distribution of modality gap between text representation and global / local image representation respectively.}{}{}}
\newlabel{table:subregion}{{1}{3}{In this table, we show the mean, max and min value of similarity between text feature and global/mix feature. We find that after adding subregion information, the mean, max and min value all increase. This observation shows that introducing subregion image information benefit the alleviation of modality gap.}{}{}}
\newlabel{method:definition}{{4.1}{3}{}{}{}}
\newlabel{method:train}{{4.2}{3}{}{}{}}
\citation{Vaswani2017AttentionIA}
\newlabel{figure:pipeline}{{2}{4}{\textbf  {An overview of MacCap pipeline.} MacCap learns to generate text based on region noise injected CLIP text feature in text reconstruction training. During inference, MacCap can generate caption without paired data in training. The CLIP and language model are kept frozen in both stages. }{}{}}
\newlabel{method:subregion}{{4.3}{4}{}{}{}}
\citation{mscoco}
\citation{Flickr30k}
\citation{CapDec}
\citation{cc3m}
\citation{DeCap}
\citation{Papineni2002BleuAM}
\citation{Banerjee2005METEORAA}
\citation{Vedantam2014CIDErCI}
\citation{Anderson2016SPICESP}
\citation{vqav2}
\citation{CapDec,MAGIC,ZeroCap,DeCap}
\citation{Vaswani2017AttentionIA}
\citation{CapDec}
\citation{Zhang2022OPTOP}
\citation{DeCap}
\citation{cc3m}
\citation{MAGIC}
\citation{MAGIC}
\citation{MAGIC}
\citation{CapDec}
\citation{Zeng2022SocraticMC}
\citation{gpt3}
\citation{clipcap}
\citation{Shen2021HowMC}
\citation{Liu2021AligningSV}
\citation{Barraco2022TheUE}
\citation{Yu2022MultimodalKA}
\citation{Yu2022MultimodalKA}
\citation{ZeroCap}
\citation{MAGIC}
\citation{MAGIC}
\citation{CapDec}
\citation{DeCap}
\citation{MAGIC}
\citation{CapDec}
\citation{DeCap}
\citation{MAGIC}
\citation{MAGIC}
\newlabel{figure:rerank}{{3}{5}{\textbf  {Multiple sampling and filtering pipeline} During inference, each image uses noise to generate several different captions, which are reranked by CLIP to output the best.}{}{}}
\newlabel{tab:cross_domain_results}{{2}{6}{\textbf  {Zero-shot Cross Domain Captioning}: We conduct experiments on cross-domain image captioning tasks. X to Y means source to target domain. We reproduce Magic, CapDec, and DeCap under the frozen LLM setting and mark them with \textbf  {$\dagger $}.}{}{}}
\newlabel{tab:in_domain_results}{{3}{6}{\textbf  {Zero-shot In Domain Captioning}: The notation "P.", "I.", and "T." are used to represent paired data, unpaired image data, and unpaired text data, respectively. We reproduce Magic, CapDec, and DeCap under frozen language model setting and mark them with \textbf  {$\dagger $}. Results tagged $\star $ are from \citep  {MAGIC}}{}{}}
\citation{mscoco}
\citation{Flickr30k}
\citation{DeCap}
\citation{Laina2019TowardsUI}
\citation{Feng2018UnsupervisedIC}
\citation{DeCap,CapDec}
\newlabel{table:VQAV2}{{4}{7}{\textbf  {Zero-shot VQA results} on VQAV2 validation set. $\dagger $ means the models come from the official release. Our method achieves superior performance under a frozen language model setting.}{}{}}
\newlabel{table:ablation}{{5}{7}{\textbf  {Ablation Results} on Flickr30K datasets. We evaluate the effectiveness of our training and inference paradigms.}{}{}}
\bibdata{aaai24}
\citation{DeCap}
\citation{CapDec}
\citation{CapDec}
\citation{MindGap}
\newlabel{figure:noise}{{4}{9}{\textbf  {Performance of MacCap with different training noise.} The MacCap is trained in CC3M dataset and tested on Flickr30K datasets.}{}{}}
\newlabel{figure:seq}{{5}{9}{\textbf  {Performance of MacCap with different patch numbers in inference.} The MacCap is trained in CC3M dataset and tested on Flickr30K datasets. The length of the text region feature in text reconstruction training is set to 10.}{}{}}
\newlabel{tab:params}{{\caption@xref {tab:params}{ on input line 195}}{10}{}{}{}}
\newlabel{figure:visulization}{{6}{10}{\textbf  {Visualization of Embedding Distributions on MSCOCO.} The red points stand the global image embedding, each green point stand a local image embedding, and the yellow points near (0, 0) stand for the text embedding.}{}{}}
\gdef \@abspage@last{10}
