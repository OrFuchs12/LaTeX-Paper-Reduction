@book{em:86,
  editor  = "Engelmore, Robert and Morgan, Anthony",
  title   = "Blackboard Systems",
  year    = 1986,
  address = "Reading, Mass.",
  publisher = "Addison-Wesley",
}

@inproceedings{c:83,
  author  = "Clancey, William J.",
  year    = 1983,
  title   = "{Communication, Simulation, and Intelligent
Agents: Implications of Personal Intelligent Machines
for Medical Education}",
  booktitle="Proceedings of the Eighth International Joint Conference on Artificial Intelligence {(IJCAI-83)}", 
  pages   = "556-560",
  address = "Menlo Park, Calif",
  publisher = "{IJCAI Organization}",
}
@inproceedings{c:84,
  author  = "Clancey, William J.",
  year    = 1984,
  title   = "{Classification Problem Solving}",
  booktitle = "Proceedings of the Fourth National 
              Conference on Artificial Intelligence",
  pages   = "45-54",
  address = "Menlo Park, Calif.",
  publisher="AAAI Press",
}
@article{r:80,
  author = {Robinson, Arthur L.},
  title = {New Ways to Make Microcircuits Smaller},
  volume = {208},
  number = {4447},
  pages = {1019--1022},
  year = {1980},
  doi = {10.1126/science.208.4447.1019},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  URL = {https://science.sciencemag.org/content/208/4447/1019},
  eprint = {https://science.sciencemag.org/content/208/4447/1019.full.pdf},
  journal = {Science},
}
@article{r:80x,
  author  = "Robinson, Arthur L.",
  year    = 1980,
  title   = "{New Ways to Make Microcircuits Smaller---Duplicate Entry}",
  journal = "Science",
  volume  =  208,
  pages   = "1019-1026",
}
@article{hcr:83,
title = {Strategic explanations for a diagnostic consultation system},
journal = {International Journal of Man-Machine Studies},
volume = {20},
number = {1},
pages = {3-19},
year = {1984},
issn = {0020-7373},
doi = {https://doi.org/10.1016/S0020-7373(84)80003-6},
url = {https://www.sciencedirect.com/science/article/pii/S0020737384800036},
author = {Diane Warner Hasling and William J. Clancey and Glenn Rennels},
abstract = {This article examines the problem of automatte explanation of reasoning, especially as it relates to expert systems. By explanation we mean the ability of a program to discuss what it is doing in some understandable way. We first present a general framework in which to view explanation and review some of the research done in this area. We then focus on the explanation system for NEOMYCIN, a medical consultation program. A consultation program interactively helps a user to solve a problem. Our goal is to have NEOMYCIN explain its problem-solving strategies. An explanation of strategy describes the plan the program is using to reach a solution. Such an explanation is usually concrete, referring to aspects of the current problem situation. Abstract explanations articulate a general principle, which can be applied in different situations; such explanations are useful in teaching and in explaining by analogy. We describe the aspects of NEOMYCIN that make abstract strategic explanations possible—the representation of strategic knowledge explicitly and separately from domain knowledge— and demonstrate how this representation can be used to generate explanations.}
}
@article{hcrt:83,
  author  = "Hasling, Diane Warner and Clancey, William J. and Rennels, Glenn R. and Test, Thomas",
  year    = 1983,
  title   = "{Strategic Explanations in Consultation---Duplicate}",
  journal = "The International Journal of Man-Machine Studies",
  volume  = 20,
  number  = 1,
  pages   = "3-19",
}
@techreport{r:86,
  author  = "Rice, James",
  year    = 1986,
  title   = "{Poligon: A System for Parallel Problem Solving}",
  type    = "Technical Report", 
  number  = "KSL-86-19", 
  institution = "Dept.\ of Computer Science, Stanford Univ.",
}
@phdthesis{c:79,
  author  = "Clancey, William J.",
  year    = 1979,
  title   = "{Transfer of Rule-Based Expertise
through a Tutorial Dialogue}",
  type    = "{Ph.D.} diss.",
  school  = "Dept.\ of Computer Science, Stanford Univ.",
  address = "Stanford, Calif.",
}
@unpublished{c:21,
  author  = "Clancey, William J.",
  title   = "{The Engineering of Qualitative Models}",
  year    = 2021,
  note    = "Forthcoming",
}
@misc{c:22,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{c:23,
  title        = "Pluto: The 'Other' Red Planet",
  author       = "{NASA}",
  howpublished = "\url{https://www.nasa.gov/nh/pluto-the-other-red-planet}",
  year         = 2015,
  note         = "Accessed: 2018-12-06"
}


@article{VSG_net,
  title={VSGNet: Spatial Attention Network for Detecting Human Object Interactions Using Graph Convolutions},
  author={Oytun Ulutan and A S M Iftekhar and B.S. Manjunath},
  journal={computer vision and pattern recognition},
  year={2020}
}

@article{DongmingYang2020AGI,
  title={A Graph-based Interactive Reasoning for Human-Object Interaction Detection},
  author={Dongming Yang and Yuexian Zou},
  journal={international joint conference on artificial intelligence},
  year={2020}
}

@inproceedings{ZeroCap,   title={ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic  Arithmetic},  url={http://dx.doi.org/10.1109/cvpr52688.2022.01739},  DOI={10.1109/cvpr52688.2022.01739},  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},  author={Tewel, Yoad and Shalev, Yoav and Schwartz, Idan and Wolf, Lior},  year={2022},  month={Sep},  language={en-US}  }


@article{DeCap,
  title={DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training},
  author={Li, Wei and Zhu, Linchao and Wen, Longyin and Yang, Yi},
  journal={arXiv preprint arXiv:2303.03032},
  year={2023}
}

@article{MAGIC,
  title={Language models can see: plugging visual controls in text generation},
  author={Su, Yixuan and Lan, Tian and Liu, Yahui and Liu, Fangyu and Yogatama, Dani and Wang, Yan and Kong, Lingpeng and Collier, Nigel},
  journal={arXiv preprint arXiv:2205.02655},
  year={2022}
}

@article{CapDec,
  title={Text-Only Training for Image Captioning using Noise-Injected CLIP},
  author={Nukrai, David and Mokady, Ron and Globerson, Amir},
  journal={arXiv preprint arXiv:2211.00575},
  year={2022}
}

@article{MindGap,
  title={Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning},
  author={Liang, Victor Weixin and Zhang, Yuhui and Kwon, Yongchan and Yeung, Serena and Zou, James Y},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17612--17625},
  year={2022}
}

@inproceedings{Changpinyo,   title={Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},  url={http://dx.doi.org/10.1109/cvpr46437.2021.00356},  DOI={10.1109/cvpr46437.2021.00356},  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},  author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},  year={2021},  month={Nov},  language={en-US}  }

@misc{Wang_Yu_Yu_Dai_Tsvetkov_Cao_2021,   title={SimVLM: Simple Visual Language Model Pretraining with Weak Supervision},  journal={Cornell University - arXiv},  author={Wang, Zirui and Yu, Jiahui and Yu, AdamsWei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},  year={2021},  month={Aug},  language={en-US}  }

@article{flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}


@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{align,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={International Conference on Machine Learning},
  pages={4904--4916},
  year={2021},
  organization={PMLR}
}

@article{glip,
  title={Grounded Language-Image Pre-training},
  author={Liunian Harold Li and Pengchuan Zhang and Haotian Zhang and Jianwei Yang and Chunyuan Li and Yiwu Zhong and Lijuan Wang and Lu Yuan and Lei Zhang and Jenq-Neng Hwang and Kai-Wei Chang and Jianfeng Gao},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={10955-10965}
}

@article{glip2,
  title={GLIPv2: Unifying Localization and Vision-Language Understanding},
  author={Haotian Zhang and Pengchuan Zhang and Xiaowei Hu and Yen-Chun Chen and Liunian Harold Li and Xiyang Dai and Lijuan Wang and Lu Yuan and Jenq-Neng Hwang and Jianfeng Gao},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.05836}
}

@inproceedings{blip,
  title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  author={Junnan Li and Dongxu Li and Caiming Xiong and Steven C. H. Hoi},
  booktitle={International Conference on Machine Learning},
  year={2022}
}

@article{hoiclip,
  title={HOICLIP: Efficient Knowledge Transfer for HOI Detection with Vision-Language Models},
  author={Sha Ning and Longtian Qiu and Yongfei Liu and Xuming He},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.15786}
}

@article{styleclip,
  title={StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery},
  author={Or Patashnik and Zongze Wu and Eli Shechtman and Daniel Cohen-Or and Dani Lischinski},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={2065-2074}
}

@inproceedings{maskclip,
  title={Extract Free Dense Labels from CLIP},
  author={Chong Zhou and Chen Change Loy and Bo Dai},
  booktitle={European Conference on Computer Vision},
  year={2021}
}

@article{clipcap,
  title={ClipCap: CLIP Prefix for Image Captioning},
  author={Ron Mokady},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.09734}
}

@article{pointclip,
  title={PointCLIP: Point Cloud Understanding by CLIP},
  author={Renrui Zhang and Ziyu Guo and Wei Zhang and Kunchang Li and Xupeng Miao and Bin Cui and Yu Jiao Qiao and Peng Gao and Hongsheng Li},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={8542-8552}
}

@article{calip,
  title={CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention},
  author={Ziyu Guo and Renrui Zhang and Longtian Qiu and Xianzheng Ma and Xupeng Miao and Xuming He and Bin Cui},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.14169}
}

@article{denclip,
  title={DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting},
  author={Yongming Rao and Wenliang Zhao and Guangyi Chen and Yansong Tang and Zheng Zhu and Guan Huang and Jie Zhou and Jiwen Lu},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={18061-18070}
}

@article{Shen2021HowMC,
  title={How Much Can CLIP Benefit Vision-and-Language Tasks?},
  author={Sheng Shen and Liunian Harold Li and Hao Tan and Mohit Bansal and Anna Rohrbach and Kai-Wei Chang and Zhewei Yao and Kurt Keutzer},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.06383}
}

@article{clipbagofwords,
  title={When and why vision-language models behave like bags-of-words, and what to do about it?},
  author={Mert Yuksekgonul and Federico Bianchi and Pratyusha Kalluri and Dan Jurafsky and James Y. Zou},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.01936}
}

@article{Wang2020UnderstandingCR,
  title={Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere},
  author={Tongzhou Wang and Phillip Isola},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.10242}
}

@article{Wang2020UnderstandingTB,
  title={Understanding the Behaviour of Contrastive Loss},
  author={Feng Wang and Huaping Liu},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={2495-2504}
}

@article{Agrawal2015VQAVQ,
  title={VQA: Visual Question Answering},
  author={Aishwarya Agrawal and Jiasen Lu and Stanislaw Antol and Margaret Mitchell and C. Lawrence Zitnick and Devi Parikh and Dhruv Batra},
  journal={International Journal of Computer Vision},
  year={2015},
  volume={123},
  pages={4-31}
}

@article{Das2016VisualD,
  title={Visual Dialog},
  author={Abhishek Das and Satwik Kottur and Khushi Gupta and Avi Singh and Deshraj Yadav and Jos{\'e} M. F. Moura and Devi Parikh and Dhruv Batra},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={1080-1089}
}

@article{conimgcap1,
  title={Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
  author={Peter Anderson and Xiaodong He and Chris Buehler and Damien Teney and Mark Johnson and Stephen Gould and Lei Zhang},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2017},
  pages={6077-6086}
}

@article{conimgcap2,
  title={Unified Vision-Language Pre-Training for Image Captioning and VQA},
  author={Luowei Zhou and Hamid Palangi and Lei Zhang and Houdong Hu and Jason J. Corso and Jianfeng Gao},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.11059}
}

@inproceedings{conimgcap3,
  title={Exploring Visual Relationship for Image Captioning},
  author={Ting Yao and Yingwei Pan and Yehao Li and Tao Mei},
  booktitle={European Conference on Computer Vision},
  year={2018}
}

@inproceedings{conimgcap4,
  title={Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
  author={Ke Xu and Jimmy Ba and Ryan Kiros and Kyunghyun Cho and Aaron C. Courville and Ruslan Salakhutdinov and Richard S. Zemel and Yoshua Bengio},
  booktitle={International Conference on Machine Learning},
  year={2015}
}

@article{mscoco,
  title={Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge},
  author={Oriol Vinyals and Alexander Toshev and Samy Bengio and D. Erhan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2016},
  volume={39},
  pages={652-663}
}

@article{Flickr30k,
  title={Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models},
  author={Bryan A. Plummer and Liwei Wang and Christopher M. Cervantes and Juan C. Caicedo and J. Hockenmaier and Svetlana Lazebnik},
  journal={International Journal of Computer Vision},
  year={2015},
  volume={123},
  pages={74-93}
}

@inproceedings{Papineni2002BleuAM,
  title={Bleu: a Method for Automatic Evaluation of Machine Translation},
  author={Kishore Papineni and Salim Roukos and Todd Ward and Wei-Jing Zhu},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2002}
}

@inproceedings{Banerjee2005METEORAA,
  title={METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments},
  author={Satanjeev Banerjee and Alon Lavie},
  booktitle={IEEvaluation@ACL},
  year={2005}
}

@article{Vedantam2014CIDErCI,
  title={CIDEr: Consensus-based image description evaluation},
  author={Ramakrishna Vedantam and C. Lawrence Zitnick and Devi Parikh},
  journal={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2014},
  pages={4566-4575}
}

@inproceedings{Anderson2016SPICESP,
  title={SPICE: Semantic Propositional Image Caption Evaluation},
  author={Peter Anderson and Basura Fernando and Mark Johnson and Stephen Gould},
  booktitle={European Conference on Computer Vision},
  year={2016}
}

@inproceedings{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={NIPS},
  year={2017}
}

@article{Zhang2022OPTOP,
  title={OPT: Open Pre-trained Transformer Language Models},
  author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.01068}
}

@article{cc3m,
  title={Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},
  author={Soravit Changpinyo and Piyush Kumar Sharma and Nan Ding and Radu Soricut},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={3557-3567}
}

@article{Zeng2022SocraticMC,
  title={Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
  author={Andy Zeng and Adrian S. Wong and Stefan Welker and Krzysztof Choromanski and Federico Tombari and Aveek Purohit and Michael S. Ryoo and Vikas Sindhwani and Johnny Lee and Vincent Vanhoucke and Peter R. Florence},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.00598}
}

@article{gpt3,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and T. J. Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165}
}

@article{Feng2018UnsupervisedIC,
  title={Unsupervised Image Captioning},
  author={Yang Feng and Lin Ma and Wei Liu and Jiebo Luo},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018},
  pages={4120-4129}
}

@article{Laina2019TowardsUI,
  title={Towards Unsupervised Image Captioning With Shared Multimodal Embeddings},
  author={Iro Laina and C. Rupprecht and Nassir Navab},
  journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2019},
  pages={7413-7423}
}

@article{Anderson2017BottomUpAT,
  title={Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
  author={Peter Anderson and Xiaodong He and Chris Buehler and Damien Teney and Mark Johnson and Stephen Gould and Lei Zhang},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2017},
  pages={6077-6086}
}

@article{Barraco2022TheUE,
  title={The Unreasonable Effectiveness of CLIP Features for Image Captioning: An Experimental Analysis},
  author={Manuele Barraco and Marcella Cornia and Silvia Cascianelli and Lorenzo Baraldi and Rita Cucchiara},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year={2022},
  pages={4661-4669}
}

@article{Liu2021AligningSV,
  title={Aligning Source Visual and Target Language Domains for Unpaired Video Captioning},
  author={Fenglin Liu and Xian Wu and Chenyu You and Shen Ge and Yuexian Zou and Xu Sun},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  volume={44},
  pages={9255-9268}
}

@article{Yu2022MultimodalKA,
  title={Multimodal Knowledge Alignment with Reinforcement Learning},
  author={Youngjae Yu and Jiwan Chung and Heeseung Yun and Jack Hessel and Jae Sung Park and Ximing Lu and Prithviraj Ammanabrolu and Rowan Zellers and Ronan Le Bras and Gunhee Kim and Yejin Choi},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.12630}
}

@inproceedings{vqav2,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2425--2433},
  year={2015}
}

@article{ViT,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.11929},
  url={https://api.semanticscholar.org/CorpusID:225039882}
}

@article{Guo2022FromIT,
  title={From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models},
  author={Jiaxian Guo and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Boyang Li and Dacheng Tao and Steven Hoi},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.10846},
  url={https://api.semanticscholar.org/CorpusID:254926627}
}

@inproceedings{Changpinyo2022AllYM,
  title={All You May Need for VQA are Image Captions},
  author={Soravit Changpinyo and Doron Kukliansky and Idan Szpektor and Xi Chen and Nan Ding and Radu Soricut},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:248512968}
}

@article{Tiong2022PlugandPlayVZ,
  title={Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training},
  author={Anthony Meng Huat Tiong and Junnan Li and Boyang Li and Silvio Savarese and Steven C. H. Hoi},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.08773},
  url={https://api.semanticscholar.org/CorpusID:252917791}
}

@article{gqa,
  title={GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering},
  author={Drew A. Hudson and Christopher D. Manning},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={6693-6702},
  url={https://api.semanticscholar.org/CorpusID:152282269}
}

@article{okvqa,
  title={OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge},
  author={Kenneth Marino and Mohammad Rastegari and Ali Farhadi and Roozbeh Mottaghi},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={3190-3199},
  url={https://api.semanticscholar.org/CorpusID:173991173}
}

@article{Niu2018RecursiveVA,
  title={Recursive Visual Attention in Visual Dialog},
  author={Yulei Niu and Hanwang Zhang and Manli Zhang and Jianhong Zhang and Zhiwu Lu and Ji-Rong Wen},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018},
  pages={6672-6681},
  url={https://api.semanticscholar.org/CorpusID:54446647}
}

@article{miniGPT4,
  title={MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
  author={Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.10592},
  url={https://api.semanticscholar.org/CorpusID:258291930}
}

@article{llava,
  title={Visual Instruction Tuning},
  author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.08485},
  url={https://api.semanticscholar.org/CorpusID:258179774}
}
