\relax 
\bibstyle{aaai24}
\citation{li2019relation,li2021cross,li2019semi,wu2019mutual,wu2019enhancing,huang2023divide}
\citation{FedAvg}
\citation{healthcare1}
\citation{recommendation1}
\citation{smartcity1}
\citation{tanno2019learning,kuznetsova2020open}
\citation{xu2022fedcorr}
\citation{kim2022fedrn,RoFL}
\citation{xu2022fedcorr,kim2022fedrn}
\citation{xu2022fedcorr,kim2022fedrn}
\citation{FedAvg,FedProx}
\@LN@col{1}
\@LN@col{2}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Figure:Overview}{{1}{1}{ (a) Local noise filtering may have limited capabilities as each client develops its own local noise filter using its own private data only. Especially on clean clients, such filters would incorrectly identify a subset of clean samples to be noisy. (b) Collaborative noise filtering proposed by us significantly improves the performance of label noise filtering on each client as a federated noise filter is learned by distilling knowledge from all clients. PDF: Probability density function.}{}{}}
\citation{kim2022fedrn,xu2022fedcorr}
\citation{noisememory6}
\citation{holland1986statistics,debiasedlearning}
\citation{cifar}
\citation{cifar}
\citation{clothing1m}
\citation{li2023betweenness,li2023idm}
\citation{JointOpt}
\citation{li2019dividemix}
\citation{cheng2022instance}
\citation{ren2018learning}
\citation{englesson2021generalized}
\citation{li2022neighborhood}
\citation{xu2022fedcorr}
\citation{kim2022fedrn}
\citation{RoFL}
\citation{xu2022fedcorr}
\citation{LID}
\citation{FedAvg}
\@LN@col{1}
\@LN@col{2}
\citation{fedgmm2022}
\citation{GMM_EM}
\newlabel{Figure:Framework}{{2}{3}{An overview of the training procedure proposed in \texttt  {FedDiv}. In this work, the parameters of a local neural model and a local noise filter are simultaneously learned on each client during the local training sessions, while both types of parameters are aggregated on the server.}{}{}}
\@LN@col{1}
\@LN@col{2}
\newlabel{Equation:FedAvg}{{1}{3}{}{}{}}
\citation{BMM,li2019dividemix}
\citation{GMM_EM}
\citation{FedAvg}
\citation{FedProx}
\citation{RoFL}
\citation{ARFL}
\citation{JointOpt}
\citation{li2019dividemix}
\citation{xu2022fedcorr}
\citation{FedAvg}
\citation{FedProx}
\citation{RoFL}
\citation{ARFL}
\citation{JointOpt}
\citation{li2019dividemix}
\citation{xu2022fedcorr}
\@LN@col{1}
\newlabel{Equation:E-step}{{3}{4}{}{}{}}
\@LN@col{2}
\newlabel{Equation:M-Step-at-Server}{{5}{4}{}{}{}}
\newlabel{Equation:Label-Splitting}{{7}{4}{}{}{}}
\newlabel{Equation:Relabel}{{8}{4}{}{}{}}
\citation{imbalance}
\citation{holland1986statistics,pearl2009causal,debiasedlearning}
\citation{FedAvg}
\citation{FedProx}
\citation{RoFL}
\citation{ARFL}
\citation{JointOpt}
\citation{li2019dividemix}
\citation{xu2022fedcorr}
\citation{FedAvg}
\citation{FedProx}
\citation{RoFL}
\citation{ARFL}
\citation{JointOpt}
\citation{li2019dividemix}
\citation{xu2022fedcorr}
\citation{RoFL}
\citation{ARFL}
\citation{JointOpt}
\citation{li2019dividemix}
\citation{xu2022fedcorr}
\newlabel{Table:CIFAR10-IID}{{1}{5}{Best test accuracy (\%) of \texttt  {FedDiv} and existing SOTA methods on CIFAR-10 with IID setting at diverse noise levels. Algorithms marked with $\S  $ indicate the re-implementations based on public code.}{}{}}
\@LN@col{1}
\newlabel{Tabel:CIFAR10-NonIID}{{2}{5}{Best test accuracy (\%) of \texttt  {FedDiv} and existing SOTA methods on CIFAR-10 with non-IID setting at the noise level $(\rho , \tau )=(6.0, 0.5)$.}{}{}}
\@LN@col{2}
\newlabel{Table:CIFAR100-IID}{{3}{5}{Best test accuracy (\%) of \texttt  {FedDiv} and existing SOTA methods on CIFAR-100 with IID setting at diverse noise levels.}{}{}}
\newlabel{Table:CIFAR100-NonIID}{{4}{5}{Best test accuracy (\%) of \texttt  {FedDiv} and existing SOTA methods on CIFAR-100 with non-IID setting at the noise level $(\rho , \tau )=(4.0, 0)$.}{}{}}
\newlabel{Equation:Logit-Debias}{{9}{5}{}{}{}}
\newlabel{Equation:Reselect}{{10}{5}{}{}{}}
\citation{zhang2018mixup}
\citation{tanaka2018joint,arazo2019unsupervised}
\citation{xu2022fedcorr}
\citation{cifar}
\citation{cifar}
\citation{clothing1m}
\citation{xu2022fedcorr}
\citation{lin2020ensemble}
\citation{xu2022fedcorr}
\citation{xu2022fedcorr}
\citation{xu2022fedcorr}
\newlabel{Table:Clothing-1M}{{5}{6}{Best test accuracy (\%) of \texttt  {FedDiv} and existing SOTA methods on Clothing1M with non-IID setting.}{}{}}
\@LN@col{1}
\newlabel{Equation:Data-Split}{{11}{6}{}{}{}}
\newlabel{Equation:Momentum-p}{{12}{6}{}{}{}}
\newlabel{Equation:Final-Loss}{{15}{6}{}{}{}}
\@LN@col{2}
\newlabel{Equation:Noise}{{16}{6}{}{}{}}
\citation{fedgmm2022}
\citation{xu2022fedcorr}
\@LN@col{1}
\newlabel{Figure:Filtering-Accuracy-per-Method}{{3}{7}{The accuracy of noisy label identification across different clients. The lightblue dotted line represents the actual noise level of each client, while the brightly colored dotted lines indicate the noise filtering performance w.r.t different noise filters. The experiment is conducted on CIFAR-100 with the non-IID data partition under the noise setting $(\rho , \tau )=(0.4, 0.0)$. (\textbf  {Best viewed zoomed in.})}{}{}}
\@LN@col{2}
\newlabel{Table:Ablation}{{6}{7}{Ablation study results for CIFAR-10 and CIFAR-100, in varying noise levels and both IID and non-IID data partition settings, with $\dagger $ indicating the IID data partitions.}{}{}}
\@LN@col{1}
\@LN@col{2}
\@LN@col{1}
\@LN@col{2}
\bibdata{aaai24}
\citation{lin2020ensemble}
\citation{xu2022fedcorr}
\citation{resnet}
\citation{resnet}
\citation{resnet}
\citation{xu2022fedcorr}
\citation{FedAvg}
\citation{FedProx}
\citation{RoFL}
\citation{RoFL}
\citation{JointOpt}
\citation{li2019dividemix}
\citation{xu2022fedcorr}
\citation{xu2022fedcorr}
\newlabel{supp}{{}{10}{}{}{}}
\@LN@col{1}
\newlabel{Table:Notations}{{7}{10}{Key notations of F-LNL and \texttt  {FedDiv}.}{}{}}
\@LN@col{2}
\newlabel{Table:Hyper-Parameter}{{8}{10}{Hyper-parameters on different datasets.}{}{}}
\citation{xu2022fedcorr}
\citation{LID}
\citation{FedAvg}
\@LN@col{1}
\newlabel{PseudoCode:FedDiv}{{1}{11}{The training procedure of \texttt  {FedDiv}}{}{}}
\@LN@col{2}
\newlabel{PseudoCode:Local-Model-Training}{{2}{11}{Local filter training}{}{}}
\citation{FedProx,xu2022fedcorr}
\citation{FedProx,xu2022fedcorr}
\citation{kim2022fedrn,RoFL}
\@LN@col{1}
\newlabel{Figure:Instability}{{4}{12}{The evolution of quantized training stability \textit  {v.s.} test classification performance across epochs for various F-LNL algorithms. We quantitatively assess training stability in F-LNL by computing the average proximal regularization metric\nobreakspace  {}\citep  {FedProx, xu2022fedcorr} between the weights of local and global neural network models in the current training round. The experiments are conducted on CIFAR-10 with $(p, \alpha _{Dir})=(0.3, 10.0)$ and $(\rho , \tau )=(6.0, 0.5)$. }{}{}}
\@LN@col{2}
\newlabel{Figure:Filtering-Accuracy}{{5}{13}{The accuracy of noisy label identification vs. different clients. The lightblue dotted line represents the actual noise level of each client, while the (dotted) lines in deep bright colors indicate the noise filtering performance with respect to different noise filters. The experiment is conducted on CIFAR-100 with the IID data partition under the noise setting $(\rho , \tau )=(0.4, 0.5)$. TOP: Evaluation in the $50$-th communication round of the usual training stage; BOTTOM: Evaluation in the $500$-th communication round of the usual training stage.}{}{}}
\newlabel{Figure:Performance-On-Different-Filter}{{6}{14}{Two examples to illustrate the performance of three noise filters to identify label noise on a clean client (with the lowest accuracy of label noise filtering) and a noisy client. We show the probability distributions of samples being clean predicted by each filter, with the samples ranked according to the per-sample loss function values. In the legend, the percentages show the accuracy of noisy label identification with respect to each filter. In addition, as illustrated by the black line, a sample that is considered clean should have a predicted probability higher than $\kappa \cdot 100\%$. Furthermore, the green and blue bars represent, respectively, the distribution of the given \textbf  {\textcolor {green}{clean}} and \textbf  {\textcolor {blue}{noisy}} samples. The evaluation is performed at the end of the 50-th communication round in the usual federated training stage. The experiment is conducted on CIFAR-100 with the IID data partition under the noise setting $(\rho , \tau )=(0.4, 0.5)$. TOP: Evaluation on the clean client; BOTTOM: Evaluation on the noisy client with $\delta =0.725$. (\textbf  {Best viewed zoomed in.})}{}{}}
\newlabel{Figure:Heatmap}{{7}{15}{An evaluation of label noise filtering, noisy sample relabeling and labeled sample re-selection on five representative clients in the proposed \texttt  {FedDiv}. As indicated by the heat maps, three confusion matrices for each client are associated to the actual labels v.s. the given labels before processing, the corrected labels after label noise filtering and noisy sample relabeling (named Thread 1), and the corrected labels after labeled sample re-selection (named Thread 2), respectively. Note that, in practice, noisy label relabeling and labeled sample re-selection may not necessarily be conducted on clean clients (e.g., Client 1) during local model training, in accordance with Eq. (\textcolor {red}{10}). The experiment is conducted on CIFAR-10 with the IID data partition under the noise setting $(\rho , \tau )=(0.8, 0.5)$. }{}{}}
\newlabel{Figure:Hyper-Parameter-Sensitivity}{{8}{15}{Sensitivity with respect to the hyper-parameter $\zeta $. We show the test accuracy to illustrate the classification performance of the final FL model when we set $\zeta $ to $0.00$, $0.25$, $0.50$, $0.75$, and $1.00$, respectively. We conduct these experiments on CIFAR-10 with both IID and non-IID data partitions. For the non-IID data partition, the noise level is set to $(\rho , \tau )=(0.6, 0.5)$.}{}{}}
\gdef \@abspage@last{15}
