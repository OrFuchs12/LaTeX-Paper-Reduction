\relax 
\bibstyle{aaai24}
\citation{zhou2022large,openai2023gpt4}
\citation{galassi2020attention}
\citation{mishra2017local,lundberg2017unified}
\citation{wu2021polyjuice,ross2021explaining}
\citation{losch2019interpretability}
\citation{wang2023intepreting,abraham2022cebab}
\citation{tan2023cbm}
\citation{koh2020concept}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaser}{{1}{1}{The illustration includes: (a) \textit  {Attention visualization} provides a localized, attention-driven explanation. While insightful, this might be less decipherable or intuitive for users outside the realm of computer science. (b) \textit  {CBMs} deliver a broader, concept-level understanding, resonating naturally with human cognition. However, they sometimes miss out on the nuanced, granular insights of the LLM's workings. (c) \textit  {SparseCBMs} outline a holistic decision pathway for each input, seamlessly progressing from tokens, via pertinent subnetworks and concepts, to the final task label. This approach marries the strengths of both local and global explanations, addressing their respective shortcomings.}{}{}}
\citation{koh2020concept,li2023inference}
\citation{vig2019multiscale,galassi2020attention}
\citation{ribeiro2016should}
\citation{lundberg2017unified}
\citation{koh2020concept,abraham2022cebab,wang2023intepreting}
\citation{koh2020concept,oikarinenlabel}
\citation{tan2023cbm}
\citation{li2022explanations}
\citation{lecun1990optimal,han2015deep,magnitude,hessian,liu2017learning,he2017channel,zhou2016less}
\citation{michel2019sixteen}
\citation{lagunas2021block}
\citation{gale2019state}
\citation{liu2023sparsity}
\citation{subramanian2018spine}
\citation{meister2021sparse}
\citation{liu2022improve}
\citation{oikarinenlabel}
\citation{tan2023cbm}
\citation{tan2023cbm}
\citation{tan2023cbm}
\citation{koh2020concept,tan2023cbm}
\newlabel{sec:setup}{{}{3}{}{}{}}
\newlabel{eq:joint}{{1}{3}{}{}{}}
\newlabel{eq:decompose}{{2}{3}{}{}{}}
\newlabel{eq:path}{{3}{3}{}{}{}}
\citation{hassibi1992second,kurtic2022optimal}
\citation{hassibi1992second,kurtic2022optimal}
\citation{hassibi1992second,kurtic2022optimal}
\citation{koh2020concept}
\citation{evci2020rigging,sun2023simple}
\citation{abraham2022cebab}
\citation{tan2023cbm}
\newlabel{eq:optimization}{{6}{4}{}{}{}}
\citation{devlin2018bert,liu2019roberta,sanh2019distilbert}
\citation{zhang2022opt}
\citation{openai2023gpt4}
\newlabel{tab:data}{{1}{5}{Statistics of experimented datasets and concepts.}{}{}}
\newlabel{tab:compare}{{2}{5}{Comparisons of task accuracy and interpretability using \texttt  {CEBaB} and \texttt  {IMDB-C} datasets with BERT-family and OPT-family models as the backbones. Metrics for both task and concept labels are \textit  {Accuracy}/\textit  {Macro F1} in $\%$. A score in \textbf  {bold} indicate that the SparseCBM under the current setting outperforms its dense CBM counterpart.}{}{}}
\newlabel{fig:example}{{2}{6}{{The illustration of a decision pathway of a toy example from the SparseCBM framework with BERT as the backbone. The binary weight masks for each concept is represented as a heatmap.}}{}{}}
\newlabel{fig:intervene}{{3}{6}{The results of Test-time Intervention. ``NI'' denotes ``no intervention'', ``SI'' denotes ``Sparsity-based Intervention''. (a) and (b) represent the results for concept and task label prediction respectively. The x-axis indicates the proportion ($r$) of the weights to perform the intervention.}{}{}}
\newlabel{fig:inter}{{4}{7}{Illustration of the explainable prediction for a real-world example from the \texttt  {IMDB-C} dataset using OPT-350m as the backbone. The brown boxes with dash lines indicate the test-time intervention on corresponding concepts by modulating the corresponding mask. $\bm  {M}_2$ and $\bm  {M}_2^\prime $ denote the parameter masks for the second concept, ``Acting'', before and after the intervention, respectively. We visualize $\bm  {M}_2^\prime $ after seeing all test samples.}{}{}}
\newlabel{fig:logit_0}{{5}{7}{The performance of SparseCBMs across varying LLM backbones in relation to the target sparsity $s$ on the \texttt  {CEBaB} dataset. Solid lines delineate scores for concept label predictions. Dashed lines capture those for task label predictions. Notably, larger LLM backbones are adept at handling increased sparsity without compromising on prediction efficacy. Nonetheless, excessive pruning invariably impinges on the performance across all LLM backbones.}{}{}}
\citation{tan2023cbm,wang2023noise}
\citation{li2023disc,li2023csgnn}
\citation{wang2023knowledge}
\citation{wang2020learn,chen2021long}
\citation{tan2023cbm,wang2023contrastive}
\citation{tan2022graph}
\citation{wang2022neural}
\citation{jiang2023disinformation,chen2023combating}
\bibdata{aaai24}
\citation{tan2023cbm}
\citation{paszke2017automatic}
\citation{wolf2020huggingfaces}
\citation{abraham2022cebab}
\citation{hassibi1992second}
\citation{kurtic2022optimal}
\newlabel{app:def}{{}{16}{}{}{}}
\newlabel{app:implement}{{}{16}{}{}{}}
\newlabel{app: optim}{{}{16}{}{}{}}
\newlabel{eq:lagrange}{{9}{16}{}{}{}}
\newlabel{tab:app_para}{{3}{17}{Key parameters in this paper with their annotations and evaluated values. Note that \textbf  {bold} values indicate the optimal ones.}{}{}}
\newlabel{tab:symbols}{{3}{17}{Key parameters in this paper with their annotations and evaluated values. Note that \textbf  {bold} values indicate the optimal ones.}{}{}}
\newlabel{fig:example1}{{6}{17}{{The illustration of a decision pathway of an real-world example (\texttt  {CEBaB} dataset) from the SparseCBM framework with BERT as the backbone. The binary weight masks for each concept is represented as a heatmap.}}{}{}}
\newlabel{fig:example2}{{7}{17}{{The illustration of a decision pathway of an real-world example (\texttt  {IMDB-C} dataset) from the SparseCBM framework with BERT as the backbone. The binary weight masks for each concept is represented as a heatmap.}}{}{}}
\gdef \@abspage@last{17}
