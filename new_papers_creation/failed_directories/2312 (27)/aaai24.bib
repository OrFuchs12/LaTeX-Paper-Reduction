% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{koh2020concept,
  title={Concept bottleneck models},
  author={Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
  booktitle={ICML},
  pages={5338--5348},
  year={2020}
}

@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv:1710.09412},
  year={2017}
}

@article{gilardi2023chatgpt,
  title={Chatgpt outperforms crowd-workers for text-annotation tasks},
  author={Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l},
  journal={arXiv:2303.15056},
  year={2023}
}

@article{liao2023ai,
  title={AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap},
  author={Liao, Q Vera and Vaughan, Jennifer Wortman},
  journal={arXiv:2306.01941},
  year={2023}
}

@article{cheng2021socially,
  title={Socially responsible ai algorithms: Issues, purposes, and challenges},
  author={Cheng, Lu and Varshney, Kush R and Liu, Huan},
  journal={Journal of Artificial Intelligence Research},
  volume={71},
  pages={1137--1181},
  year={2021}
}

@article{berthelot2019mixmatch,
  title={Mixmatch: A holistic approach to semi-supervised learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
  journal={NeurIPS},
  volume={32},
  year={2019}
}

@article{abraham2022cebab,
  title={CEBaB: Estimating the causal effects of real-world concepts on NLP model behavior},
  author={Abraham, Eldar D and D'Oosterlinck, Karel and Feder, Amir and Gat, Yair and Geiger, Atticus and Potts, Christopher and Reichart, Roi and Wu, Zhengxuan},
  journal={NeurIPS},
  volume={35},
  pages={17582--17596},
  year={2022}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv:1810.04805},
  year={2018}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv:1907.11692},
  year={2019}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv:1301.3781},
  year={2013}
}

@inproceedings{maas2011learning,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies},
  pages={142--150},
  year={2011}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on CVPR},
  pages={770--778},
  year={2016}
}

@inproceedings{oikarinenlabel,
  title={Label-free Concept Bottleneck Models},
  author={Oikarinen, Tuomas and Das, Subhro and Nguyen, Lam M and Weng, Tsui-Wei},
  booktitle={ICLR},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={NeurIPS},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{xieexplanation,
  title={An Explanation of In-context Learning as Implicit Bayesian Inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  booktitle={ICLR},
  year={2022}
}

@article{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv:2202.12837},
  year={2022}
}

@article{wu2022causal,
  title={Causal Proxy Models for Concept-Based Model Explanations},
  author={Wu, Zhengxuan and D'Oosterlinck, Karel and Geiger, Atticus and Zur, Amir and Potts, Christopher},
  journal={arXiv:2209.14279},
  year={2022}
}

@article{yang2017yedda,
  title={YEDDA: A lightweight collaborative text span annotation tool},
  author={Yang, Jie and Zhang, Yue and Li, Linwei and Li, Xingxuan},
  journal={arXiv:1711.03759},
  year={2017}
}

@inproceedings{cai2021aspect,
  title={Aspect-category-opinion-sentiment quadruple extraction with implicit aspects and opinions},
  author={Cai, Hongjie and Xia, Rui and Yu, Jianfei},
  booktitle={ACL-IJCNLP},
  pages={340--350},
  year={2021}
}

@inproceedings{kim2018feels,
  title={Who feels what and why? annotation of a literature corpus with semantic roles of emotions},
  author={Kim, Evgeny and Klinger, Roman},
  booktitle={Proceedings of the 27th International Conference on Computational Linguistics},
  pages={1345--1359},
  year={2018}
}


@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on EMNLP},
  pages={1532--1543},
  year={2014}
}

@article{bojanowski2017enriching,
  title={Enriching word vectors with subword information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={Transactions of the association for computational linguistics},
  volume={5},
  pages={135--146},
  year={2017},
  publisher={MIT Press}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={NeurIPS},
  volume={30},
  year={2017}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv:2211.05100},
  year={2022}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv:2302.13971},
  year={2023}
}

@article{belinkov2019analysis,
  title={Analysis methods in neural language processing: A survey},
  author={Belinkov, Yonatan and Glass, James},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={49--72},
  year={2019},
  publisher={MIT Press}
}

@article{madsen2022post,
  title={Post-hoc interpretability for neural nlp: A survey},
  author={Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
  journal={ACM Computing Surveys},
  volume={55},
  number={8},
  pages={1--42},
  year={2022},
  publisher={ACM New York, NY}
}

@article{wallace2019allennlp,
  title={Allennlp interpret: A framework for explaining predictions of nlp models},
  author={Wallace, Eric and Tuyls, Jens and Wang, Junlin and Subramanian, Sanjay and Gardner, Matt and Singh, Sameer},
  journal={arXiv:1909.09251},
  year={2019}
}

@article{yin2022interpreting,
  title={Interpreting language models with contrastive explanations},
  author={Yin, Kayo and Neubig, Graham},
  journal={arXiv:2202.10419},
  year={2022}
}

@article{losch2019interpretability,
  title={Interpretability beyond classification output: Semantic bottleneck networks},
  author={Losch, Max and Fritz, Mario and Schiele, Bernt},
  journal={arXiv:1907.10882},
  year={2019}
}

@inproceedings{zarlenga2022concept,
  title={Concept Embedding Models},
  author={Zarlenga, Mateo Espinosa and Barbiero, Pietro and Ciravegna, Gabriele and Marra, Giuseppe and Giannini, Francesco and Diligenti, Michelangelo and Precioso, Frederic and Melacci, Stefano and Weller, Adrian and Lio, Pietro and others},
  booktitle={NeurIPS 2022-36th Conference on NeurIPS},
  year={2022}
}

@article{liu2022identifiability,
  title={Identifiability of label noise transition matrix},
  author={Liu, Yang and Cheng, Hao and Zhang, Kun},
  journal={arXiv:2202.02016},
  year={2022}
}

@article{englesson2021generalized,
  title={Generalized jensen-shannon divergence loss for learning with noisy labels},
  author={Englesson, Erik and Azizpour, Hossein},
  journal={NeurIPS},
  volume={34},
  pages={30284--30297},
  year={2021}
}

@article{sohn2020fixmatch,
  title={Fixmatch: Simplifying semi-supervised learning with consistency and confidence},
  author={Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang, Zizhao and Zhang, Han and Raffel, Colin A and Cubuk, Ekin Dogus and Kurakin, Alexey and Li, Chun-Liang},
  journal={NeurIPS},
  volume={33},
  pages={596--608},
  year={2020}
}

@inproceedings{zhuincorporating,
  title={Incorporating BERT into Neural Machine Translation},
  author={Zhu, Jinhua and Xia, Yingce and Wu, Lijun and He, Di and Qin, Tao and Zhou, Wengang and Li, Houqiang and Liu, Tieyan},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{liu2019text,
  title={Text Summarization with Pretrained Encoders},
  author={Liu, Yang and Lapata, Mirella},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={3730--3740},
  year={2019}
}

@article{singh2023mind,
  title={Mind meets machine: Unravelling GPT-4's cognitive psychology},
  author={Singh, Manmeet and SB, Vaisakh and Malviya, Neetiraj and others},
  journal={arXiv:2303.11436},
  year={2023}
}

@misc{bommasani2022opportunities,
      title={On the Opportunities and Risks of Foundation Models}, 
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
      year={2022},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{wang2021robustness,
  title={Robustness to spurious correlations in text classification via automatically generated counterfactuals},
  author={Wang, Zhao and Culotta, Aron},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={14024--14031},
  year={2021}
}

@inproceedings{udomcharoenchaikit2022mitigating,
  title={Mitigating Spurious Correlation in Natural Language Understanding with Counterfactual Inference},
  author={Udomcharoenchaikit, Can and Ponwitayarat, Wuttikorn and Payoungkhamdee, Patomporn and Masuk, Kanruethai and Buaphet, Weerayut and Chuangsuwanich, Ekapol and Nutanong, Sarana},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={11308--11321},
  year={2022}
}

@article{zhang2022survey,
  title={A survey on aspect-based sentiment analysis: tasks, methods, and challenges},
  author={Zhang, Wenxuan and Li, Xin and Deng, Yang and Bing, Lidong and Lam, Wai},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2022},
  publisher={IEEE}
}

@article{nemeth2020machine,
  title={Machine learning of concepts hard even for humans: The case of online depression forums},
  author={N{\'e}meth, Ren{\'a}ta and Sik, Domonkos and M{\'a}t{\'e}, Fanni},
  journal={International Journal of Qualitative Methods},
  volume={19},
  pages={1609406920949338},
  year={2020},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{diao2022black,
  title={Black-box prompt learning for pre-trained language models},
  author={Diao, Shizhe and Huang, Zhichao and Xu, Ruijia and Li, Xuechun and Lin, Yong and Zhou, Xiao and Zhang, Tong},
  journal={arXiv:2201.08531},
  year={2022}
}

@inproceedings{mishra2017local,
  title={Local interpretable model-agnostic explanations for music content analysis.},
  author={Mishra, Saumitra and Sturm, Bob L and Dixon, Simon},
  booktitle={ISMIR},
  volume={53},
  pages={537--543},
  year={2017}
}

@article{galassi2020attention,
  title={Attention in natural language processing},
  author={Galassi, Andrea and Lippi, Marco and Torroni, Paolo},
  journal={IEEE transactions on neural networks and learning systems},
  volume={32},
  number={10},
  pages={4291--4308},
  year={2020},
  publisher={IEEE}
}

@article{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  journal={NeurIPS},
  volume={30},
  year={2017}
}

@inproceedings{wu2021polyjuice,
  title={Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models},
  author={Wu, T and Ribeiro, M Tulio and Heer, J and Weld, D},
  booktitle={ACL-IJCNLP},
  year={2021}
}

@inproceedings{ross2021explaining,
  title={Explaining NLP Models via Minimal Contrastive Editing (MiCE)},
  author={Ross, Alexis and Marasovi{\'c}, Ana and Peters, Matthew E},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={3840--3852},
  year={2021}
}

@article{goyal2019explaining,
  title={Explaining classifiers with causal concept effect (cace)},
  author={Goyal, Yash and Feder, Amir and Shalit, Uri and Kim, Been},
  journal={arXiv:1907.07165},
  year={2019}
}

@inproceedings{kim2018interpretability,
  title={Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)},
  author={Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and others},
  booktitle={ICML},
  pages={2668--2677},
  year={2018}
}

@article{mu2020compositional,
  title={Compositional explanations of neurons},
  author={Mu, Jesse and Andreas, Jacob},
  journal={NeurIPS},
  volume={33},
  pages={17153--17163},
  year={2020}
}

@misc{bills2023language,
         title={Language models can explain neurons in language models},
         author={
            Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William
         },
         year={2023},
         howpublished = {\url{https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html}}
      }

@article{vig2020investigating,
  title={Investigating gender bias in language models using causal mediation analysis},
  author={Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  journal={NeurIPS},
  volume={33},
  pages={12388--12401},
  year={2020}
}

@misc{
wang2023intepreting,
title={Intepreting \& Improving Pretrained Language Models: A Probabilistic Conceptual Approach},
author={Hengyi Wang and Zhiqing Hong and Desheng Zhang and Hao Wang},
eprint={id=kwF1ZfHf0W},
archivePrefix={Openreview},
year={2023},
url={https://openreview.net/forum?id=kwF1ZfHf0W}
}

@inproceedings{paszke2017automatic,
  title={Automatic differentiation in pytorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle={NeurIPS},
  year={2017}
}

@misc{wolf2020huggingfaces,
      title={HuggingFace's Transformers: State-of-the-art Natural Language Processing}, 
      author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
      year={2020},
      eprint={1910.03771},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={ICLR},
  year={2015}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@inproceedings{zhou2022large,
  title={Large Language Models are Human-Level Prompt Engineers},
  author={Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  booktitle={ICLR},
  year={2022}
}

@article{tan2023cbm,
  title={Interpreting Pretrained Language Models via Concept Bottlenecks},
  author={Tan, Zhen and Cheng, Lu and Wang, Song and Bo, Yuan and Li, Jundong and Liu, Huan},
  journal={arXiv:2311.05014},
  year={2023}
}

@article{wang2022intepreting,
  title={Intepreting \& Improving Pretrained Language Models: A Probabilistic Conceptual Approach},
  author={Wang, Hengyi and Hong, Zhiqing and Zhang, Desheng and Wang, Hao},
  year={2022}
}

@article{li2023inference,
  title={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={arXiv:2306.03341},
  year={2023}
}

@inproceedings{vig2019multiscale,
  title={A Multiscale Visualization of Attention in the Transformer Model},
  author={Vig, Jesse},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  pages={37--42},
  year={2019}
}

@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD Conference},
  pages={1135--1144},
  year={2016}
}

@article{li2022explanations,
  title={Explanations from large language models make small reasoners better},
  author={Li, Shiyang and Chen, Jianshu and Shen, Yelong and Chen, Zhiyu and Zhang, Xinlu and Li, Zekun and Wang, Hong and Qian, Jing and Peng, Baolin and Mao, Yi and others},
  journal={arXiv:2210.06726},
  year={2022}
}

@inproceedings{liu2022improve,
  title={Improve Interpretability of Neural Networks via Sparse Contrastive Coding},
  author={Liu, Junhong and Lin, Yijie and Jiang, Liang and Liu, Jia and Wen, Zujie and Peng, Xi},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={460--470},
  year={2022}
}

@inproceedings{subramanian2018spine,
  title={Spine: Sparse interpretable neural embeddings},
  author={Subramanian, Anant and Pruthi, Danish and Jhamtani, Harsh and Berg-Kirkpatrick, Taylor and Hovy, Eduard},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  year={2018}
}

@inproceedings{he2017channel,
  title={Channel pruning for accelerating very deep neural networks},
  author={He, Yihui and Zhang, Xiangyu and Sun, Jian},
  booktitle={Proceedings of ICCV},
  year={2017}
}

@inproceedings{zhou2016less,
  title={Less is more: Towards compact cnns},
  author={Zhou, Hao and Alvarez, Jose M and Porikli, Fatih},
  booktitle={ECCV},
  pages={662--677},
  year={2016},
  organization={Springer}
}

@inproceedings{liu2017learning,
  title={Learning efficient convolutional networks through network slimming},
  author={Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  booktitle={Proceedings of ICCV},
  pages={2736--2744},
  year={2017}
}

@incollection{hessian,
title = {Optimal Brain Damage},
author = {LeCun, Yann and John S. Denker and Sara A. Solla},
booktitle = {NeurIPS},
editor = {D. S. Touretzky},
pages = {598--605},
year = {1990},
publisher = {Morgan-Kaufmann},
url = {http://papers.nips.cc/paper/250-optimal-brain-damage.pdf}
}

@inproceedings{magnitude,
title = {Learning both Weights and Connections for Efficient Neural Network},
author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
booktitle = {NeurIPS},
pages = {1135--1143},
year = {2015}
}

@inproceedings{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  booktitle={ICLR},
  year={2016}
}

@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={NeurIPS},
  pages={598--605},
  year={1990}
}

@inproceedings{meister2021sparse,
  title={Is Sparse Attention more Interpretable?},
  author={Meister, Clara and Lazov, Stefan and Augenstein, Isabelle and Cotterell, Ryan},
  booktitle={ACL-IJCNLP},
  pages={122--129},
  year={2021}
}

@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={NeurIPS},
  volume={32},
  year={2019}
}

@article{liu2023sparsity,
  title={Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!},
  author={Liu, Shiwei and Chen, Tianlong and Zhang, Zhenyu and Chen, Xuxi and Huang, Tianjin and Jaiswal, Ajay and Wang, Zhangyang},
  journal={arXiv:2303.02141},
  year={2023}
}

@article{lagunas2021block,
  title={Block pruning for faster transformers},
  author={Lagunas, Fran{\c{c}}ois and Charlaix, Ella and Sanh, Victor and Rush, Alexander M},
  journal={arXiv:2109.04838},
  year={2021}
}

@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv:1902.09574},
  year={2019}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv:1910.01108},
  year={2019}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv:2205.01068},
  year={2022}
}

@article{hassibi1992second,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David},
  journal={NeurIPS},
  volume={5},
  year={1992}
}

@inproceedings{kurtic2022optimal,
  title={The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models},
  author={Kurtic, Eldar and Campos, Daniel and Nguyen, Tuan and Frantar, Elias and Kurtz, Mark and Fineran, Benjamin and Goin, Michael and Alistarh, Dan},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={4163--4181},
  year={2022}
}

@article{sun2023simple,
  title={A Simple and Effective Pruning Approach for Large Language Models}, 
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, Zico},
  year={2023},
  journal={arXiv:2306.11695}
}

@inproceedings{evci2020rigging,
  title={Rigging the lottery: Making all tickets winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={ICML},
  pages={2943--2952},
  year={2020}
}

@article{wang2023noise,
  title={Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance},
  author={Wang, Song and Tan, Zhen and Guo, Ruocheng and Li, Jundong},
  journal={arXiv:2311.01108},
  year={2023}
}

@article{wang2023knowledge,
  title={Knowledge Editing for Large Language Models: A Survey},
  author={Wang, Song and Zhu, Yaochen and Liu, Haochen and Zheng, Zaiyi and Chen, Chen and others},
  journal={arXiv:2310.16218},
  year={2023}
}

@inproceedings{li2023disc,
  title={DISC: Learning from Noisy Labels via Dynamic Instance-Specific Selection and Correction},
  author={Li, Yifan and Han, Hu and Shan, Shiguang and Chen, Xilin},
  booktitle={Proceedings of the IEEE/CVF Conference on CVPR},
  pages={24070--24079},
  year={2023}
}

@inproceedings{wang2020learn,
  title={Learn-prune-share for lifelong learning},
  author={Wang, Zifeng and Jian, Tong and Chowdhury, Kaushik and Wang, Yanzhi and Dy, Jennifer and Ioannidis, Stratis},
  booktitle={2020 ICDM},
  pages={641--650},
  year={2020},
  organization={IEEE}
}

@inproceedings{chen2021long,
  title={Long live the lottery: The existence of winning tickets in lifelong learning},
  author={Chen, Tianlong and Zhang, Zhenyu and Liu, Sijia and Chang, Shiyu and Wang, Zhangyang},
  booktitle={ICLR},
  year={2021}
}

@article{li2023csgnn,
  title={CSGNN: Conquering Noisy Node labels via Dynamic Class-wise Selection},
  author={Li, Yifan and Tan, Zhen and Shu, Kai and Cao, Zongsheng and Kong, Yu and Liu, Huan},
  journal={arXiv:2311.11473},
  year={2023}
}

@article{jiang2023disinformation,
  title={Disinformation Detection: An Evolving Challenge in the Age of LLMs},
  author={Jiang, Bohan and Tan, Zhen and Nirmal, Ayushi and Liu, Huan},
  journal={arXiv:2309.15847},
  year={2023}
}

@article{chen2023combating,
  title={Combating misinformation in the age of llms: Opportunities and challenges},
  author={Chen, Canyu and Shu, Kai},
  journal={arXiv:2311.05656},
  year={2023}
}

@inproceedings{
anonymous2023tuningfree,
title={Tuning-Free Accountable Intervention for {LLM} Deployment - A Metacognitive Approach},
author={Anonymous},
booktitle={Submitted to The Twelfth ICLR},
year={2023},
url={https://openreview.net/forum?id=ARFRZh6pzI},
note={under review}
}

@inproceedings{wang2022neural,
  title={Neural implicit dictionary learning via mixture-of-expert training},
  author={Wang, Peihao and Fan, Zhiwen and Chen, Tianlong and Wang, Zhangyang},
  booktitle={ICML},
  pages={22613--22624},
  year={2022}
}

@inproceedings{tan2022graph,
  title={Graph few-shot class-incremental learning},
  author={Tan, Zhen and Ding, Kaize and Guo, Ruocheng and Liu, Huan},
  booktitle={Proceedings of the Fifteenth ACM International Conference on WSDM},
  pages={987--996},
  year={2022}
}

@inproceedings{wang2023contrastive,
  title={Contrastive Meta-Learning for Few-shot Node Classification},
  author={Wang, Song and Tan, Zhen and Liu, Huan and Li, Jundong},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference},
  pages={2386--2397},
  year={2023}
}