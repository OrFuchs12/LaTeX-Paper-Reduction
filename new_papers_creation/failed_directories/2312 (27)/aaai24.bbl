\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Abraham et~al.(2022)Abraham, D'Oosterlinck, Feder, Gat, Geiger, Potts, Reichart, and Wu}]{abraham2022cebab}
Abraham, E.~D.; D'Oosterlinck, K.; Feder, A.; Gat, Y.; Geiger, A.; Potts, C.; Reichart, R.; and Wu, Z. 2022.
\newblock CEBaB: Estimating the causal effects of real-world concepts on NLP model behavior.
\newblock \emph{NeurIPS}, 35: 17582--17596.

\bibitem[{Chen and Shu(2023)}]{chen2023combating}
Chen, C.; and Shu, K. 2023.
\newblock Combating misinformation in the age of llms: Opportunities and challenges.
\newblock \emph{arXiv:2311.05656}.

\bibitem[{Chen et~al.(2021)Chen, Zhang, Liu, Chang, and Wang}]{chen2021long}
Chen, T.; Zhang, Z.; Liu, S.; Chang, S.; and Wang, Z. 2021.
\newblock Long live the lottery: The existence of winning tickets in lifelong learning.
\newblock In \emph{ICLR}.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova}]{devlin2018bert}
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv:1810.04805}.

\bibitem[{Evci et~al.(2020)Evci, Gale, Menick, Castro, and Elsen}]{evci2020rigging}
Evci, U.; Gale, T.; Menick, J.; Castro, P.~S.; and Elsen, E. 2020.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In \emph{ICML}, 2943--2952.

\bibitem[{Galassi, Lippi, and Torroni(2020)}]{galassi2020attention}
Galassi, A.; Lippi, M.; and Torroni, P. 2020.
\newblock Attention in natural language processing.
\newblock \emph{IEEE transactions on neural networks and learning systems}, 32(10): 4291--4308.

\bibitem[{Gale, Elsen, and Hooker(2019)}]{gale2019state}
Gale, T.; Elsen, E.; and Hooker, S. 2019.
\newblock The state of sparsity in deep neural networks.
\newblock \emph{arXiv:1902.09574}.

\bibitem[{Han, Mao, and Dally(2016)}]{han2015deep}
Han, S.; Mao, H.; and Dally, W.~J. 2016.
\newblock Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.
\newblock In \emph{ICLR}.

\bibitem[{Han et~al.(2015)Han, Pool, Tran, and Dally}]{magnitude}
Han, S.; Pool, J.; Tran, J.; and Dally, W. 2015.
\newblock Learning both Weights and Connections for Efficient Neural Network.
\newblock In \emph{NeurIPS}, 1135--1143.

\bibitem[{Hassibi and Stork(1992)}]{hassibi1992second}
Hassibi, B.; and Stork, D. 1992.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock \emph{NeurIPS}, 5.

\bibitem[{He, Zhang, and Sun(2017)}]{he2017channel}
He, Y.; Zhang, X.; and Sun, J. 2017.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{Proceedings of ICCV}.

\bibitem[{Jiang et~al.(2023)Jiang, Tan, Nirmal, and Liu}]{jiang2023disinformation}
Jiang, B.; Tan, Z.; Nirmal, A.; and Liu, H. 2023.
\newblock Disinformation Detection: An Evolving Challenge in the Age of LLMs.
\newblock \emph{arXiv:2309.15847}.

\bibitem[{Koh et~al.(2020)Koh, Nguyen, Tang, Mussmann, Pierson, Kim, and Liang}]{koh2020concept}
Koh, P.~W.; Nguyen, T.; Tang, Y.~S.; Mussmann, S.; Pierson, E.; Kim, B.; and Liang, P. 2020.
\newblock Concept bottleneck models.
\newblock In \emph{ICML}, 5338--5348.

\bibitem[{Kurtic et~al.(2022)Kurtic, Campos, Nguyen, Frantar, Kurtz, Fineran, Goin, and Alistarh}]{kurtic2022optimal}
Kurtic, E.; Campos, D.; Nguyen, T.; Frantar, E.; Kurtz, M.; Fineran, B.; Goin, M.; and Alistarh, D. 2022.
\newblock The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, 4163--4181.

\bibitem[{Lagunas et~al.(2021)Lagunas, Charlaix, Sanh, and Rush}]{lagunas2021block}
Lagunas, F.; Charlaix, E.; Sanh, V.; and Rush, A.~M. 2021.
\newblock Block pruning for faster transformers.
\newblock \emph{arXiv:2109.04838}.

\bibitem[{LeCun, Denker, and Solla(1990{\natexlab{a}})}]{lecun1990optimal}
LeCun, Y.; Denker, J.~S.; and Solla, S.~A. 1990{\natexlab{a}}.
\newblock Optimal brain damage.
\newblock In \emph{NeurIPS}, 598--605.

\bibitem[{LeCun, Denker, and Solla(1990{\natexlab{b}})}]{hessian}
LeCun, Y.; Denker, J.~S.; and Solla, S.~A. 1990{\natexlab{b}}.
\newblock Optimal Brain Damage.
\newblock In Touretzky, D.~S., ed., \emph{NeurIPS}, 598--605. Morgan-Kaufmann.

\bibitem[{Li et~al.(2023{\natexlab{a}})Li, Patel, Vi{\'e}gas, Pfister, and Wattenberg}]{li2023inference}
Li, K.; Patel, O.; Vi{\'e}gas, F.; Pfister, H.; and Wattenberg, M. 2023{\natexlab{a}}.
\newblock Inference-Time Intervention: Eliciting Truthful Answers from a Language Model.
\newblock \emph{arXiv:2306.03341}.

\bibitem[{Li et~al.(2022)Li, Chen, Shen, Chen, Zhang, Li, Wang, Qian, Peng, Mao et~al.}]{li2022explanations}
Li, S.; Chen, J.; Shen, Y.; Chen, Z.; Zhang, X.; Li, Z.; Wang, H.; Qian, J.; Peng, B.; Mao, Y.; et~al. 2022.
\newblock Explanations from large language models make small reasoners better.
\newblock \emph{arXiv:2210.06726}.

\bibitem[{Li et~al.(2023{\natexlab{b}})Li, Han, Shan, and Chen}]{li2023disc}
Li, Y.; Han, H.; Shan, S.; and Chen, X. 2023{\natexlab{b}}.
\newblock DISC: Learning from Noisy Labels via Dynamic Instance-Specific Selection and Correction.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on CVPR}, 24070--24079.

\bibitem[{Li et~al.(2023{\natexlab{c}})Li, Tan, Shu, Cao, Kong, and Liu}]{li2023csgnn}
Li, Y.; Tan, Z.; Shu, K.; Cao, Z.; Kong, Y.; and Liu, H. 2023{\natexlab{c}}.
\newblock CSGNN: Conquering Noisy Node labels via Dynamic Class-wise Selection.
\newblock \emph{arXiv:2311.11473}.

\bibitem[{Liu et~al.(2022)Liu, Lin, Jiang, Liu, Wen, and Peng}]{liu2022improve}
Liu, J.; Lin, Y.; Jiang, L.; Liu, J.; Wen, Z.; and Peng, X. 2022.
\newblock Improve Interpretability of Neural Networks via Sparse Contrastive Coding.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2022}, 460--470.

\bibitem[{Liu et~al.(2023)Liu, Chen, Zhang, Chen, Huang, Jaiswal, and Wang}]{liu2023sparsity}
Liu, S.; Chen, T.; Zhang, Z.; Chen, X.; Huang, T.; Jaiswal, A.; and Wang, Z. 2023.
\newblock Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!
\newblock \emph{arXiv:2303.02141}.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov}]{liu2019roberta}
Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv:1907.11692}.

\bibitem[{Liu et~al.(2017)Liu, Li, Shen, Huang, Yan, and Zhang}]{liu2017learning}
Liu, Z.; Li, J.; Shen, Z.; Huang, G.; Yan, S.; and Zhang, C. 2017.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In \emph{Proceedings of ICCV}, 2736--2744.

\bibitem[{Losch, Fritz, and Schiele(2019)}]{losch2019interpretability}
Losch, M.; Fritz, M.; and Schiele, B. 2019.
\newblock Interpretability beyond classification output: Semantic bottleneck networks.
\newblock \emph{arXiv:1907.10882}.

\bibitem[{Lundberg and Lee(2017)}]{lundberg2017unified}
Lundberg, S.~M.; and Lee, S.-I. 2017.
\newblock A unified approach to interpreting model predictions.
\newblock \emph{NeurIPS}, 30.

\bibitem[{Meister et~al.(2021)Meister, Lazov, Augenstein, and Cotterell}]{meister2021sparse}
Meister, C.; Lazov, S.; Augenstein, I.; and Cotterell, R. 2021.
\newblock Is Sparse Attention more Interpretable?
\newblock In \emph{ACL-IJCNLP}, 122--129.

\bibitem[{Michel, Levy, and Neubig(2019)}]{michel2019sixteen}
Michel, P.; Levy, O.; and Neubig, G. 2019.
\newblock Are sixteen heads really better than one?
\newblock \emph{NeurIPS}, 32.

\bibitem[{Mishra, Sturm, and Dixon(2017)}]{mishra2017local}
Mishra, S.; Sturm, B.~L.; and Dixon, S. 2017.
\newblock Local interpretable model-agnostic explanations for music content analysis.
\newblock In \emph{ISMIR}, volume~53, 537--543.

\bibitem[{Oikarinen et~al.(2023)Oikarinen, Das, Nguyen, and Weng}]{oikarinenlabel}
Oikarinen, T.; Das, S.; Nguyen, L.~M.; and Weng, T.-W. 2023.
\newblock Label-free Concept Bottleneck Models.
\newblock In \emph{ICLR}.

\bibitem[{OpenAI(2023)}]{openai2023gpt4}
OpenAI. 2023.
\newblock GPT-4 Technical Report.
\newblock arXiv:2303.08774.

\bibitem[{Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin, Desmaison, Antiga, and Lerer}]{paszke2017automatic}
Paszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.; DeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer, A. 2017.
\newblock Automatic differentiation in pytorch.
\newblock In \emph{NeurIPS}.

\bibitem[{Ribeiro, Singh, and Guestrin(2016)}]{ribeiro2016should}
Ribeiro, M.~T.; Singh, S.; and Guestrin, C. 2016.
\newblock " Why should i trust you?" Explaining the predictions of any classifier.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD Conference}, 1135--1144.

\bibitem[{Ross, Marasovi{\'c}, and Peters(2021)}]{ross2021explaining}
Ross, A.; Marasovi{\'c}, A.; and Peters, M.~E. 2021.
\newblock Explaining NLP Models via Minimal Contrastive Editing (MiCE).
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, 3840--3852.

\bibitem[{Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf}]{sanh2019distilbert}
Sanh, V.; Debut, L.; Chaumond, J.; and Wolf, T. 2019.
\newblock DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.
\newblock \emph{arXiv:1910.01108}.

\bibitem[{Subramanian et~al.(2018)Subramanian, Pruthi, Jhamtani, Berg-Kirkpatrick, and Hovy}]{subramanian2018spine}
Subramanian, A.; Pruthi, D.; Jhamtani, H.; Berg-Kirkpatrick, T.; and Hovy, E. 2018.
\newblock Spine: Sparse interpretable neural embeddings.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~32.

\bibitem[{Sun et~al.(2023)Sun, Liu, Bair, and Kolter}]{sun2023simple}
Sun, M.; Liu, Z.; Bair, A.; and Kolter, Z. 2023.
\newblock A Simple and Effective Pruning Approach for Large Language Models.
\newblock \emph{arXiv:2306.11695}.

\bibitem[{Tan et~al.(2023)Tan, Cheng, Wang, Bo, Li, and Liu}]{tan2023cbm}
Tan, Z.; Cheng, L.; Wang, S.; Bo, Y.; Li, J.; and Liu, H. 2023.
\newblock Interpreting Pretrained Language Models via Concept Bottlenecks.
\newblock \emph{arXiv:2311.05014}.

\bibitem[{Tan et~al.(2022)Tan, Ding, Guo, and Liu}]{tan2022graph}
Tan, Z.; Ding, K.; Guo, R.; and Liu, H. 2022.
\newblock Graph few-shot class-incremental learning.
\newblock In \emph{Proceedings of the Fifteenth ACM International Conference on WSDM}, 987--996.

\bibitem[{Vig(2019)}]{vig2019multiscale}
Vig, J. 2019.
\newblock A Multiscale Visualization of Attention in the Transformer Model.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations}, 37--42.

\bibitem[{Wang et~al.(2023{\natexlab{a}})Wang, Hong, Zhang, and Wang}]{wang2023intepreting}
Wang, H.; Hong, Z.; Zhang, D.; and Wang, H. 2023{\natexlab{a}}.
\newblock Intepreting \& Improving Pretrained Language Models: A Probabilistic Conceptual Approach.
\newblock Openreview:id=kwF1ZfHf0W.

\bibitem[{Wang et~al.(2022)Wang, Fan, Chen, and Wang}]{wang2022neural}
Wang, P.; Fan, Z.; Chen, T.; and Wang, Z. 2022.
\newblock Neural implicit dictionary learning via mixture-of-expert training.
\newblock In \emph{ICML}, 22613--22624.

\bibitem[{Wang et~al.(2023{\natexlab{b}})Wang, Tan, Guo, and Li}]{wang2023noise}
Wang, S.; Tan, Z.; Guo, R.; and Li, J. 2023{\natexlab{b}}.
\newblock Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance.
\newblock \emph{arXiv:2311.01108}.

\bibitem[{Wang et~al.(2023{\natexlab{c}})Wang, Tan, Liu, and Li}]{wang2023contrastive}
Wang, S.; Tan, Z.; Liu, H.; and Li, J. 2023{\natexlab{c}}.
\newblock Contrastive Meta-Learning for Few-shot Node Classification.
\newblock In \emph{Proceedings of the 29th ACM SIGKDD Conference}, 2386--2397.

\bibitem[{Wang et~al.(2023{\natexlab{d}})Wang, Zhu, Liu, Zheng, Chen et~al.}]{wang2023knowledge}
Wang, S.; Zhu, Y.; Liu, H.; Zheng, Z.; Chen, C.; et~al. 2023{\natexlab{d}}.
\newblock Knowledge Editing for Large Language Models: A Survey.
\newblock \emph{arXiv:2310.16218}.

\bibitem[{Wang et~al.(2020)Wang, Jian, Chowdhury, Wang, Dy, and Ioannidis}]{wang2020learn}
Wang, Z.; Jian, T.; Chowdhury, K.; Wang, Y.; Dy, J.; and Ioannidis, S. 2020.
\newblock Learn-prune-share for lifelong learning.
\newblock In \emph{2020 ICDM}, 641--650. IEEE.

\bibitem[{Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Scao, Gugger, Drame, Lhoest, and Rush}]{wolf2020huggingfaces}
Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.; Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davison, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu, J.; Xu, C.; Scao, T.~L.; Gugger, S.; Drame, M.; Lhoest, Q.; and Rush, A.~M. 2020.
\newblock HuggingFace's Transformers: State-of-the-art Natural Language Processing.
\newblock arXiv:1910.03771.

\bibitem[{Wu et~al.(2021)Wu, Ribeiro, Heer, and Weld}]{wu2021polyjuice}
Wu, T.; Ribeiro, M.~T.; Heer, J.; and Weld, D. 2021.
\newblock Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models.
\newblock In \emph{ACL-IJCNLP}.

\bibitem[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt}
Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X.~V.; et~al. 2022.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv:2205.01068}.

\bibitem[{Zhou, Alvarez, and Porikli(2016)}]{zhou2016less}
Zhou, H.; Alvarez, J.~M.; and Porikli, F. 2016.
\newblock Less is more: Towards compact cnns.
\newblock In \emph{ECCV}, 662--677. Springer.

\bibitem[{Zhou et~al.(2022)Zhou, Muresanu, Han, Paster, Pitis, Chan, and Ba}]{zhou2022large}
Zhou, Y.; Muresanu, A.~I.; Han, Z.; Paster, K.; Pitis, S.; Chan, H.; and Ba, J. 2022.
\newblock Large Language Models are Human-Level Prompt Engineers.
\newblock In \emph{ICLR}.

\end{thebibliography}
