





\section{Value of Information criterion completeness} \label{sec:model-definition}

In \autoref{sec:preliminaries-systems-and-trees}
we have shown that if an infolink $X \to D$ is present in the minimal $d$-reduction of 
a soluble ID graph $\calG$, 
then we can choose a graph and tree $\calG^3, T^3$
so that $\calG^3$ is homomorphic to $\calG$, and $T^3$ is in normal form.
In this section, we will prove that 
we can use $T^3$ to parameterise $\calG^3$
so that optimal performance can only be 
achieved by a policy that has
$\pi^D(\pa)=f(x)$
for a specific $f$
given every $\pa\in \dom(\Pa(D))$ with $P(\pa)>0$.






\subsection{Constructing an ID on nodes in a normal form tree}
We will define an ID for only the nodes in the tree, excluding the root info link, assuming that there is already an ID (possibly trivial) defined for all the other nodes (including those in the root infolink). 
This result is more general than is needed to prove positive VoI (wherein we will chose a trivial ID)
but this is done in order to help with generalizing to the $\sTaskify$ construction in the next section.~\looseness=-1


\newcommand{\sGoriginalInModelConstruction}{\sG^0}
\newcommand{\sGNewInModelConstruction}{\sG^3}
\newcommand{\MoriginalInModelConstruction}{M^0}
\newcommand{\MnewInModelConstruction}{M^3}


\begin{definition}[Parameterization of a normal form tree] \label{def:model-construction}
Let $\sGNewInModelConstruction$ be a soluble ID graph together with a normal form tree of systems $T^3$ with root info-link $X\to D$. Let $\sGoriginalInModelConstruction$ be the subgraph consisting of $X,D$, and all nodes in $\sGNewInModelConstruction$ that are not in $T^3$.
Let $\MoriginalInModelConstruction=(\sGoriginalInModelConstruction, \dom^0, P)$ be an ID on on $\sGoriginalInModelConstruction$, and let $\piTask^{D}$ (which we call \emph{the task for $D$}) be a deterministic decision rule for $D$ that depends only on $X$. 
Then we define the ID $\MnewInModelConstruction=(\sGNewInModelConstruction, \dom, P)$, which are defined as follows:


\begin{itemize}
    \item
    For each node $N$ in $\sGoriginalInModelConstruction$ except $D$, let $\dom(N)=\dom^0(N)$, and let 
    $\dom(D)=\dom_{\sbase}(D)\times \bool$, where
    $\dom_{\sbase}(D)=\dom^0(D)$. 
    For any other chance or decision node $N$, we define the domain of a node by recursion on the tree $T^3$. Let $s$ be the base system of $N$.\footnote{This uses the assumption that $T^3$ is normal form, and hence satisfies \autosubref{def:sep5.3-normal-form-graph-with-tree}{def:sep5.3c-unique-systems-and-paths} (\systemsAndPathsUniqueness), and using the properties that this implies by \autoref{le:21-may-9-position-in-tree-uniqueness-properties}} 
    Assume that the domains of the info node and decision node $X^s, D^s$ of $N$'s base system $s$ are already defined.\footnote{This is well-founded recursion, and for the base case of $s=\sRoot^T$, the domains of $X^s=X$ and $D^s=D$ were already defined above.} 
    Then, if $s$ is of the non-directed-info case then
        \[\dom_\sbase(N) =
            \begin{cases} 
            \bool \casesif  {$N$ is on $\sobspaths^s(C^1)$, incl. $C^1$, the first obspath of $s$}\\
            \dom{(X^s)} \casesif  {$N$ is in between $X^s\pathto C^1$ on $\sinfo^s$ }\\
            \bool^{|\dom_\sbase{(D^s)}|} \casesif  {$N$ in any other part of  $\sinfo^s$, or any other $\sobspaths^s$}\\
            \dom(D^s) \casesif {$N$ in $\scontrol^s$}
            \end{cases},\]
    and if it is of the directed-info case then $\dom_\sbase(N) = \dom(X^s)$. 
    Based on this:
        \[\dom(N) =
            \begin{cases} 
            \dom_\sbase(N) \times \bool \casesif  {$N=D^{s'}$ for a non-directed-info descendant  $s'$}\\
            \dom_\sbase(N) \casesotherwise
            \end{cases}.\]
    \ryan{Relatedly, can we just refer to dom-base with $dom(Q^s)$ or something, or do we need this term?}
    \chris{We might be able to get rid of it, but I'm not immediately sure how.}
    
    \ryan{It seems like the domains are implied by  the functions and domains of the parents in all cases other than forks, and the root systems so can we just define the functions directly?}
    \chris{We maybe should consider this, but it's probably not a priority. It maybe has some downsides in terms of understandability, but my guess is it'd be better if it can be done clearly and rigorously. Probably the main problem is that decision nodes need domains too.}
    
    
    \item For each chance node $N$ in $\sGoriginalInModelConstruction$ (including $X=X^{s^\sRoot}$), let $P_{\MnewInModelConstruction}^N=P^N_{\MoriginalInModelConstruction}$. For any other chance node $N$, let $s$ be the base system of $N$ and $p$ the base path of $N$,\footnote{This uses the assumption that $T^3$ is normal form, and thus satisfies (c) \systemsAndPathsUniqueness} and let $\piTask^{D^s}$ be the task of the decision of system $s$, defined for $s=\sRoot$ as the task $\piTask^{D}$ given above, and as the identity operation \emph{$\id^{\spath}$}
    for every other system. Then, writing $\piTask^{D^s}(x)$ to refer to $d$ s.t. $\piTask^{D^s}(d|x)=1$, we let $P^N(n|\pa)=1$ iff $n=f^N(\pa)$, where
    \[f^N =
        \begin{cases} 
        {\id}^{\spath} \casesif {$\to N\to$ or  $\leftarrow N \leftarrow$ in $\spath$}\\
        N^2[\piTask^{D^s}(N^1)] \casesif {$N^1\to N\gets N^3$ is the first collider on $\sinfo^s$}\\
        {\XORtext}^{\spath}\casesif {$\to N \leftarrow$ is a collider on $p$ other than the first}\\
        \srandom^\spath \casesif {$\leftarrow N \to$ in $\spath$}
        \end{cases}
    \]
    where $\id^{\spath}$ copies the output of the $\dom_\sbase$ part of the previous node on $\spath$, $\XORtext^{\spath}$ takes the bitwise \emph{exclusive OR} operation on bitstrings,\footnote{Note that the domain of any such collider is a bitstring.} $\srandom$ copies a uniform random bitstring from $\Epsilon^N$, and \emph{$b[x]$} outputs the $x^\text{th}$ digit of a bitstring $b\in \bool^n$. 
    
    \item For each utility node $U$ in $\sGoriginalInModelConstruction$, let $f_{\MnewInModelConstruction}^U=f^U_{\MoriginalInModelConstruction}$. For any other utility node $U$,
    if $\sinfo^s$ is non-directed for the base system $s$ of $U$, letting $i$ be the value that $U$ receives from the penultimate node in the infopath and $(c^h,c^r)$ the value it receives from the penultimate node of the control path, we let $P^U(u|\pa)=1$ iff $u=f^U(\pa)$, where
    \begin{align*}
    f^U(i,(c^h,c^r)) &=
        \begin{cases} 
            \Umax \casesif {$c^h[i]=c^r$}\\
            0 \casesotherwise
        \end{cases},
    \end{align*}
    and if instead $\sinfo^s$ is directed then
    \begin{align*}
    f^U(i,c) &=
        \begin{cases} 
            \Umax \casesif {$i=c$}\\
            0 \casesotherwise
        \end{cases},
    \end{align*}
    where $\Umax=1+\sum\limits_{U\in \sU^{\Goriginal}}\left(\max\limits_{\pa} f^U_{\MoriginalInModelConstruction}(\pa)-\min\limits_{\pa} f^U_{\MoriginalInModelConstruction}(\pa)\right)$.
\end{itemize}
\end{definition}



\subsection{The ID forces decision nodes to ``perform their task"} \label{sec:task-performance}

We will show here that we have constructed the ID in such a way that a decision $D^s$ of a system $s$ can only achieve optimal utility if it performs its task. Recall that we write $\piTask^{D^s}(x)=d$ to refer to $d$ s.t. $\piTask^{D^s}(d|\pa)=1$ where $x$ is the value of $X^s$ under $\pa$.


\begin{definition}[Task performance] \label{def:aug23.4-task-performance}
Let $s$ be a system in some tree $T$.
Given a decision context $\pa\in \dom(\Pa(D^s))$, we say that $D^s$ \textit{performs the task $\piTask^{D^s}$ with $\pi$ given $\pa$} if 
\begin{itemize}    
    \item $\sinfo^s$ is directed, and $\pi^{D^s}(d|\pa)=\piTask^{D^s}(d|\pa)$; or
    
    \item $\sinfo^s$ is non-directed, and $\pi^{D^s}(d|\pa)=1$ iff $d=(\piTask^{D^s}(\pa), \hat R)$, where $\hat R$ is the value of $Q^s[\piTask^{D^s}(X^s)]$.
\end{itemize}
And we say that \emph{$D^s$ performs the task $\piTask^{D^s}$ with $\pi$} if it does so for any $\pa$ that has positive probability of occurring under $\pi$. 
\end{definition}


\begin{lemma}[\editingMode{le:3.3 - }\modelKnowledgeLemmaName]\label{le:3.3-model-knowledge-lemma}
Let $M^3$ be the ID based on some $T^3$, $M^0$ and $\piTask^{D}$ (\autoref{def:model-construction}), and let $\piTask^{D^s}$ be the task of system $s$ in $M^3$, as defined in  \autoref{def:model-construction}.\footnote{recall that the task of all decisions in the tree except for the root decision, are the identity operations.}
Let $s$ be a system with non-directed infopath, and assume that for all child systems $s'$ of $s$, $D^{s'}$ performs its task $\piTask^{D^{s'}}$ with $\pi$.
Then $Q^s[\piTask^{D^s}(X^s)]$ can be expressed as a function of $\Pa(D^s)$
but $P(Q^s[\tilde y]=1\mid \Pa(D^s)=\pa)=\frac{1}{2}$ for all $\pa\in \dom (\Pa(D^s))$
for any $\tilde{y} \neq \piTask^{D^s}[X^s]$.
\end{lemma}

\begin{proof} 
Let $x^s$ be any value in $\dom(X^s)$, and for brevity, let $y=\piTask^{D^s}(x^s)$.\chris{should be $\piTask^{D^s}(X^s)$ I think? or something like that, bc now its quantifying over $x$} Moreover, let $\xor$ denote the exclusive or operator (XOR) on boolean strings.
We will first show that (1) $Q^s[y]$ is a function of $\Pa(D^s)$, 
and then that (2) $P(Q^s[\tilde y] \; \mid \;\Pa(D^s))$ is uniformly random for $\tilde y \in \dom_{\sbase}(D^s) \setminus \{y\}$. 


First we show (1).
First note that $Q^s$ and each fork $F^i,1\leq i\leq k$ in the infopath is a random bitstring of length $\bool^{|\dom_\sbase{(D^s)}|}$ (\autoref{def:model-construction}). And since each obsnode $O^i$ equals the collider $C^i$ on the info path (where $O^i$ is the obs node of the obs path $\sobspaths^s(C^i)$), by the same construction, $O^1=F^1[\piTask^{D^s}(x)]$, and for $1<i< k$, $O^i=F^{i-1} \xor F^i$, and $O^k=F^k \xor Q^s$.
Then, the decision can recover $Q^s[y]$ by taking the XOR of the $y^{\text{th}}$ element of each observation, by setting (letting $x$ be the value of $X^s$ in $\pa$):~\looseness=-1
\begin{alignat*}{2}
    &\;\pi^D(\pa, \varepsilon)\\
    &:= \;O^1 \xor O^2[y]\;\xor\;...\;\xor\; O^{k-1}[y] \xor O^k[y] && \quad : \text{where $y=\piTask^{D^s}(x)$}\\
    &=\; F^1[y] \xor (F^1[y]\xor F^2[y])\;\xor \;... \;\xor\; 
    (F^k[y] \xor Q^s[y]) &&\quad : \text{by \autoref{def:model-construction}} \\
    &=\; (F^1[y] \xor F^1[y] )\;\xor \;... \;\xor\; (F^k[y] \xor F^k[y] )\xor Q^s[y] &&\quad : \text{associativity of $\xor$} \\
    &=\; Q^s[y] &&\quad : \text{$b\xor b=0$ and $0\xor b=b$}
\end{alignat*}




Now we will show (2). Firstly, \autoref{le:2v2-graph-knowledge-lemma} directly implies that $(\Pa(D^s)\setminus (\sV^s\cup \sO)$ are $d$-separated from $Q^s$ conditional on $(\Pa(D^s)\cap \sV^s)\cup \sO$.  Hence by the standard $d$-separation criterion, they are probabilistically independent as well, i.e. $P(Q^s[\tilde{y}]=b \mid \pa(D^s), \sO)=P(Q^s[\tilde{y}]=b \mid \pa(D^s)\cap \sV^s, \sO)$. Moreover, since by
\autosubref{le:20dec14.1-basic-properties-of-trees-with-SR}{le:20dec14.1b-only-info-links-from-observation-nodes-to-ancestor-decisions} only observation nodes in this subtree can be parents, and since conditioning on more information does not increase uncertainty, it will suffice to show that
$P(Q^s[\tilde y] = b \; \mid \; X^s=x^s, \sO=\so)= \frac 1 2$ for all $b, x^s, \so$.






Now we will show (2). 
For notational brevity, we define:
$\sQ=\bigtimes_{s \in \Desc_s} Q^s$,
$\sF=\bigtimes_{s \in \Desc_s,1\leq i\leq \mid \sF^s \mid} F^{s,i}$,
$\sO=\bigtimes_{s \in \Desc_s,1\leq i\leq \mid \sO^s \mid} O^{s,i}$.\chris{sentence below is not exactly clear what the proposition is. }
Then, using \autoref{le:2v2-graph-knowledge-lemma}, we can express the probability of $Q^s[\tilde{y}]$ as a 
probability, conditional on observation nodes of the subtree.
\begin{align*}
&P(Q^s[\tilde{y}]=b \mid \pa(D^s)) \\
&= \sum_{\so \setminus \Pa(D^s)} P(\so \setminus \Pa(D^s)) \cdot P(b \mid \so \setminus \Pa(D^s), \pa(D^s))  && : \text{product rule} \\
&= \sum_{\so \setminus \Pa(D^s)} P(\so \setminus \Pa(D^s)) \cdot P(b \mid \so \setminus \Pa^s, \pa^s, \pa^{-s} \setminus \sO) && : \text{regroup terms in conditional} \\
&= \sum_{\so \setminus \Pa(D^s)} P(\so \setminus \Pa(D^s)) \cdot P(b \mid \so \setminus \Pa^s, \pa^s)  && :\autoref{le:2v2-graph-knowledge-lemma} \\
&= \sum_{\so \setminus \Pa(D^s)} P(\so \setminus \Pa(D^s)) \cdot P(b \mid x^s, \so))  && (\dagger)
\end{align*}



So it will suffice to show that 
$P(Q^s[\tilde y] = b \; \mid \; X^s=x^s, \sO=\so)= \frac 1 2$ for all $b, x^s, \so$.
Let $\mathfrak{X}^{\so,b,x^s}_{\sQ \cup \sF} = \{\sq,\sf : P(Q^s[\tilde{y}]=b,\so \mid \sq,\sf,x^s)=1\}$.
In the ID $\calM^3$, the event $Q^s[\tilde y] = b,\sO=\so$ may be equivalently stated as follows:

\begin{enumerate}
    \item $Q^s[\tilde{y}]=b $
    \item $f^{s,1}[\piTask^{D^s}[x^s]]=o^{s,1} (\iff O^1=o^{s,1})$
    \item $f^{s',1}[\piTask^{D^s}[x^{s'}]=o^{s',1} \text{ for } s' \neq s (\iff O^{s',1}= \so^{s',1})$
    \item $f^{s',i} \xor f^{s',i+1}=o^{s,i} \text{ for } 1 < i \leq \mid F^{s'}\mid \text{ for } s' \in \calS (\iff \sO^{s',i} = \so^{s',i},i>1)$
\end{enumerate}

That is to say that 
$\mathfrak{X}^{\so,b,x^s}_{\sQ \cup \sF}= \{(\sq,\sf) \in \dom(\sQ \cup \sF):\text{s.t. (1-4) satisfied}\}$, 
and that for $(\sq,\sf) \not \in \mathfrak{X}^{\so,b,x^s}_{\sQ \cup \sF}$, 
$P(Q^s[\tilde{y}]=b,\so \mid \sq,\sf,x^s)=0$.


We can use the definition of $\calM^3$ to yield a convenient equivalent expression for 
$\mathfrak{X}^{\so,b,x^s}_{\sQ \cup \sF}$ that substitutes $Q^s$ in for $F^{s,1}$.

\begin{enumerate}[label={\arabic*$'$.}] \setcounter{enumi}{1}
\item $(Q^s[\piTask^{D^s}(x^s)]=o^{s,1}\xor \bigoplus_{1 < i \leq \mid F^s\mid} o^{s,i}) \Leftarrow (2.,4.)$
\item $q^{s'}[\piTask^{D^{s'}}(X^{s'})]=o^{s',1} \xor \bigoplus_{1 < i \leq \mid F^{s'}\mid} \Leftarrow (3.,4.)$.
\end{enumerate}
Since XOR with a bitstring is a bijective operation, we also have: $(2'.,4.) \Rightarrow 2$ and $(3'.,4.) \Rightarrow 3$.
Thus $\mathfrak{X}^{\so,b,x^s}_{\sQ \cup \sF}= \{(\sq,\sf) \in \dom(\sQ \cup \sF):\text{s.t. (1.,2',3',4) satisfied}\}$

To obtain a $(\sq,\sf) \in \mathfrak{X}^{\so,b,x^s}_{\sQ \cup \sF}$, one can then carry out the following algorithm starting from $s^0=s$:
\begin{enumerate}[label=\alph*)]
\item choose any $q^s$ that satisfies (1,2') (there are $\mid Q^s \mid / 4$ possible assignments because $\piTask^{D^s}[x^s] \neq \tilde y$)
\item choose each $F^{s,i}$ to satisfy (4) given $q^s$ (one possible assignment)
\item for each child system $s^j$, choose $Q^{s^j}$ to satisfy (3') given $q^s$ ($\mid Q^{s^j} \mid / 2$ possible assignments).
\item choose each $F^{s^j,i}$ to satisfy (4) given $q^{s^j}$ (one possible assignment)
\item repeat (c-e) for children of $s^j$
\end{enumerate}

Any $(\sq,\sf)$ computed by this algorithm will clearly satisfy (1.,2',3',4) 
and is therefore in $\mathfrak{X}^{\so,b,x^s}_{\sQ \cup \sF}$.
Conversely, if at any step, a non-allowed assignment is selected, then one of (1.,2',3',4) 
is violated, and so $(\sq,\sf) \not \in \mathfrak{X}^{\so,b,x^s}_{\sQ \cup \sF}$.
The number of possible assignments after (a-b) (is clearly $\mid Q^s \mid / 4=\mid Q^{s^0} \mid / 2^{j+2}$.
For the $j$th system, the number of possible assignments is then 
$\mid Q^{s^0}/4 \cdot Q^{s^1}/2 \ldots Q^{s^j}/2 = \bigtimes_{j'\leq j} \frac{\mid Q^{s^{j'}}\mid}{2^{\mid 2^j}}$.
So $\mid \mathfrak{X}^{\so,b,x^s}_{\sQ \cup \sF} \mid = \bigtimes_{j'\leq \mid \calS \mid} \frac{\mid Q^{s^{j'}}\mid}{2^{2+j}}$


This is independent of the value $b$ selected for $Q^s[\tilde{y}]$, so 
for any $\so,b,x^s, \tilde{y}\neq \piTask^{D^s}(x^s)$:
\begin{align*}
    P(Q^s[\tilde{y}]=b,\so \mid x^s) &= \sum_{\sf \in \dom(\sF),\sq \in \dom (\sQ)} p(Q^s[\tilde{y}]=b,\so\mid \sq,\sf, x^s)p(\sq,\sf \mid x^s) \\
    &= \sum_{(\sf,\sq) \in \mathfrak{X}^{\so,b,x^s}_{\sQ \cup \sF}} p(Q^s[\tilde{y}]=b,\so\mid \sq,\sf, x^s)p(\sq,\sf \mid x^s) \\
    &= \mid \mathfrak{X}^{\so,b,x^s}_{\sQ \cup \sF}\mid p(\sq,\sf \mid x^s) & (*)
\end{align*}\chris{add better explanation for last step}

Thus we can compute the conditional:
\begin{align*}
P(Q^s[\tilde{y}]=b \mid \so, x^s)
&= \frac{P(Q^s[\tilde{y}=b],\so \mid x^s)}{P(\so \mid x^s)} & \\
&= \frac{P(Q^s[\tilde{y}=b],\so \mid x^s)}{\sum_{b \in \bool} P(Q^s[\tilde{y}]=b,\so \mid x^s)}& \\
&= \frac{\mid \mathfrak{X}^{\so,b,x^s}_{\sQ \cup \sF}\mid p(\sq,\sf \mid x^s)}{(\mid \mathfrak{X}^{\so,0,x^s}_{\sQ \cup \sF}\mid + \mid \mathfrak{X}^{\so,1,x^s}_{\sQ \cup \sF}\mid ) p(\sq,\sf \mid x^s)}&&:\text{substituting in from (*)}& \\
&= \frac{1}{2} && :\mathfrak{X}^{\so,b,x^s}\text{ is constant in $b,x^s$} %
\end{align*}
Substituting into $(\dagger)$ yields the result.
\end{proof}




We now show that in the ID that we constructed in \autoref{def:model-construction}, the set of optimal policies are exactly the set of policies in which all decisions perform their task.

\begin{definition} [Locally optimal decision rule] \label{def:21feb6.2-local-optimality}
The decision rule $\pi^D$
is locally optimal with respect to $\spi^{-D}=\{\pi^{D'}\}_{D' \neq D}$
if the policy $\pi^D \cup \spi^{-D}$
has greater or equal expected utility
as the policy $\tilde{\pi}^D \cup \spi^{-D}$ 
obtained by any alternative decision rule $\tilde \pi^D$.
\end{definition}


\begin{lemma}[Decisions in the tree are optimal iff they perform their task] \label{le:1.7-optimal-iff-truthful}
Let $\pi$ be any policy on the taskified ID $M^3$ obtained from some $T^3$, $M^0$, and task $\piTask^{D}$, and let $\piTask^{D^s}$ be the task of system $s$ in $M^3$, as defined in  \autoref{def:model-construction}. Then the decisions $D^s$ in the tree $T$ are optimal under $\pi$ if and only if they all perform their task $\piTask^{D^s}$ with $\pi$.  Moreover, then $\EE_\pi(U^s)=\Umax$ for all $s$.
\end{lemma}

\begin{proof}
We first prove an intermediate step, to be used for induction to show the result. It essentially says that as long as the decisions of the child systems of $s$ perform their task, then so will $D^s$:

(Induction-step) : Assume $\pi$ is a policy such that for any child system $s'$ of $s$ in $T$, the decision $D^{s'}$ performs its task with $\pi$. Then for any $\pa\in \Pa(D^s)$ with positive probability of occuring under $\pi$, it holds that $U^s=\Umax$ with probability $1$ if $D^s$ performs its task with $\pi$ given $\pa$, and $U^s=0$ with probability $1$ if it outputs any other decision. Moreover, such a policy $\pi$ exists.

We show (Induction-step) as follows: Since by assumption all the decisions on the info and control paths perform their task, they copy the previous node's value. The chance nodes in the front section do the same by definition of the ID.
    Hence for any $\pa\in \dom(\Pa(D^s))$ that occurs with positive probability, we have that with probability $1$, $U^s$ receives from the penultimate node on the control path a value that 
    equals that of $D^s$, and from the penultimate node in the info path a value that, if $s$ is of the directed-info case equals that of $X^s$, and if it is of the non-directed-info case equals that of $Q^s$.
    
    Hence if we assume that $s$ is of the directed-info case, then substituting this into the definition of $f^{U^s}$, and using \autoref{def:aug23.4-task-performance} (task performance), this implies that conditional on $\pa$, $\Umax$ is attained with probability $1$ if and only if $D^s$ copies:
        \[U^s = 
        \begin{cases}
        \Umax \casesif {$D^s=X^s$}\\
        0  \casesotherwise 
        \end{cases}.\]
    
    This obviously implies $\EE_\pi[U^s]=\Umax$. Similarly, if we assume that $s$ is of the non-directed-info case, then substituting the same into the definition of $f^{U^s}$, and using \autoref{def:aug23.4-task-performance} (task performance), this implies that conditional on $\pa$, $\Umax$ is attained with probability $1$ if and only if $D^s$ reports consistently: Letting
    $D^s= (\hat T,\hat R)$:
    \[U^s = 
        \begin{cases}
        \Umax \casesif {$Q^s[\hat T]=\hat R$}\\
        0  \casesotherwise 
        \end{cases}.\]
    Hence $\EE_\pi[U^s]=\Umax$ if and only if $D^s$ reports consistently with probability $1$ conditional on its parents. Moreover, by \autoref{le:3.3-model-knowledge-lemma} the value of $Q^s$ is known conditional on its parents only for the true value of the info node $X^s$. Hence $\EE_\pi[U^s]=\Umax$ if and only if $D^s$ reports consistently and outputs $\hat X=X^s$. The latter is equivalent to $D^s$ performing its task, and hence also shows that it is possible for $D^s$ to perform its task. This shows (Induction-step).

    Now we prove the $\Leftarrow$ direction of the main result: Let $\pi$ be any policy so that all decisions $D$ perform their task with $\pi$. Then the $\Leftarrow$ direction of (Induction-Step) implies that for any system $s$, $U^s=\Umax$ with probability $1$ given $\pa$, and since $\Umax$ is defined such that it is larger than the sum of the possible range of utility for all utility nodes in the original graph, by not performing its task any such $D^s$ would decrease $U^s$ by $\Umax$ and increase the other utility nodes by at most $\Umax -1$.
    Since such a policy exists by (Induction-step), this also shows that optimal policies achieve $\EE_\pi(U^s)=\Umax$ for all $s$.~\looseness=-1
    
    We now  prove the $\Rightarrow$ direction of the main result: Assume all decisions $D^s$ in the tree are optimal under $\pi$. We prove the statement by backward induction on decisions using (Induction-Step), where we show the base step by applying the induction step to the final decision: As the induction hypothesis, assume that all $D^{s'}$ with $D^{s'}>D^s$ 
    perform their task. Then since decisions of descendant systems are descendants (\autosubref{le:20dec14.1-basic-properties-of-trees-with-SR}{le:20dec14.1a-decisions-in-descendant-systems-are-descendants}), all the decisions of child systems of $D^s$ perform their task. This implies by the (Induction-Step) that for any $\pa$ with positive probability of occurring under $\pi$, 
    $U^s=\Umax$ with probability $1$ given $\pa$ if $D^s$ performs its task with $\pi$ given $\pa$, and $U^s=0$ if it takes any other decision. And since $\Umax$ is defined such that it is larger than the sum of the possible range of utility for all utility nodes in the original graph, by not performing its task any such $D^s$ would decrease $U^s$ by $\Umax$ and increase the other utility nodes by at most $\Umax -1$. Hence since by assumption $D^s$ is optimal with $\pi$ it must perform its task with $\pi$ given all $\pa$ that occur with positive probability under $\pi$.~\looseness=-1
\end{proof}



\subsection{Showing positive VoI on an ID graph that has a normal form tree} \label{sec:model-has-incentives}

In this section we will show that we can use the ID from \autoref{def:model-construction} applied to the info link $X\to D$ to show that $X$ has positive VoI for $D$.
In the previous section we showed that a policy is optimal if and only if it performs its task. We now show that in order to perform its task, $D$ in fact has to observe $X$. These together will be used to show that there is in fact positive VoI for $X$ on $D$ on the ID $\calM$ as desired.~\looseness=-1


\ryan{Need to replace materiality with reference to VoI}
\chris{Isn't this done already?}





Here is the main completeness result for the ID $\calM$:

\materialitysat*

\begin{proof}
Let the normal form tree be $T$.
Then, let the ID $\calM$ be obtained from $\MoriginalInModelConstruction$ and $\piTask^{D}$ and $T$ (\autoref{def:model-construction}), where $\MoriginalInModelConstruction$ is the ID that assigns boolean domains to $X$ and $D$ and trivial domains to all other nodes in $\sGoriginalInModelConstruction$, and has $X$ generate a random bit, and where $\piTask^{D}$ is the identity function.

We know that a policy in $\calM$ is optimal iff it performs its task (\autoref{le:1.7-optimal-iff-truthful}), i.e. for the particular ID $\calM$, to output $\piTask^{D^s}(X)=X$.
Therefore in order to show that $X$ has positive VoI for $D$, it suffices to show that any policy on $\calM$ that performs its task does not factor over $M_{X \not \to D}$, for which it suffices to show that $P[X | \Pa(D) \setminus \{X\}]=\frac{1}{2}$. 

\ryan{Add the missing bit from the previous materiality completeness proof here.}

Next, we will prove that $(\sQ,\sF)$ is independent of $X$. For any $\sq,\sf,x'$:

\begin{align*}
    P(\sq,\sf\mid x') &= P(\sq,\sf) \frac{P(x'\mid \sq,\sf)}{P(x')} && : \text{Bayes Theorem} \\
    &= P(\sq,\sf) \frac{P(x' \mid \doo(\sq,\sf))}{P(x')} && :\text{do-calc rule 2; $\Pa_{\sQ}=\emptyset$ in $\calG^3$} \\
    &= P(\sq,\sf) \frac{P(x'}{P(x')} && :\text{do-calc rule 1; $\Pa_{X}=\emptyset$ in VoI ID} \\
    &= P(\sq,\sf) && (*)
\end{align*}
\ryan{What do we mean by ``VoI ID'' here?}
\begin{align*}
P(x \mid \so) &= P(x) \frac{P(\so\mid x)}{P(\so)} && : \text{Bayes Theorem} \\
 &= P(x) \frac{P(\so\mid x)}{\sum_{x'}P(\so\mid x')P(x')} && : \text{Addition rule} \\
 &= P(x) \frac{\sum_b P(Q^s[\tilde y]=b,\so \mid \sq,\sf,x) P(\sq,\sf \mid x)}{\sum_{x,b} P(Q^s[\tilde y]=b,\so \mid \sq,\sf,x) P(\sq,\sf \mid x)} \text{ for any } \tilde y && : \text{Addition, product rules} \\
&=P(x) \frac{\sum_b \mathfrak{X}^{\so,b,x^s}_{\sQ \cup \sF} P(\sq,\sf \mid x)}{\sum_{x,b}  \mathfrak{X}^{\so,b,x^s}_{\sQ \cup \sF} P(\sq,\sf \mid x)P(x')} && : \text{Property of $\mathfrak{X}^{\so,b,x^s}_{\sQ \cup \sF}$} \\
&=P(x) \frac{P(\sq,\sf)}{\sum_{x'} P(\sq,\sf)P(x')} && :\mathfrak{X}^{\so,b,x^s}\text{constant in $b$; (*)} \\
&=P(x) \frac{P(\sq,\sf)}{P(\sq,\sf)} &&: \sum_{x'} P(x')=1 \\
&=P(x) && :\text{$P(\sq,\sf)>0$} \\
&= \frac{1}{2}
\end{align*}


\end{proof}



