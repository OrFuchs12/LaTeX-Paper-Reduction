\def\year{2020}\relax
%File: main.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai20}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{graphicx}  % DO NOT CHANGE THIS
\usepackage{xcolor}
\usepackage{amssymb}

\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS


\newcommand{\commentout}[1]{}
\newcommand{\eliran}[1]{\textbf{[\color{red}ELIRAN:#1]}}
\newcommand{\ronen}[1]{\textbf{[\color{blue}RONEN:#1]}}
\newcommand{\guy}[1]{\textbf{[\color{orange}GUY:#1]}}

\newcommand{\cbp}[0]{Collaborative Box-Pushing}
\newcommand{\mitg}[0]{Meet In The Grid}
\newcommand{\crs}[0]{Cooperative Rock-Sampling}
\newcommand{\macor}[0]{Multi-Agent Corridor}

\newcommand{\Tau}{\mathrm{T}}

%\nocopyright
%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
 \pdfinfo{
/Title (A Factored Approach To Solving Dec-POMDPs)
/Author (Eliran Abdoo, Ronen I. Brafman, Guy Shani)
} %Leave this	
% /Title ()
% Put your actual complete title (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case
% Leave the space between \Title and the beginning parenthesis alone
% /Author ()
% Put your actual complete list of authors (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case. 
% Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands, 
% remove them. 

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{caption} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \natbib} -- This package is specifically forbidden -- use the following workaround:
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai20.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\setlength\titlebox{2.5in} % If your paper contains an overfull \vbox too high warning at the beginning of the document, use this
% command to correct it. You may not alter the value below 2.5 in
\title{A Factored Approach To Solving Dec-POMDPs }
%Your title must be in mixed case, not sentence case. 
% That means all verbs (including short verbs like be, is, using,and go), 
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\author{Eliran Abdoo, Ronen I Brafman, Guy Shani
}
 \begin{document}

\maketitle

\begin{abstract}
Dec-POMDPs model planning problems under uncertainty and partial observability for a distributed team of cooperating agents. This problem is
very challenging computationally -- it is NEXP-Time Complete -- and consequently, exact methods have had difficulty scaling up. In this paper
we present a heuristic approach for solving certain instances of Factored Dec-POMDP that tries to reduce the problem of planning in Dec-POMDPs to multiple
problems of planning in a POMDP. First, we solve a team version of the Dec-POMDP in which agents have a shared belief state, and then, each agent attempts to solve the problem of executing its part of the team plan. Finally, the different solutions are aligned to improve synchronization. Using this approach we are able to solve much larger Dec-POMDP problems, limited only by the abilities of the underlying POMDP solver.
\end{abstract}

\section{Introduction}

Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are the most popular model for planning under uncertainty with partial
observability by a distributed team of agents~\cite{}. In this model, a team of agents attempts to maximize the team's commulative reward in a stochastic
environment where each agent has only partial information about the state of the system during execution. That is, an agent's observations are available
only to it. Communication is possible only if explicit communication actions are available to the agents.  To achieve this goal agents must coordinate their
actions in two ways: First, they must coordinate sequentially, i.e., current actions must help steer the system towards states in which greater reward will be possible,
e.g., to be rewarded for serving coffee, it must first be prepared. This is also a standard requirement in single-agent planning problems. Second, they may need
to coordinate their simultaneous actions because their effect are dependent, e.g., a heavy box can be pushed only if two agents push it simultaneously. 
As with previous work in this area, our focus is on centralized off-line planning for distributed execution. That is, off-line, a solver with access to the
complete model must generate a set of policies -- one for each agent -- that determine what action each agent must take as a function of its {\em own} history of
actions and observations. The difficulty lies in generating policies that provide sufficient coordination despite the fact that each agent may make different
observations at run-time, and so the belief states of different agents can be quite different. 

Dec-POMDPs are notoriously hard to solve -- they are NEXP-Time hard~\cite{} implying that only the smallest toy problems are perfectly solvable.
However, quite a lot of methods for solving it have been proposed, with steady progress. Some of these methods generate solutions with bounds on
their optimality~\cite{}, and some are more heuristic in nature~\cite{}. However, current methods seem to scale up to state spaces with roughly 200 states.


In this paper we describe a heuristic approach for solving Dec-POMDPs that scales up to much larger problems. The key idea is solve the Dec-POMDP by
solving multiple POMDPs. First, we solve the POMDP obtained from the Dec-POMDP when we assume that all agents have the same belief state -- i.e.,
that any observation by one agent is immediately available to the other agents. We refer to this as the {\em team POMDPs}. The solution of the team
POMDP is a policy tree -- the {\em team policy tree.} . It provides us with a skeleton for the solution of the Dec-POMDP as it basically tells us what each agent needs to provide the team. Naturally, this policy is not executable by the agents. Hence, in the next stage, we let each agent solve a POMDP in which it is rewarded
for behaving in a manner similar to what it would do in the team policy. This leads to the generation of a policy tree for each agent. 
However, these policy trees are not well synchronized with each other, and in the last step, we synchronize them by delaying the actions of agents to
improve the chance of good coordination. 

%It is heuristic in the sense that
%it is based on using a certain abstraction to generate the solution and as such, the solution is not guaranteed to be optimal, not can it be optimal,
%in general, with more computational time -- certain
%
%
%
%
%\\Multi agent planning in decentralized and partially observable environments is well known for being a difficult problem due to the difficult of reasoning with no explicit communication mechanisms while operating in an uncertain world. Such difficulty stems from the need for coordination, and let alone the need for collaboration. Planning for this sort of multi-agent problems, assuming there is indeed some necessity for coordination, require the agent to reason not only about it's uncertainty of the state, but also about the uncertainty of it's co-agents. This leads in turn to an infinite cycle: The agent needs to act based on its own perception of the state, by reasoning about the other agents' beliefs, and the same applies in vice versia for the other agents, creating such cycle.\\
%\guy{Specific problem in the area}
%\eliran{I'm not sure about what to write in this section}
%\\In this area of research there are numerous ways to model different problems, many of them distinguish between different representations of the world and different representations of communication mechanisms. We focus on an instance of the former, Factored Dec-POMDP, in which the world is factored into state variables and the environment is a Partially-Observable Markovian Decision Process.
%\guy{Solution method - team plan first, then single agent extensions, alignment}
%\\We approach the problem from a different angle by trying to avoid the explicit planning for decentralized multi-agent problem with no communication.\\
%We start by creating the Team-Problem, which is the centralized version of the problem at hand, and let the planner compute a solution. Such solution is much less difficult to compute, as there's essentially full communication between the agents, where each agent notifies all of it's co-agents about its observations.\\
%Following that, we project the problem with respect to each agent using the team solution. Each projection represents a single-agent problem in which the agent is rewarded for fulfilling his part of the solution that is relevant to its co-agents, while still being rewarded for achieving his private rewards. In each such single-agent problem, the agent makes certain actions on behalf of its co-agents in a rather superficial manner, yet in a way that allows him to properly plan essential sensing actions that weren't needed in the Team-Problem.\\
%Finally, we solve the single-agent problems and then align the resulting policies, in order to synchronize the agents and make sure that each agent performs his part in the appropriate time.\\
%\guy{Experiments}
We implemented our algorithm and tested it on several benchmark problems: \cbp, \mitg, \macor, \crs. We show that the algorithm manages to scale well beyond
current Dec-POMDP solvers on \cbp, \mitg ,\macor and explain the properties of these domains that help our planner perform well.
%that help our , due to the fact that different solutions doesn't contain public actions with inverse effects, hence making the reward shaping of the single-agent problems highly beneficial for the planner, as it has very strict way of achieving it's intermediate rewards and cannot incur rewarding cycles.\\




\section{Background}
In this section we provide the needed background on POMDPs, Dec-POMDPs, their factored representation,
and policies. We also introduce the concept of private and public variables and actions in Dec-POMDPs.

\subsection{POMDPs}
A POMDP  is model for single-agent sequential decision making under uncertainty and partial observability.
Formally, it is a tuple $P=(S, A, T, R, \Omega, O, \gamma, h, b_0)$, where:
\begin{itemize}
\item
$S$ is the set of all states
\item
$A$ is the set of actions
\item
$\tau:S \times A \rightarrow \prod(S)$  is the transition function, i.e., $\tau(a, s)[s']$ is the probability of moving to $s'$ when applying $a$ from $s$.
\item
$R:S \times A \times S \rightarrow \mathbb{R}$  is the immediate reward function, i.e., $R(s,a, s')$ is the reward obtained after performing $a$ in $s$  and reaching $s'$.
\item
$\Omega$ is the set of observations.
\item
$O:S \times A \rightarrow \prod (\Omega)$  is the observations function, i.e., $O(a,s)[o]$ is the probability of observing $o\in \Omega$ when performing $a$ and reaching $s$.
\item
$\gamma$  is the discount factor.
\item
$h\in\mathbb{N}\cup\{\infty\}$ is the horizon.
\item
$b_0\in \prod(S)$ is a distribution over $S$ that represents the initial belief state.
\end{itemize}
%POMDP is used to described a single agent problem, where the agent is not aware of the current state following an action and instead maintains a belief state, which specifies the distribution of the states the world may be in. The agent may receive observations that expose some information about the state, through actions.
%\eliran{The following part regarding POMDP should probably be omitted}
%In this framework, instead of maintaining the current state itself (which is unknown), the agent maintains a distribution over all states, namely, a belief state. The initial belief state $b_0$, is exactly $I$.
%The goal is to find a policy $\rho$ (a mapping from belief states to actions) that maximizes $E\left[\sum_{t=1}^{h} \gamma^t \cdot R(s_t, \rho(b_{t-1}))\right]$, where $s_t$  and $b_t$ are the true state of the world and the belief state of the agent, respectively.
%The agent can update its belief state following every observation. After being in a belief state $b$, performing $a$, and observing $o$, the update is done as follows:
%\begin{equation}
%b_{a}^{o}[s'] = Pr(s'|b,a,o)=\frac{Pr(o|s',a,b)\cdot Pr(s'|a,b)}{Pr(o|a,b)} = \frac{Pr(o|s', a)}{Pr(o|a,b)}\cdot \sum_{s\in S}b[s]Pr(s'|a, s) \propto O(a, s')[o]\cdot \sum_{s\in S}b[s]\tau(s,a)[s']
%\end{equation}
%Usually, we evaluate each entry to the right hand side of the expression, and then normalize it by $Pr(o|a,b)$ or by its sum, as it is a probability distribution. Hence, the new belief state is $b'$, where for each state $s\in S$, $b'[s]=b_{a}^{o}[s]$.\\

Often, the state space $S$ is structured, i.e., it consists of assignments to some set of variables $X_1,\ldots X_k$, and the observation space $\Omega$ is
also structured, consisting of a set of observation variables $W_1,\ldots, W_D$. 
\ronen{try to be consistent with notation. If W is the observation variable, use it everywhere. The index denoting the maximal value (D and k) should
either be upper case or lower case. Lower case is the usual. Also, the number of agents, should probably also be lower case, although does not have to.}
Thus, $S=Dom(X_1)\times\cdots\times Dom(X_k)$ and
$\Omega = Dom(W_1)\times\cdots\times Dom(W_D)$. In that case, $\tau$ can be represented compactly, e.g., using a dynamic Bayesian network~\cite{},
which can also be used to specify the observation function. The reward function, too, can be specified compactly as a function of $X_1,\ldots X_k$ and
the action. Formats such as RDDL~\cite{} and PPDDL~\cite{} exploit these ideas to specify POMDPs compactly.
%. In that case, the transition, reward, and observation variables 
%functions can be specified in a more compact manner by referring to this variable set explicitly. This is often called a {\em factored} POMDP.
%%\guy{Factored representations}
%%\\When considering a POMDP, we usually address the state space as flat, meaning that each state encapsulates all the facts about the world, and the model dynamics are defined using these encapsulated elements. In a factored representation of POMDP, the state space $S$ is decomposed into state variables. Each state variables represents a more specific parameter of the world, for example, the location of a certain agent in a grid. Using the decomposition, we can define the transition function more concisely by addressing a state variable or a subset of state variables in each function, instead of addressing the whole state at once.\\
%Formally, a factored POMDP is a tuple $(\{X_i\}_{i=1}^{K}, S, A, \{\tau_i\}_{i=1}^{K}, \{R_i\}_{i=1}^{L}, \{\rho_i\}_{i=1}^{L}, \{\omega_i\}_{i=1}^{D}, \Omega,\\ \{Z_i\}_{i=1}^{D}, \gamma,
%h, I)$
%\begin{itemize}
%\item $\{X_i\}_{i=1}^{K}$ are the state variables.
%\item $S=X_1 \times X_2 \times ... \times X_K$ is the state space.
%\item $A$ is the set of actions.
%\item $\{\tau_i\}_{i=1}^{K}$. For $SRC_i \in P(S)$, $\tau_i:SRC_i \times A \rightarrow \prod(X_i)$ is the transition function of the state variable $X_i$. For $s'\in X_i$ and a tuple of state variable values $s \in SRC_i$, $\tau_i(a, s)[s']$ defines the probability of $X_i$ to be $s'$ when applying action $a$ in a state $(s_1,..., s_K)$ that satisfies $(s_1,..., s_K)|_{SRC_i}=s$
%\item $\{R_i\}_{i=1}^{L}$ are the reward variables. These form the total accumulated reward, where in each step, the reward received is the sum of the values of all variables.
%\item $\{\rho_i\}_{i=1}^{L}$ are the reward functions, one per variable. For $SRC_i, DST_i \in P(S)$,  $\rho_i:SRC_i \times A \times DST_i \rightarrow \mathbb{R}$ is the $i$-th reward function. Given tuples of the source and destination state variable values $s \in SRC_i$, $s' \in DST_i$, $r_i(s, a, s')$ defines the reward given when applying $a$ in state $s=(s_1,..., s_K)$ and reaching $s'=(s'_1,..., s'_K)$, where $(s_1,..., s_K)|_{SRC_i}=s$ and $(s'_1,..., s'_K)|_{DST_i}=s'$
%\item $\{\omega_i\}_{i=1}^{D}$ are the observation variables. Each variable represents a certain observable piece of information in the world.
%\item $\Omega=\omega_1 \times \omega_2 \times... \times \omega_D$. Is the observation space.
%\item $\{Z_i\}_{i=1}^{D}$ are the observation functions, one per variable. For $DST_i \in P(S)$, $Z_i:DST_i \times A \rightarrow \prod(\omega_i)$ is the observation function of the observation variable $\omega_i$. For $o \in \omega_i$ and a tuple of state variable values $s \in DST_i$, $Z_i(a, s)[o]$ defines the probability of $\omega_i$ to receive the observation $o$ when applying action $a$ and reaching a state $(s_1,..., s_K)$ that satisfies $(s_1,..., s_K)|_{DST_i}=s$
%\item The discount factor $\gamma$, horizon $h$ and initial state $I$ remain the same as in the regular POMDP.
%\end{itemize}

A solution to a POMDP is called a policy. In general, a policy assigns to each history of actions and observation ({\em AO-history}) the next action to execute.
Such a policy is often represented using a {\em policy tree} or, more generally, a {\em policy graph} (which is also called a finite-state controller). 
A policy-graph $G=(V,E)$ is a directed simple graph, in which each vertex is associated with an action, and each edge is associated with an observation.
For every edge $v\in V$ and every observation $o\in\Omega$ exactly one edge emanates from $v$ with the label $o$.
The graph has a single root which acts as its entry point. Every AO-history $h$ can be associated with some path from the root to some vertex $v$,
and the action labelling $v$ is the action that the policy associates with $h$. 

\subsection{Dec-POMDP}
A Dec-POMDP extends POMDP to problems where there are $N$ acting agents for some $N>1$. 
These agents are part of a team -- and hence, share the reward -- but they act in a distributed manner,
and hence may be exposed to different observations. Thus, their information state can be different. 
Formally, a Dec-POMDP for $N$ agents is a tuple  $P=(S, {\{A_i\}}_{i=1}^{N}, A, T, R, {\{\Omega_i\}}_{i=1}^{N}, \Omega, O, \gamma, h, {\{I_i\}}_{i=1}^{N})$, where:
\begin{itemize}
\item
$S$ is the set of all states
\item
$A_i$ is the set of actions available to agent $i$. We assume $A_i$ contains the special {\em no-op} action.
\item
$A=A_1 \times A_2 \times .. \times A_N$ is the set of joint actions.
\item
$\tau:S \times A \rightarrow \prod(S)$  is the transition function. Notice that it refers to joint-actions.
\item
$R:S \times A \rightarrow \mathbb{R}$  is the reward function. 
\item
$\Omega = \Omega_1 \times \Omega_2 \times .. \times \Omega_N$ is the set of joint observations.
\item
$O:S \times A \rightarrow \prod(\Omega)$  is the observations function. 
\item
$\gamma$  is the discount factor.
\item
$h\in\mathbb{N}\cup\{\infty\}$ is the horizon.
\item
$b_0 \in \prod(S)$ is the initial belief state. In principle, different agents may have different initial belief states, but
we make the (common) assumption that the initial belief state is identical. 
\end{itemize}
As in the case of POMDPs, Dec-POMDPs can also be represented in a factored manner, although most work to date uses the flat-state representation.
An important element of a factored specification of Dec-POMDPs is a compact formalism for specifying joint-actions. If possible, we do not want
to specify all $O(|A|^N)$ possible joint actions separately. Indeed, we expect that most actions do not interact with each other and impact different
variables, except when explicitly specified. For this, we use the concept of collaborative actions below. For a more detailed discussion of the issue
of compact specification of joint-actions, see~\cite{}. 
A solution to a Dec-POMDP is a set of policies $\rho_i$, one for each agent. It maps action-observation sequences of this agent to $A_i$.
As in POMDPs, these policies can be specified using a policy-graph for each agent. The policy graph for agent $i$ associates nodes with elements of $A_i$
and edges with elements of $\Omega_i$. 

%\guy{POMDP solvers, SARSOP}
%We use offline POMDP solvers to solve different single-agent POMDPs we derive from the Dec-POMDPs.
%In general, an offline POMDP solver receives a POMDP problem together with a precision parameter $p$, and outputs a policy graph $G$ whose value $V(G)$ satisfies $|V(G)-V(G^*)|<p$, where $G^*$ is the optimal policy-graph for the problem \eliran{not really sure it's true in general}. Following that, it is also possible to simulate the policy on the problem, and produce simulation trajectories (traces in short).\\
%Note that online solvers, unlike offline solvers, can only produce traces when solving the problem as they plan their actions on-the-fly, instead of trying to build a policy-graph.
%SARSOP (Kurniawati, Hsu and Lee 2009) is state-of-the-art point based algorithm for solving POMDPs that we used as our offline POMDP solver. It's an anytime algorithm, meaning it can be halted in any point of the run and produce the current best result found.\\

\subsection{Private and Public Variables and Actions}
The concept of \emph{private} and \emph{public} (or \emph{local} and \emph{global}) variables was introduced by
Brafman and Domshlak~\cite{} in the context of their work on factored planning. This concept has been used extensively
in work on privacy-preserving multi-agent planning (e.g., ~\cite{}) and, more recently in work on solving qualitative variants
of Dec-POMDPs~\cite{}. Since we are building on ideas in this latter work, we now explain how we extend them to the
case of factored Dec-POMDPs.

%Similarly to Factored POMDP, we can also define a factored Dec-POMDP by decomposing the state, observation and reward components, where action and observation are replaced by joint-action and joint-observation.\\
%\eliran{Should we define it formally as well?}
%When dealing with multiple agents, we can specify the relation of each agent to each of the variables.
%We define these relations according to the agent's actions, by first defining the $\mathbf{subjects}$ of each action.

We associate with each action $a$ the set of variables it can affect or it is affected by, which we refer to as  {\em subjects}$(a)$. 
The effect could take place through the transitions, rewards, or observations associated with $a$.
Let $a\in A_i$ be an action of agent $i$. We identify $a_i$ with the joint action $(\mbox{no-op},\ldots, a_i,\ldots,\mbox{no-op})$.
%\ronen{This is slightly problematic. Imagine that $a_i$ can affect $x$, but only if it is done together with $a_j$, where $a_j$ on its own does not affect $x$}
We say that variable $X_i$ is a subject of $a\in A_j$ if there is some state $s$ for which there is a positive probability that the value 
$X_i$ changes following $a$, or if there are two states $s,s'$ that differ only in the value of $X_i$ such that $R(s,a,s'')\neq R(s',a,s")$.
Similarly, $\omega_i$ is a subject of $a\in A_j$ if there exists a state $s$ such that there is a positive probability
of observing $\omega_i$ when $a$ is executed and $s$ is reached. 
% if $\exists{s, s'\in S}.s|_{S_i} \neq s'|_{X_i}.\tau_i(s, a_i)[s'] > 0$\\
%An observation variable $\omega_i$ is a subject of an action $a\in A_j$ if $\exists{s'\in S}.\exists{o \in \omega_i}.o \neq NullObs.Z_i[a_{iso}, s', o] > 0$\\
%A reward variable $R_i$ is a subject of an action $a\in A_j$ if $\exists{s, s'\in S}.\rho_i[s, a_{iso}, s'] \neq 0$\\

As we work in multi-agent environment, some actions may affect some variables only when applied jointly with other actions.
We refer to these as $\emph{collaborative}$ actions. 
%Hence, we need to be able to define subjects of actions even when they alone can't affect the variable at question.
%We start by defining what collaborative actions are:
\ronen{I don't understand the definition and notation. Start with an explanation in words. }
Let $a$ be an action. We say that $a$ is a \emph{collaborative} action if we can find a set of actions apart from $a$ itself, each applied by a different agent, so that if applied simultaneously may cause an effect, while the application of any strict subset of these actions and $a$ yields no effect whatsoever.
Formally, let $a\in A_j$, then $a$ is a \emph{collaborative} action if one of the following holds:
\begin{itemize}
    \item There exists a set of actions, $(a_1,..., a_c)$ such that $\forall 1\leq t \leq c$, $a_t\in A_{i_t}$, $a_t \not\in A_j$, where $\forall 1\leq t\neq r \leq c. i_t \neq i_r$
    \item We define the following joint-action $ac_{iso}$: $ac_{iso}[j]=a$, $\forall 1\leq t \leq c. ac_{iso}[i_t]=a_t$ and else $ac_{iso}=NoOp$
    \item There exists a variable $V$ such that the following holds:
    \begin{itemize}
        \item If $V$ is a state variable $S_i$: $\exists{s, s'\in S}.s|_{S_i} \neq s'|_{S_i}.\tau_i[s, ac_{iso}, s'] > 0$ and $\forall 1 \leq t \leq c.\tau_i[s, ac_{iso}|_{a_t=NoOp}, s'] = 0$
        \item If $V$ is an observations variable $\omega_i$: $\exists{s'\in S}.\exists{o\in \omega_i}.o \neq NullObs.o_i[ac_{iso}, s', o] > 0$ and $\forall 1\leq t \leq c.o_i[ac_{iso}|_{a_t=NoOp},s',NullObs]= 1$
        \item If $V$ is a reward variable $R_i$: $\exists{s, s'\in S}.R_i[s, ac_{iso}, s'] \neq 0$ and $\forall 1\leq t \leq c.R_i[s, ac_{iso}|_{a_t=NoOp}, s'] = 0$
    \end{itemize}
\end{itemize}
Given these definitions, we can naturally expand the definition of an action's subjects in case it is a collaborative action, by considering all the variables that makes them a collaborative action.

We can now define the concept of {\em private} and {\em public} variables.
$X_i$ is {\em private} to agent $j$ if $X_i$ is only the subject of actions $a\in A_j$. 
Otherwise, $X_i$ is {\em public}. 
%Having defined the group of $Subjects$ of an action, we can intuitively define the ownership of a variable by an agent. A variable $V$ is owned by agent $i$, if $\exists a\in A_i$ such that $V\in Subjects(a)$.
% We then consider a variable as $\mathbf{private}$ with respect to an agent, if that agent is the only agent that possesses it. Otherwise, it is considered a public variable.
An action $a\in A_j$ is public if at least one of its subjects $X_i$ is public. Otherwise, it is private.
Notice that by definition, a collaborative action is always public, since all other actions that collaborate with it also make their agent an owner of the variable, hence making it a public variable.

\subsection{Projected Contexed Actions}
In the absence of explicit preconditions in MDP based models, it is possible to mimic preconditioning for actions via the state itself. For example, if we would like an action to be applied only in a certain state $s$, we can incur high penalty for applying the action in any other state.
To do that, we use the notion of \emph{contexed} action. A contexed action is the combination of an action and a state. When working with models where the state space is structured, it is also possible to project some of the state variable, and achieve wider contexts without having to explicitly specify all the states that compose the context.
We will consider two types of contexed actions:
\begin{itemize}
\item fully-contexed action - a combination of an action and an assignment to $\mathbf{all}$ state variables
\item projected-context action - a combination of an action and an assignment to a subset of the state variables, while the remaining state variables are considered "wild-carded", meaning that they can be considered as any of their values.
\end{itemize} 

\section {The Algorithm}
%\guy{High level description}
%\\The algorithm consists of three main parts:
%\begin{itemize}
%    \item Extracting traces of the team solution.
%    \item Constructing single-agent policies.
%    \item Process and align the policies.
%\end{itemize}
%\eliran{things to add to description}
%\begin{itemize}
%    \item reason we use traces instead of policy
%\end{itemize}
%
%\eliran{old description}
%Given the team problem, which is the centralized POMDP version of the Dec-POMDP problem at hand, we aim at generating a series of policy-graphs, one for each agent.\\
%We start off by generating a set of $n_{traces}$ simulation traces using a black-box POMDP solver according to our confidence parameter $\alpha$ and precision requirement $p_{team}$.\\
%Having the team problem solved, we process the resulted traces in order to extract the contexed public actions that were applied by each agent.\\
%Now, we project each instance of contexed public action according to the agent's view, and use a heuristic to determine the reward that will be given to each such projected-context public action.
%We then project the problem with respect to each agent, given its set of projected-contex public actions (with their respective rewards) and the team problem. Following that, we solve each single-agent problem, again by some black-box POMDP solver, where now we're intrested in obtaining a policy-graph for the agent, instead of run traces. The precision is set differently for each agent according to the rewards given for his contexed public actions, and denoted by $p_i$ for $i \in {1...M}$.\\
%We perform an iterative alignment of the resulted policy-graphs until the convergence criteria is reached, using the Align procedure.\\
%Finally, we apply the PostProcess procedure to the aligned policy-graphs, that makes the policy-graphs applicable for the Dec-POMDP problem, as well as performs several local optimizations to the policies.

Starting with the input Factored Dec-POMDP $P$, we first generate the team POMDP $P_{team}$. $P_{team}$ is simply the Dec-POMDP when we
ignore its underlying multi-agent structure. That is, the actions are the joint actions and the observations are the joint observation, viewed as applied 
and observed by a single agent. Equivalently, this can be viewed as a Dec-POMDP in which all observations are communicated accurately and instantaneously 
to all agents. We solve $P_{team}$ using an of-the-shelf POMPD solver. Some solvers will provide us with a policy-tree. Instead, we simply 
generate many traces from the policy, obtaining many action-observation traces. Using these traces and the factored Dec-POMDP we now generate
one single-agent problem for each agent. The dynamics of each single-agent problem is similar to that of the Dec-POMDP, except that some variables
are projected away. We design the reward function to be such that agents are rewarded for behaving in a manner that is similar to how they behave
in the team solution. 
\ronen{In fact, it seems we could use an RL algorithm here to generate a policy for each agent, as we can simulate as many traces as we wish.
In fact, we could use the RL algorithm to solve the team POMDP. This would generate the needed traces as well. We would learn incrementally
both the team solution and the single-agent solution. Using this idea, we could probably scale up to very large problems. In fact, we can use this
approach to do MA RL. If we can do the projections. What we need is a simulator that let's us control all the agents at once.}
Finally, we analyze the single-agent policies and align them so us to try to ensure that actions are properly synchronized  when they are
executed in a decentralized manner. The high-level pseudo-code is described below. We described the first step (generating $P_{team}$)
above. In the rest of this section we explain the rest of these step in more detail.
\ronen{How important is $\alpha$ for the description? Since we do not provide guarantees, I'm not sure it is worth mentioning}

%\eliran {pseudocode-part0}
\begin{algorithm}
\caption{GenerateAgentPolicies}
\begin{algorithmic}[tbph]
\State Input: $P$, $\alpha$, $p_{team}$
\State $P_{team} \gets \Call{Centralize}{P}$
\State {\em Traces} $\gets \Call{ExtractTraces}{P_{team}, \alpha, p_{team}}$
\State {\em RawSAPolicies}$ \gets \Call{ProjectAndSolve}{P_{team},{\mathit Traces}}$
\State {\em SAPolicies} $\gets \Call{ProcessAndAlign}{\mathit{RawSAPolicies}}$
\State {return {\em SAPolicies}}
\end{algorithmic}
\end{algorithm}

%\subsection{Creating the Team Problem}
%
%\guy{What is the input}\\
%We are given with a Factored Dec-POMDP problem $P$, a confidence parameter $\alpha\in \mathbb{R}$, and a precision parameter $p_{team} \geq 0$
%\eliran{talk about metadata stuff? we can essentially extract the relations using the definition, perhaps without even specifying an algorithm}
%along with metadata that specifies each agent's variables, which actions are public and which are private, and the objectives of each action.\\
%\guy{The team problem format}\\
%Initially we need to convert the Factored Dec-POMDP problem at hand to a Factored POMDP problem which represents the team problem.\\
%So given a Factored Dec-POMDP problem $P=(\{X_i\}_{i=1}^{K}, S, {\{A_i\}}_{i=1}^{M}, A, \{\tau_i\}_{i=1}^{K}, \{R_i\}_{i=1}^{L}, \{\rho_i\}_{i=1}^{L}, \{\omega_i\}_{i=1}^{Z}, \Omega, \{Z_i\}_{i=1}^{Z}, \gamma, h, I)$, we produce a new Factored POMDP problem, $P_{team}=(\{X_i\}_{i=1}^{K}, S, A, \{\tau_i\}_{i=1}^{K}, \{R_i\}_{i=1}^{L}, \{\rho_i\}_{i=1}^{L}, \{\omega_i\}_{i=1}^{Z}, \Omega, \{Z_i\}_{i=1}^{Z}, \gamma, h, I)$. The only difference is that now there's no separation to different agent, and instead there's a "master" agent that perform all joint-actions and receive all joint-observations at once.\\
%\guy{Solving the team problem}\\
%When solving the team problem, we use an offline POMDP solver as a black-box that receives the team problem $P_{team}$ and a precision parameter $p_{team}$. The solver outputs a policy-graph $G_{team}$, which given that $G^*$ is the optimal policy-graph, satisfies $|V(G_{team})-V(G^*)| \leq p_{team}$. We then take the problem $P_{team}$ and produce $n_{t}$ simulation traces of $G_{team}$ over $P_{team}$.\\
%

\guy{Output - trajectories}
\\Simulation traces are recordings of a world's progress in a specific run of the problem. Each trace starts with an initial state, which is sampled from the initial belief state, and then alternates between being in a state and taking an action, to receiving an observation and a reward, according to the simulated policy-graph. Formally, a trace $T$ of length $l$ is a sequence of quintuplets $c_i = (s_i, a_i, s'_i, o_i, r_i)$ where:
\begin{itemize}
    \item $s_i$ is the world's real state in step $i$ before taking an action. $s_0$ is the initial state.
    \item $a_i$ is the action taken in step $i$
    \item $s'_i$ is the world's real state  in step $i$ after taking an action.
    \item $o_i$ is the observation received after taking the action $a_i$ and reaching state $s'_i$
    \item $r_i$ is the reward received for taking the action $a_i$ and transitioning from $s_i$ to $s'_i$
\end{itemize}
Notice that $\forall 0\leq i \leq l-1$, $s'_i=s_{i+1}$.\\
Having traces defined, we need to determine the number of traces to produce, denoted by $n_t$.
To calculate $n_t$, we use the confidence parameter to choose such $n_t$ which ensures that with probability greater than $\alpha$ our traces cover all possible initial states.
As generating traces is rather cheap, we use a rough upper bound on the probability using the union bound. Let $E$ denote the event in which we cover all initial states $s \in I$ in $n_t$ traces, then we want to pick $n_t$ such that $Pr(E)\geq \alpha$, using the union bound we get:
$Pr(E)\geq 1-\sum_{s\in I}(1-Pr(I=s))^{n_t}$.
Hence if we pick $n_t$ large enough such that $\sum_{s\in I}(1-Pr(I=s))^{n_t} \leq 1-\alpha$, we achieve the required property.
\eliran{pseudocode-part1}
\begin{algorithm}
\caption{ExtractTraces}
\begin{algorithmic}[tbph]
\State Input: $P_{team}$, $\alpha$, $p_{team}$
\State {$TeamPolicy \gets \Call{POMDPSolver}{P_{team}, p_{team}}$}
\State {$n_t \gets \Call{NumTracesRequired}{P, \alpha}$}
\State {$Traces \gets \Call{Simulate}{TeamPolicy, n_t}$}
\State {return $Traces$}
\end{algorithmic}
\end{algorithm}

\subsection{Single Agent Projection}
Once we obtained the traces, we want to construct and solve the single-agent problems. As mentioned earlier, the single-agent problems are POMDPs in which the agent is rewarded for accomplishing the objectives that are relevant to the team, as well as his own private objectives.

To do the projection for agent $i$, we are given with the team problem $P_{team}$ and the contexed actions to be rewarded accompanied by their corresponding rewards, $Actions_i$, $Rewards_i$.
We apply a series of operations to alter $P_{team}$, into the desired single-agent POMDP $P_i$:
\begin{itemize}
\item{Remove all the rewards given upon the change of a public state variable, namely, public objectives. By that we change the agent's target, from achieving the public goals to applying his public actions. We leave private objectives without change, as we still want to agent to achieve them}
\item{Remove any public action that doesn't appear along the simulation traces, whether its a co-agent's action or an action of the agent itself. As public actions alter information that is relevant for more than one agent, the agent can't perform public actions or simulate co-agents' public actions that weren't already planned when solving the team solution}
\item{Add default penalties for the application of all remaining public actions in any context. By default, we want to penalize the agent for applying a public action out of context, to prevent small reward cycles which is a well known issue in reward shaping based methods. The default penalty is chosen to be $-max(Rewards_i)\cdot|Actions_i|$, as a rough upper bound on the sum of rewards that can be achieved from the rewarded contexed-action. This ensures that no forbidden action is worth applying in exchange for applying all the rewarded contexed-actions}
\item{Add the rewards for the agent's contexed-actions, as appears in $Actions_i, Rewards_i$. By this we override the penalty that was given, but only under the specificed context for each action.}
\item{Neutralize the penalty for co-agents's contexed actions, as appears in all other $Actions_j, \forall j \neq i$. We allow the agent to simulate his co-agents' public actions, in order to plan his sensing accordingly and to alter the state of the world as needed.}
\item{Remove co-agents sensing actions. The agent is not supposed to have any access to his co-agents observations, and he must plan as the only sensing entity in his view}
\item{Convert all co-agents private actions to deterministic actions. Private actions of co-agents alter only their private variables, hence by definition cannot be sensed by the agent itself. Therefore, those private actions can become deterministic as the agent doesn't need to consider them as neither rewarding nor penalizing, but only as state changers.}
\end{itemize}

\guy{Trajectories - rewards}

\guy{Trajectories - action order}

\guy{Definition of single agent problem}

\guy{Solving the single agent problem}

\guy{Output - policy}

\eliran{pseudocode-part2}
\begin{algorithm}
\caption{ProjectAndSolve}
\begin{algorithmic}[tbph]
\State {}
\end{algorithmic}
\end{algorithm}

\subsection{Plan Alignment}

\guy{Removing private actions of other agents}

\guy{Alignment}

\guy{Removing public actions of other agents}

\guy{Handling collaborative actions}

\eliran{pseudocode-part3}
\begin{algorithm}
\caption{ProcessAndAlign}
\begin{algorithmic}[tbph]
\State {}
\end{algorithmic}
\end{algorithm}



\subsection{Description}
\eliran{All the content left here should be split between the previous subsections and rewritten to match the background, especially private and public notations}
We now present the algorithm, which consists of four main procedures:
\begin{itemize}
\item{GeneratePolicyGraphs - The main procedure}
\item{Project}
\item{Align}
\item{PostProcess}
\end{itemize}
\begin{algorithm}
\caption{GeneratePolicyGraphs}
\begin{algorithmic}[tbph]
\State Input: Dec-POMDP Problem $P$, Confidence $\alpha$
\State Set $P_{team} \gets Centralize(P)$
\State {$TeamPolicy \gets \Call{POMDPSolver}{P_{team}, p_{team}}$}
\State {$n_{traces} \gets \Call{NumRequiredTraces}{P_{team}, \alpha}$}
\State {$Traces \gets \Call{Simulate}{TeamPolicy, n}$}
\For {all agents $\phi_i$}
\State {$ Actions_i \gets \Call{GetProjectedActions}{Traces, \phi_i}$}
\State {$ Rewards_i \gets \Call{GetRewards}{Actions_i, P_{team}, Traces}$}
\State {$ p_i \gets \Call{GetAgentPrecision}{Rewards_i, Traces}$}
\State {$ P_i \gets \Call{Project}{P_{team}, Actions_i, Rewards_i, \phi_i}$}
\State {$ G_i \gets \Call{POMDPSolver}{P_i, p_i}$}
\EndFor
\While {true}:
\State {$AG_1,..., AG_M \gets \Call{Alignment}{G_1,..., G_M }$}
\If {$AG_1,..., AG_M == G_1,..., G_M$}
\State {break}
\Else
\State {$G_1,..., G_M \gets AG_1,..., AG_M$}
\EndIf
\EndWhile
\State {$FinalPolicies \gets \Call{PostProcess}{AG_1,..., AG_M}$}
\State {return $FinalPolicies$}
\end{algorithmic}
\end{algorithm}

By deriving the single-agent problem using Project, we obtain a POMDP in which the agent is rewarded for applying the public actions in the currect context and filling his share of the team effort, rather than being rewarded for achieveing the goal of the original team problem, which in any interesting Dec-POMDP would be infeasible for a single-agent to achieve.\\

Once we have the single-agent policies, $\{G_i\}_{i=1}^M$, we need to perform alignment. The requirement for alignment is rooted in the fact that each agent policy contains a very superficial plan for the other agents, if at all, therefore it needs to align it's actions with respect to the policies of other agents. Since we're dealing with policy graphs rather than trees, and cycles do exist, it's impossible to perform an alignment in which all action are perfectly coordinated (at least in terms of the graph only), but we need the alignment to postpone actions to the minimal necessary time so that the agents remain synchronized with each other.
For each policy graph, we seek for a set of NoOps requirements that defines the number of required NoOps that should precede each instance of a public action in the graph. \\
We traverse the graph in a breadth-first manner, and upon encountering a node that contains a public action, we extract the $identifier$ of that node in the graph. The identifier is a description of that node, that can be searched for in any other policy graph in the set $\{G_i\}_{i=1}^M$.\\
Given the identifier, we extract a noop requirement from each of the other policy graphs in the set, by trying to match the identifier to a node in the graph.\\
Then, we take the maximum NoOp requirement found, and deduct a compensation term that consists of the actual depth of the node in the graph and the number of NoOps that were already added to any of its predecesors, in order to determine the exact addition of NoOps it needs to meet the NoOp requirement.\newline
We perform the alignment iteratively until a convergence criteria is met, as the postponing of certain actions may require further postponing other actions.

\begin{algorithm}
\caption{Align}
\begin{algorithmic}[tbph]
\State Input: PolicyGraphs $G_1,..., G_M$
\For{$G_i,  i\in\{1,..m\}$}
	\State {$NoopsReqs \gets EmptyMapping()$}
      \State {$CurrBFS \gets \Call{BFS}{G_i}$}
      \While {$CurrBFS.queue$ is not empty}
	\State {$CurrNode \gets CurrBFS.queue.pop()$}
	\State {$Action \gets CurrNode.action$}
	\If {$Action.IsPublic()$}
	\State {$ID \gets \Call{GetIdentifier}{CurrNode}$}
	\State {$MaxNoop \gets 0$}
	\For {$G_j,  j\in\{1,..M\}\setminus\{i\}$}
	\State {$CurrNoop \gets \Call{NoopReq}{G_j, ID}$}
	\State {$MaxNoop \gets max(MaxNoop, CurrNoop)$}
	\EndFor
	\State {$NoopsReqs[CurrentNode] \gets MaxNoop - CompensationTerm$}
	\EndIf
	\EndWhile
	\State {$AG_i \gets \Call{AddNoops}{G_i, NoopsReqs}$}
\EndFor
\State {return $AG_i, i\in\{1,...M\}$}
\end{algorithmic}
\end{algorithm}

As agents make some planning on behalf of their co-agents, each policy-graph $G_i$ contains actions of agent $i$'s co-agents, which are not valid as part of a policy that can be run solely by agent $i$, in the Dec-POMDP problem. Hence, we apply the PostProcess procedure that aims at removing these actions. We peform this by traversing the policy-graph, and if we encounter a node that represents a "foreign" action, namely an actions that belongs to some other agent, we first convert it into a NoOp. Next, if such node participates in a cycle, we want to remove it from the cycle and connect it's two adjacent nodes with respect to that cycle. 
Finally, in order to prevent deadlocks when collaborative actions appear in cycles, we inject repetitions of collaborative actions that participate in cycles, according to the number of agents that collaborate that specific action.


\section{Empirical Study}

\section{Related Work}
QDec-POMDPs~\cite{} are qualitative version of Dec-POMDPS that examines a conceptually simpler, but more structured model. In QDec-POMDPs 
non-determinism replaces stochastic uncertainty, the model is factored (i.e., described at the level of state variables rather than states), and actions 
are described using preconditions and non-determinstic effects. Although QDec-POMDPs are also NEXT-Time hard, recent work in the area that
leverages heuristic-search planners, has been able to scale up to much larger domains (e.g., box pushing on a grid of size 24,  12 agents and 12 boxes, implying a state space of $24^24$)
albeit, under the assumptions that actions are deterministic.

Dec-POMDP algorithms.

\section{Conclusion}

\bibliography{Bibliography-File}
\bibliographystyle{aaai}
\end{document}
