\commentout{
To identify an appropriate order we use the set of traces $T$. We define a partial order $\prec_T$ over the contexted actions such that $ca_i \prec_T ca_j$ if in all traces $ca_j$ never appears before $ca_i$. \guy{I do not understand (from the text later) what happens if there are cycles - just assume no order?}\eliran{we just consider the first ordering that appeared. If we find two PCAs that created a cycle, we can't say which should come first} We now iterate over all the contexted actions in reverse topological order of $\prec_T$.

To execute a context action the agent may need to execute first a sequence of other actions, designed to achieve the appropriate influencers values. We design the rewards so that no sequence of non-rewarding actions would cost more than the contexted action reward.


\begin{algorithm}
\caption{ProjectAndSolve}
\begin{algorithmic}[tbph]
\State Input: $P_{team}$, $Traces$
\For {all agent $i$}
\State $\pcact{i},\mathit{Rewards_i} \gets \Call{ExtractPCAs}{P_{team}, Traces, i}$
\State {$\mathit{SAProblem_i}$ $\gets \Call{Project}{P_{team}, \pcact{i}, Rewards_i}$}
\State {$\epsilon_i \gets \Call{GetSAPrecision}{\mathit{MaxTraceLength}, \gamma}$}
\State {$\mathit{RawSAPolicy_i} \gets \Call{POMDPSolver}{\mathit{SAProblem_i}, \epsilon_i}$}
\EndFor
\State {return $\mathit{RawSAPolicy_1}, \mathit{RawSAPolicy_2},..., \mathit{RawSAPolicy_n}$}
\end{algorithmic}
\end{algorithm}



\subsection{Single Agent Projection - OLD}
Our next step is to define a POMDP for each agent. This POMDP will contain some of agent $i$'s action and some actions of other agents,
as well as appropriate rewards for applying its own actions in a
projected context. 
%Given the PCAs and their rewards, we construct single-agent problems that reward each agent for following its role in the team plan, as well as its own private objectives. The single agent POMDP of agent $i$ contains some actions of $i$, and some actions of other agents. 
The actions of other agents are used to "simulate" some of the behaviors of the other agents -- behaviors that eventually enable the agent to carry out its own actions.

The single-agent POMDP $P_i$ for agent $i$ is obtained by modifying $P_{team}$. In terms of
as follows:
\ronen{it is not clear what are the variables/states in this problem}\eliran{not 100\% sure what you mean. the state variables and states remain the same. Added to point 2 the fact that we also remove other agents observation variables.}
\ronen{state it positively: the state variables are...} 
\eliran{done}
\begin{enumerate}
\item Remove from the problem any \emph{public} action that does not appear in the traces, regardless of the agent applying it. As public actions alter facts in the world that are relevant to more than one agent, we  forbid the application of a public action that was not part of the team solution.
\item Remove sensing actions and variables of other agents because the agent does
not have access to their results. The state variables remain the same as in the team problem, and are left untouched.
\item The \emph{private} actions of other agents are transformed into deterministic actions that succeed with probability 1.
As private actions change only private variables, and an agent cannot sense other agents' private variables, this change allows the agent to progress the world towards states in which it must act more easily.
If we cannot assume that actions have a clear success outcome, we can omit this step. This will result in larger, more complicated single-agent policies.
%\ronen{omit what - making them probability 1?}\eliran{yes}

\item Add penalties for the application of all remaining \emph{public} actions in contexts that did not appear in the team plan - whether of the agent or of the other agents. We want to discourage an agent from applying public actions out of their context
in the team plan - as in terms of public action, it must conform to the team plan in order to stay coordinated with its co-agents. Moreover, positive reward cycles may occur~\cite{RLCYCLES}, which must be dealt with so to prevent the agent from trying to endlessly trying to achieve one of the sub-goals. The penalty is chosen to be $-1\cdot\max_{r\in Rewards_i}\{r\} \cdot|\pcact{i}|$, as an upper bound on the sum of rewards that can be achieved from applying PCAs. This ensures that no undesirable contexted action is worth applying, even in exchange for applying all the PCAs. There is no penalty for other agents' PCAs
(i.e., contexted-actions in $\bigcup_{j\neq i} \pcact{j}$). We want
to allow the agent to simulate other agents' PCAs in order to plan the execution of its own actions at appropriate times.
%\ronen{this is not clear. do we allow other agents action only in their context, or anywhere?}\eliran{only in their context, hope it is clear now}
\item Add the rewards for $\pcact{i}$ (explained later) according to $Rewards_i$
This reward will override the above penalty, but only when the action is applied in its projected context.
\item 
Remove rewards related to public variables the agent
can achieve. The goal of the single-agent POMDP is to imitate the team policy, not compute an alternative solution.
% Therefore, given a public variable $X_i$, two states $s, s'$ that differ only on $X_i$, and an action $a\in A_i$ such that $R(s, a, s'')\neq R(s', a, s'')$, or $R(s'', a, s) \neq R(s'', a, s)$, we set any of the involved \emph{positive} rewards to 0.
Therefore, given a public state variable $X_i$, an action $a \in A_i$ , a source state $s$, an outcome state $s'$ that differ from $s$ on $X_i$ and is reachable from $s$ using $a$ such that $R(s, a, s')\neq R(s, a, s)$, we set $R(s,a,s') = R(s,a,s') - \textit{max}\left(0, R(s,a,s') - R(s,a,s)\right)$. That way we cancel any positive reward that is achieved via the alternation of $X_i$.
% we set any of the involved \emph{positive} rewards to 0. Since we are dealing with varying reward functions, we need to make sure we don't reduce reward that is not specific to the alternation. To do so, we define
% $cost(a)=min_{s, s'\in S}R(s,a,s')$ and $cost(s,s')=min_{a\inA_i}R(s,a,s')$. These represent the constant 
\ronen{this is a bit problematic. What if we have negative rewards, too?
For example, undesirable states. What if we cannot reach the same state s'' from s and s'?}
\eliran{I still think there could be a problem here, for example if the model's reward function is always negative, and "rewards" are just inducing lower penalty, but perhaps in this case the reward function could be altered by adding a constant to all entries. Should we attend to that now or leave it with the current reformulation?}
\ronen{then say that you are assuming it. It is true that you can always add a constant value to the reward.}\eliran{I think I've found a better way to define it, without assuming anything on the reward function. Is it ok?}
\end{enumerate}

After applying these changes, we obtain the single-agent problem $P_i$. %Before solving it, we need to specify a precision hyperparameter that determines how close to optimal the policy should be. In oppose to the team problem case, we can pick this parameter wisely to ensure that if the output policy reaches the precision, it must satisfy our goals.
Ideally, the solution to $P_i$ should be sufficiently optimal so that %$\epsilon_i$-optimal to guarantee that if the policy is $\epsilon_i$-optimal, then 
no PCA will be ignored by the policy. Yet, the policy might fail to satisfy it for certain scenarios, for example where actions fail far beyond their failure expectancy. The optimality criteria for the problem $P_i$ is denoted by $\epsilon_i$. In the next section we specify how this parameter is chosen.
%\ronen{what do you mean by "on average"}\eliran{refactored it}

\begin{example}
%\ronen{this is strange that you are using agent 1 now, after you computed the PCA for agent 2}\eliran{changed that to agent2}
We now construct Agent2's single-agent problem. We denote the PCAs and their respective rewards with $\{pca_1:r_1, pca_2: r_2\}$. We follow the projection stages one by one:
\begin{enumerate}
    \item As the push action is the only public action in the problem, we remove all push actions except for the ones that are observed in the traces: Agent-1-CPush-Right-Box1, Agent-2-Push-Left-Box2 and Agent-2-CPush-Right-Box1. Notice that we keep Agent1's CPush-Left action, as we might need to simulate it.
    \item We remove the sensing action of Agent1, as well as its observation variable.
    \item We don't have any non-deterministic private actions, so no actions are turned to deterministic.
    \item We add a penalty of $-2\cdot max\{r_1, r_2\}$ to the remaining public actions, applied in any context except for the PCA's contexts.
    \item We add the rewards ${r_1,  r_2}$ to their corresponding PCAs.
    \item The rewards for pushing the boxes to the target tiles are set to 0 - we reward the agent only for doing its public action in context.
\end{enumerate}
\end{example}

Finally, we solve each single-agent problem $P_i$ using our off-the-shelf POMDP solver, SARSOP, resulting in a $\epsilon_i$-optimal policy for each agent. The generated policies will likely
contain non-sensing actions of other agents, which require
some handling, explained later on.

\begin{algorithm}
\caption{ProjectAndSolve}
\begin{algorithmic}[tbph]
\State Input: $P_{team}$, $Traces$
\For {all agent $i$}
\State $\pcact{i},\mathit{Rewards_i} \gets \Call{ExtractPCAs}{P_{team}, Traces, i}$
\State {$\mathit{SAProblem_i}$ $\gets \Call{Project}{P_{team}, \pcact{i}, Rewards_i}$}
\State {$\epsilon_i \gets \Call{GetSAPrecision}{\mathit{MaxTraceLength}, \gamma}$}
\State {$\mathit{RawSAPolicy_i} \gets \Call{POMDPSolver}{\mathit{SAProblem_i}, \epsilon_i}$}
\EndFor
\State {return $\mathit{RawSAPolicy_1}, \mathit{RawSAPolicy_2},..., \mathit{RawSAPolicy_n}$}
\end{algorithmic}
\end{algorithm}





\subsection{Rewards and Precision in the Projection}

After we compute the PCAs for the agent, we need to specify their respective rewards. The rewards should encourage the agent to perform the team plan actions in their corresponding contexts, in the same order they appeared in the team solution. 
To do so, we exploit the discount factor, which makes it beneficial for the planner to apply higher rewarding actions earlier. Hence, we need to order the PCAs, and assign higher rewards to actions that appear earlier in the order.

The main thing to notice is that the planner might need to insert costly actions that precede a PCA to achieve some needed influencer for it. This may lead to a scenario where it is more beneficial to apply the PCAs in a different order or even not to apply a PCA at all, as the costs might be higher than the reward of the PCAs.

The first problem, applying PCAs in a different orders, is automatically taken care of because effects are part of the
projected context. 
%Public actions are responsible for altering the agent's objectives, 
By penalizing public actions that are applied out of context and rewarding those that are applied in context, we effectively force PCAs to be applied only in an appropriate order - assuming that one of the ways of getting from one context to the next is preferable over others. When this assumption doesn't hold and there are "ties" between different ways of progression, we break these ties via small changes in the rewards we assign to the PCAs, as we explain later on.
%\ronen{not 100\% sure of this}\eliran{tried to make it clearer}

The second problem of making the PCAs worth applying, is handled via the rewards we assign to each PCA. 
%We want to make sure that in the projected problem it is beneficial for the agent to apply a sequence of PCAs that will achieve its desired goals. Since we no longer pursue the team goals, the reward function does not necessarily make these application beneficial, and therefore we insert artificial rewards.
%
%To do so, we tackle the problem more locally, and 
We design the rewards so that no sequence of non-rewarding actions would "cancel" the need for applying the PCA it precedes, that is - would cost more than the PCA's reward.

To compute the rewards, we need to first determine an order over the PCAs we extracted for the agent.
We go over the traces and construct a \emph{directed acyclic graph} of PCAs, ensuring it is acyclic by not adding edges that form cycles.
We start with an empty graph, and go over all traces. Let $l$ be a trace.
%\ronen{define event or change the term}\eliran{we defined event earlier on, in the definition of trace, but I now changed it to "step" instead, feels more intuitive. Should we use the same exact notation as in the definition of "step"? it seems unnecessarily formal here}
For each step $e_j$ in the trace, we compute the PCA of its contexed action with respect to agent $i$. We denote this PCA with $pca_j$.
Then, we add a vertex that represents $pca_j$, assuming it doesn't already exist. We add an edge between the vertex corresponding to $pca_{j-1}$ and $pca_j$.
This implies that the current action cannot appear before the one preceding it.
Once we went over all traces and constructed the graph, we compute a topological order
for it, giving us a sequence of PCAs $(\pi_1, a_1),..., (\pi_q, a_q)$, where the first instance corresponds to the action that appeared earliest.

\eliran{Not Implemented - Replace sequence with sets representing the levels of the DAG}
We begin by associating a reward for the last PCA in the sequence, namely the \emph{base} reward, and then iteratively calculate the reward for each preceding PCA.
The base reward is chosen so that applying the PCA will be beneficial in any possible scenario. Recall that we need to avoid a scenario where many costly actions are required to reach a state satisfying the context of the action, so that their total cost surpasses the reward, making it non-beneficial to apply.


%Next, we describe the formulas that are used to compute to rewards. In these calculations, we assume that each action has a success probability. Following that, we describe how we can we calculate the rewards without assuming these success probabilities exist.

Recalling that we assume each action has one successful outcome,
let $MaxCost$ be the maximal negative reward received by an action, that appeared in the traces.
%\ronen{Why do we care about negative reward after the action?}
%\eliran{sorry, just a bad formulation - I meant the negative reward caused by an action}
Let $MinSP$ be the minimal success probability of all actions in the problem, and $MinPCASP$ the minimal success probability of the actions appearing in the PCAs. Let $MTL$ be the Maximal Trace Length we produced, and $MPG$ be the Maximal Public Gap -- the maximal number of \emph{private} actions that precede a public action in the traces. Let $sp_i$ denote the success probability of action $a_i$. We set $\epsilon > 0$ to some arbitrary positive real. We compute $r_q$, the base reward:
\begin{equation}
\label{eqn:rq}
   r_q = \frac{\sum_{i=MTL - MPG}^{MTL}(\frac{MaxCost}{MinSP} \cdot \gamma^{i-1})+ \epsilon}{\gamma^{MTL-1}\cdot MinPCASP} 
\end{equation}%
The numerator is an upper bound on the expected discounted cost we would pay before applying the last PCA. The denominator amplifies that cost to be beneficial when scheduled as the last action in the policy.

%Equation~\ref{eqn:rq} ensures that the planner will always find the application of $a_q$ in the context $\pi_q$ beneficial, and thus will insert it to the policy.

After calculating the base reward for action $a_q$, 
%which is set as the reward of $(\pi_q, a_q)$, 
we can calculate the rewards associated with the rest of the PCAs in the sequence. Since the application order of the PCAs is enforced using the context, we simply need to perform tie breaking for cases in which several PCAs may be applied, so that the agents would produce coordinated solutions.
To do so, we set $r_i=(1+\epsilon)\cdot r_{i+1}$, where again $\epsilon$ can be an arbitrarily small positive real, and $r_i$ is
the reward associated with $a_i$ in context $\pi_i$.


% To ensure that the order is maintained, using the same intuition as with the base reward, we compute the reward for $(\pi_i, a_i)$ so that the planner will always prefer to apply $a_i$ in $\pi_i$ before applying $a_{i+1}$ in $\pi_{i+1}$. 
% An interesting cases arises when the failure probability of $a_i$ is small. Then, after applying the PCA once, it may be better for the planner to assume that the PCA succeeded and apply $a_{i+1}$, without any form of verification, causing the order to break.
% \guy{I do not understand why we enforce "verification" - if it is beneficial to observe, the planner should do so, and if not, then not. Such "vitrification" should appear in the team plan to begin with, assuming that they are needed.}
% \eliran{Even if the verification appears in the team plan, the artificial rewards we add to the public actions would cause the planner to postpone this verification to a later stage and violate the order, as it would be more beneficial for him to apply the public actions as soon as possible. I hope that later on we would find other ways to design the reward function so that the team plan behavior could be more naturally preserved.}\guy{I disagree. Rewards are given for executing an action at a state. If the planner is unsure of the state, it will observe, if need be.}\eliran{Open Issue - I understand your meaning. The problem is that in the code, we still didn't implement the projection so that it would be only with respect to the variables relevant to the agent, but with respect to the action's context. This point is very crucial, though I think that both options (projecting onto context and onto relevant variables) would yield good results for now. Let's discuss this as well}
% We set $r_i=\gamma \cdot \frac{r_{i+1}}{1-sp_i} \cdot sp_{i+1} + \epsilon$ \eliran{explain further?}
% and by that we ensure that regardless of whether $a_i$ fails or not, it would still be beneficial to verify that and apply $a_i$ again if necessary, before applying $a_{i+1}$. We compute $r_i$ until we reach $r_1$, and by that associating a reward to each of the agent's PCAs.

When defining $MinSP$ in the base reward calculation, we assumed that each action has a clear success probability. If we do not want to assume that actions have a unique successful outcome, we can instead incorporate into each PCA, in addition to its projected context and action, a contexted minimal probability. That is, for each PCA we consider all its possible effects under contexts that match the projected-context of the PCA, and take the minimal probability among all of these effects' probabilities.
%\ronen{the above is not clear}\eliran{is it ok?}

To compute the $\epsilon_i$, the optimality parameter from the
POMDP's solution, we use a concept similar to the base reward calculation, but now we rely on the observed MaxTraceLength to capture the total number of costly action without amplifying their cost with their success probability.
\begin{align*}
\label{eqn:rq}
   \epsilon_i = \gamma^{MTL-1}\cdot r_q \cdot MinPCASP \\
   -\sum_{i=MTL - MPG }^{MTL}(MaxCost \cdot \gamma^{i-1})
\end{align*}
This is purely a heuristic choice.
\eliran{It is heuristic, but yields the best results on all problem configurations. Should we leave it as is for the workshop paper?}
\ronen{For now, yes}.

\begin{example}
We go back to our example to demonstrate how these rewards are calculated.
First, recall that our only non-deterministic action is the push action, which has a success probability of 0.8, hence $MinSP=MinPCASP=0.8$. In addition, the action costs are -1, -10, -3 and -2 for sense, move, push and cpush respectively. Hence $MaxCost$ is set to 10. We also set $\gamma=0.99$, $\epsilon=0.01$.
Taking our two traces, we have $MTL=7$ and $MPG=2$


As mentioned previously, we have two PCAs when considering Agent1's problem.
We construct the DAG from the traces and extract the topological order, which yields the following order between the two PCAs.
\begin{enumerate}
    \item $pca_1$ = \emph{(*, L, L, R), Agent-2-Push-Left-Box2}
    \item $pca_2$ = \emph{(L, L, L, L), Agent-2-CPush-Right-Box1}
\end{enumerate}


We calculate the base reward, $r_2$:
\begin{equation}
     r_2 = \frac{\sum_{i=7-2}^{7}(\frac{10}{0.8} \cdot \gamma^{i-1})+ \epsilon}{\gamma^{7-1}\cdot 0.8} = 47.36
\end{equation}
To calculate $r_1$ we just amplify by $1+\epsilon$ and get $r_1=47.83$

Considering the PCAs and their rewards, \emph{Agent2} will be rewarded with $r1$ for pushing \emph{Box2} to the left regardless of \emph{Agent1}'s position, but to push $Box1$ it will need to reason about agent 1's position - although in a very shallow manner as it is a private variable of agent 1.
Notice that the contexts of both PCAs, enforces $pca_2$ to be applied only once \emph{Box2} is pushed left - meaning after $pca_1$.


Next we calculate the precision of \emph{Agent2}'s single-agent problem: \begin{equation}
     \epsilon_2 = \gamma^{7-1}\cdot r_2 \cdot 0.8 - \sum_{i=7-2}^{7}(10 \cdot \gamma^{i-1}) = 7.14
\end{equation}
\end{example}
}