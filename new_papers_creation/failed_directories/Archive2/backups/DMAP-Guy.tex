\def\year{2020}\relax
%File: main.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai20}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{graphicx}  % DO NOT CHANGE THIS
\usepackage{xcolor}
\usepackage{amssymb}

\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS


\newcommand{\commentout}[1]{}
\newcommand{\eliran}[1]{\textbf{[\color{red}ELIRAN:#1]}}
\newcommand{\ronen}[1]{\textbf{[\color{blue}RONEN:#1]}}
\newcommand{\guy}[1]{\textbf{[\color{orange}GUY:#1]}}

\newcommand{\cbp}[0]{Collaborative Box-Pushing}
\newcommand{\mitg}[0]{Meet In The Grid}
\newcommand{\crs}[0]{Cooperative Rock-Sampling}
\newcommand{\macor}[0]{Multi-Agent Corridor}

\newcommand{\Tau}{\mathrm{T}}

%\nocopyright
%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
 \pdfinfo{
/Title (A Factored Approach To Solving Dec-POMDPs)
/Author (Eliran Abdoo, Ronen I. Brafman, Guy Shani)
} %Leave this	
% /Title ()
% Put your actual complete title (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case
% Leave the space between \Title and the beginning parenthesis alone
% /Author ()
% Put your actual complete list of authors (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case. 
% Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands, 
% remove them. 

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{caption} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \natbib} -- This package is specifically forbidden -- use the following workaround:
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai20.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\setlength\titlebox{2.5in} % If your paper contains an overfull \vbox too high warning at the beginning of the document, use this
% command to correct it. You may not alter the value below 2.5 in
\title{A Factored Approach To Solving Dec-POMDPs }
%Your title must be in mixed case, not sentence case. 
% That means all verbs (including short verbs like be, is, using,and go), 
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\author{Eliran Abdoo, Ronen I Brafman, Guy Shani}
 \begin{document}

\maketitle

\begin{abstract}
Dec-POMDPs model planning problems under uncertainty and partial observability for a distributed team of cooperating agents planning together but executing their plans in a distributed manner. This problem is
very challenging computationally (NEXP-Time Complete) and consequently, exact methods have difficulty scaling up. In this paper
we present a heuristic approach for solving certain instances of Factored Dec-POMDP that tries to reduce the problem of planning in Dec-POMDPs to multiple
problems of planning in a POMDP. First, we solve a team version of the Dec-POMDP in which agents have a shared belief state, and then, each agent attempts to solve the problem of executing its part of the team plan. Finally, the different solutions are aligned to improve synchronization. Using this approach we are able to solve larger Dec-POMDP problems, limited only by the abilities of the underlying POMDP solver.
\end{abstract}




\section{Introduction}

Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are the most popular model for planning in stochastic environments under uncertainty with partial observability by a distributed team of agents~\cite{}. In this model, a team of agents attempts to maximize the team's cumulative reward where each agent has only partial information about the state of the system during execution. That is, each agent is aware of it's own observations only. Communication is possible only through explicit communication actions, if these are available.  

To achieve their common goal agents must coordinate their actions in two ways: First, as in single agent problems, actions must be coordinated sequentially. That is, current actions must help steer the system towards states in which greater reward will be possible,
e.g., to be rewarded for shipping a product, it must first be assembled. 
Second, agents may need to coordinate their simultaneous actions because their effects are dependent, e.g., a heavy box can only be pushed if two agents push it simultaneously. 

As with previous work in this area, our focus is on centralized off-line planning for distributed execution. That is, offline, a solver with access to the complete model must generate a policy for each agent. An agent's policy specifies which action must be taken as a function of the agent's history of
actions and observations. Such policies can be represented by a {\em policy graph} where nodes are labeled by actions, and edges are labeled by observations. Online, each agent executes it's own policy independently of the other agents.
The difficulty lies in generating policies that provide sufficient coordination, even though each agent may make different observations at run-time, and so the agent's beliefs over which states are possible are typically different. 

Dec-POMDPs are notoriously hard to solve -- they are NEXP-Time hard~\cite{} implying that only the smallest toy problems are optimally solvable.
However, many approximate methods for solving Dec-POMDPs have been proposed, with steady progress. Some of these methods generate solutions with bounds on
their optimality~\cite{}, and some are heuristic in nature~\cite{}. However, current methods typically do not scale to state spaces with more than a few hundreds of states.


In this paper we describe a heuristic approach for solving Dec-POMDPs that scales up to much larger state spaces. The key idea is to solve a Dec-POMDP by
solving multiple POMDPs. First, we solve a POMDP obtained by assuming that all agents have the same belief state. That is, that any observation by one agent is immediately available to the other agents. We refer to this as the {\em team POMDP}. The solution of the team
POMDP can be represented by a policy graph --- the {\em team policy graph.} . It provides us with a skeleton for the solution of the Dec-POMDP, specifying what each agent needs to provide for the team. Naturally, this policy is not executable by the agents, because agents cannot condition their actions on the observations of other agents in the real world.

Hence, in the next stage, we let each agent solve a POMDP in which it is rewarded for behaving in a manner similar to the specification in the team policy. This leads to the generation of a policy tree for each agent. 
These policy trees are not well synchronized with each other, and in the last step, we synchronize them by delaying the actions of agents to
improve the probability of good coordination. 

%It is heuristic in the sense that
%it is based on using a certain abstraction to generate the solution and as such, the solution is not guaranteed to be optimal, not can it be optimal,
%in general, with more computational time -- certain
%
%
%
%
%\\Multi agent planning in decentralized and partially observable environments is well known for being a difficult problem due to the difficult of reasoning with no explicit communication mechanisms while operating in an uncertain world. Such difficulty stems from the need for coordination, and let alone the need for collaboration. Planning for this sort of multi-agent problems, assuming there is indeed some necessity for coordination, require the agent to reason not only about it's uncertainty of the state, but also about the uncertainty of it's co-agents. This leads in turn to an infinite cycle: The agent needs to act based on its own perception of the state, by reasoning about the other agents' beliefs, and the same applies in vice versia for the other agents, creating such cycle.\\
%\guy{Specific problem in the area}
%\eliran{I'm not sure about what to write in this section}
%\\In this area of research there are numerous ways to model different problems, many of them distinguish between different representations of the world and different representations of communication mechanisms. We focus on an instance of the former, Factored Dec-POMDP, in which the world is factored into state variables and the environment is a Partially-Observable Markovian Decision Process.
%\guy{Solution method - team plan first, then single agent extensions, alignment}
%\\We approach the problem from a different angle by trying to avoid the explicit planning for decentralized multi-agent problem with no communication.\\
%We start by creating the Team-Problem, which is the centralized version of the problem at hand, and let the planner compute a solution. Such solution is much less difficult to compute, as there's essentially full communication between the agents, where each agent notifies all of it's co-agents about its observations.\\
%Following that, we project the problem with respect to each agent using the team solution. Each projection represents a single-agent problem in which the agent is rewarded for fulfilling his part of the solution that is relevant to its co-agents, while still being rewarded for achieving his private rewards. In each such single-agent problem, the agent makes certain actions on behalf of its co-agents in a rather superficial manner, yet in a way that allows him to properly plan essential sensing actions that weren't needed in the Team-Problem.\\
%Finally, we solve the single-agent problems and then align the resulting policies, in order to synchronize the agents and make sure that each agent performs his part in the appropriate time.\\
%\guy{Experiments}

We implement our algorithm and test it on several benchmark problems: \cbp, \mitg, \macor, \crs. We show that the algorithm manages to scale well beyond
current Dec-POMDP solvers on \cbp, \mitg ,\macor and \ronen{explain the properties of these domains that help our planner perform well.} \eliran{this is what I found as the most influencing property. It was very evident after trying to solve the rock sampling whose policies were consisted mostly of public actions}\guy{need to add in the experiments numbers to support that - e.g., the portion of public actions in each domain}\eliran{will do}
One of the main properties of the domains where our planner performs well, is that agents policies are only loosely coupled. That is, the need for actions that affect state components that are relevant to all agent, is sparse. That sparsity allows for each agent to independently construct a plan that consists mostly of its own private actions without requiring it to consider the other agents' behavior. This allows us to achieve good decentralized policies even when achieving the goal requires many steps, compared to planning directly over the Dec-POMDP model.
%that help our , due to the fact that different solutions doesn't contain public actions with inverse effects, hence making the reward shaping of the single-agent problems highly beneficial for the planner, as it has very strict way of achieving it's intermediate rewards and cannot incur rewarding cycles.\\



\section{Background}

In this section we provide needed background on POMDPs, Dec-POMDPs, their factored representation,
and policies. We also introduce the concept of private and public variables and actions in Dec-POMDPs.

\subsection{POMDPs}


A POMDP is a model for single-agent sequential decision making under uncertainty and partial observability.
Formally, it is a tuple $P=\langle S, A, T, R, \Omega, O, \gamma, h, b_0 \rangle$, where:
\begin{itemize}
\item
$S$ is the set of states. Each state captures all the relevant information for making decisions.
\item
$A$ is the set of actions. An action modifies the state of the world and may provide information about the current state.
\item
$T: S \times A \rightarrow \prod(S)$ is the state transition function.  $T(s, a, s')$ is the probability of transitioning to $s'$ when applying $a$ in $s$. 
\item
$R:S \times A \times S \rightarrow \mathbb{R}$  is the immediate reward function. $R(s,a, s')$ is the reward obtained after performing $a$ in $s$  and reaching $s'$. 
\item
$\Omega$ is the set of observations. An observation is obtained by the agent following an action, and provides some limited information about the world.
\item
$O:S \times A \rightarrow \prod (\Omega)$ is the observation function, specifying the likelihood of sensing a specific observation following an action. $O(s', a, o)$ is the probability of observing $o\in \Omega$ when performing $a$ and \emph{reaching} $s'$. 
\item
$\gamma \in (0,1)$ is the discount factor, modeling the higher importance of immediate rewards over future rewards.
\item
$h\in\mathbb{N}$ is the problem horizon --- the  amount of actions that an agent executes before terminating. \guy{Are we really in a finite horizon setting?}\eliran{corrected}
\item
$b_0\in \prod(S)$ is a distribution over $S$ specifying the probability that the agent begins the execution in each state.
\end{itemize}
%POMDP is used to described a single agent problem, where the agent is not aware of the current state following an action and instead maintains a belief state, which specifies the distribution of the states the world may be in. The agent may receive observations that expose some information about the state, through actions.
%\eliran{The following part regarding POMDP should probably be omitted}
%In this framework, instead of maintaining the current state itself (which is unknown), the agent maintains a distribution over all states, namely, a belief state. The initial belief state $b_0$, is exactly $I$.
%The goal is to find a policy $\rho$ (a mapping from belief states to actions) that maximizes $E\left[\sum_{t=1}^{h} \gamma^t \cdot R(s_t, \rho(b_{t-1}))\right]$, where $s_t$  and $b_t$ are the true state of the world and the belief state of the agent, respectively.
%The agent can update its belief state following every observation. After being in a belief state $b$, performing $a$, and observing $o$, the update is done as follows:
%\begin{equation}
%b_{a}^{o}[s'] = Pr(s'|b,a,o)=\frac{Pr(o|s',a,b)\cdot Pr(s'|a,b)}{Pr(o|a,b)} = \frac{Pr(o|s', a)}{Pr(o|a,b)}\cdot \sum_{s\in S}b[s]Pr(s'|a, s) \propto O(a, s')[o]\cdot \sum_{s\in S}b[s]\tau(s,a)[s']
%\end{equation}
%Usually, we evaluate each entry to the right hand side of the expression, and then normalize it by $Pr(o|a,b)$ or by its sum, as it is a probability distribution. Hence, the new belief state is $b'$, where for each state $s\in S$, $b'[s]=b_{a}^{o}[s]$.\\

\guy{I like having a running example, and tying each concept to the example. I would introduce here the example (say, box pushing), and show how all the concepts - states, actions, observations, ..., manifest in the example}


We assume that agent actions are either sensing actions or non-sensing actions. An agent that applies a non-sensing action always receives the observation {\em null-obs}. If all actions in the joint action are sensing actions or {\em no-op}s, then the state does not change. In addition, we assume that every action has a success probability, that is, it has an effect with constant probability that we consider as the successful outcome, while all the other effects are considered failures.
\guy{This is non-standard. Is this crucial? If so, we should either motivate it well, or cite someone else making this assumption.}\eliran{I guess you're talking only about the success prob thing, not the sensing/non-sensing separation. We need it mainly for the reward heuristic, and also for turning private action of other agents to deterministic in the single-agent problems. I'll think of a way to motivate it, perhaps we could also show a conversion between any discrete effects actions to a set of binary actions like these.}\guy{It is better to find someone who has already assumed success/fail and cite them}

Often, the state space $S$ is structured, i.e., it consists of assignments to some set of variables $X_1,\ldots X_k$, and the observation space $\Omega$ is
also structured, consisting of a set of observation variables $W_1,\ldots, W_d$. 
Thus, $S=Dom(X_1)\times\cdots\times Dom(X_k)$ and
$\Omega = Dom(W_1)\times\cdots\times Dom(W_d)$. 

In that case, $\tau$, $O$, and $R$ can be represented compactly by, e.g., using a dynamic Bayesian network~\cite{}. Formats such as RDDL~\cite{} and POMDPX~\cite{} exploit factored representations to specify POMDPs compactly.

A solution to a POMDP is called a policy. In general, a policy assigns to each history of actions and observation ({\em $AO$-history}) the next action to execute. 
Such a policy is often represented using a {\em policy tree} or, more generally, a {\em policy graph} (also called a finite-state controller). 
A policy graph $G=(V,E)$ is a directed simple graph, in which each vertex is associated with an action, and each edge is associated with an observation.

For every edge $v\in V$ and every observation $o\in\Omega$ exactly one edge emanates from $v$ with the label $o$.
The graph has a single root which acts as its entry point. Every $AO$-history $h$ can be associated with some path from the root to some vertex $v$,
and the action labelling $v$ is the action that the policy associates with $h$.

\guy{Are we using the concept of a  belief state? If so, we need to introduce it here.}\eliran{we don't use it for anything that is part of the algorithm. I'll see if we can omit the few usages that we have}

\guy{Do we really need both trace and history? Why not introduce this concept earlier - it does not fit in this section}\eliran{History is more relevant to policy graphs as part of Dec-POMDP solution, but I think we also need to explain about traces as we rely on them later on. What do you think about moving it to the end of the POMDP subsection? We can connect it to the policy graph notion by saying its the execution of it}\guy{Yes, move it to the POMDP section}\eliran{done, is the connection to policies ok?}
Finally, a policy graph can be run on the problem and produce an execution trajectory (trace for short). A trace $T$ of length $l$ is a sequence of quintuplets $e_i = (s_i, a_i, s'_i, o_i, r_i)$, namely \emph{events}, that occurred during a possible policy execution where:
\begin{itemize}
    \item $s_i$ is a state in step $i$ and $s_0$ is the initial state.
    \item $a_i$ is the action taken in step $i$
    \item $s'_i$ is the result of applying
    $a_i$ in $s_i$.
    \item $o_i$ is the observation received after taking $a_i$ and reaching $s'_i$.
    \item $r_i$ is the reward received for taking the $a_i$ in $s_i$ and reaching $s'_i$.
\end{itemize}
$\forall 0\leq i \leq l-1$, $s'_i=s_{i+1}$.

\subsection{Dec-POMDP}

A Dec-POMDP extends POMDP to problems where there are $n$ acting agents for some $n>1$. 
These agents are part of a team, sharing the same reward, but they act in a distributed manner,
sensing different observations. Thus, their information state can be different. 
Formally, a Dec-POMDP for $n$ agents is a tuple  $P=(S, A=\bigcup_{i=1}^{n}{\{A_i\}}, T, R, \Omega=\bigcup_{i=1}^{n}{\{\Omega_i\}},  O, \gamma, h, {\{I_i\}}_{i=1}^{n})$, where: \guy{Why do you have both $A_i$ and $A$, also for $\Omega$?}\eliran{is it ok if we drop $A$ and $\Omega$ from the tuple while still specifying them in the definitions?} \guy{see if this looks ok}
\begin{itemize}
\item
$S$ is the set of all states. Each state captures all the relevant information for making decisions. 
\item
$A_i$ is the set of actions available to agent $i$. We assume that $A_i$ contains a special {\em no-op} action, which does not change the state of the world, and does not provide any informative observation.
$A=A_1 \times A_2 \times .. \times A_n$ is the set of joint actions. On every step each agent $i$ chooses an action $a_i \in A_i$ to execute, and all agents execute their actions jointly. $\langle a_1,...,a_n \rangle$ is known as a joint action.
\item
$T:S \times A \rightarrow \prod(S)$  is the transition function. Transitions are specified for joint actions, that is, $T(s, \langle a_1,...,a_n \rangle, s')$ is the probability of transitioning from state $s$ to state $s'$ when each agent $i$ executes action $a_i$.
\item
$R:S \times A \times S \rightarrow \mathbb{R}$  is the reward function. Rewards are also specified over joint actions.
\item
$\Omega = \Omega_1 \times \Omega_2 \times .. \times \Omega_n$ is the set of joint observations. We assume $\Omega_i$ contains a special {\em null-obs} observation, which is the observation received when applying a non-sensing action.
\item
$O:S \times A \rightarrow \prod_{i=1..n}(\Omega_i)$  is the observation function, specified over joint actions. $O(s',\langle a_1,...,a_n \rangle,\langle o_1,...,o_n \rangle)$ is the probability that when all agents execute $\langle a_1,...,a_n \rangle$ jointly and reach $s'$, each agent $i$ observes $o_i$.
\item
$\gamma$  is the discount factor.
\item
$h\in\mathbb{N}\cup\{\infty\}$ is the horizon.
\item
$b_0 \in \prod(S)$ is a distribution over $S$ specifying the probability that each agent begin its execution in each state. In principle, different agents may have different initial belief states, but
we make the (common) assumption that the initial belief state is identical. 
\end{itemize}


\guy{Again, I would go back to the running example, extend it to a multi agent scenario, and demonstrate all concepts over the example}

As in the case of POMDPs, Dec-POMDPs can also be represented in a factored manner \cite{}, although most work to date uses the flat-state representation \cite{}.\guy{citations needed for both statements}
An important element of a factored specification of Dec-POMDPs is a compact formalism for specifying joint-actions. If there are $|A|$ actions in the domain, then, in principle, there are $O(|A|^n)$ possible joint actions. Specifying all joint actions explicitly is unrealistic for large domains. 

In practice, we may expect
that most actions will not interact with each other. A pair of actions $a\in A_i$, $a' \in A_j$ is said to be non interacting, if their effects when applied jointly (in the same joint action) is identical to their effects when applied separately.
Thus, our specification language focuses on specifying
the effects of single-agent actions and specific
combinations of single-agent actions that interact with each other, which we refer to as {\em collaborative} actions~\cite{}. For a more detailed discussion of the issue
of compact specification of joint-actions, see~\cite{}. 


A solution to a Dec-POMDP is a set of policies $\rho_i$, one for each agent. It maps action-observation sequences of this agent to actions in $A_i$.
As in POMDPs, these policies can be specified using a policy graph for each agent. The policy graph for agent $i$ associates nodes with actions in $A_i$
and edges with observations in $\Omega_i$. 

%\guy{POMDP solvers, SARSOP}
%We use offline POMDP solvers to solve different single-agent POMDPs we derive from the Dec-POMDPs.
%In general, an offline POMDP solver receives a POMDP problem together with a precision parameter $p$, and outputs a policy graph $G$ whose value $V(G)$ satisfies $|V(G)-V(G^*)|<p$, where $G^*$ is the optimal policy graph for the problem \eliran{not really sure it's true in general}. Following that, it is also possible to simulate the policy on the problem, and produce simulation trajectories (traces in short).\\
%Note that online solvers, unlike offline solvers, can only produce traces when solving the problem as they plan their actions on-the-fly, instead of trying to build a policy graph.
%SARSOP (Kurniawati, Hsu and Lee 2009) is state-of-the-art point based algorithm for solving POMDPs that we used as our offline POMDP solver. It's an anytime algorithm, meaning it can be halted in any point of the run and produce the current best result found.\\

\subsection{Public and Private Actions and Variables}

Public variables are state variables that several agents manipulate directly, while private variables are manipulated by a single agent only. 
The concept of \emph{private} and \emph{public} (or \emph{local} and \emph{global}) variables was introduced by
Brafman and Domshlak~\cite{} in the context of their work on factored planning. This concept has been used extensively
in work on privacy-preserving multi-agent planning (e.g., ~\cite{}) and, more recently in work on solving qualitative variants of Dec-POMDPs~\cite{}. As we are building on ideas in this latter work, we now explain how we extend them to the
case of factored Dec-POMDPs.

%Similarly to Factored POMDP, we can also define a factored Dec-POMDP by decomposing the state, observation and reward components, where action and observation are replaced by joint-action and joint-observation.\\
%\eliran{Should we define it formally as well?}
%When dealing with multiple agents, we can specify the relation of each agent to each of the variables.
%We define these relations according to the agent's actions, by first defining the $\mathbf{subjects}$ of each action.

We associate with each action $a$ the set of variables it can affect or it is affected by, which we refer to as  {\em subjects}$(a)$. 
The effect could take place through the transitions, rewards, or observations associated with $a$.

More specifically, let $a\in A_i$ be an action of agent $i$. We identify $a_i$ with the joint action $(\mbox{no-op},\ldots, a_i,\ldots,\mbox{no-op})$.
We say that variable $X_i$ is a subject of $a\in A_j$ if there is some state $s$ for which there is a positive probability that the value 
$X_i$ changes following $a$, or if there are two states $s_1,s_2$ that differ only in the value of $X_i$ such that $R(s_1,a,s')\neq R(s_2,a,s')$ for some $s'$.
Similarly, $\omega_i$ is a subject of $a\in A_j$ if there exists a state $s$ such that there is a positive probability
of observing $\omega_i$ when $a$ is executed and $s$ is reached. 
% if $\exists{s, s'\in S}.s|_{S_i} \neq s'|_{X_i}.\tau_i(s, a_i)[s'] > 0$\\
%An observation variable $\omega_i$ is a subject of an action $a\in A_j$ if $\exists{s'\in S}.\exists{o \in \omega_i}.o \neq NullObs.Z_i[a_{iso}, s', o] > 0$\\
%A reward variable $R_i$ is a subject of an action $a\in A_j$ if $\exists{s, s'\in S}.\rho_i[s, a_{iso}, s'] \neq 0$\\
\guy{After the definition of subject I was expecting the definition of private and public}\eliran{the problem is that we need the private/public variables definitions to define public/private actions, and for that we need to first define collaborative actions so that the definition of subjects is complete}

As explained above, we expect that most actions do not interact. In that case, it is straightforward to get the post-action distribution for their combination from the specification of the single-agent actions they contain. But, as we work in multi-agent environment, some actions may affect some variables only when applied jointly with other actions, or may have a different effect on these variables when applied jointly.
We refer to these as $\emph{collaborative}$ actions. 
%Hence, we need to be able to define subjects of actions even when they alone can't affect the variable at question.
%We start by defining what collaborative actions are:

%\ronen{the definition below is restrictive}
%Let $a$ be an action. We say that $a$ is a \emph{collaborative} action if we can find a set of actions apart from $a$ itself, each applied by a different agent, so that if applied simultaneously may cause an effect, while the application of any strict subset of these actions and $a$ yields no effect whatsoever.
% Formally, let $a\in A_j$, then $a$ is a \emph{collaborative} action if the following holds:
% \begin{itemize}
%     \item There exists a set of actions, $(a_1,..., a_c)$ such that $\forall 1\leq t \leq c$, $a_t\in A_{i_t}$, $a_t \not\in A_j$, where $\forall 1\leq t\neq r \leq c. i_t \neq i_r$. Simply put, a set of actions of disjoint agents.
%     \item We define the joint-action $ac_{iso}$ which is assigned with $a, a_1,..., a_c$ on their respective indices, and a $no-op$ otherwise.
%     \item There exists a variable $V$ such that the following holds:
%     \begin{itemize}
%         \item $V$ is a state variable $X_i$ and $\exists{s, s'\in S}.s|_{X_i} \neq s'|_{X_i}.\tau[s, ac_{iso}, s'] > 0$ and $\forall 1 \leq t \leq c.\tau_i(s, ac_{iso}|_{a_t=\textit{no-op}}, s') = 0$
%         \item $V$ is a state variable $X_i$ and $\exists{s, s', s''\in S}.R(s, ac_{iso}, s'') \neq R(s', ac_{iso}, s'')$ and $\forall 1\leq t \leq c.R(s, ac_{iso}|_{a_t=\textit{no-op}}, s'') = R(s', ac_{iso}|_{a_t=\textit{no-op}}, s'')$
%         \item $V$ is an observations variable $\Omega_i$ and $\exists{s'\in S}.\exists{o\in \Omega_i}.o \neq \textit{null-obs}.O[ac_{iso}, s', o] > 0$ and $\forall 1\leq t \leq c.o_i[ac_{iso}|_{a_t=NoOp},s',NullObs]= 1$
%         \item If $V$ is a reward variable $R_i$: $\exists{s, s'\in S}.R_i[s, ac_{iso}, s'] \neq 0$ and $\forall 1\leq t \leq c.R_i[s, ac_{iso}|_{a_t=NoOp}, s'] = 0$
%     \end{itemize}
% \end{itemize}
%Given these definitions, we can naturally expand the definition of an action's subjects in case it is a collaborative action, by considering all the variables that makes them a collaborative action.
We say that $X_i$ is the subject of a collaborative action $a$ that consists of single agent actions
$a_{i_1},\ldots,a_{i_k}$ (and {\em no-ops}) if $X_i$ is the subject of $a$, as defined above, and is not the subject of each of the actions $a_{i_j}$.%
\footnote{A complete treatment of the subtleties of this issue is beyond the scope of this paper. The above definition will be sufficient for our purpose.}
%If $X_i$ is the subject of a collaborative action
%$a$, then $X_i$ is considered {\em public}.
%
%\eliran{I defined the \emph{relevant} notion, and then defined the private/public using that}
%Having defined the group of $subjects$ of an action, we can intuitively define the variables that are \emph{relevant} to an agent. 
We say that a variable $X_j$ is \emph{relevant} to agent $i$, if  $X_j\in subjects(a)$ for some $a\in A_i$.

We can now define the concept of {\em private} and {\em public} variables.
$X_i$ is {\em private} to agent $j$ if $X_i$, if it is not relevant to any agent $k\neq j$.
Otherwise, if there are other agents whose $X_i$ is relevant to, we say that $X_i$ is {\em public}. 

% We then consider a variable as $\mathbf{private}$ with respect to an agent, if that agent is the only agent that possesses it. Otherwise, it is considered a public variable.
An action $a\in A_j$ is public if at least one of its subjects $X_i$ is public. 
%Otherwise, it is private.
%Notice that by definition, 
A collaborative action is always public. \guy{this is interesting - I think that there can be a collaborative action only over private variables}\eliran{A variable affected by a collaborative action is a public variable - it's contained in two different subjects groups, which turns the action to public as it has a public variable as one of its subjects. We could have used different terminology where a variable that is relevant to more than one agent is not necessarily public, but is perhaps private to both of them.}\guy{I disagree. We can have private variables $p_1,p_2$ private to agents 1 and 2, respectively, and have actions $a_{11},a_{12},a_{21},a_{22}$ where action $a_{ij}$ belongs to agent $i$. The effect of the joint action $\langle a_{11},a_{21} \rangle$ is $p_1=true$, and the effect of $\langle a_{11},a_{22} \rangle$ is $p_1=false$. I see nothing above to forbid this.}\eliran{But if the effect of $\langle a_{11}, a_{21} \rangle$ is always $p_1=true$ (and I assume you also meant that applying $a_{21}$ alone doesn't affect $p_1$), then from a state in which $p_1=false$ the application of $\langle a_{11}, a_{21} \rangle$ changes $p_1$, hence making $p_1$ a subject of both $a_{11}$ and $a_{21}$}
%since all other actions that collaborate with it also make their agent an owner of the variable, hence making it a public variable.

%\subsection{Sensing and Non-Sensing Actions}
%\eliran{focus on the fact that we require that separation}

% For the description of our algorithm, we need two more
%concepts: a {\em contexted} action is a pair $(\pi,a)$
%where $\pi$ is a (possibly partial) assignment to the
%state variables. Intuitively, we use this pair when we want %to restrict the application of $a$ to states that satisfy $\pi$. 

%\subsection{contexted Actions}
%In the absence of explicit preconditions in MDP-based models, it is possible to mimic preconditioning for actions via the state itself. For example, if we would like an action to be applied only in a certain state $s$, we can incur high penalty for applying the action in any other state.
%To do that, we use the notion of \emph{contexted} action. A contexted action is simply the combination of an action and a set of states, which is the context. When working with models where the state space is structured, it is also possible to project some of the state variable \eliran{elaborate further regarding the functionality of the agent in the single-agent problem, what he can and cannot do}, and achieve wider contexts without having to explicitly specify all the states the compose the context.
%We will consider two types of contexted actions:
%\begin{itemize}
%\item fully-contexted action - a combination of an action and an assignment to $\mathbf{all}$ state variables
%\item projected-context action - a combination of an action and an assignment to a subset of the state variables, while the remaining state variables are considered "wild-carded", meaning that they any of their domain values can be assigned to them.
%\end{itemize} 
%
%\subsection{Traces}
%\eliran{I'm not sure where to place it actually, but I %find it better placed here than directly in the trace extraction subsection, as we use the term "trace" quite alot.}
%\\When having a POMDP model and a policy solving it, it is possible to produce traces which are recordings of a world's progress in a specific run of the problem, according the policy. Each trace starts with an initial state, which is sampled from the initial belief state of the problem, and then cycles between being in a state, taking an action, and receiving an observation and a reward. Formally, a trace 


\section{The Algorithm}
\guy{We need a name for the algorithm} \eliran{ How about DMAFP? Distributed multi agent factorized planning }
%\guy{High level description}
%\\The algorithm consists of three main parts:
%\begin{itemize}
%    \item Extracting traces of the team solution.
%    \item Constructing single-agent policies.
%    \item Process and align the policies.
%\end{itemize}
%\eliran{things to add to description}
%\begin{itemize}
%    \item reason we use traces instead of policy
%\end{itemize}
%
%\eliran{old description}
%Given the team problem, which is the centralized POMDP version of the Dec-POMDP problem at hand, we aim at generating a series of policy graphs, one for each agent.\\
%We start off by generating a set of $n_{traces}$ simulation traces using a black-box POMDP solver according to our confidence parameter $\alpha$ and precision requirement $p_{team}$.\\
%Having the team problem solved, we process the resulted traces in order to extract the contexted public actions that were applied by each agent.\\
%Now, we project each instance of contexted public action according to the agent's view, and use a heuristic to determine the reward that will be given to each such projected-context public action.
%We then project the problem with respect to each agent, given its set of projected-contex public actions (with their respective rewards) and the team problem. Following that, we solve each single-agent problem, again by some black-box POMDP solver, where now we're intrested in obtaining a policy graph for the agent, instead of run traces. The precision is set differently for each agent according to the rewards given for his contexted public actions, and denoted by $p_i$ for $i \in {1...M}$.\\
%We perform an iterative alignment of the resulted policy graphs until the convergence criteria is reached, using the Align procedure.\\
%Finally, we apply the PostProcess procedure to the aligned policy graphs, that makes the policy graphs applicable for the Dec-POMDP problem, as well as performs several local optimizations to the policies.

We now describe our approach for producing policy graphs for agents acting in a Dec-POMDP. Our approach first generates a team solution, and then generates single agent policies where each agent attempts to fulfil its part in the team solution. Finally, these policies are synchronized. 

\guy{The high level description here may be too detailed and technical.}

From the input factored Dec-POMDP problem $P$, we first generate the team POMDP $P_{team}$. $P_{team}$ is identical to $P$, ignoring the underlying multi-agent structure. That is, the actions are the joint actions and the observations are the joint observation, viewed as applied 
and observed by a single agent. Equivalently, this can be viewed as a Dec-POMDP in which all observations are communicated accurately and instantaneously 
to all agents.


We solve $P_{team}$ using an off-the-shelf POMDP solver, SARSOP \cite{}. Some solvers provide a policy graph directly, but SARSOP provides a policy in the form of a set of $\alpha$-vectors.\eliran{not sure whether you or Ronen added it, but why is it relevant? SARSOP also provides a way to convert the alpha vectors into a graph anyway} Instead, we use SARSOP to generate execution traces from the policy. From the traces we can learn each agent's behavior in the team solution.
We use the traces to extract contexted actions for the next step in the algorithm. That is, we identify in which context each agent executes a public action in the plan.
Policy graphs are difficult to work with, since they are based on the state distribution of the initial belief state, and most of the time represent highly obfuscated contexts. \guy{I do not understand the last sentence - are we trying to explain why policy graphs are not good for us? I am not sure that the argument holds - if we had a graph we could have used it to get the info that we need.}\eliran{We can get a graph, but getting proper contexts in the form of action-state would be inaccurate. We start from an initial belief and then can't know exactly which action was applied in which state. We should should later on extract this from the policy graph but it's more difficult, since we'll be calculating all possible traces. Should I use that argument? that we could use the policy graph to extract all possible traces but it's more difficult and a bit of an overkill?}

Using these traces and the factored Dec-POMDP we then generate
one single-agent problem for each agent. The dynamics of each single-agent problem is similar to that of the Dec-POMDP, except that some variables are projected away. We design the reward function to be such that agents are rewarded for behaving in a manner that is similar to how they behave in the team solution.

Finally, we process the single-agent policies and align them to try and ensure that actions are properly synchronized when they are executed in a decentralized manner. The high-level pseudo-code is described below. We described the first step (generating $P_{team}$) above. In the rest of this section we explain these steps in more detail.


\begin{algorithm}
\caption{GenerateAgentPolicies \guy{This pseudo code is pretty useless. It just lists the phases. I don't think that it adds anything.} }
\begin{algorithmic}[tbph]
\State Input: $P$, $\alpha$, $p_{team}$
\State $P_{team} \gets \Call{Centralize}{P}$
\State {\em Traces} $\gets \Call{ProduceTraces}{P_{team}, \alpha, p_{team}}$
\State {\em RawSAPolicies}$ \gets \Call{ProjectAndSolve}{P_{team},{\mathit Traces}}$
\State {\em SAPolicies} $\gets \Call{ProcessAndAlign}{\mathit{RawSAPolicies}}$
\State {return {\em SAPolicies}}
\end{algorithmic}
\end{algorithm}

\subsection{Producing the Traces}
% \ronen{the reason I do not like these parameters is that from the description it seems that the user provides them as part of the problem. But this is not the case. The user just provides P. alpha and p-team are simply hyper-parameters that you need to select to get a good solution. At the very least, I would say that they are hyper-parameters. Moreover, we cannot say at this point anything about how they impact the quality of the final solution. I updated the text below accordingly.}
% \eliran{I understand. When solving the single-agent problems I also stated there's a hyper-parameter for the precision, though it's a bit more principled there since we choose it so that all the contexted-actions are applied in the policy. Perhaps we can omit the precision hyper-parameter in the team problem case} 
% \ronen{OK}

Having generated the team problem, $P_{team}$, we 
must specify two hyper-parameters: a confidence parameter $\alpha$ and a precision parameter $\epsilon_{team}$. \guy{It makes no sense to have $P_{team}$ as the problem and $p_{team}$ as a precision parameter. Find a different name.}\eliran{changed to $\epsilon_{team}$} We generate a $\epsilon_{team}$-optimal solution \guy{what does this means?}\eliran{shortly explained below}
to $P_{team}$ using an off-the-shelf POMDP solver.
Then we generate sufficiently many traces so that with probability of at least $\alpha$, for every initial state $s$, there is a trace that starts at $s$. An $\epsilon$-optimal solution is a solution whose value differs by at most $\epsilon$ from the value of an optimal solution. \eliran{do we need to explain what a solution's value is?}
%all the possible initial states all calculate the number of traces that is required to satisfy the confidence parameter $\alpha$, and produce that number of traces using the team policy at hand. We denote this number by $n_t$.

\guy{I do not understand this - why is it sufficient to have a single trace that starts at a state $s$? This is only important if we have deterministic transitions. Otherwise, we need many traces from each state.}\eliran{its not principled, it's just the best we can do. Perhaps we can calculate something from the policy graph - taking the belief state with the maximal number of states, and replace $I$ with it in the formula}

\eliran{change initial belief to trace perhaps}
For that, it is sufficient to select the number of traces $n_t$ to be such that $\sum_{s\in supp(b_0)}(1-Pr(b_0=s))^{n_t} \leq 1-\alpha$. \guy{The notation here is not clear. What does $I$ mean? I don't think that it was defined earlier. Also you use it here as both a set $s \in I$ and something else $I=s$}\eliran{changed it to support of $b_0$, is it ok now? } \guy{Do not use footnotes}\eliran{actually Ronen suggested we move it to a footnote, do you generally avoid using them?}
Let $E$ denote the event in which we cover all possible initial states, $s \in supp(b_0)$, in $n_t$ traces. Then we want to pick $n_t$ such that $Pr(E)\geq \alpha$, using the union bound we get: $Pr(E)\geq 1-\sum_{s\in supp(b_0)}(1-Pr(b_0=s))^{n_t}$.
%Hence if we pick $n_t$ large enough such that $\sum_{s\in I}(1-Pr(I=s))^{n_t} \leq 1-\alpha$, we achieve the required property.

%\subsection{Creating the Team Problem}
%
%\guy{What is the input}\\
%We are given with a Factored Dec-POMDP problem $P$, a confidence parameter $\alpha\in \mathbb{R}$, and a precision parameter $p_{team} \geq 0$
%\eliran{talk about metadata stuff? we can essentially extract the relations using the definition, perhaps without even specifying an algorithm}
%along with metadata that specifies each agent's variables, which actions are public and which are private, and the objectives of each action.\\
%\guy{The team problem format}\\
%Initially we need to convert the Factored Dec-POMDP problem at hand to a Factored POMDP problem which represents the team problem.\\
%So given a Factored Dec-POMDP problem $P=(\{X_i\}_{i=1}^{K}, S, {\{A_i\}}_{i=1}^{M}, A, \{\tau_i\}_{i=1}^{K}, \{R_i\}_{i=1}^{L}, \{\rho_i\}_{i=1}^{L}, \{\omega_i\}_{i=1}^{Z}, \Omega, \{Z_i\}_{i=1}^{Z}, \gamma, h, I)$, we produce a new Factored POMDP problem, $P_{team}=(\{X_i\}_{i=1}^{K}, S, A, \{\tau_i\}_{i=1}^{K}, \{R_i\}_{i=1}^{L}, \{\rho_i\}_{i=1}^{L}, \{\omega_i\}_{i=1}^{Z}, \Omega, \{Z_i\}_{i=1}^{Z}, \gamma, h, I)$. The only difference is that now there's no separation to different agent, and instead there's a "master" agent that perform all joint-actions and receive all joint-observations at once.\\
%\guy{Solving the team problem}\\
%When solving the team problem, we use an offline POMDP solver as a black-box that receives the team problem $P_{team}$ and a precision parameter $p_{team}$. The solver outputs a policy graph $G_{team}$, which given that $G^*$ is the optimal policy graph, satisfies $|V(G_{team})-V(G^*)| \leq p_{team}$. We then take the problem $P_{team}$ and produce $n_{t}$ simulation traces of $G_{team}$ over $P_{team}$.\\
%

%\eliran{pseudocode-part1}
\begin{algorithm}
\caption{ProduceTraces}
\begin{algorithmic}[tbph]
\State Input: $P_{team}$, $\alpha$, $\epsilon_{team}$
\State {\em{TeamPolicy} $\gets \Call{POMDPSolver}{P_{team}, \epsilon_{team}}$}
\State {$n_t \gets \Call{NumTracesRequired}{P, \alpha}$}
\State {\em{Traces} $\gets \Call{Simulate}{\mathit{TeamPolicy}, n_t}$}
\State {return \em{Traces}}
\end{algorithmic}
\end{algorithm}

% Having the team problem solved, we process the resulted traces in order to extract the contexted public actions that were applied by each agent.\\
% Now, we project each instance of contexted public action according to the agent's view, and use a heuristic to determine the reward that will be given to each such projected-context public action.
% We then project the problem with respect to each agent, given its set of projected-contex public actions (with their respective rewards) and the team problem. Following that, we solve each single-agent problem, again by some black-box POMDP solver, where now we're intrested in obtaining a policy graph for the agent, instead of run traces. The precision is set differently for each agent according to the rewards given for his contexted public actions, and denoted by $p_i$ for $i \in {1...M}$.\\
% We perform an iterative alignment of the resulted policy graphs until the convergence criteria is reached, using the Align procedure.\\
% Finally, we apply the PostProcess procedure to the aligned policy graphs, that makes the policy graphs applicable for the Dec-POMDP problem, as well as performs several local optimizations to the policies.
% \ronen{The postprocessing step is not clear. What does it mean to make the policy graph applicable to the dec-pomdp? It seems like an implementation issue, not an algorithmic one.}
% \eliran{The processing stuff refer to the removal of actions of other agents ("foreign actions") from the policy graph. Before the alignment we remove private foreign actions, and after the alignment we turn foreign public actions into no-ops, and also insert repetitions of collaborative action to avoid the deadlock issue. Anyway I don't think that the high-level description should contain any details about it apart from mentioning it exists. Perhaps adding the fact that it is necessary so that the policies won't contain foreign actions, but certainly nothing more. Let me know what you think}
% \ronen{Ok - we should not mention this at the high level. At the end of the section on alignment, we can briefly mention this.}

\subsection{Extracting Contexted Actions}
\guy{Is "contexted" proper English?}\eliran{according to wiktionary, yes}
When constructing a POMDP for each agent, we attempt for the optimal policy in that POMDP to lead the agent to behave similarly to how it behaves in the team plan. That is, the agent should execute the same public actions that it executes in the team plan, in the same context that each action was executed in the team plan. We must define appropriate conditions for rewarding the agent when it executes an action from the team plan, which we
call the \emph{projected context}. 

Yet, similar behavior doesn't necessarily mean identical one, and is affected by the way we project the contexts. \guy{Never ever use questions in a paper. You shouldn't have even a single question mark.}
The projected context can be associated with a specific state. However, requiring that an action would be executed only in the same state as in the team plan is too restrictive. Various aspects of the state may be too specific or irrelevant, and can be generalized to other states. \guy{This is very vague and unclear. What does "too specific" means. What does "irrelevant" means.}\eliran{Ronen offered we add this paragraph to give some general sense of why we project the contexts. I removed the question mark anyway.The "too specific" or "irrelevant" is not something concrete, do you still think we should elaborate on that?}


%When projecting the context of a contexted-action $(\pi, a)$ with respect to an agent, we refer to the resulting contexted action as the \emph{projected-context} action of $(\pi, a)$ (again, with respect to the agent). Notice that by projecting the context onto a subset of state variables, we obtain a partial assignment.

We start with the states in the trace in which an action is executed by an agent. We refer to a pair of state in which
an action is executed in the trace and the action itself,
as a \emph{contexted action}. \guy{This is also unclear - you refer to a pair of states as contexted actions? This makes no sense. What I think you mean is "Given a public action $a$ in the team plan, we refer to all states where $a$ was executed as the context of $a$." This is also not great, because it may be that an action, say "push-left" must be executed in a few different contexts. Not sure how to fix this. I am stopping here with this part. I am not sure how to properly describe this.}\eliran{fixed it. it was just a bad formulation. a contexted action is a pair of state + action, that's it}

For agent $i$, we denote with $CActions_i$ the set of all the contexted actions in the traces, that contain a \emph{public} action of our agent.
%, and their corresponding rewards with $Rewards_i$.
$CActions_i$ contains \emph{public} actions only since the private objectives of the agent are left untouched when transitioning to the single-agent problem.
Next, we project the context of all contexted actions to
the set of variables \emph{relevant} to agent $i$,
% \eliran{I think that the subjects of the action should also be incorporated, perhaps subjects+private relevant variables, but we can leave it like that for now since some of our definitions here still don't exist in the code}
and obtain a set of projected contexted-action, denoted by $PCActions_i$, consisting of
a projected-context and an action. We will associate
the rewards with the elements of this latter set. We will refer to projected contexted-actions as PJAs from now on. \guy{Why do you need both PCActions and PJA?} \eliran{the set PCActions contains the PJAs} Notice that when projecting different contexted actions, we might result with identical PJAs.

%We could reward the agent for performing action $a$ at state $s$ whenever there is a trace in which it performs $a$ at $s$. However, this is too narrow a scope, as it also contains various facts about the states of the other agents that are not important for the execution of $a$.

%Therefore, we need to first be able to perform a relaxation of the context that appears in the traces so that unnecessary facts are omitted. We refer to that relaxed context as \emph{projected-context}. When projecting the context of a contexted-action $(\pi, a)$ with respect to an agent, we refer to the resulting contexted action as the \emph{projected-context} action of $(\pi, a)$ (again, with respect to the agent). Notice that by projecting the context onto a subset of state variables, we obtain a partial assignment.

%\ronen{The text is not clear. Do you project to  subjects(a) only, or are there additional elements? Give a clear definition and say it in one line, if possible.}
%\eliran{rewritten}
%To compute the \emph{projected-context} of a contexted-action $(\pi, a)$ with respect to agent $i$, we project the context $\pi$ onto $subjects(a)$ and the variables owned by agent $i$.

% The subjects of $a$ are necessary to the projected-context as they are relevant to the \emph{action} itself, regardless of the agent. The variables owned by the agent are necessary as they represent all the information of the agent's progression in the team solution.

\guy{This should have been a part of your running example}
As an example, we can think of a simple Box-Pushing 1 by 2 grid, where there's an agent and a box in both tiles. The left tile is marked by \emph{L} and the right tile by \emph{R}. \emph{Agent1} and \emph{Box1} are placed on the left, while \emph{Agent2} and \emph{Box2} on the right. The state is composed of 4 variables: the location of each box -- $(X_1, X_2)$ -- and the location of each agent -- $(X_3, X_4)$. Each agent can push only its respective box. The goal of each agent is to push the box to his fellow agent, but the push action has a positive failure probability. In the team policy traces, we would find the following \emph{public contexted} actions:
\begin{itemize}
    \item \emph{(L, R, L, R),Agent-1-Push-Right}
    \item \emph{(L, R, L, R),Agent-2-Push-Left}
    \item \emph{(L, R, L, L),Agent-1-Push-Right}
    \item \emph{(L, R, R, R),Agent-2-Push-Left}
\end{itemize}

Each action appears in two different contexts, as throughout the traces there may be scenarios in which one of the push actions fail while the other does not.

When extracting the PJAs for \emph{Agent1}, we would encounter two possible candidates:
\begin{itemize}
    \item \emph{(L, R, L, R),Agent-1-Push-Right}
    \item \emph{(L, R, L, L),Agent-1-Push-Right}
\end{itemize}

We want to determine the variables onto which we project the contexts with respect to \emph{Agent1} and its action \emph{Agent-1-Push-Right}. The only subject of \emph{Agent-1-Push-Right} is $X_1$, the location of \emph{Box1}, and the additional variable that is owned by \emph{Agent1} is $X_3$ which is the agent's location. Therefore, the contexts are projected onto the variable $X_1, X_3$. 

When projecting both candidates, we get the same projected-context, \emph{(L, *, L, *)}, hence obtaining a single PJA.
Thus, in the single-agent problem, \emph{Agent1} will be rewarded for pushing \emph{Box1} to the left when both itself and the box are in the left tile, regardless of the locations of \emph{Box2} and \emph{Agent2}.

We will now describe how the single-agent problems are constructed, given the PJAs and their rewards.
How the reward values are computed will be described afterwards. We will use $Rewards_i$ to refer to the set
of reward values associated with $PCActions_i$.


\subsection{Single Agent Projection}

Given the PJAs and their rewards, we construct single-agent problems that reward each agent for following its role in the team plan, as well as its own private objectives. The single agent POMDP of agent $i$ contains some the actions of $i$, and some actions of other agents. The actions of other agents are used to ``simulate'' some of the behaviors of the other agents -- behaviors that eventually enable the agent to carry out its own actions.

The single-agent POMDP $P_i$ for agent $i$ is obtained by modifying $P_{team}$
as follows:
%o do the projection for agent $i$, we are given with the team problem $P_{team}$ and the contexted actions to be rewarded accompanied by their corresponding rewards, $CActions_i$ and $Rewards_i$.
%We apply a series of operations to alter $P_{team}$, into the desired single-agent POMDP $P_i$:
\begin{enumerate}
\item Remove from the problem any \emph{public} action that does not appear in the traces, regardless of the agent applying it. As public actions alter facts in the world that are relevant to more than one agent, we completely forbid the application of a public action that was not part of the team solution.
%\ronen{This is not clear. $CActions_i$ are contexted actions of agent i. From the above it is implied that it can contain actions of other agents.}
%\eliran{Ok sorry I indeed got a bit confused earlier. We remove public actions that don't appear in the traces, or as you said equivalently, in any of the $CActions$.}
\item Remove any sensing actions of other agents.
In the real world, the agent does not have any access to other agents' observations and must plan based on its local view only.
\item The \emph{private} actions of other agents are transformed into deterministic actions that succeed with probability 1.
As private actions change only private variables, and an agent cannot sense other agent's private variables, this projection allows the agent to progress the world towards states in which it must act.
\item Add penalties for the application of all remaining \emph{public} actions in any context that did not appear in the team plan. We want to discourage an agent from applying public actions out of the context of the team plan. The penalty is chosen to be $-1\cdot\max_{r\in Rewards_i}\{r\} \cdot|PCActions_i|$, as an upper bound on the sum of rewards that can be achieved from applying the contexted action. This ensures that no undesirable action is worth applying, even in exchange for applying all the contexted actions. There is no penalty for other agents' PJAs
(i.e., actions in $\bigcup_{j\neq i} PCActions_j$). We want
to allow the agent to simulate other agents' contexted public actions in order to plan the execution of its own
actions at appropriate times.
\item Add the rewards for PJAs (explained later).
This reward will override the above penalty, but only under the specified context of each action. 
%For example, if a public action appears in $CActions_i$ with the context $c$, then in the previous alternation it would be penalized when applied in any state, but now the penalty will be overridden and a reward will be given instead when applied in states expressed by $c$, while still yielding a penalty in any other state.
\item Remove rewards related to public variables the agent
can achieve -- the goal of the single-agent POMDP is to imitate the team policy, not compute an alternative solution. More specifically, given a public variable $X_i$, two state $s, s'$ that differ only on $X_i$, and an action $a\in A_i$ such that $R(s, a, s'')\neq R(s', a, s'')$, we set both rewards to 0.
% \item{Remove all the rewards given upon the change of a public state variable, namely, public objectives. By that we change the agent's target, from achieving the public goals to applying his public actions. We leave private objectives without change, as we still want to agent to achieve them}
\end{enumerate}

After applying these changes, we obtain the single-agent problem $P_i$. Before solving it, we need to specify a precision hyperparameter that determines how close to optimal the policy should be, similarly to the one that we specified for the team problem, but now picked separately for each single-agent problem.
%in order to make sure all objectives are achieved. 
%\eliran{this is similar to $p_{team}$ but a bit more important, again let me know if you find it unnecessary}. 
\guy{What is "precision" here exactly? Do you have a different precision for each agent? This is unclear.}\eliran{hope it's clear now}
For the single-agent problem $P_i$, we pick the precision $\epsilon_i$ to guarantee that no PJA will be ignored by the policy. We take the maximum trace length $l_m$ and choose the precision so that even after performing $l_m$ steps, the policy will be $\epsilon_i$-optimal, where $\epsilon_i$ is smaller than the minimal reward associated with a PJA in $PCActions_i$,
%\ronen{not sure I understand the formula below}
%\eliran{Thinking of it again I may have made a mistake. The idea was to take the largest step possible and calculate the reward given for applying the minimally rewarding action in the farthest step, i meaning with a discount of $\gamma^{l_m}$. But perhaps I should've taken into account any preceding costly actions. We should discuss it in the meeting}
i.e., $\epsilon_i=\min_{r\in Rewards_i}\{r\}\cdot\gamma^{l_m}$


Finally, we solve each single-agent problem $P_i$ using our off-the-shelf POMDP solver, SARSOP, resulting in a $\epsilon_i$-optimal policy for each agent. The generated policies will likely
contain non-sensing actions of other agents, which require
some post-processing, explained later on.
% In the following section we explain how to eliminate these actions and synchronize the agents' policies, as much as possible.
%In the next section, we describe how we calculate the set of rewards $Rewards_i$.

%\eliran{pseudocode-part2}
\begin{algorithm}
\caption{ProjectAndSolve}
\begin{algorithmic}[tbph]
\State Input: $P_{team}$, ${\{PCActions_i\}}_{i=1}^{n}$, ${\{Rewards_i\}}_{i=1}^{n}$
\For {all agent $i$}
\State {$\mathit{SAProblem_i}$ $\gets \Call{Project}{P_{team}, PCActions_i, Rewards_i}$}
\State {$\epsilon_i \gets \Call{GetSAPrecision}{\mathit{MaxTraceLength}, \gamma}$}
\State {$\mathit{RawSAPolicy_i} \gets \Call{POMDPSolver}{\mathit{SAProblem_i}, \epsilon_i}$}
\EndFor
\State {return $\mathit{RawSAPolicy_1}, \mathit{RawSAPolicy_2},..., \mathit{RawSAPolicy_n}$}
\end{algorithmic}
\end{algorithm}


\subsection{Rewards in the Projection}

After we compute the PJAs for the agent, we need to specify their respective rewards. The rewards should encourage the agent to perform the team plan actions in their corresponding contexts, in the same order they appeared in the team solution. 
To do so, we exploit the discount factor, which makes it beneficial for the planner to apply higher rewarding actions earlier. Hence, we need to order the PJAs, and assign higher rewards to actions that appear earlier in the order.
\ronen{The sentences below are not clear to me}\eliran{tried to clarify, and added the PJA abbreviation} 

Arbitrary increasing rewards may not be sufficient. First, the planner might need to insert costly actions that precede a PJA for achieving some needed precondition. This may lead to a scenario where it is more beneficial to apply the PJAs in a different order or even not to apply a PJA at all, as the costs might be higher than the reward of the PJAs. 
\guy{The argument below is unclear to me. You say that {\bf because} actions are not deterministic the planner would apply a PJA only once? This does not make sense. The planner should know to apply the PJA as many times as need be.}\eliran{The "once" terminology is indeed bad, I'll change it. Eventually we need to make sure that the actions succeed in the same order that we determined. In problems where the team solution contain alot of "jumpings" between the public actions, we would naturally get worse results as the order we determined wouldn't reflect the team solution well. But again, the main idea is to preserve the order, and because of that the verification is needed}
Furthermore, when we consider non-deterministic actions, the planner might find it beneficial to apply a PJA without verifying -- to some extent -- it had succeeded, hence possibly violating the order.

Therefore, we need to design the rewards, and especially the differences between the rewards, so that the desired behavior is manifested through them. The planner should find it beneficial to execute the actions in the right order. That is, striving for achieving the effects of the actions in the correct order should increase the expected discounted reward of the single-agent problem.

We compute the rewards given to the PJAs as the sum of two terms:
The first term is used to motivate the agents to perform the actions in the same order in which they appeared in the team solution.
\ronen{don't understand the second term. do you discuss it below?}
\eliran{Sorry, I really forgot to write about it. Added it, though I now think we should omit it for this version, as it's really unnecessary for the BoxPushing and perhaps should be better explain/modifier}\guy{Why is the average reward important? Also, why is an average needed? isn't it just $R(s,a,s')$? Or do you mean the average cumulative reward of the trace?}\eliran{since we're attaching the rewards to a PJA rather then action + state, we cant really use $R(s,a,s')$, as a PJA might match several entries.}
The second term is the average reward that was obtained by the PJA throughout the traces.
We start by computing the first term for all PJAs, and then add the second term to each PJA.

To compute the first term, we need to determine an order over the PJAs we extracted for the agent.
We go over the traces and construct a \emph{directed acyclic graph} of PJAs, forcing a DAG by not adding edges that form cycles.
\guy{the definition below is vague. I suggest something like: "Let $l$ be a trace. For each event $e_i$ in the trace, we add a PJA, and add an edge between the vertex corresponding to $e_{i-1}$ and $e_i$."}\eliran{done. also added the terminology of event to the traces explanation}
We start with an empty graph, and go over all traces. Let $l$ be a trace. For each event $e_j$ in the trace, we compute the PJA of its contexed action with respect to agent $i$. We denote this PJA as $pja_j$.
Then, we add a vertex that represents $pja_j$, assuming it doesn't already exist. We add an edge between the vertex corresponding to $pja_{j-1}$ and $pja_j$.
This implies that the current action cannot appear before the one preceding it.
Once we went over all traces and constructed the graph, we compute a topological order
for it, giving us a sequence of PJAs $(\pi_1, a_1),..., (\pi_q, a_q)$, where the first instance corresponds to the action that appeared earliest.
\ronen{Also, the logic is not clear. How can the reward influence the order. You need to explain that because of the discount factor, there is motivation to perform actions with higher reward first.
Then how this translates into an actual number.}
\eliran{explained above}

\guy{Is the sequence really important? Isn't it sufficient to go over the DAG in reversed topological order? That is, do you consider the sequence, or just the parents in the DAG?}\eliran{Perhaps we can really replace the sequence with sets of PJAs, where each set represent a specific depth in the tree, though I'd like to think about it further and check whether it yields the same results before changing it}
Now, we begin by associating a reward for the last PJA in the sequence, namely the \emph{base} reward, and then iteratively calculate the reward for each preceding one.
The base reward is chosen so that applying the PJA will be beneficial in any possible scenario. We need to avoid a scenario where many costly actions are required to reach a state satisfying the context of the action, so that their total cost surpasses the reward, making it non-beneficial.

Let $MaxCost$ be the maximal negative reward that can be achieved in the problem. Let $MinSP$ be the minimal success probability of actions that yield negative rewards. Let $MaxTraceLength$ be the maximal trace length we produced, and let $sp_i$ denote the success probability of action $a_i$. We set $\epsilon > 0$ to some arbitrary positive scalar. We compute $r_q$, the base reward:
\begin{equation}
\label{eqn:rq}
   r_q = \frac{\sum_{i=1}^{MaxTraceLength-1}(\frac{MaxCost}{MinSP} \cdot \gamma^{i-1})}{\gamma^{MaxTraceLength}\cdot sp_q} + \epsilon
\end{equation}%
The numerator \guy{enumerator?}\eliran{sorry, numerator} is an upper bound for the expected discounted cost we would pay before applying the action. The denominator amplifies that cost to be beneficial when scheduled as the last action in the policy.
\eliran{In fact we can do something better but harder to explain, instead of using MaxTraceLength we can use the maximal number of actions that appear between consequent pairs of contexted actions, which will yield lower base reward as it is almost always lower. Also, we might need to divide MaxCost by MaxFailureProbability to really ensure the desired property}\guy{I suggest that you would change it as in your comment}

Equation~\ref{eqn:rq} ensures that the planner will always find the application of $a_q$ in the context $\pi_q$ beneficial, and thus will insert it to the policy. \eliran{should we explain the formula?}\ronen{yes}
\eliran{added in footnote}\guy{do not use footnotes}

After calculating the base reward, which is set as the reward of $(\pi_q, a_q)$, we can calculate the rewards associated with the rest of the PJAs in the sequence. To ensure that the order is maintained, using the same intuition as with the base reward, we compute the reward for $(\pi_i, a_i)$ so that the planner will always prefer to apply $a_i$ in $\pi_i$ before applying $a_{i+1}$ in $\pi_{i+1}$. 
An interesting cases arises when the failure probability of $a_i$ is small. Then, after applying the PJA once, it may be better for the planner to assume that the PJA succeeded and apply $a_{i+1}$, without any form of verification, causing the order to break.
\guy{I do not understand why we enforce "verification" - if it is beneficial to observe, the planner should do so, and if not, then not. Such "vitrification" should appear in the team plan to begin with, assuming that they are needed.}
\eliran{Even if the verification appears in the team plan, the artificial rewards we add to the public actions would cause the planner to postpone this verification to a later stage and violate the order, as it would be more beneficial for him to apply the public actions as soon as possible. I hope that later on we would find other ways to design the reward function so that the team plan behavior could be more naturally preserved.}
We set $r_i=\gamma \cdot \frac{r_{i+1}}{1-sp_i} \cdot sp_{i+1} + \epsilon$ \eliran{explain further?}
and by that we ensure that regardless of whether $a_i$ fails or not, it would still be beneficial to verify that and apply $a_i$ again if necessary, before applying $a_{i+1}$.

\guy{below - the intuition of why this is needed is unclear. We said above that we do not want the planner to opitmize the problem, possibly arriving at alternative solutions}\eliran{I'm not sure what you mean in not wanting to optimize the problem - you mean that we change the objective?. Anyway, The idea is to still give some weight to the original rewards, to balance between the new goal of applying the public actions correctly, to the old goal of optimizing the problem's expected reward. But as I wrote above, I'd prefer to omit it for this version as we don't use it for the box pushing anyway, and leave only the first term as the reward heuristic. I'll give this idea some more thought later on. What do you think?}
Having set the first term for each PJA, we add the second term. Given a PJA $(\pi_i, a_i)$, we scan the traces, and take the second term to be the average reward that was obtained under all applications of $a_i$ in a state that matches the context $\pi_i$. The second term's purpose is to embed the original rewards of the problem into the reward function, so that the resulting policy would still optimize the expected discounter reward to some extent.

%\eliran{commented below is the other case which generally doesn't happen, since the success probability are usually higher 0.5}
%There are two cases we need to consider:
% \begin{itemize}
%     \item Reordering of actions that appeared in-between $a_i$ and $a_{i+1}$ in the team solution, that might offer a more beneficial planning where $a_{i+1}$ precedes $a_i$
%     \item The failure probability of $a_i$ is small enough, such that after applying it once it would be better for the planner to assume it succeeded and apply $a_{i+1}$, without verifying it first.
% \end{itemize}
% To overcome these two difficulties, we provide a heuristic for handling each one of them, and then take the maximal result among the two. We denote by $sp_i$ the success probability of $a_i$:
% \begin{itemize}
%     \item Setting $r_i = r_{i+1} +     \frac{\sum_{i=1}^{MaxTraceLength-1}(MaxCost \cdot \gamma^{i-1})\cdot (1-sp_{i})}{\gamma^{MaxTraceLength}\cdot sp_{i}} $
%     \item Setting $r_i = \frac{r_{i+1}}{1-sp_i}$ 
% \end{itemize}



\subsection{Policy Adjustment and Alignment}

We run the planner on each of the single agent projections that we generate, constructing a set of single agent policies for the projection. We now adapt the policies to apply to the joint problem. First, the projection of agent $i$ contains actions of other agents that must be removed.

We now need to align the policies in order to synchronize different agent actions to occur in the same order as in the team plan. We need to ensure that an action $a_1$ of one agent that generates a precondition for an action $a_2$ of another agent would be executed before $a_2$. We also need to ensure that collaborative actions are executed at the same time by all participating agents. 

Given a policy graph of agent $i$, we consider the actions of any other agent as \emph{foreign} actions with respect to that policy graph. The policy adjustment process for agent $i$ can be divided into three steps:
\guy{I don't like the "pre-processing" "post-processing" - maybe avoid giving names to the steps?}\eliran{done}
\begin{enumerate}
    \item Remove all the \emph{private} actions of agents other than $i$ in the agent $i$'s policy graph.
    \item Insert \emph{no-op} actions into the graph to synchronize it, as much as possible, with other agents' policy graphs. This is the alignment procedure.
    \item Convert all the \emph{public} actions of  agents other than $i$ to no-ops.
    \item Handle potential \emph{collaborative} actions issue that might occur, in which slight unsynchronization leads the collaborating agents to enter an infinite loop. \guy{maybe split into two phases - turn public to no-op, and handle collaborative actions}\eliran{done}
%    \eliran{I though instead of writing "deadlocks" here, to just use the deadlock terminology later, and provided a more basic explanation}
\end{enumerate}
We now describe the steps in detail.

\guy{I think that the paragraph below adds nothing}\eliran{removed it. Not sure whether the start of the paragraph is elegant enough now though}
% Starting with the first step, we go over each of the raw single-agent policies, and eliminate every \emph{foreign} private actions. Since all coordination between agents can be determined by inspecting the public actions, we can perform this step prior to the alignment.

%\eliran{Next is the "motivation" paragraph we talked about}
Starting with the first step, we remove all the foreign private action from the policy graph, and proceed to the second step, which is the alignment procedure.

Each single-agent policy contains only a superficial plan for the other agents, if at all, and therefore its actions need to be aligned with respect to the true policies of other agents. 
\guy{It is not because of the policy graph, but because of the non-deterministic structure}\eliran{If we had a partial policy that is specified as a tree (like you had in the QDECPOMDP I suppose), we could essentially perform a perfect alignment with respect to that specific policy (yet not with respect to the problem itself). The cycles preventing us from performing a perfect alignment even to the policy itself. Am I missing something?}
As we are dealing with policy graphs rather than trees, cycles do exist, thus it is impossible to perform an alignment in which all action are perfectly coordinated. 
\guy{below is very imprecise - you say "minimal", but you do not guarantee minimality. You say "remain synchronized" but you only try to synchronize, and cannot be sure about it}\eliran{I agree that it is imprecise but in some sense we do postpone them to the minimal necessary, since we take the maximal path but assume no cycles. I changed it to something less strict though, if its worth elaborating on let me know}
% Instead, we expect the alignment to merely postpone actions to the minimal necessary time so that the agents remain synchronized with each other.
Instead, we expect the alignment to merely postpone actions in order to improve the coordination between the agents under various scenarios, yet without any guarantees.

\guy{I think that we had enough high level explanations above. I was expecting here an exact, algorithmic like, description. Something like "For each public action vertex $v$ in the policy graph, we identify..."}\eliran{Ronen suggested to have the algorithmic description only after we have everything explained in high-level. I'll talk to him about it}
We alter the graphs by inserting \emph{no-op} actions, so that corresponding public action vertices will appear in the same depth in all graphs, thus preventing the agents from applying actions prematurely.
To define what corresponding vertices are, we use the notion of an \emph{identifier} of a public action vertex. The identifier of a public action vertex in the policy graph is the
sequence of public actions that appear along the \emph{simple} path from the root to the vertex. \eliran{this is the easy version. in fact there can be several identifiers to each vertex, since there can be more than one path from the root to the vertex}
\ronen{In the conf version, we need to be more precise.
This is fine now.}\guy{I disagree. I would have preferred to see an exact description here.}

The sequence of public actions represents the team-relevant actions that took place prior to the current action, and serves as a summarization of the sequence of events. As the identifier is not agent-specific, it allows us to look for it in other policy graphs and determine which vertices represent similar \guy{identical?}\eliran{not necessarily, there may be intermediate private events or even public events that are not related to our agent.} sequences of events, namely corresponding vertices.

We now describe the algorithm in detail. Algorithm~\ref{} provides the high-level pseudo-code.
The alignment is an iterative process. We are given with policy graphs $G_1,...,G_n$ which are the output of the last iteration, and initially set to be the raw policy graphs. In each iteration, we align every graph with respect to all other graphs, and stop once convergence is reached. The process is applied iteratively as some alignment might need to propagate through iterations. 
\guy{This is not an example - you cannot ground it on some actual actions (say, box pushing). It adds nothing to the understanding. Again, an example here should be based on the running example.}\eliran{It might be hard to give a simple example that also requires more than one iteration. Hope I could find something}
For example, consider the case of three ordered agents, where each agent relies on the actions of his preceding agents. In the first alignment iteration, Agent 2 will delay its actions according to Agent 1's no-op requirements. Agent 3 would need to wait for the second alignment iteration before observing the proper time in which Agent 2's actions should be applied, and only then could delay its own actions accordingly. \eliran{should we explain why one iteration may not be enough?}\ronen{I think so. But briefly.}\eliran{done}
%For this criteria, We can use any graph similarity measurement, or just iterate until the set of graphs in two consecutive alignment rounds is identical.\eliran{I still didn't get to explore the convergence matter at all, but I assume there are certain graph similarity measurements after a quick look at the internet}


\guy{I don't think that out-edge is a standard term. Also, I think that this repetition of the well known concept of a policy graph is not needed.}\eliran{is outgoing edge better? though I'm pretty sure I've seen out-edge/in-edge here and there. Anyway I commented this part, I'll also ask Ronen about it tomorrow}
Recall that in a policy graph, each vertex represents an action and each edge represents an observation. From a vertex that represents a non-sensing action, there is a single out-edge that represents the \emph{null-obs} observation. From a vertex representing a sensing action there is one out-edge for each possible observation.

\guy{I am adding here a high level description of what I think is happening here - please check.}\eliran{so since we indeed take the maximal path, $a$ occurs at the same time as in the graph with the maximal path. But yea overall that's what's happening.}
We now describe the alignment of a single policy graph. For each public action $a$ in the policy graph $G_i$ of agent $i$, we identify this action in the graphs of other agents.
Then, we postpone $a$ by inserting no-op actions, so that $a$ occurs at the same time in all policy graphs, given the maximal path to $a$ in all policy graphs.



\guy{The description below would be much easier using mathematical notation. Something like "For each vertex $v \in G_i$ representing ...". This allows you to avoid "this vertex", "that vertex", "our graph".}\eliran{added new description}
We traverse $G_i$ using breadth-first search. For a vertex $v\in G_i$ representing a public action of agent $i$, we extract the $identifier$ of $v$. Having the identifier of $v$, we traverse all other graphs $G_j$, and in each we search for a vertex $v'$ that matches the identifier. $v'$ matches the identifier if there's a simple path from the root of $G_j$ to it that contains the identifier.
If we found such $v'$ that matches $v$, we set the {\em no-op} amount of $G_j$ that is required for $v$, denoted by $m_j$, to be the length of the \emph{longest simple path} from the root to $v'$. Else we set $m_j$ to 0.
Once we calculated $m_j$ for all $j\neq i$, we need to determine the final amount of {\em no-ops} added to $v$ in $G_i$. First, we consdier $max_{j\neq i}\{m_j\}$. However, the maximal requirement does not take into account {\em no-ops} that were already added to preceding vertices in $G_i$, or the depth of $v$ itself, and must be
corrected by subtracting from it a \emph{compensation term} which is the sum of the \emph{longest simple path} from the root to $v$ and the \emph{minimal} {\em no-op} amount added to any of the predecessors of $v$. Overall, we set the {\em no-op} amount of $v$ to be $max_{j\neq i}\{m_j\} - \mathit{CompensationTerm}$.

Having computed the  {\em no-op} amount for each of the public action vertices in $G_i$, we insert them to it by appending the required number of consecutive {\em no-op} vertices \emph{prior} to the public action vertex, thus postponing it.
As we mentioned, in each iteration of the algorithm we apply this procedure to all policy graphs, one by one, and halt upon convergence.



\eliran{up until here is the new explanation}
We add to each path to a vertex a sequence of preceding no-ops
that represent %public
actions of other agents that must be executed prior to this vertex.
For each public action vertex, we obtain a \emph{no-op requirement} from each of the other graphs, and compute a final requirement for that vertex.\guy{What exactly is a "no-op requirement" - the number of needed no-ops in that graph? Please define explicitly}\eliran{in the new paragraph}
We traverse our graph using breadth-first search. When we encounter a a public action vertex, we extract the \emph{identifier} of that vertex in the graph. 
%The \emph{identifier} is a representation of the vertex, that can be searched for in any policy graph. 
\eliran{I thought of changing the terminology of requirement to just counter/number/amount}\guy{amount}\eliran{done in the new paragraph}
We then traverse the policy graphs of all other agents, and in each one we look for a vertex that matches the identifier.
\eliran{In fact we would consider all vertices that match it in the graph, and stop the BFS expansion only for a matching vertex, then take the max among all matches. On the other hand there's usually only one vertex that matches so it might be better to keep it simple, it still reflects the main concept}
\ronen{I suggest to keep it simple for the first version. Later, we should be more precise.}
\guy{What does "a vertex that matches the identifier" mean? A vertex with the same action in the same context? Just the same action?}\eliran{in the new paragraph}
If we find a vertex that matches the identifier, we set the no-op requirement of that graph to be the length of the  \emph{longest simple path} from the root to that vertex. In case no vertex is found, we just set the requirement to 0, meaning the graph does not contain any actions that must precede our agent's current public action.
Having a no-op requirement from each of the other agents' graphs, we consider the \emph{maximal} requirement among them. 

\guy{I do not understand this. It seems to me that the simplest way to do this and avoid multiple iterations, is by running BFS on all graphs jointly. Whenever you encounter an action that needs no-ops, you add them in all graphs, and then move on. This way, you would not need to correct for anything later on.}\eliran{I tried to do something similar to this back then. I'll think about it again}
However, the maximal requirement does not take into account the {\em no-op} requirements for the preceding vertices, or the depth of the vertex itself, and must be
corrected by subtracting from it a \emph{compensation term} which is the sum of the \emph{longest simple path} from the root to our vertex and the \emph{minimal} {\em no-op} requirement of any of the predecessors of our vertex. Overall, we set the {\em no-op} requirement of our current vertex to be the maximal  {\em no-op} requirement we received from the other agents' graphs, minus the compensation term.


Having computed the  {\em no-op} requirement for each of our public action vertices in the graph, we insert them to the graph by appending the required number of consecutive  {\em no-op} vertices \emph{prior} to the public action vertex, thus postponing it.
As we mentioned, in each iteration of the algorithm we apply this procedure to all policy graphs, one by one, and halt upon convergence.
%, so we can avoid adding unnecessary \emph{no-ops} that would harm the solution quality.

After the alignment procedure has converged, we proceed to the the third step. Unlike the case of \emph{private} actions that were previously eliminated, \emph{public} actions of other agents were left in the graphs to guide the alignment process. Following the alignment, we convert them into \emph{no-ops}, and by that make the agent wait while other agents are performing their public actions.

%\eliran{Following is a detailed explanation about the deadlocks thing. I've written a shorter version afterwards, but I'm not sure whether its intuitive to understand the problem, though once the problem is understood the solution is quite straightforward.
%Also in second thought, I'm not sure the term deadlock fits because the agents don't wait for one another}
Finally, in the fourth step, we handle the problem of a potential ``livelock''
between \emph{collaborative} actions. Consider a scenario where two agents need to perform a non-deterministic collaborative action whose effect can be directly sensed. The action is costly so they must apply it the minimal necessary number of times. To do so, following every application, they perform a \emph{sensing} action that senses the effect of that collaborative action.
Given non-deterministic actions -- causing the alignment to be imperfect -- there might be scenarios in which the agents become unsynchronized. Then, they might be applying the collaborative and sensing actions in an alternating manner, where one agent performs the collaborative action while the other performs the sensing action, causing them to enter a livelock. To handle that, given a collaborative action with $n$ collaborating agents, we modify the graph so that every collaborative action that is part of a cycle is repeated by every agent for $n$ times instead of just once. This way, a livelock can never occur.

%\eliran{the shorter deadlocks explanation}
%Secondly, we need to handle the potential risk of agents getting into deadlocks as a result of unsychronized application of \emph{collaborative} actions. We do so by repeating every collaborative action that is part of a cycle for $n$ times, where $n$ is the number of collaborating agents in it, thus eliminating the possibility for deadlocks.

% As a result of that modification, we might encounter into cycles that contain a \emph{no-op} action as their entry point. For example - A public action of other action followed by our agent's sensing action. We want the \emph{no-op} action to be applied only once to express.

%After the process is finished, we are left with the final policies that can be now run in the decentralized environment.

\begin{algorithm}
\caption{Alignment Iteration}
\begin{algorithmic}[tbph]
\State Input: PolicyGraphs $G_1, ..., G_M$
\For{$G_i,  i\in\{1, ..., M\}$}
	\State {$\mathit{NoopsReqs} \gets \mathit{VertexToIntMapping}$}
      \State {$\mathit{CurrBFS} \gets \Call{BFS}{G_i}$}
      \While {$\mathit{CurrBFS.queue}$ is not empty}
	\State {$v \gets \mathit{CurrBFS.queue.pop}$}
	\State {$a \gets v.action$}
	\If {$a$ is public action}
	\State {$\mathit{identifier} \gets \Call{GetIdentifier}{v}$}
	\State {$\mathit{MaxNoop} \gets 0$}
	\For {$G_j,  j\in\{1, ..., M\}\setminus\{i\}$}
	\State {$\mathit{CurrNoop} \gets \Call{NoopReq}{G_j, \mathit{identifier}}$}
	\State {$\mathit{MaxNoop} \gets max(\mathit{MaxNoop}, \mathit{CurrNoop})$}
	\EndFor
	\State {$\mathit{NoopsReqs}[v] \gets \mathit{MaxNoop} - \mathit{CompensationTerm}$}
	\EndIf
	\EndWhile
	\State {$G_i' \gets \Call{AddNoops}{G_i, \mathit{NoopsReqs}}$}
\EndFor
\State {return $G_1', ..., G_M'$}
\end{algorithmic}
\end{algorithm}


% \guy{Removing private actions of other agents}

% \guy{Alignment}

% \guy{Removing public actions of other agents}

% \guy{Handling collaborative actions}

% Once we have the raw single-agent policies, $\{G_i\}_{i=1}^M$, we need to perform alignment. The requirement for alignment is rooted in the fact that each agent policy contains a very superficial plan for the other agents, if at all, therefore it needs to align it's actions with respect to the policies of other agents. Since we're dealing with policy graphs rather than trees, and cycles do exist, it's impossible to perform an alignment in which all action are perfectly coordinated (at least in terms of the graph only). Instead, we expect the alignment to postpone actions to the minimal necessary time so that the agents remain synchronized with each other.
% For each policy graph, we seek for a set of no-ops requirements that defines the number of required no-ops that should precede each instance of a public action in the graph. \\
% We traverse the graph in a breadth-first manner, and upon encountering a node that contains a public action, we extract the $identifier$ of that node in the graph. The identifier is a description of that node, that can be searched for in any other policy graph in the set $\{G_i\}_{i=1}^M$.\\
% Given the identifier, we extract a no-op requirement from each of the other policy graphs in the set, by trying to match the identifier to a node in the graph.\\
% Then, we take the maximum no-ops requirement found, and deduct a compensation term that consists of the actual depth of the node in the graph and the number of no-ops that were already added to any of its predecesors, in order to determine the exact addition of no-ops it needs to meet the no-ops requirement.\newline
% We perform the alignment iteratively until a convergence criteria is met, as the postponing of certain actions may require further postponing other actions.

% \begin{algorithm}
% \caption{Align}
% \begin{algorithmic}[tbph]
% \State Input: PolicyGraphs $G_1,..., G_M$
% \For{$G_i,  i\in\{1,..m\}$}
% 	\State {$NoopsReqs \gets EmptyMapping()$}
%       \State {$CurrBFS \gets \Call{BFS}{G_i}$}
%       \While {$CurrBFS.queue$ is not empty}
% 	\State {$CurrNode \gets CurrBFS.queue.pop()$}
% 	\State {$Action \gets CurrNode.action$}
% 	\If {$Action.IsPublic()$}
% 	\State {$ID \gets \Call{GetIdentifier}{CurrNode}$}
% 	\State {$MaxNoop \gets 0$}
% 	\For {$G_j,  j\in\{1,..M\}\setminus\{i\}$}
% 	\State {$CurrNoop \gets \Call{NoopReq}{G_j, ID}$}
% 	\State {$MaxNoop \gets max(MaxNoop, CurrNoop)$}
% 	\EndFor
% 	\State {$NoopsReqs[CurrentNode] \gets MaxNoop - CompensationTerm$}
% 	\EndIf
% 	\EndWhile
% 	\State {$AG_i \gets \Call{AddNoops}{G_i, NoopsReqs}$}
% \EndFor
% \State {return $AG_i, i\in\{1,...M\}$}
% \end{algorithmic}
% \end{algorithm}

% As agents make some planning on behalf of their co-agents, each policy graph $G_i$ contains actions of agent $i$'s co-agents, which are not valid as part of a policy that can be run solely by agent $i$, in the Dec-POMDP problem. Hence, we apply the PostProcess procedure that aims at removing these actions. We peform this by traversing the policy graph, and if we encounter a node that represents a "foreign" action, namely an actions that belongs to some other agent, we first convert it into a NoOp. Next, if such node participates in a cycle, we want to remove it from the cycle and connect it's two adjacent nodes with respect to that cycle. 
% Finally, in order to prevent deadlocks when collaborative actions appear in cycles, we inject repetitions of collaborative actions that participate in cycles, according to the number of agents that collaborate that specific action.

% \eliran{pseudocode-part3}
% \begin{algorithm}
% \caption{ProcessAndAlign}
% \begin{algorithmic}[tbph]
% \State {}
% \end{algorithmic}
% \end{algorithm}

\section{Empirical Study}






\section{Related Work}
QDec-POMDPs~\cite{} are qualitative version of Dec-POMDPS that tackle a conceptually simpler, more structured, model. In QDec-POMDPs 
non-determinism replaces stochastic uncertainty. The model is factored (i.e., described at the level of state variables rather than states), and actions 
are described using preconditions and non-determinstic effects. Although QDec-POMDPs are also NEXT-Time hard, recent work in the area that leverages heuristic-search planners, has been able to scale up to much larger domains (e.g., box pushing on a grid of size 24,  12 agents and 12 boxes, implying a state space of $24^24$)
albeit, under the assumptions that actions are deterministic.

Dec-POMDP algorithms.

\section{Conclusion}

\section{Future Research}
\eliran{its more of a sketch for now so we don't forget what we've talked about}
There are two directions in which the solver can be improved -  scalability and solution quality.
The use of online planners instead of offline ones can greatly improve the scale of solvable problems. The changes in terms of algorithm architecture are minor, as we merely need to be able to produce the single agent policy graphs using an online solver. \ronen{In fact, it seems we could use an RL algorithm here to generate a policy for each agent, as we can simulate as many traces as we wish.
In fact, we could use the RL algorithm to solve the team POMDP. This would generate the needed traces as well. We would learn incrementally
both the team solution and the single-agent solution. Using this idea, we could probably scale up to very large problems. In fact, we can use this
approach to do MA RL. If we can do the projections. What we need is a simulator that let's us control all the agents at once.}
In terms of solution quality, we would want to use more principled methods of reward shaping, that come from the worlds of reinforcement learning, in order to define the reward heuristic. We would want to achieve both the properties we already achieved using our current heuristic, and to still be able to optimize the expected discounted reward of the problem.
Also, in order to fully achieve that, the alignment algorithm will need to include some form of confidence aspect, where we no longer ignore cycles but rather look at non-simple paths and try to increase the certainty about the world's state. 

\bibliography{Bibliography-File}
\bibliographystyle{aaai}
\end{document}
