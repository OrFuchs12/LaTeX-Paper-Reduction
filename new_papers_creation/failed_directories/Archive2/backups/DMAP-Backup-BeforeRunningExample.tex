\def\year{2020}
%File: main.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai20}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{subfig}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{graphicx}  % DO NOT CHANGE THIS
\usepackage{xcolor}
\usepackage{amssymb}

\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS

\newtheorem{example}{Example}

\newcommand{\commentout}[1]{}
\newcommand{\eliran}[1]{\textbf{[\color{red}ELIRAN:#1]}}
\newcommand{\ronen}[1]{\textbf{[\color{blue}RONEN:#1]}}
\newcommand{\guy}[1]{\textbf{[\color{orange}GUY:#1]}}
%\newcommand{\begin{example}}[1]{\textbf{[\color{purple}RunningExample:#1]}}

\newcommand{\cbp}[0]{Collaborative Box-Pushing}
\newcommand{\mitg}[0]{Meet In The Grid}
\newcommand{\crs}[0]{Cooperative Rock-Sampling}
\newcommand{\macor}[0]{Multi-Agent Corridor}

\newcommand{\cact}[1]{{\em CActions$_#1$}}
\newcommand{\pcact}[1]{{\em \textit{PCA$_#1$}}}

\newcommand{\Tau}{\mathrm{T}}

%\nocopyright
%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
 \pdfinfo{
/Title (A Factored Approach To Solving Dec-POMDPs)
/Author (Eliran Abdoo, Ronen I. Brafman, Guy Shani)
} %Leave this	
% /Title ()
% Put your actual complete title (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case
% Leave the space between \Title and the beginning parenthesis alone
% /Author ()
% Put your actual complete list of authors (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case. 
% Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands, 
% remove them. 

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{caption} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \natbib} -- This package is specifically forbidden -- use the following workaround:
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai20.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\setlength\titlebox{2.5in} % If your paper contains an overfull \vbox too high warning at the beginning of the document, use this
% command to correct it. You may not alter the value below 2.5 in
\title{A Factored Approach To Solving Dec-POMDPs }
%Your title must be in mixed case, not sentence case. 
% That means all verbs (including short verbs like be, is, using,and go), 
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\author{Eliran Abdoo, Ronen I Brafman, Guy Shani}
\begin{document}

\maketitle

\begin{abstract}
Dec-POMDPs model planning problems under uncertainty and partial observability for a distributed team of cooperating agents planning together but executing their plans in a distributed manner. This problem is
very challenging computationally (NEXP-Time Complete) and consequently, exact methods have difficulty scaling up. In this paper
we present a heuristic approach for solving certain instances of Factored Dec-POMDP that tries to reduce the problem of planning in Dec-POMDPs to multiple
problems of planning in a POMDP. First, we solve a team version of the Dec-POMDP in which agents have a shared belief state, and then, each agent attempts to solve the problem of executing its part of the team plan. Finally, the different solutions are aligned to improve synchronization. Using this approach we are able to solve larger Dec-POMDP problems, limited only by the abilities of the underlying POMDP solver.
\end{abstract}




\section{Introduction}
Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are the most popular model for planning in stochastic environments under uncertainty with partial observability by a distributed team of agents~\cite{}. In this model, a team of agents attempts to maximize the team's cumulative reward where each agent has only partial information about the state of the system during execution. That is, each agent is aware of its own observations only. Communication is possible only through explicit communication actions, if these are available.  

To achieve their common goal agents must coordinate their actions in two ways: First, as in single agent problems, actions must be coordinated sequentially. That is, current actions must help steer the system towards states in which greater reward will be possible,
e.g., to be rewarded for shipping a product, it must first be assembled. 
Second, agents may need to coordinate their simultaneous actions because their effects are dependent, e.g., a heavy box can only be pushed if two agents push it simultaneously. 

As with previous work in this area, our focus is on centralized off-line planning for distributed execution. That is, offline, a solver with access to the complete model must generate a policy for each agent. An agent's policy specifies which action must be taken as a function of the agent's history of
actions and observations. Such policies can be represented by a {\em policy graph} where nodes are labeled by actions, and edges are labeled by observations. Online, each agent executes its own policy independently of the other agents.
The difficulty lies in generating policies that provide sufficient coordination, even though each agent may make different observations at run-time, and so each agent's beliefs over which states are possible are typically different. 

Dec-POMDPs are notoriously hard to solve -- they are NEXP-Time hard~\cite{}, implying that only the smallest toy problems are optimally solvable.
However, many approximate methods for solving Dec-POMDPs have been proposed, with steady progress. Some of these methods generate solutions with bounds on
their optimality~\cite{}, and some are heuristic in nature~\cite{} \eliran{citations GMAA-ICE, DICEPS, JESP(heuristic), MBDP?}. However, current methods typically do not scale to state spaces with more than a few hundreds of states.


In this paper we describe a heuristic approach for solving Dec-POMDPs that scales up to much larger state spaces. The key idea is to solve a Dec-POMDP by
solving multiple POMDPs. First, we solve a POMDP obtained by assuming that all agents have the same belief state. That is, that any observation by one agent is immediately available to the other agents. We refer to this as the {\em team POMDP}. The solution of the team
POMDP can be represented by a policy graph --- the {\em team policy graph.} . It provides us with a skeleton for the solution of the Dec-POMDP, specifying what each agent needs to provide for the team. Naturally, this policy is not executable by the agents, because agents cannot condition their actions on the observations of other agents in the real world.
%
Hence, in the next stage, we let each agent solve a POMDP in which it is rewarded for behaving in a manner similar to the specification in the team policy. This leads to the generation of a policy tree for each agent. 
These policy trees are not well synchronized with each other, and in the last step, we synchronize them by delaying the actions of agents to
improve the probability of good coordination. 

We implemented our algorithm and tested it on several configurations of a benchmark problem \cbp which is a variation of the Cooperative Box Pushing problem. We show that the algorithm manages to scale well beyond
current Dec-POMDP solvers.
One of the main properties of the domain, is that agents policies are only loosely coupled. That is, the need for actions that affect state components that are relevant to all agent, is sparse. That sparsity allows for each agent to independently construct a plan that consists mostly of its own private actions without requiring it to consider the other agents' behavior. This allows us to achieve good decentralized policies even when achieving the goal requires many steps, compared to planning directly over the Dec-POMDP model.

\section{Background}

In this section we provide needed background on POMDPs, Dec-POMDPs, their factored representation,
and policies. We also introduce the concept of private and public variables and actions in Dec-POMDPs.

\subsection{POMDPs}


A POMDP is a model for single-agent sequential decision making under uncertainty and partial observability.
Formally, it is a tuple $P=\langle S, A, T, R, \Omega, O, \gamma, h, b_0 \rangle$, where:
\begin{itemize}
\item
$S$ is the set of states. The future is independent of the past, given the current state.
\item
$A$ is the set of actions. An action may modify the state and/or
provide information about the current state.
\item
$T: S \times A \rightarrow \prod(S)$ is the state transition function.  $T(s, a, s')$ is the probability of transitioning to $s'$ when applying $a$ in $s$. 
\item
$R:S \times A \times S \rightarrow \mathbb{R}$  is the immediate reward function. $R(s,a, s')$ is the reward obtained after performing $a$ in $s$  and reaching $s'$. 
\item
$\Omega$ is the set of observations. An observation is obtained by the agent following an action, and provides some  information about the world.
\item
$O:S \times A \rightarrow \prod (\Omega)$ is the observation function, specifying the likelihood of sensing a specific observation following an action. $O(s', a, o)$ is the probability of observing $o\in \Omega$ when performing $a$ and \emph{reaching} $s'$. 
\item
$\gamma \in (0,1)$ is the discount factor, quantifying the relative importance of immediate rewards vs.~future rewards.
\item
$h\in\mathbb{N}\cup\{\infty\}$ is the planning horizon --- the amount of actions that an agent executes before terminating. 
% \eliran{Open issue - included infinity as our algorithm outputs a policy for infinite horizon, where we state this?}
% \ronen{Do we really plan for an infinite horizon? Perhaps you mean unbounded horizon?}\eliran{I'm not sure actually. The policies we output are for infinite horizons (there's an edge for every possible observation of the node's action), but perhaps it is because we plan for enough steps so that the policy graph closes, and then it might be for an unbounded horizon. I'll look into it}
%\eliran{we said it can be considered as "unbounded horizon", should we drop the $\infty$ sign?}

\item
$b_0\in \prod(S)$ is a distribution over $S$ specifying the probability that the agent begins the execution in each state.
\end{itemize}

We assume that agent actions are either sensing actions or non-sensing actions. An agent that applies a non-sensing action always receives the observation {\em null-obs}. In addition, we assume for simplicity that every action has 
%a success probability, that is, it has 
an effect 
%with constant probability 
that we consider as the successful outcome, while all other effects are considered failures. We will explain how this assumption can be omitted in the relevant parts.

Often, the state space $S$ is structured, i.e., it consists of assignments to some set of variables $X_1,\ldots X_k$, and the observation space $\Omega$ is
also structured, consisting of a set of observation variables $W_1,\ldots, W_d$. 
Thus, $S=Dom(X_1)\times\cdots\times Dom(X_k)$ and
$\Omega = Dom(W_1)\times\cdots\times Dom(W_d)$. 
In that case, $\tau$, $O$, and $R$ can be represented compactly by, e.g., using a dynamic Bayesian network~\cite{}. Formats such as RDDL~\cite{} and POMDPX~\cite{} exploit factored representations to specify POMDPs compactly.

\begin{example}
As an example, we can think of a simple Box-Pushing 1 by 2 grid, where there is an agent and a box in the left tile. The left tile is marked by \emph{L} and the right tile by \emph{R}. The agent can either move, sense its current tile or push a box from its current tile. Both move and push can be done in any direction - left and right. The agent's goal is to push the box to the right tile.
The state is composed of 2 variables: the location of the agent and the location of the box. Each variable can take one of two values: \emph{L} or \emph{R}.
The sense action returns an observation telling whether there's a box in the agent's tile, while the move and push actions are non-sensing actions always returning {\em null-obs}.
The push action has a success probability of 0.8.
\end{example}

A solution to a POMDP is called a {\em policy}. In general, a policy assigns to each history of actions and observation ({\em $AO$-history}) the next action to execute. 
Such a policy is often represented using a {\em policy tree} or, more generally, a {\em policy graph} (also called a finite-state controller). 
A policy graph $G=(V,E)$ is a directed simple graph, in which each vertex is associated with an action, and each edge is associated with an observation.
%
For every edge $v\in V$ and every observation $o\in\Omega$ exactly one edge emanates from $v$ with the label $o$.
The graph has a single root which acts as its entry point. Every $AO$-history $h$ can be associated with some path from the root to some vertex $v$,
and the action labelling $v$ is the action that the policy associates with $h$.

Finally, a policy graph can be run on the problem and produce an execution trajectory (trace for short). A trace $T$ of length $l$ is a sequence of quintuplets $e_i = (s_i, a_i, s'_i, o_i, r_i)$, namely \emph{events}, that occurred during a possible policy execution where: $s_i$ is a state in step $i$ and $s_0$ is the initial state;
$a_i$ is the action taken in step $i$;
    $s'_i$ is the result of applying $a_i$ in $s_i$;
    $o_i$ is the observation received after taking $a_i$ and reaching $s'_i$; and is the reward received for taking the $a_i$ in $s_i$ and reaching $s'_i$. Naturally, $\forall i$ such
    that $0\leq i \leq l-1$, we have $s'_i=s_{i+1}$.

\eliran{examples for traces and policy graphs will come later on, in the trace extraction and alignment section respectively
}

\subsection{Dec-POMDP}

A Dec-POMDP extends POMDP to problems where there are $n$ acting agents for some $n>1$. 
These agents are part of a team, sharing the same reward, but they act in a distributed manner,
sensing different observations. Thus, their information state can be different. 
Formally, a Dec-POMDP for $n$ agents is a tuple  $P=(S, A=\bigcup_{i=1}^{n}{\{A_i\}}, T, R, \Omega=\bigcup_{i=1}^{n}{\{\Omega_i\}},  O, \gamma, h, {\{I_i\}}_{i=1}^{n})$, where:
\begin{itemize}
\item
$S,\gamma,h,b_0$ are defined as in a POMDP.
\item
$A_i$ is the set of actions available to agent $i$. We assume that $A_i$ contains a special {\em no-op} action, which does not change the state of the world, and does not provide any informative observation. 
$A=A_1 \times A_2 \times .. \times A_n$ is the set of joint actions. On every step each agent $i$ chooses an action $a_i \in A_i$ to execute, and all agents execute their actions jointly. $\langle a_1,...,a_n \rangle$ is known as a joint action. We often treat the
single-agent action $a_i$ as a joint-action, with the understanding that it refers to the joint-action $\langle$no-op,$\ldots,a_i,\ldots,$no-op$\rangle$
\item
$T:S \times A \rightarrow \prod(S)$  is the transition function. Transitions are specified for joint actions, that is, $T(s, \langle a_1,...,a_n \rangle, s')$ is the probability of transitioning from state $s$ to state $s'$ when each agent $i$ executes action $a_i$.
\item
$R:S \times A \times S \rightarrow \mathbb{R}$  is the reward function. Rewards are also specified over joint actions.
\item
$\Omega = \Omega_1 \times \Omega_2 \times .. \times \Omega_n$ is the set of joint observations. We assume $\Omega_i$ contains a special {\em null-obs} observation, which is the observation received when applying a non-sensing action.
\item
$O:S \times A \rightarrow \prod_{i=1..n}(\Omega_i)$  is the observation function, specified over joint actions. $O(s',\langle a_1,...,a_n \rangle,\langle o_1,...,o_n \rangle)$ is the probability that when all agents execute $\langle a_1,...,a_n \rangle$ jointly and reach $s'$, each agent $i$ observes $o_i$.
\item
$\gamma$  is the discount factor.
\item
$h\in\mathbb{N}\cup\{\infty\}$ is the horizon.
\item
$b_0 \in \prod(S)$ is a distribution over $S$ specifying the probability that each agent begin its execution in each state. In principle, different agents may have different initial belief states, but
we make the (common) assumption that the initial belief state is identical. 
\end{itemize}

\begin{example}
We now take the previous example and convert it to a Dec-POMDP by adding an agent and a box to the right tile as well. The agents are denoted by \emph{Agent1} and \emph{Agent2} and the boxes by \emph{Box1}, and \emph{Box2}. The goal of each agent is to push its box to its fellow agent.
\end{example}

As in the case of POMDPs, Dec-POMDPs can also be represented in a factored manner \cite{}, although most work to date uses the flat-state representation \cite{}.\eliran{citations}
We add the notion of \emph{observation variables}, which are the variables that take the observation value of each agent following an action. Each observation variable is denoted by $\omega_i$ which takes values in $\Omega_i$, and represents the observation of agent $i$.

\begin{example}
In our example, the state is now composed of 4 state variables: the location of each box -- $(X_{B1}, X_{B2})$ -- and the location of each agent -- $(X_{A1}, X_{A2})$. In addition, there are two observation variables -- $(\omega_1, \omega_2)$.
\end{example}

An important element of a factored specification of Dec-POMDPs is a compact formalism for specifying joint-actions. If there are $|A|$ actions in the domain, then, in principle, there are $O(|A|^n)$ possible joint actions. Specifying all joint actions explicitly is unrealistic for large domains. 

In practice, we may expect
that most actions will not interact with each other. A pair of actions $a\in A_i$, $a' \in A_j$ is said to be non interacting, if their effects when applied jointly (in the same joint action) is the union of their effects when applied separately.
Thus, our specification language focuses on specifying
the effects of single-agent actions and specific
combinations of single-agent actions that interact with each other, which we refer to as {\em collaborative} actions~\cite{}. For a more detailed discussion of the issue
of compact specification of joint-actions, see~\cite{}. 

\begin{example}
We alter our example further by introducing a collaborative action. To do so, we need to convert one of the boxes to a "heavy" box - a box that requires both agents to push it. We will convert \emph{Box1} to such box. Both agents now have also the option to apply a \emph{collaborative-push} action in any specified direction. If both agents apply that action to push \emph{Box1} while in the same tile with it, the box will transit.
\end{example}

Finally, a solution to a Dec-POMDP is a set of policies $\rho_i$, one for each agent. It maps action-observation sequences of this agent to actions in $A_i$.
As in POMDPs, these policies can be specified using a policy graph for each agent. The policy graph for agent $i$ associates nodes with actions in $A_i$
and edges with observations in $\Omega_i$. 
%\eliran{add an example decentralized policy graph? I think it could be fairly understood when showing the policies in the alignment part, the idea is very straightforward after all, two graphs that are performed simultaneously}

\subsection{Public and Private Actions and Variables}

Public variables are state variables that several agents manipulate directly, while private variables are manipulated by a single agent only. The concept of \emph{private} and \emph{public} (or \emph{local} and \emph{global}) variables was introduced by
Brafman and Domshlak~\cite{} in the context of their work on factored planning. This concept has been used extensively
in work on privacy-preserving multi-agent planning (e.g., \cite{}) and, more recently in work on solving qualitative variants of Dec-POMDPs~\cite{}. As we are building on ideas in this latter work, we now explain how we extend them to the
case of factored Dec-POMDPs. It is important to note that the definitions below are making some simplifying assumption and should be viewed as heuristics for helping our algorithm focus on relevant variables. These definitions are based on the notions of
preconditions and effects, as used in classical planning formalisms.



Let $a\in A_i$ be an action of agent $i$. As noted, we identify $a_i$ with the joint action $(\mbox{no-op},\ldots, a_i,\ldots,\mbox{no-op})$.
We say that a state variable $X_i$ is an {\em effect} of $a\in A_j$ if there is some state $s$ for which there is a positive probability that the value 
$X_i$ changes following $a$.
Similarly, we say that observation variable $\omega_i$ is an {\em effect} of $a\in A_j$ if there exists a state $s$ such that there is a positive probability
of observing $\omega_i \neq$ {\em null-obs} when $a$ is executed and $s$ is reached. We denotes the effects of $a$ by $eff(a)$.

We say that a state variable $X_i$ is an
{\em influencer} of $a$, if there are two states $s_1,s_2$ that differ only in the value of $X_i$ such that $R(s_1,a,s')\neq R(s_2,a,s')$ or $T(s_1,a,s')\neq T(s_2,a,s')$ for some state $s'$, or $O(s_1,a,o)\neq O(s_2,a,o)$ for some observation $o$.
We denote the influencers of $a$ by
$inf(a)$.
We refer to the union of the influencers and effects of $a$ as
the {\em relevant} variables of $a$,
denoted $rel(a)$.
\ronen{This is problematic. Perhaps this is related to the footnote. For example, suppose that p influences a only if q has a certain value, and similary for q. That is, the effect of a changes only if both p and q are true.}
\eliran{Why is that problematic? If this is the case, then on states where we fix q to be true, we can see the effect of $a$ when transitioning between p=false and p=true, and vice versia.}

%We associate with each action $a$ the set of variables it can affect or it is affected by, which we refer to as  {\em context}$(a)$. The effect could take place through the transitions, rewards, or observations associated with $a$. {\em context}$(a)$ is the union of two sets, the preconditions of $a$ denoted by {\em pre(a)} and the objectives of $a$ denoted by {\em obj(a)}. 

As explained above, we expect that most actions do not interact. In that case, it is straightforward to get the post-action distribution for their combination from the specification of the single-agent actions they contain. But some actions may behave differently when applied jointly with other actions. We refer to a combination of single-agent actions whose conditional effects are not the union of the single-agent actions as $\emph{collaborative}$ actions.

We say that $X_i$ is an effect of a collaborative action $a$ that consists of single agent actions
$a_{i_1},\ldots,a_{i_k}$ (and {\em no-ops} for the non-collaborating agents) if $X_i$ is an effect of $a$, as defined above, if $X_i$ has a positive probability of changing following an application of $a$.
We also say that $X_i$ is an influencer of $a$ if there are two states $s_1,s_2$ that differ only in the value of $X_i$ such that $R(s_1,a,s')\neq R(s_2,a,s')$ or $T(s_1,a,s')\neq T(s_2,a,s')$ for some state $s'$, or $O(s_1,a,o)\neq O(s_2,a,o)$ for some observation $o$.
\eliran{Updated -In second thought it seems quite simple. In term of effects and influencers, we only care about variables that fit the same definitions as with the single action, regardless of the single actions themselves. A variable can be an influencer or an effect of one of the single action but not of the collaborative actions, and vice versia. This is due to the fact that we don't define a collaborative action's effect through its factors nor defining its factors' effects as a derivation of its own effect.}
Finally, the {\em relevant} variables of a collaborative actions $a$ is the union of its influencers and effects.
\ronen{Why are the effects of a collaborative action only those that are not of its single-agent actions? It is also possible that  some other variable will affect a collaborative action}
%
\ronen{If the picture is more complicated, then we need to say something. I do think there are complications.As I noted above}

\footnote{A complete treatment of the subtleties of this issue is beyond the scope of this paper. The above definition will be sufficient for our purpose.}

We say that a variable $X_i$ is an \emph{effect} of agent $j$, denoted,
$eff(j)$, if  $X_i\in eff(a)$ for some $a\in A_j$.
%\eliran{note the addition,in the code relevants are only objectives}

We can now define the concept of {\em private} and {\em public} variables. $X_i$ is {\em private} to agent $j$ if $X_i$, if it an effect of agent $j$ but it is not an effect of any agent $k\neq j$. If $X_i$ is an effect of more than one agent, we say that $X_i$ is {\em public}. 
An action $a\in A_j$ is public if at least one of its relevant variables, $X_i$ is public. A collaborative action is always public.

\begin{example}
In our running example we have that:
\begin{itemize}
    \item $X_{B1}$ and $X_{B2}$ are both public variables, as they are effects of both agents' push actions.
    \item $X_{A1}$, $X_{A2}$ are private variables of $Agent1$ and $Agent2$ respectively, since they are the effects of each respective agent's move actions.
    \item The same holds for $\omega_1$ and $\omega_2$ with respect to the sensing actions.
    \item move and sense actions are private actions, while the push actions are public.
\end{itemize}
\end{example}

\section{FDMAP - Factorized Distributed MAP}

We now describe our approach for producing policy graphs for agents acting in a Dec-POMDP. First, we generate a team solution. Next,
we generate single agent policies in which each agent attempts to fulfill its part in the team solution. Finally, these policies are synchronized. 

From the input factored Dec-POMDP problem $P$, we first generate the team POMDP $P_{team}$. $P_{team}$ is identical to $P$, ignoring the underlying multi-agent structure. That is, the actions are the joint actions and the observations are the joint observation, viewed as applied 
and observed by a single agent. Equivalently, this can be viewed as a Dec-POMDP in which all observations are communicated accurately and instantaneously 
to all agents.

We solve $P_{team}$ using an off-the-shelf POMDP solver --
we used SARSOP \cite{} \eliran{citation} -- and output the team policy. We then use the team policy to produce traces, which are simulations of the team policy over the team problem. With the traces at hand, we start projecting the team problem with respect to each agent.

First, for each agent, we extract from the traces a set of public actions and the context in which they were applied, which we call a {\em contexted actions}. Then, we associate a reward with each such contexted action.  Using these contexted actions and their rewards, together with the factored Dec-POMDP, we  generate
one single-agent problem for each agent. The dynamics of each single-agent problem is similar to that of the Dec-POMDP, except that some variables are projected away. The reward associated with the contexted action is designed
so that agents will be rewarded for acting in a manner similar to its
behavior in the team solution.

Finally, we process the single-agent policies and align them to try and ensure that actions are properly synchronized when they are executed in a decentralized manner. 
%The high-level pseudo-code is described below. We described the first step (generating $P_{team}$) above. 
In the rest of this section we explain the steps that follow the
generation of the team solution in more detail.

%\begin{algorithm}
%\caption{GenerateAgentPolicies \eliran{keep this?}}
%\begin{algorithmic}[tbph]
%\State Input: $P$, $\alpha$, $p_{team}$
%\State $P_{team} \gets \Call{Centralize}{P}$
%\State {\em Traces} $\gets \Call{ProduceTraces}{P_{team}, \alpha, p_{team}}$
%\State {\em RawSAPolicies}$ \gets \Call{ProjectAndSolve}{P_{team},{\mathit %Traces}}$
%\State {\em SAPolicies} $\gets \Call{ProcessAndAlign}{\mathit{RawSAPolicies}}$
%\State {return {\em SAPolicies}}
%\end{algorithmic}
%\end{algorithm}

\subsection{Producing the Traces}

Having generated the team problem, $P_{team}$, we 
must specify two hyper-parameters: a confidence parameter $\alpha$ and a precision parameter $\epsilon_{team}$. We generate a $\epsilon_{team}$-optimal solution
to $P_{team}$ using an off-the-shelf POMDP solver.
Then we generate sufficiently many traces so that the distance between the
empirical distribution of initial states in the traces and the initial belief state is less then $\alpha$,
as measured using ???.
%An $\epsilon$-optimal solution is a solution whose value differs by at most $\epsilon$ from the value of an optimal solution.
%Both parameters should be specified by the user, where $\epsilon_{team}$ should be small enough so that the team policy achieves the user's requirements - the final decentralized policy can only be as good as the team policy.

\eliran{Open issue - capturing every initial state is both very heuristic and quite weak -- should we change that?}
\ronen{I agree. I think what you want is that the distance between the
distribution of initial states in your trace and the initial belief state is
sufficiently small. E.g., in terms of KL divergence or some other distance measure. This is related to the error of estimating a multinomial distribution by sampling. I think the bound you are using can be adjusted to that,
e.g., some norm.}
\eliran{I think that the problem here is not only about producing enough traces to capture the initial state correctly, but to also produce enough so that we capture all possible scenarios, an issue which is not tied only to the number of possible initial distributions - but we can extract some measure from the team policy graph for it. For now perhaps we could do with the initial state matter, so developing your idea - if we were to do that without a predetermined bound we start by sampling an initial number of states and construct their distribution to define an initial distribution, then we iteratively sample more traces and update the distribution, and stop when the KL divergence between the resulted distribution and the initial belief state are close enough. Is that correct?}
Now transforming it t
For that, it is sufficient to select the number of traces $n_t$ to be such that $\sum_{s\in supp(b_0)}(1-Pr(b_0=s))^{n_t} \leq 1-\alpha$.
Let $E$ denote the event in which we cover all possible initial states, $s \in supp(b_0)$, in $n_t$ traces. Then we want to pick $n_t$ such that $Pr(E)\geq \alpha$, using the union bound we get: $Pr(E)\geq 1-\sum_{s\in supp(b_0)}(1-Pr(b_0=s))^{n_t}$.

%\begin{algorithm}
%\caption{ProduceTraces \eliran{keep this?}}
%\begin{algorithmic}[tbph]
%\State Input: $P_{team}$, $\alpha$, $\epsilon_{team}$
%\State {\em{TeamPolicy} $\gets \Call{POMDPSolver}{P_{team}, \epsilon_{team}}$}
%\State {$n_t \gets \Call{NumTracesRequired}{P, \alpha}$}
%\State {\em{Traces} $\gets \Call{Simulate}{\mathit{TeamPolicy}, n_t}$}
%\State {return \em{Traces}}
%\end{algorithmic}
%\end{algorithm}

\begin{example}
In our example, solving the team problem would result in a very simple policy graph. 
$Agent2$ first pushes $Box2$ to the left, making sure the push has succeeded using the sensing action. It then moves left to assist $Agent1$ to push $Box1$ to the right.

Next, we use the policy graph to produce the traces. Different
traces will differ by the number of pushes each agent performs until success.
Suppose we generated the following two traces, where odd numbered elements
denote the state, agent1's action, and agent2's action, and even numbered
elements denote the two agents' observation. Recall that the state is composed of 4 state variables: $(X_{A1}, X_{A2}, X_{B1}, X_{B2})$, where each variables can take values in \emph{(L, R)}. \emph{CPush} stands for Collaborative-Push. We will use these 

\paragraph{Trace 1}
\begin{enumerate}
    \item \emph{(L,R,L,R),IDLE,Agent2-Push-Left-Box2}
    \item \emph{null-obs, null-obs}
    \item \emph{(L,R,L,L),IDLE, Agent2-Sense-Box2}
    \item \emph{null-obs, no}
    \item \emph{(L,R,L,L),IDLE, Agent2-Move-Left}
    \item \emph{null-obs, null-obs}
    \item \emph{(L,L,L,L),Agent1-CPush-Right-Box1,Agent2-CPush-Right-Box1}
    \item \emph{null-obs, null-obs}
    \item \emph{(L,L,R,L),Agent1-Sense-Box1, IDLE}
    \item \emph{no, null-obs}
\end{enumerate}
\paragraph{Trace 2}
\begin{enumerate}
    \item \emph{(L,R,L,R),IDLE , Agent2-Push-Left-Box2}
    \item \emph{null-obs, null-obs}
    \item \emph{(L,R,L,L),IDLE, Agent2-Sense-Box2}
    \item \emph{null-obs, no}
    \item \emph{(L,R,L,L),IDLE, Agent2-Move-Left}
    \item \emph{null-obs, null-obs}
    \item \emph{(L,L,L,L),Agent1-CPush-Right-Box1,Agent2-CPush-Right-Box1}
    \item \emph{null-obs, null-obs}
    \item \emph{(L,L,L,L),Agent1-Sense-Box1, IDLE}
    \item \emph{\textbf{yes}, null-obs}
    \item \emph{(L,L,L,L),Agent1-CPush-Right-Box1,Agent2-CPush-Right-Box1}
    \item \emph{null-obs, null-obs}
    \item \emph{(L,L,R,L),Agent1-Sense-Box1, IDLE}
    \item \emph{no, null-obs}
\end{enumerate}
\end{example}
\eliran{talk about single action-variable problem}


\subsection{Extracting Contexted Actions}
We seek a policy for each agent that will cause it to behave a manner that is similar to how it would behave in the team plan.
 That is, the agent should execute the same public actions it executes in the team plan and in the same contexts. To generate such a policy,
 we must define an appropriate reward function for each agent.
 But first, we must define the concept of the context in which an action is executed, better.
% We must define appropriate conditions for rewarding the agent when it executes an action from the team plan, which we
%call the \emph{projected context}. 

A first attempt would be to equate the context of an action with the
state in which it is executed. This, however, is too restrictive,
as the state might contain various variables that are irrelevant to the
action.

We proceed as follows: denote by the term {\em contexted action},
a pair of the form {\em (state,action)}, such that {\em action} was performed in some trace in {\em state}. 
A contexted action is said to be \emph{public} contexted action, if its action is public.

For agent $i$, we denote with \cact{i}
the set of all the public contexted actions in the traces that contain an action of our agent.
%, and their corresponding rewards with $Rewards_i$.
\cact{i} contains \emph{public} actions only since the private objectives of the agent are left untouched when transitioning to the single-agent problem.
Next, we project the state in each contexted action to
the set of variables that are either relevant to the action \emph{or} effects of the executing agent $i$.
By that we obtain a set of projected contexted-actions(PCA), denoted by $\pcact{i}$, consisting of
a projected-context and an action. We will associate
the rewards with the elements of this latter set.  Notice that   different contexted actions may yield identical PCAs.
\begin{example}
Returning to our Box-Pushing example: we find the following \emph{public contexted} actions in the traces:
\begin{itemize}
    \item \emph{(L,R,L,R),Agent2-Push-Left}
    \item \emph{(L,L,L,L),Agent1-CPush-Right-Box1}
    \item \emph{(L,L,L,L),Agent2-CPush-Right-Box1}
\end{itemize}



We extract the PCAs for \emph{Agent2}. When doing so, we encounter two possible candidates:
\begin{itemize}
    \item \emph{(L,R,L,R),Agent2-Push-Left}
    \item \emph{(L,L,L,L),Agent2-CPush-Right-Box1}
\end{itemize}
\end{example}

Next we project the contexts with respect to \emph{Agent2} and its actions. The relevant variables (influencers + effects) of \emph{Agent-2-Push-Left} are $X_{A2}$ and $X_{B1}$. The effects of \emph{Agent2} are both $X_{B1}$ and $X_{B2}$, as it can push both boxes. Therefore, the context is projected onto the variable $X_{A2}, X_{B1}, X_{B2}$. The location of \emph{Agent1} is taken out of the context. 
For the collaborative push, we also add $X_{A1}$ to the projected-context, as it is an influencer of the collaborative push.

This result in the following PCAs:
\begin{itemize}
    \item \emph{(*,R,L,R),Agent2-Push-Left}
    \item \emph{(L,L,L,L),Agent2-CPush-Right-Box1}
\end{itemize}
\ronen{the commented out part below should move to the next example, after you explain how
you define the rewards}
\commentout{
Thus, in the single-agent problem, \emph{Agent2} will be rewarded for pushing \emph{Box2} to the left regardless of \emph{Agent1}'s position, but to push $Box1$ it will need to reason about $Agent1$'s position - although in a very shallow manner as it is a private variable of $Agent1$.
Each of the PCAs will be assigned with a different reward. Such difference will assist in determining both the priority and the order in which the agent chooses to apply the PCAs.
We will now describe how the single-agent problems are constructed, given the PCAs and their rewards.
How the reward values are computed will be described afterwards. We will use $Rewards_i$ to refer to the set
of reward values associated with $\pcact{i}$.
}

\subsection{Single Agent Projection}
Our next step is to define a POMDP for each agent. This POMDP will contain some of agent $i$'s action and some actions of other agents,
as well as appropriate rewards for applying its own actions in a
projected context. 
%Given the PCAs and their rewards, we construct single-agent problems that reward each agent for following its role in the team plan, as well as its own private objectives. The single agent POMDP of agent $i$ contains some actions of $i$, and some actions of other agents. 
The actions of other agents are used to "simulate" some of the behaviors of the other agents -- behaviors that eventually enable the agent to carry out its own actions.

The single-agent POMDP $P_i$ for agent $i$ is obtained by modifying $P_{team}$
as follows:
\begin{enumerate}
\item Remove from the problem any \emph{public} action that does not appear in the traces, regardless of the agent applying it. As public actions alter facts in the world that are relevant to more than one agent, we  forbid the application of a public action that was not part of the team solution.
\item Remove sensing actions of other agents because the agent does
not have access to their results.
\item The \emph{private} actions of other agents are transformed into deterministic actions that succeed with probability 1.
As private actions change only private variables, and an agent cannot sense other agents' private variables, this change allows the agent to progress the world towards states in which it must act more easily.
If we cannot assume that actions have a clear success outcome, we can omit this step. This will result in larger, more complicated single-agent policies.
\ronen{omit what - making them probability 1?}\eliran{yes}

\item Add penalties for the application of all remaining \emph{public} actions in any context that did not appear in the team plan. We want to discourage an agent from applying public actions out of their context
in the team plan. The penalty is chosen to be $-1\cdot\max_{r\in Rewards_i}\{r\} \cdot|\pcact{i}|$, as an upper bound on the sum of rewards that can be achieved from applying the contexted action. This ensures that no undesirable action is worth applying, even in exchange for applying all the contexted actions. There is no penalty for other agents' PCAs
(i.e., actions in $\bigcup_{j\neq i} \pcact{j}$). We want
to allow the agent to simulate other agents' contexted public actions in order to plan the execution of its own actions at appropriate times.
\ronen{this is not clear. do we allow other agents action only in their context, or anywhere?}
\item Add the rewards for $\pcact{i}$ (explained later) according to $Rewards_i$
This reward will override the above penalty, but only when the action is applied in its projected context.
\item 
Remove rewards related to public variables the agent
can achieve. The goal of the single-agent POMDP is to imitate the team policy, not compute an alternative solution. Therefore, given a public variable $X_i$, two states $s, s'$ that differ only on $X_i$, and an action $a\in A_i$ such that $R(s, a, s'')\neq R(s', a, s'')$, we set both rewards to 0.
\ronen{this is a bit problematic. What if we have negative rewards, too?
For example, undesirable states. What if we cannot reach the same state s'' from s and s'?}
\end{enumerate}

After applying these changes, we obtain the single-agent problem $P_i$. %Before solving it, we need to specify a precision hyperparameter that determines how close to optimal the policy should be. In oppose to the team problem case, we can pick this parameter wisely to ensure that if the output policy reaches the precision, it must satisfy our goals.
Ideally, the solution to $P_i$ should be sufficiently optimal so that %$\epsilon_i$-optimal to guarantee that if the policy is $\epsilon_i$-optimal, then 
on average no PCA will be ignored by the policy. In the next section we specify how this parameter is chosen.
\ronen{what do you mean by "on average"}

\begin{example}
\ronen{this is strange}
We now construct Agent1's single-agent problem. We denote the PCAs and their respective rewards with $\{pca_1:r_1, pca_2: r_2\}$. We follow the projection stages one by one:
\begin{enumerate}
    \item As the push action is the only public action in the problem, we remove all push actions except for the two that are observed in the traces: Agent-1-Push-Right and Agent-2-Push-Left. Notice that we keep Agent2's Push-Left action, as we might need to simulate it.
    \item We remove the sensing action of Agent2.
    \item We don't have any non-deterministic private actions, so no actions are turned to deterministic.
    \item We add a penalty of $-2\cdot max\{r_1, r_2\}$ to the remaining public actions, applied in any context except for the PCA's contexts.
    \item We add the rewards ${r_1,  r_2}$ to their corresponding PCAs.
    \item The rewards for pushing the boxes to the target tiles are removed - we reward the agent only for doing its public action in context.
\end{enumerate}
\end{example}

Finally, we solve each single-agent problem $P_i$ using our off-the-shelf POMDP solver, SARSOP, resulting in a $\epsilon_i$-optimal policy for each agent. The generated policies will likely
contain non-sensing actions of other agents, which require
some handling, explained later on.

\begin{algorithm}
\caption{ProjectAndSolve}
\begin{algorithmic}[tbph]
\State Input: $P_{team}$, $Traces$
\For {all agent $i$}
\State $\pcact{i},\mathit{Rewards_i} \gets \Call{ExtractPCAs}{P_{team}, Traces, i}$
\State {$\mathit{SAProblem_i}$ $\gets \Call{Project}{P_{team}, \pcact{i}, Rewards_i}$}
\State {$\epsilon_i \gets \Call{GetSAPrecision}{\mathit{MaxTraceLength}, \gamma}$}
\State {$\mathit{RawSAPolicy_i} \gets \Call{POMDPSolver}{\mathit{SAProblem_i}, \epsilon_i}$}
\EndFor
\State {return $\mathit{RawSAPolicy_1}, \mathit{RawSAPolicy_2},..., \mathit{RawSAPolicy_n}$}
\end{algorithmic}
\end{algorithm}


\subsection{Rewards and Precision in the Projection}

After we compute the PCAs for the agent, we need to specify their respective rewards. The rewards should encourage the agent to perform the team plan actions in their corresponding contexts, in the same order they appeared in the team solution. 
To do so, we exploit the discount factor, which makes it beneficial for the planner to apply higher rewarding actions earlier. Hence, we need to order the PCAs, and assign higher rewards to actions that appear earlier in the order.

The main thing to notice is that the planner might need to insert costly actions that precede a PCA to achieve some needed influencer for it. This may lead to a scenario where it is more beneficial to apply the PCAs in a different order or even not to apply a PCA at all, as the costs might be higher than the reward of the PCAs.

The first problem, applying PCAs in a different order, is already handled due to the existence of the agent's objectives as part of the projected context. As public actions are responsible for altering the agent's objectives, by penalizing public actions that are applied out of context and rewarding those that are applied in context, we enforce PCAs to be applied only in an appropriate order.

The second problem of making the PCAs worth applying, is handled via the rewards we assign to each PCA. We want to make sure that in the projected problem it is beneficial for the agent to apply a sequence of PCAs that will achieve its desired goals. Since we no longer pursue the team goals, the reward function does not necessarily make these application beneficial, and therefore we insert artificial rewards.

To do so, we tackle the problem more locally, and design the rewards so that no sequence of non-rewarding actions would "cancel" the need for applying the PCA it precedes, that is - would cost more than the PCA's reward.

To compute the rewards, we need to first determine an order over the PCAs we extracted for the agent.
We go over the traces and construct a \emph{directed acyclic graph} of PCAs, forcing a DAG by not adding edges that form cycles.
We start with an empty graph, and go over all traces. Let $l$ be a trace. For each event $e_j$ in the trace, we compute the PCA of its contexed action with respect to agent $i$. We denote this PCA with $pca_j$.
Then, we add a vertex that represents $pca_j$, assuming it doesn't already exist. We add an edge between the vertex corresponding to $pca_{j-1}$ and $pca_j$.
This implies that the current action cannot appear before the one preceding it.
Once we went over all traces and constructed the graph, we compute a topological order
for it, giving us a sequence of PCAs $(\pi_1, a_1),..., (\pi_q, a_q)$, where the first instance corresponds to the action that appeared earliest.

\eliran{Not Implemented - Replace sequence with sets representing the levels of the DAG}
Now, we begin by associating a reward for the last PCA in the sequence, namely the \emph{base} reward, and then iteratively calculate the reward for each preceding PCA.
The base reward is chosen so that applying the PCA will be beneficial in any possible scenario. Recall that we need to avoid a scenario where many costly actions are required to reach a state satisfying the context of the action, so that their total cost surpasses the reward, making it non-beneficial to apply.
Next, we describe the formulas that are used to compute to rewards. In these calculations, we assume that each action has a success probability. Following that, we describe how we can we calculate the rewards without assuming these success probabilities exist.

Let $MaxCost$ be the maximal negative reward that was observed in the traces following an action. Let $MinSP$ be the minimal success probability of all actions in the problem, and $MinPCASP$ the minimal success probability of the actions appearing in the PCAs. Let $MTL$ be the Maximal Trace Length we produced, and $MPG$ be the Maximal Public Gap - the maximal number of \emph{private} actions that precede a public action in the traces. Let $sp_i$ denote the success probability of action $a_i$. We set $\epsilon > 0$ to some arbitrary positive scalar. We compute $r_q$, the base reward:
\begin{equation}
\label{eqn:rq}
   r_q = \frac{\sum_{i=MTL - MPG}^{MTL}(\frac{MaxCost}{MinSP} \cdot \gamma^{i-1})+ \epsilon}{\gamma^{MTL-1}\cdot MinPCASP} 
\end{equation}%
The numerator is an upper bound for the expected discounted cost we would pay before applying the last PCA. The denominator amplifies that cost to be beneficial when scheduled as the last action in the policy.

Equation~\ref{eqn:rq} ensures that the planner will always find the application of $a_q$ in the context $\pi_q$ beneficial, and thus will insert it to the policy.

After calculating the base reward, which is set as the reward of $(\pi_q, a_q)$, we can calculate the rewards associated with the rest of the PCAs in the sequence. Since the application order of the PCAs is enforced using the context, we simply need to perform tie breaking for cases in which several PCAs may be applied, so that the agents would produce coordinated solutions.
To do so, we set $r_i=(1+\epsilon)\cdot r_{i+1}$, where again $\epsilon$ can be an arbitrarily small positive scalar.

% To ensure that the order is maintained, using the same intuition as with the base reward, we compute the reward for $(\pi_i, a_i)$ so that the planner will always prefer to apply $a_i$ in $\pi_i$ before applying $a_{i+1}$ in $\pi_{i+1}$. 
% An interesting cases arises when the failure probability of $a_i$ is small. Then, after applying the PCA once, it may be better for the planner to assume that the PCA succeeded and apply $a_{i+1}$, without any form of verification, causing the order to break.
% \guy{I do not understand why we enforce "verification" - if it is beneficial to observe, the planner should do so, and if not, then not. Such "vitrification" should appear in the team plan to begin with, assuming that they are needed.}
% \eliran{Even if the verification appears in the team plan, the artificial rewards we add to the public actions would cause the planner to postpone this verification to a later stage and violate the order, as it would be more beneficial for him to apply the public actions as soon as possible. I hope that later on we would find other ways to design the reward function so that the team plan behavior could be more naturally preserved.}\guy{I disagree. Rewards are given for executing an action at a state. If the planner is unsure of the state, it will observe, if need be.}\eliran{Open Issue - I understand your meaning. The problem is that in the code, we still didn't implement the projection so that it would be only with respect to the variables relevant to the agent, but with respect to the action's context. This point is very crucial, though I think that both options (projecting onto context and onto relevant variables) would yield good results for now. Let's discuss this as well}
% We set $r_i=\gamma \cdot \frac{r_{i+1}}{1-sp_i} \cdot sp_{i+1} + \epsilon$ \eliran{explain further?}
% and by that we ensure that regardless of whether $a_i$ fails or not, it would still be beneficial to verify that and apply $a_i$ again if necessary, before applying $a_{i+1}$. We compute $r_i$ until we reach $r_1$, and by that associating a reward to each of the agent's PCAs.

In the base reward calculation, we assumed that each action has a success probability when defining $MinSP$. If we do not assume that actions have success probabilities, we can instead incorporate into each PCA, in addition to its projected context and action, the effect it had on the state variables, and then use the minimal probability of all these effects.

To compute the precision $\epsilon_i$ we use similar concept to the base reward calculation, but now we rely on the observed MaxTraceLength to capture the total number of costly action without amplifying their cost with their success probability.
\begin{equation}
\label{eqn:rq}
   \epsilon_i = \gamma^{MTL-1}\cdot r_q \cdot MinPCASP - \sum_{i=MTL - MPG }^{MTL}(MaxCost \cdot \gamma^{i-1})
\end{equation}
\eliran{It is heuristic, but yields the best results on all problem configurations. Should we leave it as is for the workshop paper?}

\begin{example}
We go back to our example to demonstrate how these rewards are calculated.
First, recall that our only non-deterministic action is the push action, which has a success probability of 0.8, hence $MinSP=MinPCASP=0.8$. In addition, the action costs are -1, -10, -3 and -2 for sense, move, push and cpush respectively. Hence $MaxCost$ is set to 10. We also set $\gamma=0.99$, $\epsilon=0.01$.
Taking our two traces, we have $MTL=7$ and $MPG=2$
\end{example}

As mentioned previously, we have two PCAs when considering Agent1's problem.
We construct the DAG from the traces and extract the topological order, which yields the following order between the two PCAs.
\begin{enumerate}
    \item $pca_1$ = \emph{(*, L, L, R), Agent-2-Push-Left-Box2}
    \item $pca_2$ = \emph{(L, L, L, L), Agent-2-CPush-Right-Box1}
\end{enumerate}


We calculate the base reward, $r_2$:
\begin{equation}
     r_2 = \frac{\sum_{i=7-2}^{7}(\frac{10}{0.8} \cdot \gamma^{i-1})+ \epsilon}{\gamma^{7-1}\cdot 0.8} = 47.36
\end{equation}

To calculate $r_1$ we just amplify by $1+\epsilon$ and get $r_1=47.83$

We calculate the precision of \emph{Agent2}'s single-agent problem: \begin{equation}
     \epsilon_2 = \gamma^{7-1}\cdot r_2 \cdot 0.8 - \sum_{i=7-2}^{7}(10 \cdot \gamma^{i-1}) = 7.14
\end{equation}

\subsection{Policy Adjustment and Alignment}

We run the planner on each of the single agent projections that we generate, constructing a set of single agent policies for the projection. We now adapt the policies to apply to the joint problem. First, the projection of agent $i$ contains actions of other agents that must be removed.

We now need to align the policies in order to synchronize different agent actions to occur in the same order as in the team plan. We need to ensure that an action $a_1$ of one agent that generates a influencer for an action $a_2$ of another agent would be executed before $a_2$. We also need to ensure that collaborative actions are executed at the same time by all participating agents. 

Given a policy graph of agent $i$, we consider the actions of any other agent as \emph{foreign} actions with respect to that policy graph. The policy adjustment process for agent $i$ can be divided into three steps:
\begin{enumerate}
    \item Remove all the \emph{private} actions of agents other than $i$ in the agent $i$'s policy graph.
    \item Insert \emph{no-op} actions into the graph to synchronize it, as much as possible, with other agents' policy graphs. This is the alignment procedure.
    \item Convert all the \emph{public} actions of  agents other than $i$ to no-ops.
    \item Handle potential \emph{collaborative} actions issue that might occur, in which slight unsynchronization leads the collaborating agents to enter an infinite loop.
\end{enumerate}
We now describe the steps in detail.

Starting with the first step, we remove all the foreign private action from the policy graph, and proceed to the second step, which is the alignment procedure.

Each single-agent policy contains only a superficial plan for the other agents, if at all, and therefore its actions need to be aligned with respect to the true policies of other agents. 
Since we are dealing with non-deterministic actions, it is impossible to perform an alignment in which all action are perfectly coordinated.
Instead, we expect the alignment to reduce the required postponement of actions, but currently do not guarantee a minimal waiting time.

We alter the graphs by inserting \emph{no-op} actions, so that corresponding public action vertices will appear in the same depth in all graphs, thus preventing the agents from applying actions prematurely.
To define what corresponding vertices are, we use the notion of an \emph{identifier} of a public action vertex. The identifier of a public action vertex in the policy graph is the
sequence of public actions that appear along the \emph{simple} path from the root to the vertex.
\eliran{Not specified - several identifiers}

The sequence of public actions represents the team-relevant actions that took place prior to the current action, and serves as a summarization of the sequence of events. As the identifier is not agent-specific, it allows us to look for it in other policy graphs and determine which vertices represent similar
sequences of events, namely corresponding vertices.

We now describe the algorithm in detail.
The alignment is an iterative process. We are given with policy graphs $G_1,...,G_n$ which are the output of the last iteration, and initially set to be the raw policy graphs. In each iteration, we align every graph with respect to all other graphs, and stop once convergence is reached. The process is applied iteratively as some alignment might need to propagate through iterations. Algorithm~\ref{} provides the high-level pseudo-code of a single iteration.

Recall that in a policy graph, each vertex represents an action and each edge represents an observation. From a vertex that represents a non-sensing action, there is a single outgoing edge that represents the \emph{null-obs} observation. From a vertex representing a sensing action there is one outgoing edge for each possible observation.

We now describe the alignment of a single policy graph. For each public action $a$ in the policy graph $G_i$ of agent $i$, we identify this action in the graphs of other agents.
Then, we postpone $a$ by inserting no-op actions, so that $a$ occurs at the same time in all policy graphs, given the maximal path to $a$ in all policy graphs.

We traverse $G_i$ using breadth-first search. For a vertex $v\in G_i$ representing a public action of agent $i$, we extract the $identifier$ of $v$. Having the identifier of $v$, we traverse all other graphs $G_j$, and in each we search for a vertex $v'$ that matches the identifier. $v'$ matches the identifier if there's a simple path from the root of $G_j$ to it that contains the identifier.\eliran{Not specified - exhaustive match, more precise}
If we found such $v'$ that matches $v$, we set the {\em no-op} amount of $G_j$ that is required for $v$, denoted by $m_j$, to be the length of the \emph{longest simple path} from the root to $v'$. Else we set $m_j$ to 0.
Once we calculated $m_j$ for all $j\neq i$, we need to determine the final amount of {\em no-ops} added to $v$ in $G_i$. First, we consider $max_{j\neq i}\{m_j\}$. However, the maximal requirement does not take into account {\em no-ops} that were already added to preceding vertices in $G_i$, or the depth of $v$ itself, and must be
corrected by subtracting from it a \emph{compensation term} which is the sum of the \emph{longest simple path} from the root to $v$ and the \emph{minimal} {\em no-op} amount added to any of the predecessors of $v$. Overall, we set the {\em no-op} amount of $v$ to be $max_{j\neq i}\{m_j\} - \mathit{CompensationTerm}$.

Having computed the  {\em no-op} amount for each of the public action vertices in $G_i$, we insert them to it by appending the required number of consecutive {\em no-op} vertices \emph{prior} to the public action vertex, thus postponing it.
As we mentioned, in each iteration of the algorithm we apply this procedure to all policy graphs, one by one, and halt upon convergence.

\eliran{Not Implemented - Running with concurrent BFSs}

After the alignment procedure has converged, we proceed to the the third step. Unlike the case of \emph{private} actions that were previously eliminated, \emph{public} actions of other agents were left in the graphs to guide the alignment process. Following the alignment, we convert them into \emph{no-ops}, and by that make the agent wait while other agents are performing their public actions.

Finally, in the fourth step, we handle the problem of a potential ``livelock''
between \emph{collaborative} actions. Consider a scenario where two agents need to perform a non-deterministic collaborative action whose effect can be directly sensed. The action is costly so they must apply it the minimal necessary number of times. To do so, following every application, they perform a \emph{sensing} action that senses the effect of that collaborative action.
Given non-deterministic actions -- causing the alignment to be imperfect -- there might be scenarios in which the agents become unsynchronized. Then, they might be applying the collaborative and sensing actions in an alternating manner, where one agent performs the collaborative action while the other performs the sensing action, causing them to enter a livelock. To handle that, given a collaborative action with $n$ collaborating agents, we modify the graph so that every collaborative action that is part of a cycle is repeated by every agent for $n$ times instead of just once. This way, a livelock can never occur.

\begin{example}
We again go back to our example to see how these transformations occur. Figures (a) and (b) show \emph{Agent1}'s policy graph before and after the alignment and adjustments procedure.

\begin{figure}
    \centering
    \subfloat[Before]{\includegraphics[width=0.25\textwidth,height=\textheight,keepaspectratio]{BPNEW-2x1_2A_1H_1L_REPLACE_TEAM_agent1_001AMP_COLOR.dot.png}}
      \hfill
    \subfloat[After]{\includegraphics[width=0.45\textwidth,height=\textheight,keepaspectratio]{BPNEW-2x1_2A_1H_1L_REPLACE_TEAM_ALIGNED_exactalign_postv3_repeatcol_pre_agent1_COLOR.dot.png}}
\end{figure}

\begin{itemize}
    \item We remove \emph{Agent1}'s simulation of \emph{Agent2}'s left movement.
    \item The alignment algorithm inserts a {\em no-op} following \emph{Agent1}'s first sensing action, as the collaborative push in \emph{Agent2} occurs in depth 4.
    \item We convert the first simulated push of \emph{Agent2} to a {\em no-op}.
    \item We duplicate the collaborative push to avoid a live-lock. This way, even if \emph{Agent2} fails with his first push, it could still synchronize with \emph{Agent1} by entering the collaborative-push loop which ensures no live-lock can occur.
\end{itemize}
\end{example}


\begin{algorithm}
\caption{Alignment Iteration}
\begin{algorithmic}[tbph]
\State Input: PolicyGraphs $G_1, ..., G_M$
\For{$G_i,  i\in\{1, ..., M\}$}
	\State {$\mathit{NoopsReqs} \gets \mathit{VertexToIntMapping}$}
      \State {$\mathit{CurrBFS} \gets \Call{BFS}{G_i}$}
      \While {$\mathit{CurrBFS.queue}$ is not empty}
	\State {$v \gets \mathit{CurrBFS.queue.pop}$}
	\State {$a \gets v.action$}
	\If {$a$ is public action}
	\State {$\mathit{identifier} \gets \Call{GetIdentifier}{v}$}
	\State {$\mathit{MaxNoop} \gets 0$}
	\For {$G_j,  j\in\{1, ..., M\}\setminus\{i\}$}
	\State {$\mathit{CurrNoop} \gets \Call{NoopReq}{G_j, \mathit{identifier}}$}
	\State {$\mathit{MaxNoop} \gets max(\mathit{MaxNoop}, \mathit{CurrNoop})$}
	\EndFor
	\State {$\mathit{NoopsReqs}[v] \gets \mathit{MaxNoop} - \mathit{CompensationTerm}$}
	\EndIf
	\EndWhile
	\State {$G_i' \gets \Call{AddNoops}{G_i, \mathit{NoopsReqs}}$}
\EndFor
\State {return $G_1', ..., G_M'$}
\end{algorithmic}
\end{algorithm}

\section{Empirical Study}
We provide experimental results while focusing on large planning horizons on large scale problems.
The experiments were conducted on a variation of Cooperative Box Pushing, which we specify in the next subsection.
We compare our algorithm with two Dec-POMDP solvers \eliran{citation}, GMAA-ICE \cite{} and JESP \cite{}, using MADP-tools. \cite{}.
We evaluate FDMAP on a Windows machine with 4 cores of and 8GB of memory.
We evaluate both GMMA-ICE and JESP on a Linux machine with 4 cores and 8GB of memory.


\subsection{\cbp}
We present a variation of the well known Cooperative Box Pushing domain, in which the need shifts from a good local policy for each agent that relies mostly on short responses to observations, to a decentralized policy that relies on good coordination between agents and requires larger planning horizons to be accomplished. A grid tile can contain any number of agents and boxes, and each agent has both move and push actions in each of the four possible directions, as well as a sensing action for a box and a no-op action.
An agent (or agents), can push a box only when positioned in the same grid with it.
All action except for the push actions are deterministic.
Light boxes can be pushed by a single agent, while heavy boxes can only be pushed by the collaborative push of two agents.
The goal of the agents is to move the boxes to the target tile, located at the upper left corner of the grid.
Initially, each box can appear in either the target tile (making it invaluable to handle) or the lower right tile, with equal probability.
Each action has a cost (except for no-op), and a reward is given for pushing a box to the target tile. The costs are 5 for move, 1 for sense, 30 for push, 20 for collaborative push (per agent), and the reward is 500. In addition, there's a penalty of 500 for pushing a box \emph{out} of the target tile to avoid abuse.
The rewards were chosen so that it would be more beneficial to sense rather than blindly push.
A domain instance of $m$ tiles with $n$ agents $l$ light boxes and $h$ heavy boxes, has $m^{n+l+h}$ states, $(5\cdot(k+1)+4h\cdot(n-1))^n$ actions and $3^n$ observations. 

\subsection{Domain Configurations}
We demonstrate results on six configurations of \cbp. Each box can equally start at either the top-left or bottom-lower corner, so we only specify the initial location of each agent:
\begin{enumerate}
    \item 2 by 2 grid, 1 light box and 2 agents located at tiles (1,2) and (2,2)
    \item 2 by 2 grid, 2 light boxes and 2 agents located at tiles (1,2) and (2,2)
    \item 2 by 2 grid, 3 light boxes and 2 agents located at tiles (1,2) and (2,2)
    \item 2 by 3 grid, 2 light boxes and 3 agents located at tiles (1,2), (1,3) and (2,3)
    \item 2 by 3 grid, 3 light boxes and 3 agents located at tiles (1,2), (1,3) and (2,3)
    \item 3 by 3 grid, 1 light box and 2 heavy boxes, 2 agents located at tiles (1,3) and (3,1). In this configuration we also increase the reward and penalty for pushing a box in and out of the target tile to 1000, so that it will remain beneficial for the agents to push all boxes.
\end{enumerate}

\begin{center}
    \begin{tabular}{||c|c|c|c|c||}
         \hline
         \multicolumn{5}{||c||}{\cbp \ Configurations} \\
         \hline
         Index & Name & $|S|$ & $|A|$ & $|\Omega| $ \\ 
         \hline
         1 & BP-22201 & 64 & 100 & 9 \\
         \hline
         2 & BP-22202 & 256 & 225 & 9 \\
         \hline
         3 & BP-22203 & 1024 & 400 & 9 \\
         \hline
         4 & BP-23302 & 7776 & 3375 & 27 \\ 
         \hline
         5 & BP-23303 & 46656 & 8000 & 27 \\
         \hline
         6 & BP-33221 & 59049 & 324 & 9 \\
         \hline
    \end{tabular}
\end{center}

\subsection{Comparison Setting}
We compared FDMAP with GMAA-ICE and DP-JESP. For GMAA-ICE and DP-JESP, the horizon stands for the planning horizon. Since FDMAP calculates a policy for an infinite horizon, the horizon parameter specifies the number of steps the policy was evaluated under.
The value metric for GMAA-ICE and DP-JESP specifies the policy value. In FDMAP we measured the average discounted accumulated reward of 1000 simulations, where in each simulation the policy was run for the number of steps specified in the horizon column. The last horizon prefixed with Max is the maximal horizon measured for reaching the goal state in all simulations.

GMAA-ICE and DP-JESP were given with 4000 seconds limit to solve each $\langle\textit{configuration, horizon}\rangle$ pair, FDMAP was given with the same amount to solve the problem once and output an infinite horizon solution.
The times shown for GMAA-ICE do not include the problem loading. FDMAP times do not include writing SARSOP policies and graphs to disk, as they are highly dependent on hardware quality and can effectively remain in memory throughout the whole process.
% \eliran{explain that the process is done in separated steps and should be unified?}\ronen{Not essential now. EVentually, you should have code that is usable by others.}

In both GMAA-ICE and DP-JESP, configurations BP-23302, BP-23303 and BP-33221 could not be parsed in the time given. Therefore we provide comparisons only for the first three configurations, BP22201 and BP22202. For the latter three we provide the maximal and average number of steps it took FDMAP to reach a goal state - excluding instances in which the initial state is a goal state.

In DP-JESP, X marks a general timeout. In GMAA-ICE we mark two different timeout options - FF refers to failure of finding a full policy for the required horizon, where FH refers to an earlier stage timeout when computing the heuristic function.

\begin{center}
    \begin{tabular}{||c|c|c|c|c|c|c||}
         \hline
         \multicolumn{7}{||c||}{BP-22201} \\
         \hline
         Horizon & \multicolumn{2}{|c|}{DP-JESP} & \multicolumn{2}{|c|}{GMAA-ICE} & \multicolumn{2}{|c||}{FDMAP}\\ 
         \hline
         & Time & Value & Time & Value & Time & Value \\
         \hline
         3 & 14.11 & 0 & 2.08 & 0 & 1.94 & -22.14 \\
         \hline
         4 & 1279.06 & 80 & 368.54 & 76.64 & " & -22.54 \\
         \hline
         5 & X & - & 366.58 & 108.74 & " & -30.94 \\ 
         \hline
         6 & X & - & FF & - & " & 108.96 \\
         \hline
         7 & X & - & FF & - & " & 141.245 \\
         \hline
         Max=15 & X & - & FF & - & " & 192.546 \\
         \hline
    \end{tabular}
\end{center}

\begin{center}
    \begin{tabular}{||c|c|c|c|c|c|c||}
         \hline
         \multicolumn{7}{||c||}{BP-22202} \\
         \hline
         Horizon & \multicolumn{2}{|c|}{DP-JESP} & \multicolumn{2}{|c|}{GMAA-ICE} & \multicolumn{2}{|c||}{FDMAP}\\ 
         \hline
         & Time & Value & Time & Value & Time & Value \\
         \hline
         3 & 262.28 & 0 & 50.16 & 0 & 4.37 & -25.36 \\
         \hline
         4 & X & - & FF & - & " & -33.97 \\
         \hline
         5 & X & - & FF & - & " & -41.87 \\ 
         \hline
         6 & X & - & FF & - & " & 104.83 \\
         \hline
         7 & X & - & FF & - & " & 200.21 \\
         \hline
         Max=24 & X & - & FF & - & " & 357.23 \\
         \hline
    \end{tabular}
\end{center}

\begin{center}
    \begin{tabular}{||c|c|c|c|c|c|c||}
         \hline
         \multicolumn{7}{||c||}{BP-22203} \\
         \hline
         Horizon & \multicolumn{2}{|c|}{DP-JESP} & \multicolumn{2}{|c|}{GMAA-ICE} & \multicolumn{2}{|c||}{FDMAP}\\ 
         \hline
         & Time & Value & Time & Value & Time & Value \\
         \hline
         3 & 3146.58 & 0 & 1856.81 & 0 & 27.08 & -27.51 \\
         \hline
         4 & X & - & FF & - & " & -29.13 \\
         \hline
         5 & X & - & FF & - & " & -38.04 \\ 
         \hline
         6 & X & - & FH & - & " & 96.29 \\
         \hline
         7 & X & - & FH & - & " & 121.68 \\
         \hline
         Max=33 & X &  & FH & - & " & 541.35 \\
         \hline
    \end{tabular}
\end{center}

\begin{center}
    \begin{tabular}{||c|c|c|c|c||}
         \hline
         \multicolumn{5}{||c||}{FDMAP}\\ 
         \hline
         Problem & MaxSteps & AvgSteps & Time & Value \\
         \hline
         BP-23302 & 36 & 16 & 106.4 & 250.79 \\
         \hline
         BP-23303 & 65 & 25 & 1832.55 & 351.19 \\
         \hline
         BP-33221 & 126 & 41 & 3150.02 & 244.91 \\
         \hline
    \end{tabular}
\end{center}
\section{Related Work}
QDec-POMDPs~\cite{} \eliran{citation} are qualitative version of Dec-POMDPS that tackle a conceptually simpler, more structured, model. In QDec-POMDPs 
non-determinism replaces stochastic uncertainty. The model is factored (i.e., described at the level of state variables rather than states), and actions 
are described using preconditions and non-determinstic effects. Although QDec-POMDPs are also NEXT-Time hard, recent work in the area that leverages heuristic-search planners, has been able to scale up to much larger domains (e.g., box pushing on a grid of size 24,  12 agents and 12 boxes, implying a state space of $24^{24}$)
albeit, under the assumptions that actions are deterministic.

Dec-POMDP algorithms.

\section{Conclusion}

\section{Future Research}
\eliran{its more of a sketch for now so we don't forget what we've talked about}
There are two directions in which the solver can be improved -  scalability and solution quality.
The use of online planners instead of offline ones can greatly improve the scale of solvable problems. The changes in terms of algorithm architecture are minor, as we merely need to be able to produce the single agent policy graphs using an online solver. \ronen{In fact, it seems we could use an RL algorithm here to generate a policy for each agent, as we can simulate as many traces as we wish.
In fact, we could use the RL algorithm to solve the team POMDP. This would generate the needed traces as well. We would learn incrementally
both the team solution and the single-agent solution. Using this idea, we could probably scale up to very large problems. In fact, we can use this
approach to do MA RL. If we can do the projections. What we need is a simulator that let's us control all the agents at once.}
In terms of solution quality, we would want to use more principled methods of reward shaping, that come from the worlds of reinforcement learning, in order to define the reward heuristic. We would want to achieve both the properties we already achieved using our current heuristic, and to still be able to optimize the expected discounted reward of the problem.
Also, in order to fully achieve that, the alignment algorithm will need to include some form of confidence aspect, where we no longer ignore cycles but rather look at non-simple paths and try to increase the certainty about the world's state. 

\bibliography{Bibliography-File}
\bibliographystyle{aaai}
\end{document}