@inproceedings{carlin2008value,
  title={Value-based observation compression for DEC-POMDPs},
  author={Carlin, Alan and Zilberstein, Shlomo},
  booktitle={Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems-Volume 1},
  pages={501--508},
  year={2008}
}
@article{DECPOMDPCOMP,
  title={The complexity of decentralized control of Markov decision processes},
  author={Bernstein, Daniel S and Givan, Robert and Immerman, Neil and Zilberstein, Shlomo},
  journal={Mathematics of operations research},
  volume={27},
  number={4},
  pages={819--840},
  year={2002}
}


@inproceedings{DRQN,
  author    = {Matthew J. Hausknecht and
               Peter Stone},
  title     = {Deep Recurrent Q-Learning for Partially Observable MDPs},
  booktitle = {2015 {AAAI} Fall Symposium},
  pages     = {29--37},
  publisher = {{AAAI} Press},
  year      = {2015}
}

@inproceedings{Qtran,
  author    = {Kyunghwan Son and
               Daewoo Kim and
               Wan Ju Kang and
               David Hostallero and
               Yung Yi},
  title     = {{QTRAN:} Learning to Factorize with Transformation for Cooperative
               Multi-Agent Reinforcement Learning},
  booktitle = {ICML},
  pages     = {5887--5896},
  year      = {2019}
}

@inproceedings{WQmix,
  author    = {Tabish Rashid and
               Gregory Farquhar and
               Bei Peng and
               Shimon Whiteson},
  title     = {Weighted {QMIX:} Expanding Monotonic Value Function Factorisation
               for Deep Multi-Agent Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2020}
}

@article{hussein2017imitation,
  title={Imitation learning: A survey of learning methods},
  author={Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina},
  journal={ACM Computing Surveys (CSUR)},
  volume={50},
  number={2},
  pages={1--35},
  year={2017},
  publisher={ACM New York, NY, USA}
}
@inproceedings{CMAE,
  author    = {Iou{-}Jen Liu and
               Unnat Jain and
               Raymond A. Yeh and
               Alexander G. Schwing},
  editor    = {Marina Meila and
               Tong Zhang},
  title     = {Cooperative Exploration for Multi-Agent Deep Reinforcement Learning},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning,
               {ICML}},
  volume    = {139},
  pages     = {6826--6836},
  year      = {2021}
}

@inproceedings{WuZJ13,
  author    = {Feng Wu and
               Shlomo Zilberstein and
               Nicholas R. Jennings},
  title     = {Monte-Carlo Expectation Maximization for Decentralized POMDPs},
  booktitle = {{IJCAI}},
  pages     = {397--403},
  year      = {2013}
}

@article{POMDPCOMPINF,
author = {Madani, Omid and Hanks, Steve and Condon, Anne},
year = {2000},
month = {02},
pages = {},
title = {On the Undecidability of Probabilistic Planning and Infinite-Horizon Partially Observable Markov Decision Problems},
journal = {Proceedings of the National Conference on Artificial Intelligence}
}

@article{POMDPCOMP,
author = {Papadimitriou, Christos H. and Tsitsiklis, John N.},
title = {The Complexity of Markov Decision Processes},
year = {1987},
issue_date = {August 1987},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {12},
number = {3},
issn = {0364-765X},
abstract = {We investigate the complexity of the classical problem of optimal policy computation in Markov decision processes. All three variants of the problem finite horizon, infinite horizon discounted, and infinite horizon average cost were known to be solvable in polynomial time by dynamic programming finite horizon problems, linear programming, or successive approximation techniques infinite horizon. We show that they are complete for P, and therefore most likely cannot be solved by highly parallel algorithms. We also show that, in contrast, the deterministic cases of all three problems can be solved very fast in parallel. The version with partially observed states is shown to be PSPACE-complete, and thus even less likely to be solved in polynomial time than the NP-complete problems; in fact, we show that, most likely, it is not possible to have an efficient on-line implementation involving polynomial time on-line computations and memory of an optimal policy, even if an arbitrary amount of precomputation is allowed. Finally, the variant of the problem in which there are no observations is shown to be NP-complete.},
journal = {Math. Oper. Res.},
month = aug,
pages = {441–450},
numpages = {10},
keywords = {dynamic programming, parallel computation, Markov decision processes, computational complexity}
}

@MISC{Seuken08formalmodels,
    author = {Sven Seuken and Shlomo Zilberstein},
    title = {Formal models and algorithms for decentralized decision making under uncertainty},
    year = {2008}
}

@article{DECPOMDPVP,
author = {Oliehoek, Frans},
year = {2010},
month = {01},
pages = {},
title = {Value-Based Planning for Teams of Agents in Stochastic Partially Observable Environments}
}

@book{DECPOMDPART,
    author =        {Frans A. Oliehoek and Christopher Amato},
    title =         {A Concise Introduction to Decentralized POMDPs},
    year =          2016,
    series =        {SpringerBriefs in Intelligent Systems}
}

@INPROCEEDINGS{SARSOP,
    author = {Hanna Kurniawati and David Hsu and Wee Sun Lee},
    title = {SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces},
    booktitle = {In Proc. Robotics: Science and Systems},
    year = {2008}
}

@inproceedings{MBDP,
author = {Seuken, Sven and Zilberstein, Shlomo},
title = {Memory-Bounded Dynamic Programming for DEC-POMDPs},
year = {2007},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 20th International Joint Conference on Artifical Intelligence},
pages = {2009–2015},
numpages = {7},
location = {Hyderabad, India},
series = {IJCAI’07}
}

@article{GMAAICE,
    author =    {Frans A. Oliehoek and 
                 Matthijs T. J. Spaan and 
                 Christopher Amato and 
                 Shimon Whiteson},
    title =     {Incremental Clustering and Expansion for Faster 
                 Optimal Planning in Decentralized {POMDPs}},
    journal =   {JAIR},
    volume =    {46},
    pages =     {449--509},
    year =      2013
}

@inproceedings{JESP,
author = {Nair, Ranjit and Tambe, Milind and Yokoo, Makoto and Pynadath, David and Marsella, Stacy},
booktitle = {IJCAI'03},
year = {2003},
pages = {705-711},
title = {Taming Decentralized POMDPs: Towards Efficient Policy Computation for Multiagent Settings.}
}

@article{DICEPS,
author = {Oliehoek, Frans and Kooij, Julian and Vlassis, Nikos},
year = {2008},
month = {01},
pages = {341-357},
title = {The Cross-Entropy Method for Policy Search in Decentralized POMDPs.},
volume = {32},
journal = {Informatica}
}

@article{BAYESNETWORK,
author = {Boutilier, Craig and Dean, Thomas and Hanks, Steve},
title = {Decision-Theoretic Planning: Structural Assumptions and Computational Leverage},
year = {1999},
volume = {11},
number = {1},
journal = {J. Artif. Int. Res.},
pages = {1–94}
}

@article{MAXQ,
	Author = {Thomas G. Dietterich},
	Journal = {JOURNAL OF AI RESEARCH},
	Pages = {227-303},
	Title = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition},
	Volume = {13},
	Year = {2000}}

@unpublished{RDDL,
	Author = {Scott Sanner},
	Title = {Relational Dynamic Influence Diagram Language (RDDL): Language Description},
	Year = {2010}}

@inproceedings{FDECPOMDP,
author = {Oliehoek, Frans A. and Spaan, Matthijs T. J. and Whiteson, Shimon and Vlassis, Nikos},
title = {Exploiting Locality of Interaction in Factored Dec-POMDPs},
year = {2008},
booktitle = {AAMAS},
pages = {517–524}
}

@inproceedings{QDECPOMDP,
  author={Ronen I. Brafman and Guy Shani and Shlomo Zilberstein},
  title={Qualitative Planning under Partial Observability in Multi-Agent Domains},
  booktitle={AAAI'13},
  year={2013}
}

@inproceedings{smith2005point,
  title={Point-based POMDP algorithms: improved analysis and implementation},
  author={Smith, Trey and Simmons, Reid},
  booktitle={UAI},
  pages={542--549},
  year={2005}
}

@article{amato2010optimizing,
  title={Optimizing fixed-size stochastic controllers for POMDPs and decentralized POMDPs},
  author={Amato, Christopher and Bernstein, Daniel S and Zilberstein, Shlomo},
  journal={{JAAMAS}},
  volume={21},
  number={3},
  pages={293--320},
  year={2010},
  publisher={Springer}
}

@inproceedings{nair2003taming,
  title={Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings},
  author={Nair, Ranjit and Tambe, Milind and Yokoo, Makoto and Pynadath, David and Marsella, Stacy},
  booktitle={IJCAI},
  volume={3},
  pages={705--711},
  year={2003}
}
@inproceedings{FACTOREDPLAN,
author = {Brafman, Ronen and Domshlak, Carmel},
year = {2006},
Booktitle = {AAAI},
Pages = {809-814},
title = {Factored Planning: How, When, and When Not}
}

@article{PRIVACYPLAN,
  author    = {Amos Beimel and
               Ronen I. Brafman},
  title     = {Privacy Preserving Multi-Agent Planning with Provable Guarantees},
  journal   = {CoRR},
  volume    = {abs/1810.13354},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.13354},
  archivePrefix = {arXiv},
  eprint    = {1810.13354},
  timestamp = {Thu, 08 Nov 2018 10:57:46 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-13354.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{QDECPOMDPPLAN1,
author = {Brafman, Ronen I. and Shani, Guy and Zilberstein, Shlomo},
title = {Qualitative Planning under Partial Observability in Multi-Agent Domains},
year = {2013},
publisher = {AAAI Press},
booktitle = {Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence},
pages = {130–137},
numpages = {8},
location = {Bellevue, Washington},
series = {AAAI’13}
}

@inproceedings{CPOR,
  author    = {Sagi Bazinin and Guy Shani},
  title     = {Iterative Planning for Deterministic QDec-POMDPs},
  booktitle = {GCAI-2018. 4th Global Conference on Artificial Intelligence},
  volume    = {55},
  pages     = {15--28},
  year      = {2018}
  }

@article{INTERPRIVATE,
title = "Representing and planning with interacting actions and privacy",
journal = "Artificial Intelligence",
volume = "278",
pages = "103200",
year = "2020",
issn = "0004-3702",
doi = "https://doi.org/10.1016/j.artint.2019.103200",
url = "http://www.sciencedirect.com/science/article/pii/S0004370218302832",
author = "Shashank Shekhar and Ronen I. Brafman",
keywords = "Multi-agent planning, Deterministic planning, Concurrent interacting actions, Distributed privacy preserving planning",
abstract = "Interacting actions – actions whose joint effect differs from the union of their individual effects – are challenging both to represent and to plan with due to their combinatorial nature. So far, there have been few attempts to provide a succinct language for representing them that can also support efficient centralized planning and distributed privacy preserving planning. In this paper we suggest an approach for representing interacting actions succinctly and show how such a domain model can be compiled into a standard single-agent planning problem as well as to privacy preserving multi-agent planning. We test the performance of our method on a number of novel domains involving interacting actions and privacy."
}

@article{MADP,
author = {Oliehoek, Frans A. and Spaan, Matthijs T. J. and Terwijn, Bas and Robbel, Philipp and Messias, Jo\~{a}o V.},
title = {The MADP Toolbox: An Open Source Library for Planning and Learning in (Multi-)Agent Systems},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3112–3116},
numpages = {5},
keywords = {multiagent systems, software, decision-theoretic planning, reinforcement learning}
}
  
@article{KLDIV,
  author    = {Rohit Agrawal},
  title     = {Concentration of the multinomial in Kullback-Leibler divergence near
               the ratio of alphabet and sample sizes},
  journal   = {CoRR},
  volume    = {abs/1904.02291},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.02291},
  archivePrefix = {arXiv},
  eprint    = {1904.02291},
  timestamp = {Sun, 02 Jun 2019 09:36:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-02291.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

 @misc{PomdpX,
      title  = {PomdpX File Format (version 1.0)},
      url = {https://bigbird.comp.nus.edu.sg/pmwiki/farm/appl/index.php?n=Main.PomdpXDocumentation},
      year = {2014}
}

@article{QDECPOMDPPLAN2,
	title={A Factored Approach to Deterministic Contingent Multi-Agent Planning},
	author={Shekhar, Shashank and Brafman, I. Ronen and Shani, Guy},
	journal={ICAPS},
	pages={419--427},
	year={2019}
}


@inproceedings{RLCYCLES,
author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
title = {Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},
year = {1999},
isbn = {1558606122},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning},
pages = {278–287},
numpages = {10},
series = {ICML ’99}
}

  


@article{NB14,
	Author = {Raz Nissim and Ronen I. Brafman},
	Date-Added = {2019-07-31 16:55:27 +0300},
	Date-Modified = {2019-07-31 16:55:27 +0300},
	Journal = {Journal of Artificial Intelligence Research (JAIR)},
	Pages = {293--332},
	Title = {Distributed Heuristic Forward Search for Multi-agent Planning},
	Volume = {51},
	Year = {2014}}



@inproceedings{MaliahBS17,
	Author = {Shlomi Maliah and Ronen I. Brafman and Guy Shani},
		Booktitle = {Proceedings of the Twenty-Seventh International Conference on Automated Planning and Scheduling, {ICAPS} },
	Pages = {209--217},
	Title = {Increased Privacy with Reduced Communication in Multi-Agent Planning},
	Year = {2017},
	}
	
@INPROCEEDINGS{REWARDSHAPING,
  author={T. {Brys} and A. {Harutyunyan} and P. {Vrancx} and M. E. {Taylor} and D. {Kudenko} and A. {Nowe}},
  booktitle={2014 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Multi-objectivization of reinforcement learning problems by reward shaping}, 
  year={2014},
  volume={},
  number={},
  pages={2315-2322},}
  
@inproceedings{REWARDSHAPING2,
author = {Laud, Adam and DeJong, Gerald},
title = {The Influence of Reward on the Speed of Reinforcement Learning: An Analysis of Shaping},
year = {2003},
isbn = {1577351894},
publisher = {AAAI Press},
booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
pages = {440–447},
numpages = {8},
location = {Washington, DC, USA},
series = {ICML’03}
}


