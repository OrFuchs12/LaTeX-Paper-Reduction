\documentclass[a4paper]{article}


\newcommand{\commentout}[1]{}
\newcommand{\eliran}[1]{\textbf{[\color{red}ELIRAN:#1]}}
\newcommand{\ronen}[1]{\textbf{[\color{blue}RONEN:#1]}}
\newcommand{\guy}[1]{\textbf{[\color{orange}GUY:#1]}}

\begin{document}

\subsection{Review 1}
\textbf{1. In subsection 2.1 $\tau$ is introduced without being defined what $\tau$ represents.}
Typo - $\tau$ refers to $T$.

\textbf{2. Is the term influencer really used somewhere?}
The influencers of an action, together with its effects, form the relevant variables of the actions.
We also use it in the final step of the single-agent construction to remove rewards related to public variables in the original problem.

\textbf{5. Section 3.1 makes less sense. If the authors want to have initial states similar to initial belief, why don't the authors use KL-Div (or Jensen-Shannon divergence) directly. Why with the probability shown in sect 3.1? and why is there a lower limit on this similarity? contradicting with authors' earlier statement.}
It is possible to sample belief states iteratively and measure the KL-Divergence between the empirical distribution, but this would require an unknown number of samples and translates to no probabilistic guarantee. Using concentration bound we can pre-determine the number of samples and guarantee a certain level of similarity.
Regarding the second question, we upper bound with $\alpha$ the probability of the distributions to be far from each other, specifically at-most $\beta$ far. Instead we could have lower bounded with $1-\alpha$ the probability of the distributions to be at-least $\beta$ close.

\textbf{6. Shouldn't the deterministic assumption of a non-collaborative action be a global assumption that works on the team policy too? If it only applies on the single agent policy, how do the authors convert the action since no example for this whatsoever?}
I assume the intention was to non-public (i.e. private) action rather than non-collaborative. Determinization of co-agents' private actions is optional, and can be performed only under the assumption that actions has a known desired effect. Conversion in these case is easy, the action achieves the desirable effect with probability 1.

\textbf{7. I am lost at the concept of anchoring step, especially I can not grasp what the following sentences trying to say 'To overcome that, for each CA, we cancel the penalty in contexts that the agent might find itself in when aiming to execute a CA. We refer to this broader context as the anchored context of the CA.'}
We cancel the penalty given to agents in states that specify variables that are hard to observe (e.g. the location of a box), by taking the original context and projecting away those variables, receiving a new context (the anchored context). Only in he new context, that specifies only easy-to-observe variables (anchor variables) such as the location of the agent, the penalty persists. We cancel the penalty in all states that reside in the anchored context and were previously penalized.


\subsection{Review 2}
\textbf{Regarding Figure 1, it is not very easy to follow the notations used. For example
it is not clear what the labels on the edges mean.}
This is the output of the SARSOP planner. The only important labels are $A$, referring to the action of the node, and $o$ which is the observations (e.g. $\left(yes,null\right)$, referring to the branching observations.
For completeness' sake -
$Y$ refers to the most likely state, $null$ stands for $null-obs$, $action_idle$ is our fixed idle action and the number near each observation is the probability $O(o|Y,a)$.

\subsection{Review 3}
\textbf{After reading the paper I don't have the feeling
that I could say whether the approach works particularly well for my problem,
but rather that there is this new method that works quite well, so I might try
it out, which feels less satisfactory to me.}
Research that revolves around the impacts of team problem solutions on their corresponding decentralized solution, and specifically how one's optimality relates to the other, would aid in better defining such problems.
\end{document}
