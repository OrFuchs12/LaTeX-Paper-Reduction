@inproceedings{xie2016unsupervisedDEC,
  title={Unsupervised deep embedding for clustering analysis},
  author={Xie, Junyuan and Girshick, Ross and Farhadi, Ali},
  booktitle={International conference on machine learning},
  pages={478--487},
  year={2016},
  organization={PMLR}
}
@inproceedings{IDEC2017,
author = {Guo, Xifeng and Gao, Long and Liu, Xinwang and Yin, Jianping},
title = {Improved Deep Embedded Clustering with Local Structure Preservation},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Deep clustering learns deep feature representations that favor clustering task using neural networks. Some pioneering work proposes to simultaneously learn embedded features and perform clustering by explicitly defining a clustering oriented loss. Though promising performance has been demonstrated in various applications, we observe that a vital ingredient has been overlooked by these work that the defined clustering loss may corrupt feature space, which leads to non-representative meaningless features and this in turn hurts clustering performance. To address this issue, in this paper, we propose the Improved Deep Embedded Clustering (IDEC) algorithm to take care of data structure preservation. Specifically, we manipulate feature space to scatter data points using a clustering loss as guidance. To constrain the manipulation and maintain the local structure of data generating distribution, an under-complete autoencoder is applied. By integrating the clustering loss and autoencoder's reconstruction loss, IDEC can jointly optimize cluster labels assignment and learn features that are suitable for clustering with local structure preservation. The resultant optimization problem can be effectively solved by mini-batch stochastic gradient descent and backpropagation. Experiments on image and text datasets empirically validate the importance of local structure preservation and the effectiveness of our algorithm.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {1753â€“1759},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}
@article{wang2019attributedDAEGC,
  title={Attributed graph clustering: A deep attentional embedding approach},
  author={Wang, Chun and Pan, Shirui and Hu, Ruiqi and Long, Guodong and Jiang, Jing and Zhang, Chengqi},
  journal={arXiv preprint arXiv:1906.06532},
  year={2019}
}

@article{mrabah2022rethinking,
  title={Rethinking graph auto-encoder models for attributed graph clustering},
  author={Mrabah, Nairouz and Bouguessa, Mohamed and Touati, Mohamed Fawzi and Ksantini, Riadh},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2022},
  publisher={IEEE}
}
@inproceedings{bo2020structural,
  title={Structural deep clustering network},
  author={Bo, Deyu and Wang, Xiao and Shi, Chuan and Zhu, Meiqi and Lu, Emiao and Cui, Peng},
  booktitle={Proceedings of The Web Conference 2020},
  pages={1400--1410},
  year={2020}
}

@article{su2022comprehensive,
  title={A comprehensive survey on community detection with deep learning},
  author={Su, Xing and Xue, Shan and Liu, Fanzhen and Wu, Jia and Yang, Jian and Zhou, Chuan and Hu, Wenbin and Paris, Cecile and Nepal, Surya and Jin, Di and others},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}
@article{rossetti2018community,
  title={Community discovery in dynamic networks: a survey},
  author={Rossetti, Giulio and Cazabet, R{\'e}my},
  journal={ACM computing surveys (CSUR)},
  volume={51},
  number={2},
  pages={1--37},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{zhou2022comprehensive,
  title={A comprehensive survey on deep clustering: Taxonomy, challenges, and future directions},
  author={Zhou, Sheng and Xu, Hongjia and Zheng, Zhuonan and Chen, Jiawei and Bu, Jiajun and Wu, Jia and Wang, Xin and Zhu, Wenwu and Ester, Martin and others},
  journal={arXiv preprint arXiv:2206.07579},
  year={2022}
}
@article{zhang2022embedding,
  title={Embedding graph auto-encoder for graph clustering},
  author={Zhang, Hongyuan and Li, Pei and Zhang, Rui and Li, Xuelong},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@article{GUO2004101,
title = {A pseudoinverse learning algorithm for feedforward neural networks with stacked generalization applications to software reliability growth data},
journal = {Neurocomputing},
volume = {56},
pages = {101-121},
year = {2004},
issn = {0925-2312},
doi = {https://doi.org/10.1016/S0925-2312(03)00385-0},
url = {https://www.sciencedirect.com/science/article/pii/S0925231203003850},
author = {Ping Guo and Michael R. Lyu},
keywords = {Feedforward neural networks, Generalized linear algebra, Pseudoinverse learning algorithm, Fast learning, Generalization},
abstract = {A supervised learning algorithm, Pseudoinverse Learning Algorithm (PIL), for feedforward neural networks is developed. The algorithm is based on generalized linear algebraic methods, and it adopts matrix inner products and pseudoinverse operations. Incorporating with network architecture of which the number of hidden layer neuron is equal to the number of examples to be learned, the algorithm eliminates learning errors by adding hidden layers and will give an exact solution (perfect learning). Unlike the existing gradient descent algorithm, the PIL is a feedforward only, fully automated algorithm, including no critical user-dependent parameters such as learning rate or momentum constant. The algorithm is tested on case studies with stacked generalization applications to software reliability growth data. The results indicate that the proposed algorithm is very efficient for the investigation on the computation-intensive generalization techniques.}
}

@inproceedings{Hofer17a,
    author    = {C.~Hofer, R.~Kwitt, M.~Niethammer and A.~Uhl},
    title     = {Deep Learning with Topological Signatures},
    booktitle = {NIPS},
    year      = {2017}}

@inproceedings{Hofer19a,
    author    = {C.~Hofer, R.~Kwitt, M.~Dixit and M.~Niethammer},
    title     = {Connectivity-Optimized Representation Learning via Persistent Homology},
    booktitle = {ICML},
    year      = {2019}}

@article{Hofer19b,
    author    = {C.~Hofer, R.~Kwitt, and M.~Niethammer},
    title     = {Learning Representations of Persistence Barcodes},
    booktitle = {JMLR},
    year      = {2019}}
   
@inproceedings{gabrielsson2020topology,
  title={A topology layer for machine learning},
  author={Gabrielsson, Rickard Br{\"u}el and Nelson, Bradley J and Dwaraknath, Anjan and Skraba, Primoz},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1553--1563},
  year={2020},
  organization={PMLR}
}

@book{dey2022computational,
  title={Computational topology for data analysis},
  author={Dey, Tamal Krishna and Wang, Yusu},
  year={2022},
  publisher={Cambridge University Press}
}

@article{kipf2016variational,
  title={Variational Graph Auto-Encoders},
  author={Kipf, Thomas N and Welling, Max},
  journal={stat},
  volume={1050},
  pages={21},
  year={2016}
}


@inproceedings{salehi2020graph,
  title={Graph attention auto-Encoders},
  author={Salehi, Amin and Davulcu, Hasan},
  booktitle={2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)},
  pages={989--996},
  year={2020},
  organization={IEEE Computer Society}
}
@article{creswell2018generative,
  title={Generative adversarial networks: an overview},
  author={Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A},
  journal={IEEE Signal Processing Magazine},
  volume={35},
  number={1},
  pages={53--65},
  year={2018},
  publisher={IEEE}
}
@article{zhou2020graph,
  title={Graph neural networks: a review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={AI Open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier}
}

@article{skarding2021foundations,
  title={Foundations and modeling of dynamic networks using dynamic graph neural networks: A survey},
  author={Skarding, Joakim and Gabrys, Bogdan and Musial, Katarzyna},
  journal={IEEE Access},
  volume={9},
  pages={79143--79168},
  year={2021},
  publisher={IEEE}
}

@article{medsker2001recurrent,
  title={Recurrent neural networks},
  author={Medsker, Larry R and Jain, LC},
  journal={Design and Applications},
  volume={5},
  pages={64--67},
  year={2001}
}

@inproceedings{shaw2018self,
    title = "Self-Attention with Relative Position Representations",
    author = "Shaw, Peter  and
      Uszkoreit, Jakob  and
      Vaswani, Ashish",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2074",
    doi = "10.18653/v1/N18-2074",
    pages = "464--468",
    abstract = "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.",
}

@article{zhang2019graph,
  title={Graph convolutional networks: a comprehensive review},
  author={Zhang, Si and Tong, Hanghang and Xu, Jiejun and Maciejewski, Ross},
  journal={Computational Social Networks},
  volume={6},
  number={1},
  pages={1--23},
  year={2019},
  publisher={SpringerOpen}
}
@article{kuhn1955hungarian,
  title={The Hungarian method for the assignment problem},
  author={Kuhn, Harold W},
  journal={Naval research logistics quarterly},
  volume={2},
  number={1-2},
  pages={83--97},
  year={1955},
  publisher={Wiley Online Library}
}

@inproceedings{peng2021attention,
  title={Attention-driven graph clustering network},
  author={Peng, Zhihao and Liu, Hui and Jia, Yuheng and Hou, Junhui},
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
  pages={935--943},
  year={2021}
}

@article{petri2013topological,
  title={Topological strata of weighted complex networks},
  author={Petri, Giovanni and Scolamiero, Martina and Donato, Irene and Vaccarino, Francesco},
  journal={PloS One},
  volume={8},
  number={6},
  pages={e66506},
  year={2013},
  publisher={Public Library of Science San Francisco, USA}
}

@article{huang2016persistent,
  title={Persistent homology lower bounds on high-order network distances},
  author={Huang, Weiyu and Ribeiro, Alejandro},
  journal={IEEE Transactions on Signal Processing},
  volume={65},
  number={2},
  pages={319--334},
  year={2016},
  publisher={IEEE}
}

@article{rieck2017clique,
  title={Clique community persistence: A topological visual analysis approach for complex networks},
  author={Rieck, Bastian and Fugacci, Ulderico and Lukasczyk, Jonas and Leitte, Heike},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  volume={24},
  number={1},
  pages={822--831},
  year={2017},
  publisher={IEEE}
}

@phdthesis{lozeve2018topological,
  title={Topological Data Analysis of Temporal Networks},
  author={Lozeve, Dimitri},
  year={2018},
  school={University of Oxford}
}

@inproceedings{dcrn,
  title={Deep graph clustering via dual correlation reduction},
  author={Liu, Yue and Tu, Wenxuan and Zhou, Sihang and Liu, Xinwang and Song, Linxuan and Yang, Xihong and Zhu, En},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={7},
  pages={7603--7611},
  year={2022}
}

@article{liu2013entropy,
  title={Entropy-rate clustering: Cluster analysis via maximizing a submodular function subject to a matroid constraint},
  author={Liu, Ming-Yu and Tuzel, Oncel and Ramalingam, Srikumar and Chellappa, Rama},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={36},
  number={1},
  pages={99--112},
  year={2013},
  publisher={IEEE}
}

@article{likas2003global,
  title={The global k-means clustering algorithm},
  author={Likas, Aristidis and Vlassis, Nikos and Verbeek, Jakob J},
  journal={Pattern Recognition},
  volume={36},
  number={2},
  pages={451--461},
  year={2003},
  publisher={Elsevier}
}


@inproceedings{spectralclustering,
author = {Ng, Andrew Y. and Jordan, Michael I. and Weiss, Yair},
title = {On Spectral Clustering: Analysis and an Algorithm},
year = {2001},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Despite many empirical successes of spectral clustering methodsâ€” algorithms that cluster points using eigenvectors of matrices derived from the dataâ€”there are several unresolved issues. First. there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.},
booktitle = {Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic},
pages = {849â€“856},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'01}
}

@InProceedings{glorotinit,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@article{carlsson2009topology,
  title={Topology and data},
  author={Carlsson, Gunnar},
  journal={Bulletin of the American Mathematical Society},
  volume={46},
  number={2},
  pages={255--308},
  year={2009}
}

@article{admmboyd,
  title={Distributed optimization and statistical learning via the alternating direction method of multipliers},
  author={Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan and others},
  journal={Foundations and Trends{\textregistered} in Machine learning},
  volume={3},
  number={1},
  pages={1--122},
  year={2011},
  publisher={Now Publishers, Inc.}
}

@Inbook{Corne2010,
author="Corne, David
and Handl, Julia
and Knowles, Joshua",
editor="Sammut, Claude
and Webb, Geoffrey I.",
title="Evolutionary Clustering",
bookTitle="Encyclopedia of Machine Learning",
year="2010",
publisher="Springer US",
address="Boston, MA",
pages="332--337",
isbn="978-0-387-30164-8",
doi="10.1007/978-0-387-30164-8_271",
url="https://doi.org/10.1007/978-0-387-30164-8_271"
}
@article{espra,
doi = {10.1088/1742-5468/2017/1/013401},
url = {https://dx.doi.org/10.1088/1742-5468/2017/1/013401},
year = {2017},
month = {jan},
publisher = {IOP Publishing and SISSA},
volume = {2017},
number = {1},
pages = {013401},
author = {Peizhuo Wang and Lin Gao and Xiaoke Ma},
title = {Dynamic community detection based on network structural perturbation and topological similarity},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
abstract = {Community detection in dynamic networks has been extensively studied since it sheds light on the structure-function relation of the overall complex systems. Recently, it has been demonstrated that the structural perturbation in static networks is excellent in characterizing the topology. In order to investigate the perturbation structural theory in dynamic networks, we extend the theory by considering the dynamic variation information between networks of consecutive time. Then a novel similarity is proposed by combing structural perturbation and topological features. Finally, we present an evolutionary clustering algorithm to detect dynamic communities under the temporal smoothness framework. Experimental results on both artificial and real dynamic networks demonstrate that the proposed similarity is promising in dynamic community detection since it improves the clustering accuracy compared with state-of-the-art methods, indicating the superiority of the presented similarity measure.}
}

@article{dcdsurvey,
author = {DUAN Xiangyu, YUAN Guan, MENG Fanrong},
title = {Dynamic Community Detection: A Survey},
publisher = {Journal of Frontiers of Computer Science and Technology},
year = {2021},
journal = {Journal of Frontiers of Computer Science & Technology},
volume = {15},
number = {4},
eid = {612},
pages = {612-630},
keywords = {<p>dynamic community detection; social network; network analysis; dynamic community evolution</p>},
doi = http://fcst.ceaj.org/EN/10.3778/j.issn.1673-9418.2008023
}    

@article{Zhou2009,
author={Zhou, Tao
and L{\"u}, Linyuan
and Zhang, Yi-Cheng},
title={Predicting missing links via local information},
journal={The European Physical Journal B},
year={2009},
month={Oct},
day={01},
volume={71},
number={4},
pages={623-630},
abstract={Missing link prediction in networks is of both theoreticalinterest and practical significance in modern science. In thispaper, we empirically investigate a simple framework of linkprediction on the basis of node similarity. We compare ninewell-known local similarity measures on six real networks. Theresults indicate that the simplest measure, namely CommonNeighbours, has the best overall performance, and the Adamic-Adarindex performs second best. A new similarity measure, motivated bythe resource allocation process taking place on networks, isproposed and shown to have higher prediction accuracy than commonneighbours. It is found that many links are assigned the same scoresif only the information of the nearest neighbours is used. Wetherefore design another new measure exploiting information on thenext nearest neighbours, which can remarkably enhance the predictionaccuracy.},
issn={1434-6036},
doi={10.1140/epjb/e2009-00335-8},
url={https://doi.org/10.1140/epjb/e2009-00335-8}
}

@inproceedings{yao2021interpretable,
  title={Interpretable clustering on dynamic graphs with recurrent graph neural networks},
  author={Yao, Yuhang and Joe-Wong, Carlee},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={5},
  pages={4608--4616},
  year={2021}
}

@inproceedings{park2022cgc,
  title={Cgc: Contrastive graph clustering forcommunity detection and tracking},
  author={Park, Namyong and Rossi, Ryan and Koh, Eunyee and Burhanuddin, Iftikhar Ahamath and Kim, Sungchul and Du, Fan and Ahmed, Nesreen and Faloutsos, Christos},
  booktitle={Proceedings of the ACM Web Conference 2022},
  pages={1115--1126},
  year={2022}
}

@inproceedings{you2021robust,
  title={Robust dynamic clustering for temporal networks},
  author={You, Jingyi and Hu, Chenlong and Kamigaito, Hidetaka and Funakoshi, Kotaro and Okumura, Manabu},
  booktitle={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  pages={2424--2433},
  year={2021}
}

@article{folino2013evolutionary,
  title={An evolutionary multiobjective approach for community discovery in dynamic networks},
  author={Folino, Francesco and Pizzuti, Clara},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={26},
  number={8},
  pages={1838--1852},
  year={2013},
  publisher={IEEE}
}

@article{mrabah2020deep,
  title={Deep clustering with a dynamic autoencoder: From reconstruction towards centroids construction},
  author={Mrabah, Nairouz and Khan, Naimul Mefraz and Ksantini, Riadh and Lachiri, Zied},
  journal={Neural Networks},
  volume={130},
  pages={206--228},
  year={2020},
  publisher={Elsevier}
}

@article{goyal2020dyngraph2vec,
  title={dyngraph2vec: Capturing network dynamics using dynamic graph representation learning},
  author={Goyal, Palash and Chhetri, Sujit Rokka and Canedo, Arquimedes},
  journal={Knowledge-Based Systems},
  volume={187},
  pages={104816},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{songdechakraiwut2021topological,
  title={Topological learning and its application to multimodal brain network integration},
  author={Songdechakraiwut, Tananun and Shen, Li and Chung, Moo},
  booktitle={Medical Image Computing and Computer Assisted Intervention--MICCAI 2021: 24th International Conference, Strasbourg, France, September 27--October 1, 2021, Proceedings, Part II 24},
  pages={166--176},
  year={2021},
  organization={Springer}
}

@inproceedings{songdechakraiwut2020dynamic,
  title={Dynamic topological data analysis for functional brain signals},
  author={Songdechakraiwut, Tananun and Chung, Moo K},
  booktitle={2020 IEEE 17th International Symposium on Biomedical Imaging Workshops (ISBI Workshops)},
  pages={1--4},
  year={2020},
  organization={IEEE}
}

@article{songdechakraiwut2023topological,
  title={Topological learning for brain networks},
  author={Songdechakraiwut, Tananun and Chung, Moo K},
  journal={The Annals of Applied Statistics},
  volume={17},
  number={1},
  pages={403},
  year={2023},
  publisher={NIH Public Access}
}

@inproceedings{yan2021link,
  title={Link prediction with persistent homology: An interactive view},
  author={Yan, Zuoyu and Ma, Tengfei and Gao, Liangcai and Tang, Zhi and Chen, Chao},
  booktitle={International Conference on Machine Learning},
  pages={11659--11669},
  year={2021},
  organization={PMLR}
}

@misc{bauckhage2015kmeans,
      title={k-Means Clustering Is Matrix Factorization}, 
      author={Christian Bauckhage},
      year={2015},
      eprint={1512.07548},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Blondel_2008,
doi = {10.1088/1742-5468/2008/10/P10008},
url = {https://dx.doi.org/10.1088/1742-5468/2008/10/P10008},
year = {2008},
month = {oct},
publisher = {},
volume = {2008},
number = {10},
pages = {P10008},
author = {Vincent D Blondel and Jean-Loup Guillaume and Renaud Lambiotte and Etienne Lefebvre},
title = {Fast unfolding of communities in large networks},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
abstract = {We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection methods in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2 million customers and by analysing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad hoc modular networks.}
}

@Article{math11122674,
AUTHOR = {Du, Ke-Lin and Swamy, M. N. S. and Wang, Zhang-Quan and Mow, Wai Ho},
TITLE = {Matrix Factorization Techniques in Machine Learning, Signal Processing, and Statistics},
JOURNAL = {Mathematics},
VOLUME = {11},
YEAR = {2023},
NUMBER = {12},
ARTICLE-NUMBER = {2674},
URL = {https://www.mdpi.com/2227-7390/11/12/2674},
ISSN = {2227-7390},
ABSTRACT = {Compressed sensing is an alternative to Shannon/Nyquist sampling for acquiring sparse or compressible signals. Sparse coding represents a signal as a sparse linear combination of atoms, which are elementary signals derived from a predefined dictionary. Compressed sensing, sparse approximation, and dictionary learning are topics similar to sparse coding. Matrix completion is the process of recovering a data matrix from a subset of its entries, and it extends the principles of compressed sensing and sparse approximation. The nonnegative matrix factorization is a low-rank matrix factorization technique for nonnegative data. All of these low-rank matrix factorization techniques are unsupervised learning techniques, and can be used for data analysis tasks, such as dimension reduction, feature extraction, blind source separation, data compression, and knowledge discovery. In this paper, we survey a few emerging matrix factorization techniques that are receiving wide attention in machine learning, signal processing, and statistics. The treated topics are compressed sensing, dictionary learning, sparse representation, matrix completion and matrix recovery, nonnegative matrix factorization, the Nystr&ouml;m method, and CUR matrix decomposition in the machine learning framework. Some related topics, such as matrix factorization using metaheuristics or neurodynamics, are also introduced. A few topics are suggested for future investigation in this article.},
DOI = {10.3390/math11122674}
}

@article{ALEXANDER2010376,
title = {Experimental and computational determination of tRNA dynamics},
journal = {FEBS Letters},
volume = {584},
number = {2},
pages = {376-386},
year = {2010},
note = {Transfer RNA},
issn = {0014-5793},
doi = {https://doi.org/10.1016/j.febslet.2009.11.061},
url = {https://www.sciencedirect.com/science/article/pii/S0014579309009740},
author = {Rebecca W. Alexander and John Eargle and Zaida Luthey-Schulten},
keywords = {tRNA, Flexibility, Conformational change, Induced fit, Molecular dynamics, Translation},
abstract = {As the molecular representation of the genetic code, tRNA plays a central role in the translational machinery where it interacts with several proteins and other RNAs during the course of protein synthesis. These interactions exploit the dynamic flexibility of tRNA. In this minireview, we discuss the effects of modified bases, ions, and proteins on tRNA structure and dynamics and the challenges of observing its motions over the cycle of translation.}
}

@inproceedings{Hajij2018,
  author={Hajij, Mustafa and Wang, Bei and Scheidegger, Carlos and Rosen, Paul},
  booktitle={2018 IEEE Pacific Visualization Symposium (PacificVis)}, 
  title={Visual Detection of Structural Changes in Time-Varying Graphs Using Persistent Homology}, 
  year={2018},
  volume={},
  number={},
  pages={125-134},
  doi={10.1109/PacificVis.2018.00024}}

@article{liu2020detecting,
  title={Detecting the evolving community structure in dynamic social networks},
  author={Liu, Fanzhen and Wu, Jia and Xue, Shan and Zhou, Chuan and Yang, Jian and Sheng, Quanzheng},
  journal={World Wide Web},
  volume={23},
  pages={715--733},
  year={2020},
  publisher={Springer}
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={11},
  year={2008}
}

@article{crawford2018cluenet,
  title={ClueNet: Clustering a temporal network based on topological similarity rather than denseness},
  author={Crawford, Joseph and Milenkovi{\'c}, Tijana},
  journal={PloS One},
  volume={13},
  number={5},
  pages={e0195993},
  year={2018},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{hou2020glodyne,
  title={Glodyne: Global topology preserving dynamic network embedding},
  author={Hou, Chengbin and Zhang, Han and He, Shan and Tang, Ke},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={34},
  number={10},
  pages={4826--4837},
  year={2020},
  publisher={IEEE}
}

@inproceedings{carriere2017sliced,
  title={Sliced Wasserstein kernel for persistence diagrams},
  author={Carriere, Mathieu and Cuturi, Marco and Oudot, Steve},
  booktitle={International conference on machine learning},
  pages={664--673},
  year={2017},
  organization={PMLR}
}

@article{javed2018community,
  title={Community detection in networks: A multidisciplinary review},
  author={Javed, Muhammad Aqib and Younis, Muhammad Shahzad and Latif, Siddique and Qadir, Junaid and Baig, Adeel},
  journal={Journal of Network and Computer Applications},
  volume={108},
  pages={87--111},
  year={2018},
  publisher={Elsevier}
}

@article{liu2020determine,
  title={Determine the number of unknown targets in open world based on elbow method},
  author={Liu, Fan and Deng, Yong},
  journal={IEEE Transactions on Fuzzy Systems},
  volume={29},
  number={5},
  pages={986--995},
  year={2020},
  publisher={IEEE}
}

@inproceedings{pareja2020evolvegcn,
  title={EvolveGCN: Evolving graph convolutional networks for dynamic graphs},
  author={Pareja, Aldo and Domeniconi, Giacomo and Chen, Jie and Ma, Tengfei and Suzumura, Toyotaro and Kanezashi, Hiroki and Kaler, Tim and Schardl, Tao B and Leiserson, Charles E},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020},
  organization={AAAI press}
}
@misc{liu2023deep,
      title={Deep Temporal Graph Clustering}, 
      author={Meng Liu and Yue Liu and Ke Liang and Siwei Wang and Sihang Zhou and Xinwang Liu},
      year={2023},
      eprint={2305.10738},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{fastgae,
author = {Salha, Guillaume and Hennequin, Romain and Remy, Jean-Baptiste and Moussallam, Manuel and Vazirgiannis, Michalis},
title = {FastGAE: Scalable Graph Autoencoders with Stochastic Subgraph Decoding},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {142},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2021.04.015},
doi = {10.1016/j.neunet.2021.04.015},
journal = {Neural Netw.},
month = {oct},
pages = {1â€“19},
numpages = {19},
keywords = {Scalability, Graph convolutional networks, Graph autoencoders, Graph variational autoencoders, Link prediction, Node clustering}
}





@misc{brÃ¼elgabrielsson2020topology,
      title={A Topology Layer for Machine Learning}, 
      author={Rickard BrÃ¼el-Gabrielsson and Bradley J. Nelson and Anjan Dwaraknath and Primoz Skraba and Leonidas J. Guibas and Gunnar Carlsson},
      year={2020},
      eprint={1905.12200},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{
vandaele2022topologically,
title={Topologically Regularized Data Embeddings},
author={Robin Vandaele and Bo Kang and Jefrey Lijffijt and Tijl De Bie and Yvan Saeys},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=P1QUVhOtEFP}
}
