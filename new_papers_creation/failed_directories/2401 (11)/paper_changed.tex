%File: anonymous-submission-latex-2024.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% % \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
%\title{The Role of Higher-Order Cognitive Models in Active Learning}
% \title{Active learning with higher-order cognitive models}
% \author{
%     %Authors
%     % All authors must be in the same font size and format.
%     Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
%     AAAI Style Contributions by Pater Patel Schneider,
%     Sunil Issar,\\
%     J. Scott Penberthy,
%     George Ferguson,
%     Hans Guesgen,
%     Francisco Cruz\equalcontrib,
%     Marc Pujol-Gonzalez\equalcontrib
% }
% \affiliations{
%     %Afiliations
%     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
%     % If you have multiple authors and multiple affiliations
%     % use superscripts in text and roman font to identify them.
%     % For example,

%     % Sunil Issar\textsuperscript{\rm 2},
%     % J. Scott Penberthy\textsuperscript{\rm 3},
%     % George Ferguson\textsuperscript{\rm 4},
%     % Hans Guesgen\textsuperscript{\rm 5}
%     % Note that the comma should be placed after the superscript

%     1900 Embarcadero Road, Suite 101\\
%     Palo Alto, California 94303-3310 USA\\
%     % email address must be in roman text type, not monospace or sans serif
%     proceedings-questions@aaai.org
% %
% % See more examples next
% }

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

%\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{The Role of Higher-Order Cognitive Models in Active Learning}
\author {
    % Authors
    Oskar Keurulainen, %\textsuperscript{\rm 1},
    Gokhan Alcan, %\textsuperscript{\rm 1},
    Ville Kyrki%\textsuperscript{\rm 1}
}
% \affiliations {
%     % Affiliations
%     Department of Electrical Engineering and Automation, Aalto University, Finland\\
%     %\textsuperscript{\rm 1}Aalto University\\
%     %\textsuperscript{\rm 2}Aalto University\\
%     firstname.lastname@aalto.fi
% }
%\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}

Building machines capable of efficiently collaborating with humans has been a longstanding goal in artificial intelligence. Especially in the presence of uncertainties, optimal cooperation often requires that humans and artificial agents model each other's behavior and use these models to infer underlying goals, beliefs or intentions, potentially involving multiple levels of recursion. Empirical evidence for such higher-order cognition in human behavior is also provided by previous works in cognitive science, linguistics, and robotics. We advocate for a new paradigm for active learning for human feedback that utilises humans as active data sources while accounting for their higher levels of agency. In particular, we discuss how increasing level of agency results in qualitatively different forms of rational communication between an active learning system and a teacher. Additionally, we provide a practical example of active learning using a higher-order cognitive model. This is accompanied by a computational study that underscores the unique behaviors that this model produces.

\end{abstract}

\insert\footins{\noindent\footnotesize This work was financially supported by the Academy of Finland grant numbers 345661 and 347199. \\ All authors are with the Intelligent Robotics Group, Department of Electrical Engineering and Automation, Aalto University, Finland. \\ E-mails: \texttt{firstname.lastname@aalto.fi}.}

\section{Introduction}

The active involvement of humans in training of Artificial Intelligence (AI) systems is becoming increasingly common. Compared to passive data sources, a key advantage of learning from human feedback is that humans can be queried by active learning methods, thereby maximising the usefulness of the expected data. However, a computational model of human behavior, a necessary component of such AI systems, motivates multidisciplinary research efforts to better understand human-AI interactions. A typical modeling assumption in learning from human feedback is Boltzmann rationality, which models goal-directed human behavior as approximately rational with respect to the task, but does not account for potential human situational awareness that extends beyond the task itself.

\begin{figure}[t!]
%removedVspace
	\centering
	\def\svgwidth{1\linewidth}
	{
 \fontsize{9}{9}
		\input{fig1_v2.pdf_tex}}
	\caption{Illustration of a sophisticated agent designed to integrate higher-order cognitive modeling into its learning process. The agent not only learns from human interaction but also recognizes and incorporates the human's perception and understanding of the agent itself into its model. The recursive nature of this cognitive modeling, extending through multiple levels, potentially amplifies the agent's ability to ask better questions and learn efficiently from human feedback.}
	\label{fig:fig1}
\end{figure}

In stark contrast, well established models from cognitive science \cite{Stahl1995level-k, goodman2016rsa} indicate that especially in social settings, humans may have multiple levels of such situational awareness. These models not only account for how humans model other agents they interact with, but also \textit{how they can attribute such capabilities to other agents} in a recursive fashion. Furthermore, besides these models predicting human behavior more accurately in specific contexts, efficient communication, such as that entailed by the rational speech acts (RSA) framework \cite{goodman2016rsa}, \textit{might also require} higher order recursive modeling. This highlights an opportunity for improving the sample efficiency of AI-human interaction by incorporating such models as a part of AI systems.

Moving towards the potential use of higher-order cognitive models in active learning, we present a taxonomy of interaction involving different levels of cognition (Fig.~\ref{fig:fig1}), incorporating qualitative descriptions of the types of behaviors that different levels allow for. We also provide an example of a higher-order cognitive model and demonstrate the types of behavior it can yield. Lastly, we discuss how these examples guide the conduct of future user studies and illuminate other promising research directions.



\section{Background and related work}



\subsection{Higher-order cognitive models in computational rationality}

We base our proposed framework for higher-order active learning on the theory of computational rationality, which considers human behavior to be optimal with respect to certain preferences and under constraints imposed by certain cognitive bounds \cite{gerchman2015computationalrationality}. One of the first empirically established formulations of hierarchical cognition in computational rationality is given by the level-k family of models \cite{Stahl1995level-k}, where the computational bound specifies the depth of recursion that an agent can perform. RSA \cite{goodman2016rsa} is another seminal theory based on higher-order models, which formalises pragmatic communication in terms of recursive Bayesian reasoning.

It has been shown that similar kinds of pragmatic behavior as predicted by these models can arise in AI systems as solutions to certain types of optimisation problems \cite{malik2018efficientbellman, Fisac2020Pragmatic-pedagogic}. This is especially the case in optimal solutions to problems formalised as cooperative inverse reinforcement learning \cite{Hadfield-Menell2016cirl}, where there is an asymmetry in information that two agents possess about a cooperative task, thus requiring them to share relevant information in an efficient manner. These model-free approaches speak to the potential usefulness of higher-order interaction between humans and AI systems, but they do not directly allow for the incorporation of user models that encode more specific assumptions about human cognition.

The explicit incorporation of cognitive science models for recursive reasoning has been demonstrated to improve the efficiency of both machine-machine interaction \cite{wen2018probabilistic, wu2021bayesiandelegation, Moreno2021NeuralRB} and human-machine interaction \cite{Ho2016showingversusdoing, Milli2019LiteralOP, Sumers2022howtotalk}. An alternative, yet more specific viewpoint on recursive cognition is the ability to attribute mental states to others in order to explain their behavior, often referred to as Theory of Mind (ToM). Inductive biases in AI systems inspired by ToM have been shown to facilitate inverse modeling of goal-driven behavior and multi-agent planning under uncertainty \cite{rabinowitz2018mtom, foerster2019bad, wu2021bayesiandelegation}. Further motivating our work, these classes of methods have recently been identified as a promising research direction for constructing the next generation of user models \cite{celikok2023modeling}. However, none of these works specifically consider the setting where the AI system learns actively from human feedback rather than passively. To the best of our knowledge, the potential utility of higher-order cognitive user models in an active learning setting has not been explored previously.


\subsection{Active learning with human feedback}

Learning actively from human feedback is an approach to machine learning where the learning system can influence what type of data the human provides. On an abstract level, this interaction happens such that the learning system asks \textit{questions}, to which the human provides \textit{answers}.

Active learning has been shown to improve the data efficiency of learning from expert feedback, both in computational and user studies and for multiple modalities. One such modality is active learning from demonstrations (LfD), where the system queries for a demonstration from a selected starting state \cite{Silver2012ActiveLfD, Chen2020ActiveDQN}. These approaches use the decision-making uncertainty as a heuristic for deciding when it is beneficial to query for expert demonstrations. Another modality is learning from preference queries, where an expert is provided multiple options and provides the best alternative as an answer. Compared to demonstration queries, answers to preference queries in general contain less information but have also been shown to be easier for humans to provide answers to \cite{biyik2022dempref, biyik2023ActivePreferenceGP}. Feature queries are yet a third modality where active learning has been shown to improve learning efficiency \cite{Basu2018learning}. Such queries resemble preference queries, but they also allow humans to communicate in their answers the most relevant feature. These works exemplify the diversity of settings involving expert feedback where active learning can be useful. Although the case study we present builds on top of an active learning algorithm that generates preference queries that maximise the expected information gain, similarly as in \cite{biyik2022dempref}, the main difference between these works and ours is that they do not consider the expert to have a model of the system they are teaching.

Sharing certain similarities with our work is an intriguing result by \cite{colella2020strategic}, which shows that humans can strategically steer a Bayesian optimisation active learning system for improved learning performance. Unlike our work, the authors do not propose a model for how the participant performs this task. In their study, all meaningful internal states of the active learning system are also fully transparent to the participant. This raises the question whether humans can infer the internal states of an active learning system that lacks full transparency or explainability, like a system with a belief that is not easy for humans to visualise or understand. Our work, however, does not make this assumption, instead it outlines a vision where explainability emerges as a result of higher-order interactions.


\section{Levels of agency in active learning}

Here we outline our vision for higher-order active learning, which combines the paradigms of higher-order cognitive modeling and active learning from human feedback.

We consider a setting where an active learning system or artificial agent $\pi_a(\xi | b_a)$ asks questions $\xi \in \Xi$ based on its belief $b_a \in \mathcal{B}_a$ about a human teacher, with the objective of learning a model $p(y | \xi, \theta)$ based from the answers. The human provides these answers correspondingly as $y \sim \pi_{h}(y|\xi, \theta, b_h)$, based on its level 1 model $\theta \in \Theta$ and belief $b_h \in \mathcal{B}_h$ about the artificial agent. The question, for example, could be a starting state where the human provides a demonstration trajectory, or a set of items to which the human provides a ranking. The agent's and human's belief spaces $\mathcal{B}_a$ and $\mathcal{B}_h$ may be of higher order; for example a second-order belief space consists of beliefs-of-beliefs and so on. We begin with active learning involving a level 1 human and progressively describe how each level of agency up to level 5 unlocks new opportunities for enhancing AI-human interaction.

\subsection{Levels 1 and 2: Literal active learning}

We consider a level 1 human as invariant to the internal states of the agent, thus always giving answers consistent with their model $\theta$:
\begin{equation}
    \pi^{level 1}_h(y | \xi, \theta, b_h) = p(y | \xi, \theta).
\end{equation}
An active learning system, which adapts its questions based on its belief $b_a = p(\theta)$ about the level 1 user model $\theta$, is classified as a level 2 agent. Such an agent performs a posterior update based on assumingly literal answers provided by the human by
\begin{equation}
    p(\theta | \xi, y, b_a) = \frac{p(y | \xi, \theta)b_a}{p(y|\xi)}.
\end{equation}
The updated posterior belief is then used to generate the next question by maximising an objective given by
\begin{equation}
    p(\xi|b_a) \propto \exp \left[ \beta_a \mathbb{E}_{p(y|\xi,\theta)p(\theta)}\left[U^{\text{L2}}_a(y | \xi, b_a)\right] \right],
    \label{eq:level2_objective}
\end{equation}
where $\beta_a$ is a rationality coefficient for the questions and $U^{\text{L2}}_a(\cdot|\cdot, \cdot)$ is used to denote a general level 2 utility function, which we denote by uppercase L2. This utility quantifies the value of an answer with respect to the current belief, for instance, by measuring the amount of relevant information the answer provides about a specific task. Since the answer $y$ contains both aleatoric and epistemic uncertainties, the agent must optimise the expected utility considering these uncertainties. Special cases of such a level 2 utility that have been utilised in active learning from human feedback are EIG \cite{biyik2023ActivePreferenceGP} and volume removal \cite{Basu2018learning}. Importantly, level 2 active learning views questions solely  as a means to extract relevant information from the expected answer.

\subsection{Level 3: Theory of Mind reasoning and strategic teaching}

A level 3 human provides answers conditioned on a second-order nested belief $b_h = p(b_a)$, thus affording it the capacity to incorporate a model about the agent's model about the human. This in turn allows the human to make inferences about the knowledge that an active learning system has about itself based on the questions it is asking:
\begin{equation}
    p(b_a | \xi, b_h) = \frac{p(\xi | b_a)b_h}{p(\xi)}.
    \label{eq:level3tom}
\end{equation}

Modeling the human as a level 3 agent also allows for the utilisation of such beliefs obtained through ToM for shaping the belief update of the learner, referred to as strategic teaching \cite{peltola2019machine}. We consider a strategic teacher as selecting answers that shape the belief of the learner such that the probability of the user's true model $\theta$ is maximised through the following objective:
\begin{align}
    &p(y | \xi, \theta^{\text{true}}, b_h) \propto \exp \left[ \beta_h \mathbb{E}_{p(b_a)}U^{\text{L3}}_h(y | \theta^{\text{true}}, \xi, b_a)\right] \\
    &U^{\text{L3}}_h(y | \theta^{\text{true}}, \xi, b_a) = p(\theta^{\text{true}} | \xi, y, b_a) \label{eq:L3_objective},
\end{align}
where $\beta_h$ controls the rationality of the teacher.
Since the teacher has uncertainty about the learners true belief $b_a$, an optimal level 3 teacher optimises the expected utility under their second-order belief.

\subsection{Level 4: Pragmatic questions}

An active learning system that models a human as level 3 and performs reasoning in this model to generate questions is classified as a level 4 agent. As such a level 4 active learning system can use ToM to understand the intent behind the human's responses. Moreover, active learning on level 4 adds another layer of meaning to questions; while a level 2 agent uses questions as a means for receiving information, a level 4 agent may also use questions to \textit{convey} information. A similar strategic teaching formulation from the previous section yields questions that aim to make the belief of the agent \textit{identifiable} to the human:
\begin{equation}
    U^{\text{L4}}_a(\xi | b^{\text{true}}_a, b_h) = p(b^{\text{true}}_a| \xi, b_h).
\end{equation}
Evaluating this utility for a particular question requires the agent to simulate the level 3 ToM update given by Eq. \ref{eq:level3tom}, which is performed by the human after observing that question and then evaluating the resulting posterior density for its true belief $b_a^{\text{true}}$. Alternatively, a level 4 utility may also be used for communicating the relevance of a particular feature or model parameter through the question.

A weighted combination of level 2 and level 4 utilities results in an objective that optimises a tradeoff in the \textit{bidirectional information flow} between the teacher and student.

\subsection{Level 5: Pragmatic inference}

A level 5 human can be aware of the dual nature of questions described above, therefore giving the ability of understanding the \textit{intention} behind a question. By accounting for the fact that questions might be chosen strategically, pragmatic inference from questions can for example allow for identifying what the agent considers relevant.

The possible directions of information flow can also serve as two alternative hypotheses to explain the intentions behind questions. Reasoning about the intention in such a way can be done by computing the following Bayes factor:

\begin{equation}
    BF = \frac{\int\pi_a^{L2}(\xi | b_a)p(b_a)db_a}{\int\pi_a^{L4}(\xi | b_a)p(b_a)db_a}.
\end{equation}
 the human needs to marginalise over the agent's possible beliefs $b_a$. A value of $BF>1$ supports the hypothesis that the question is \textit{literal} in nature, aiming to extract information from the answer, whereas $BF<1$ supports the alternative hypothesis that the question is \textit{rhetorical} in nature, aiming to convey information with the question. For the case where the human attributes a level 2 model to the agent that is based on EIG, an intuitive rhetorical question (e.g. "is this not how it is?") is a question which has an obvious answer for every possible agent belief $b_a$, resulting in a low marginal likelihood $\int\pi_a^{L2}(\xi | b_a)p(b_a)db_a$, therefore likely also resulting in $BF << 1$. Taking this example one step further, a level 6 agent that wishes to communicate intention through questions might strategically choose a question with an obvious answer in order to make the rhetorical nature of the question identifiable to the human.

\section{Case study: Higher-order active learning with preference queries}

We now move to a computational study of active learning with preference queries, illustrating how the general methods depicted in the previous section can be grounded into concrete models and how these models can influence the  resulting interactions. We have designed experiments for a computational model of a level 3 human, aiming to answer the following questions:

\begin{enumerate}
    \item Can a level 3 human model infer the belief of a level 2 active learning system by observing queries aimed at reducing uncertainty about a unimodal belief?
    \item Is it possible to identify such a belief as bimodal by observing queries that aim to reduce uncertainty both about \textit{the group} the human belongs to and the parameters \textit{within the group}?
    \item How does belief attribution affect level 3 strategic teaching behavior? In particular, can beliefs inferred by ToM from preference queries lead to qualitative changes in the teaching strategy?
\end{enumerate}

\subsection{Strategic teaching scenario}

To ground our case study with a concrete example, consider a recommender system for a car shopping website that has the objective of identifying the price which best aligns with the preferences of the user. The system has a bimodal prior belief for the desired price, encoding domain knowledge about the presence of two distinct user groups regularly visiting the site: one group is looking for cheaper used cars and the other group for more luxurious ones. The system offers two options at a time to the user from which they are supposed to select the more suitable one. If the user makes certain assumptions about how recommendations are produced, such as their purpose being to gather information from the answer, they may employ ToM reasoning to identify internal beliefs of the system.

\subsubsection{Human and Agent Models:}

We next proceed to a more detailed description of a computational model that can allow for such reasoning in our example scenario. This model describes a level 3 human, who itself models the agent it is interacting with as level 2. As such, the human also has a model of the level 1 model that the level 2 agent has about its own preferences.

We start by describing this level 1 model, which defines how humans respond to preference queries. In this model answers are consistent with the true preference of the human as follows:

\begin{equation}
    p(y=1 | \theta, x_1, x_2) = \sigma(r_{\theta}(x_2) - r_{\theta}(x_1)),
\end{equation}
where $r_{\theta} : \mathbb{R} \rightarrow \mathbb{R}$ is a stochastic reward function which depends on the distance to the most preferred item $\theta$, $\sigma : \mathbb{R} \rightarrow [0, 1]$ is the logistic sigmoid function, $x_1, x_2 \in \mathbb{R}$ are 1-dimensional real-valued item features (normalised prices in our example) and $y \in \{0,1\}$ is binary answer where $y=1$ implies that $x_2$ is preferred over $x_1$. We model a setting where there are two distinct groups of human preferences with the following bimodal prior for $\theta$ as

\begin{align}
    &p(z = 1) = p_z \\
    &p(\theta_1) =  \mathcal{N}(\theta_1 | \mu_1, \sigma_1^2) \\
    &p(\theta_2) =  \mathcal{N}(\theta_2 | \mu_2, \sigma_2^2) \\
    &\theta = z \theta_1 + (1 - z) \theta_2,
\end{align}
where $\mu_i$ and $\sigma^2_i$ are the mean and variance of the preferences of group $i$ and $z \in \{0, 1\}$ is a binary latent variable that identifies the group.

The objective of the active learning system is to maximise the expected utility of the answers as given by (\ref{eq:level2_objective}). A natural choice for such a local utility is the information gain, leading to a level 2 agent that maximises the mutual information between its model and the answers as

\begin{align}
    &p(x_1, x_2 | b_a) \propto \exp(\beta_a I(\theta, y | x_1, x_2, b_a)) \\
    &I(\theta, y | x_1, x_2, b_a) = \mathbb{E}_{p(y|x_1,x_2)}\left[U^{\text{IG}}(y| x_1, x_2, b_a)\right] \\
    &U^{\text{IG}}(y| x_1, x_2, b_a) = H(p(\theta)) - H(p(\theta | x_1, x_2, y)),
\end{align}
where $H(\cdot)$ is the information entropy.

After observing a set of one or multiple preference queries, $\mathcal{D}_x=\{(x_1^i, x_2^i)\}_{i=1}^N$, we estimate the agent's belief $b_a$ with maximum likelihood as follows:

% \begin{align}
%     &p(b_a | \mathcal{D}_x) = \frac{\prod_{i=1}^Np(x_1^i, x_2^i|b_a)p(b_a)}{p(\mathcal{D}_x)} \\
%     &\propto \exp\left[\beta_a \sum_{i=1}^N\left[I(\theta, y | x_1, x_2, b_a)\right]\right]p(b_a).
% \end{align}

\begin{align}
    \hat{b}_a &= \argmax_{b_a}p(\mathcal{D}_x | b_a) \\
    &=\argmax_{b_a}\prod_{i=1}^N\exp\left[\beta_a I(\theta, y | x^i_1, x^i_2, b_a)\right] \\
    &=\argmax_{b_a}\sum_{i=1}^NI(\theta, y | x^i_1, x^i_2, b_a).
\end{align}
Thus, in this model the human explains the observed questions by attributing to the agent a belief that makes those questions maximally informative.

\subsection{Identifiability of unimodal queries}

We first study how well the belief of the active learning system can be identified by our user model when queries aim to reduce uncertainty about one group only. To create such a setting, we use a prior where $\mu_1=-3, \sigma_1^2=1, \mu_2 = 3, \sigma_2^2=1$ and $p_z=0.9$.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{unimodal_mle_3.pdf}
\caption{Identifiability of agent belief $b_a$ from unimodal preference queries. Left: EIG visualised in query space for the true belief. Right: EIG prediction for the belief obtained with maximum likelihood from the 5 queries shown.}
\label{fig:unimodal_eig}
\end{figure}

Figure \ref{fig:unimodal_eig} shows sampled queries and the EIG prediction in the query space for the belief given by the maximum likelihood estimate. This example shows how it from preference queries is possible to \textit{identify the belief that was used to generate the queries}. The estimated belief allows for making predictions for the EIG that resembles those obtained from the true belief.

\subsection{Identifiability of bimodal queries}

 = 0.6$, which keeps the total uncertainty similar but spreads it over both modes. We employ $N=20$ queries here since the inference problem is more difficult in the bimodal case.

An example outcome of such inference is shown in Figure \ref{fig:bimodal_eig}. This example shows that although the exact belief may not always be accurately identified, it remains possible to detect the \textit{locations of the modes} and determine if there is \textit{uncertainty regarding the group} to which the participant belongs.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{bimodal_mle_3.pdf}
\caption{Identifiability of agent belief $b_a$ from bimodal preference queries. Top left: Samples from the true belief. Top right: EIG and queries resulting from true belief. Bottom left: Belief estimated with maximum likelihood from sampled queries. Bottom right: EIG prediction obtained from estimated belief.}
\label{fig:bimodal_eig}
\end{figure}

\subsection{Effect of belief attribution on strategic teaching}

Lastly, we show how our model of a level 3 human can detect a \textit{false belief} about itself through preference queries generated by an active learning system and use strategic teaching to correct it. We use a similar prior and dataset to that in the unimodal queries example (Figure \ref{fig:unimodal_eig}), with the user model parameter $\theta^{\text{true}}=2.0$ representing the ground truth, which belongs to the less likely mode. To illustrate the impact of the belief derived from such inference on teaching behavior, we employ a similar type of strategic teaching objective as given in (\ref{eq:L3_objective}). However, here it is applied to selecting full data points $(x_1, x_2, y)$ rather than the answers only.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{belief_correction_3.pdf}
\caption{An example of how the identification of a false belief can impact strategic teaching behavior. Left: Expected utilities of teaching data points $x_1, x_2$ for a uniform belief $b_a$. Right: Corresponding expected utilities for belief inferred after observing 5 preference queries that were generated from the false belief.}
\label{fig:belief_correction}
\end{figure}

 inference with strategic teaching. In this illustration, the non-adaptive model shows a teaching example where both options closely align with the true preference. Conversely, \textit{the adaptive model shows an example that compares the mode of the false belief with the true preference}. Integrating information about both the false belief and truth in the teaching example is an intuitive strategy for steering the agent's belief towards the correct mode.

\section{Discussion}

 as a motivating example for how a level 3 human could recognise internal states of an active learning agent, such as uncertainties or false beliefs, and then adapt their behavior accordingly to enhance the effectiveness of their teaching. The advantage of the demonstrated adaptation is also intuitive; a strategic teaching approach that incorporates information about both the false belief and the ground truth is likely to be more efficient.

 questions are generated with the objective of maximizing their EIG. Characterizing the extent to which various modeling assumptions influence the resultant behavior would be beneficial. For example, how does the identifiablity of the active learning system's belief change when queries are generated with different level 2 objectives?

 user study attribute to active learning systems could be predicted from their behavior. Specifically, if users engage in strategic teaching behavior that specifically aims to correct false beliefs, it provides evidence that they are capable of recognising such false beliefs through ToM. Furthermore, a comparison between the strategy employed by a participant and predictions given by a computational model could allow for more specific conclusions about the beliefs that humans attribute to active learning systems, as well as the factors that affect them.

 communication with a level 3 human, as described in our taxonomy, is to be reached when the agent can pragmatically query the user. However, at some stage, it is anticipated that approximations to optimal communication will become necessary, both within user models and active learning algorithms. Consequently, future research should focus on identifying the types of approximations that maintain the qualitative modes of behavior predicted by computationally rational higher-order models. This research would guide the development of active learning methods that can fully utilise the advantages of complex and nuanced human behavior.

\section{Interdisciplinary implications}

 hypotheses for user models to be tested in empirical user studies. The converse may also be true; results from the kinds of user studies we have proposed can guide the search for better machine learning methods. For example, constructing a level 5 user model that balances computational efficiency with accurate representation of human behavior might necessitate advanced algorithmic methods surpassing the state of the art. Eventually, the long-term goal of constructing AI agents that utilise the kinds of user models we advocate for, have the potential to enhance the sample efficiency, explainablity, safety and reliability of future machine learning systems.

\section{Conclusion}

 practical example where a higher-order cognitive model recognizes and corrects a false belief in an active learning system. Building on these findings, we explore the implications for designing future computational and user studies, which represents a critical next step in applying the proposed methods effectively in real-world systems.


\clearpage
\bibliography{refs}

\end{document}