\relax 
\bibstyle{aaai24}
\citation{misra2017red,purushwalkam2019task}
\citation{mit}
\citation{resnet}
\citation{mit}
\citation{resnet}
\citation{misra2017red,li2020symmetry,purushwalkam2019task,li2022siamese}
\citation{naeem2021learning,mancini2021open,mancini2022learning}
\citation{menon2020long,tang2020unbiased,kim2020detecting}
\citation{atzmon2020causal}
\citation{saini2022disentangling,wang2023learning}
\citation{menon2020long}
\newlabel{sec:intro}{{}{1}{}{}{}}
\citation{xian2017zero}
\citation{chen2022zero}
\citation{akata2013label,lampert2013attribute,parikh2011relative,frome2013devise,akata2015evaluation}
\citation{atzmon2020causal,yang2022decomposable,nagarajan2018attributes,wang2019task}
\citation{hoffman1984parts}
\citation{misra2017red,chen2014inferring,yang2020learning,lu2016visual,li2022siamese}
\citation{naeem2021learning,mancini2022learning,mancini2021open}
\citation{saini2022disentangling}
\citation{wang2023learning}
\citation{long1,long3,hou2021detecting,menon2020long,focal}
\citation{chen2022zero}
\citation{jiang2022mutual}
\citation{xian2017zero}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig1}{{1}{2}{An example of prior and posterior probabilities (predicted by model) of same classes in MIT-States \citep  {mit}. N.S. represents the prior probability calculated from the number of samples in each class. P. denotes the average value of posterior probabilities indicating the likelihood of the sample belonging to its class, which is predicted by a MLP with ResNet-18 \citep  {resnet} as backbone (same settings as $\mathcal  {C}_{o},\mathcal  {C}_{s}$ and $\mathcal  {C}_{y}$ in Implementation Details and trained via vanilla cross-entropy loss). The classes are selected on the basis of the closest sample size, and shows the results for the object (left), state (middle), and composition (right) classes. All data is simply normalized for ease of presentation.}{}{}}
\newlabel{sec.related}{{}{2}{}{}{}}
\newlabel{Sec.methodology}{{}{2}{}{}{}}
\newlabel{subsec.taskdefine}{{}{2}{}{}{}}
\citation{mancini2021open}
\citation{naeem2021learning}
\citation{saini2022disentangling,wang2023learning}
\citation{mancini2021open,naeem2021learning}
\citation{kexuefm-7615}
\citation{kraskov2004estimating}
\newlabel{subsec.empirical_analysis}{{}{3}{}{}{}}
\newlabel{tab.empirical}{{1}{3}{The results of methods that incorporate a composition classifier, along with the addition of two classifiers for states and objects on top of them (\textit  {i.e.}, model ensemble), are presented. I.C. indicates only two independent classifiers are used. M.E. indicates the utilization of model ensemble in the methods, where F denotes false and T denotes true. The metrics in the table are defined in Evaluation Protocol.}{}{}}
\newlabel{eq1}{{1}{3}{}{}{}}
\newlabel{subsec_mutal}{{}{3}{}{}{}}
\newlabel{eq_objective}{{2}{3}{}{}{}}
\newlabel{eq_condition}{{3}{3}{}{}{}}
\newlabel{eq_mutual_information}{{4}{3}{}{}{}}
\newlabel{eq_f_log}{{5}{3}{}{}{}}
\newlabel{eq_f_log_transfer}{{6}{3}{}{}{}}
\citation{menon2020long}
\citation{johnson2019survey,japkowicz2002class}
\citation{chen2022zero}
\citation{importancesampling}
\newlabel{eq_after_softmax}{{7}{4}{}{}{}}
\newlabel{subsec.from_posteri_view}{{}{4}{}{}{}}
\newlabel{eq_estimate}{{8}{4}{}{}{}}
\newlabel{eq_final_sigma_introduce}{{9}{4}{}{}{}}
\newlabel{eq_onehot}{{10}{4}{}{}{}}
\newlabel{eq_final_after_softmax}{{11}{4}{}{}{}}
\newlabel{subsec.inference}{{}{4}{}{}{}}
\newlabel{eq_ah}{{12}{4}{}{}{}}
\newlabel{eq_lowerbound}{{13}{4}{}{}{}}
\newlabel{eq_infer_1}{{14}{4}{}{}{}}
\newlabel{eq_seen}{{15}{4}{}{}{}}
\newlabel{eq_importance}{{16}{4}{}{}{}}
\newlabel{eq_khat}{{17}{4}{}{}{}}
\citation{mit}
\citation{utzappos}
\citation{naeem2021learning}
\citation{naeem2021learning}
\citation{xian2017zero}
\citation{naeem2021learning}
\citation{wang2019task}
\citation{misra2017red}
\citation{nagarajan2018attributes}
\citation{purushwalkam2019task}
\citation{li2020symmetry}
\citation{mancini2021open}
\citation{naeem2021learning}
\citation{li2022siamese}
\citation{mancini2022learning}
\ci