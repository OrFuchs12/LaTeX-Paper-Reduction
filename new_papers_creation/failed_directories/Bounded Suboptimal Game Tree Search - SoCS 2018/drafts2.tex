----------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Drafts from here on!!}
\section{General Bounded Minimax Value}






\section{Bounded Pruning for Games with Chance}

Alpha-beta pruning was introduced to games with chance in the Star-1 and Star-2 algorithms~\cite{TODO}. In this section we build on this prior work and extend Theorem~\ref{the:basic} to games with chance. 

For the following theoretical results, we need the following notation. For every node $c$ that is a child of a chance node $n$, we denote by $\pi(c)$ the probability that $c$ occurs immediately after $n$. For example, for every node $c$ that is a child of a  chance node that corresponds to throwing a dice, we have that $\pi(c)=1/6$. 

\begin{theorem}
For a game tree $G$ and a game tree search algorithm $A$, 
let $\opti$, $\pess$, $\alpha$, and $\beta$ be functions that map nodes in $A(G)$ to a real value $[\vmin, \vmax]$. 
If all the conditions in Theorem~\ref{the:basic} hold (C1-C5) 
as well as the following conditions (C6-C7), then for any $\epsilon\geq 0$ it holds that $MM(\rootnode)\in [\pess(\rootnode),\pess(\rootnode)+\epsilon]$, 
where $\rootnode$ is the root of $G$. \\
\noindent C6. For every non-leaf chance node $n$: 
(1) $\pess(n)\gets \sum_{c\in C(n)} \pi(c)\cdot \pess(c) + 
(1-\sum_{c\in C(n)} \pi(c))\cdot \vmin{}$, and 
(2) $\opti(n)\gets \sum_{c\in C(n)} \pi(c)\cdot \opti(c) + 
(1-\sum_{c\in C(n)} \pi(c))\cdot \vmax{}$. \\

\noindent C7. For every node $n$ that is a child of a chance node $p$:
(1) $\alpha(n)\gets \max(\vmin, \frac{\alpha(p)-\opti(p)+\pi(n)\cdot\opti(n)}{\pi(n)})$, and (2) $\beta(n)\gets \min(\vmax, \frac{\beta(p)-\pess(p)+\pi(n)\cdot\pess(n)}{\pi(n)})$.
\label{the:chance}
\end{theorem}
Note that condition C7 is an extension of condition C4, 
We prove Theorem~\ref{the:chance} in a similar way to our proof of Theorem~\ref{the:basic}, establishing the corresponding Lemmas to Lemma~\ref{lem:bounded-beta} and Lemma~\ref{lem:opti-pess}. 
\begin{lemma}
If $\beta(n)\leq \alpha(n)+\epsilon$ holds for all leaf nodes and all partially expanded nodes, then it must also hold for all other nodes in $A(G)$, including chance nodes. 
\label{lem:bounded-beta-chance}
\end{lemma}
\begin{proof} 
Similar to the proof for Lemma~\ref{lem:bounded-beta}, we prove Lemma~\ref{lem:bounded-beta-chance} by contradiction. Let 
$n_{bad}$ be the deepest node for which Lemma~\ref{lem:bounded-beta-chance} does not hold, i.e., for every node $n$  in the subtree beneath $n_{bad}$ it holds that 
$\beta(n)\leq \alpha(n)+\epsilon$, 
but $\beta(n_{bad})>\alpha(n_{bad})+\epsilon$. 
$n_{bad}$ must be partially expanded. Consider a node $c\in C(n_{bad})$. 
If $n_{bad}$ is either a MIN node or a MAX node, then
$\beta(c)=\min(\beta(n_{bad}), \opti(c))$
and 
$\alpha(c)=\max(\alpha(n_{bad}), \pess(c))$. 
As shown in the proof of Lemma~\ref{lem:bounded-beta}, this leads to a contradiction with the assumption that $\beta(n_{bad})>\alpha(n_{bad})+\epsilon$. 

Now, assume that $n_{bad}$ is a chance node. 
\begin{align}
\beta(c) \leq & \alpha(c)+\epsilon \\
\min(\vmax, \frac{\beta(p)-\pess(p)+\pi(n)\cdot\pess(n)}{\pi(n)}) \leq & 
\max(\vmin, \frac{\alpha(p)-\opti(p)+\pi(n)\cdot\opti(n)}{\pi(n)}) 
\end{align}
Since $\beta(n_{bad})>\alpha(n_{bad})+\epsilon$, 
it must hold that either $\opti(c)<\beta(n_{bad})$
or $\alpha(n_{bad})<\pess(c)$. 
By definition of $\alpha$ and $\beta$, neither can be true for fully expanded nodes
and for the root. Thus, $n_{bad}$ cannot exist in $A(G)$. 

Consequently, it holds that 
\begin{align}
\beta(root)\leq & \alpha(root)+\epsilon\\
\opti(root) \leq & \pess(root)+\epsilon 
\end{align}
as required.
\end{proof}
\begin{lemma}
If $\pess(n)\leq MM(n)\leq opti(n)$ holds for all leaf nodes, then it must also hold for all other nodes in $A(G)$. 
\label{lem:opti-pess}
\end{lemma}

\begin{proof}
By contradiction, assume that there is a node for which this does not hold. 
Let $n_{bad}$ be the deepest such node, i.e., 
for every other node $n$ in the subtree of $A(G)$ rooted by $n_{bad}$ 
it holds that $\pess(n)\leq MM(n)\leq \opti(n)$, except for $n_{bad}$. For $n_{bad}$, it holds that either $\pess(n)>MM(n)$ or $MM(n)>\opti(n)$.
$n_{bad}$ can be either a MAX node or a MIN node, 
and can be either fully expanded or partially expanded. We show next that in each of these four combination, it must hold that $MM(n_{bad})\in [\pess(n_{bad}),\opti(n_{bad})]$, thus contradicting our assumption about $n_{bad}$. 


\noindent \textbf{Case \#1 $n_{bad}$ is a MAX node.}
\[ MM(n_{bad}) = \max_c MM(c) \geq \max_c \pess(c) = \pess(n_{bad}) \]
and thus $MM(n_{bad})\geq \pess(n_{bad})$. 
If $n_{bad}$ is fully expanded, then 
\[ \opti(n_{bad}) = \max_c \opti(c) \geq \max_c MM(c) = MM(n_{bad}) \]
Otherwise, $n_{bad}$ is partially expanded and so $opti(c)=\vmax\geq MM(n_{bad})$. So, regardless if $n_{bad}$ is fully expanded or partially expanded, it holds that $MM(n_{bad})\in [\pess(n_{bad}), opti(n_{bad})]$. 

\noindent \textbf{Case \#2 $n_{bad}$ is a MIN node.}
\[ MM(n_{bad}) = \min_c MM(c) \leq \min_c opti(c) = opti(n_{bad}) \]
and thus $MM(n_{bad})\leq opti(n_{bad})$. 
If $n_{bad}$ is fully expanded, then 
\[ pess(n_{bad}) = \min_c pess(c) \leq \min_c MM(c) = MM(n_{bad}) \]
Otherwise, $n_{bad}$ is partially expanded and so $pess(c)=\vmin\leq MM(n_{bad})$. So, here too, it holds that $MM(n_{bad})\in
[\pess(n_{bad}), \opti(n_{bad})]$. 
\end{proof}

Finally, we use Lemma~\ref{lem:bounded-beta} and Lemma~\ref{lem:opti-pess} to prove Theorem~\ref{the:basic}. 
Since $\beta(root)=\pess(root)$ 
and $\alpha(root)=\opti(root)$, then by Lemma~\ref{lem:bounded-beta} we have that
$\opti(root)\leq \pess(root)+\epsilon$.
Then, by Lemma~\ref{lem:opti-pess} we have
\begin{equation}
\pess(root) \leq MM(root) \leq \pess(root)+\epsilon
\end{equation}
as required.















\begin{theorem}
For a game tree $G$ and a game tree search algorithm $A$, let $\opti{}$, $\pess{}$, $\alpha$, and $\beta$ be define as in Theorem~\ref{thlem:bounded-beta}. Assume that for every MIN node  in $A(G)$ and for every MAX node in $A(G)$, all the conditions in Theorem~\ref{thlem:bounded-beta} hold. 
Let $\pi(c)$ denote the probability that a child $c$ of a chance node will occur. 
Then if for every chance node $n$ in $A(G)$ it holds that:
\begin{itemize}
  \item .. 
  \item ..  
\item $\alpha(n)\gets \max(\vmin, \frac{\alpha_p-\opti(p)+\pi\cdot\opti(n)}{\pi})$
\item $\beta(n)\gets \max(\vmax, \frac{\beta_p-\pess(p)+\pi\cdot\pess(n)}{\pi})$
\item $\opti(n)\gets$ 
if $n$ is partially expanded, then 
$$
$$
\item if $n$ is fully expanded 
\end{itemize}
\end{theorem}



%This is Abdallah's version and proof. 

The key is to prove that for every node it holds that 
when we return a value then it holds that $\beta\leq \alpha+\epsilon$, 
regardless of which return statement we used. 



\begin{algorithm}
\DontPrintSemicolon
\KwIn{$n$ - a game tree node}
\KwIn{$(\alpha_p, \beta_p)$ - the $\alpha$ and $\beta$ values of the parent node}
\KwIn{$\epsilon$ - an error margin}

\If{$n$ is a terminal node}{
	$\alpha = \max(\alpha_p, \eval(n))$; $\beta = \min(\beta_p, \eval(n))$\;
	\Return $(\eval(n), \eval(n))$\;
}

$\pess(n)\gets \vmin$;
$\opti(n)\gets \vmax$\;
  \lIf{$n$ is a MAX node}{
  	$\besto\gets \vmin$
  }\lElseIf{$n$ is a MIN node}{
    $\bestp\gets \vmax$
  }
  $\alpha = \alpha_p$;
  $\beta = \beta_p$\;
\ForEach{child $c$ of $n$}{
	\If{$n$ is a MAX node or a MIN node}{
    	$\alpha_c=\alpha$; $\beta_c=\beta$
    }
    \Else{
        $\alpha_c\gets \max(\vmin, \frac{\alpha-\opti(n) + \pi \opti(c)}{\pi})$\;
        $\beta_c\gets \min(\vmax, \frac{\beta-\pess(n)+\pi \pess(c)}{\pi})$\;
    }
    $\zeta\gets$ ComputeChildBound($\epsilon$, $\opti(n)$,$\pess(n)$,$\pi(c)$)\;
    $\big(\pess(c),\opti(c)\big)\gets$ BoundedAB($c$, $\alpha_c$, $\beta_c$, $\zeta$)\;
    
    \If{$n$ is a MAX node}{
    	$\besto\gets \max(\besto,\opti(c))$\;
        $\pess(n)\gets \max(\pess(n),\pess(c))$\;
%    	$\opti(n)\gets \max(\opti(n),\opti(c))$\; Always vmax
    }
    \ElseIf{$n$ is a MIN node}{
    	$\bestp\gets \min(\bestp,\pess(c))$\;
%        $\pess(n)\gets \min(\pess(n),\pess(c))$\;  Always vmin
		$\opti(n)\gets \min(\opti(n),\opti(c))$\;
    }\Else{
        $\pess(n)\gets \pess(n)+\pi(c)(\pess(c)-\vmin)$\;
%        $\opti(n)\gets \opti(n)+\pi(\opti(c)-\vmax)$\;
		$\opti(n)\gets \opti(n)-\pi(c)(\vmax-\opti(c))$\;
    } 

    $\alpha\gets \max(\pess(n), \alpha)$\;
    $\beta\gets \min(\opti(n), \beta)$\;
       
    \lIf{$\beta\leq \alpha+\epsilon$ \fbox{and $c$ is not the last child of $n$}}{
    	\Return $(\pess(n),\opti(n))$
    }  
}
\lIf{$n$ is a MAX node}{
	$\opti(n)\gets \besto$
}\lElseIf{$n$ is a MIN node}{
	$\pess(n)\gets \bestp$
}

\Return $(\pess(n),\opti(n))$\;

\caption{BoundedAB}
\label{alg:weightedAlphaBeta}
\end{algorithm}


\begin{theorem}
\label{theorem_error_bound}
Let $n$ be a chance node in a game tree
having $M$ children $c_1,\ldots c_M$, 
where each child $c_i$ occurs with probability $\pi_i$. 

$c_i$ of chance node $n$ will occur, and $\epsilon _i$ be an error bound computed in the ComputeChildBound() method by node $n$ and given to $C_i$. Then, by satisfying the following equation, the error bound $\epsilon$ of node $n$ will remain safe.
\begin{equation} 
\label{equ:error}
	\left ( {\sum_{i=1}^{\#Children}{{\epsilon _i}\cdot{\pi _i}}} \right ) \leq {\epsilon } 
\end{equation}
\end{theorem}


\begin{itemize}
%\item if $n$ is a leaf in $G'$, then $\pess(n)\leq MM(n)$ and $\opti(n)\geq MM(n)$
\item if $n$ is a leaf in $G'$, then $\pess(n)\leq MM(n)\leq opti(n)$
\item if $n$ is a MAX node, and it is partially expanded, then $\opti(n)=\infty$
\item if $n$ is a MAX node, and it is fully expanded, then $\opti(n)=\max_c \opti(c)$
\item if $n$ is a MAX node, and it is not a leaf then $\pess(n)=\max_c \pess(c)$
\item if $n$ is a MIN node, and it is partially expanded, then $\pess(n)=-\infty$
\item if $n$ is a MIN node, and it is fully expanded, then $\pess(n)=\min_c \pess(c)$
\item if $n$ is a MIN node, and it is not a leaf then $\opti(n)=\min_c \opti(c)$
\item if $n$ is a chance node, and it is fully expanded, then 
$opti(n)=\sum_{c\in children(n)}\pi(c)\cdot opti(c)$ 
and $pess(n)=\sum_{c\in children(n)}\pi(c)\cdot pess(c)$.
\item if $n$ is a chance node, and it is partially expanded, then 
$opti(n)=\sum_{c\in children(n)}\pi(c)\cdot opti(c)
+(1-\sum_{c\in children(n)}\pi(c))\cdot \vmax$
and 
$pess(n)=\sum_{c\in children(n)}\pi(c)\cdot pess(c)
+(1-\sum_{c\in children(n)}\pi(c))\cdot \vmin$
\item if $n$ is the root of $G'$ then $\alpha(n)=\pess(n)$ and $\beta(n)=\opti(n)$ 

\item If $n$ is MAX or MIN, then $\alpha(n)=\max(p(n),\pess(n))$ 
\item If $n$ is MAX or MIN, then $\beta(n)=\min(p(n),\opti(n))$
\item $\epsilon(root)=\epsilon$
\item If $n$ is not a root and its parent $p$ is a MAX node or a MIN node, then $\epsilon(n)=\epsilon(p)$
\item If $n$ is a chance node then $\epsilon(n)\geq \sum_{c\in children(n)}\pi(c)\cdot \epsilon(c)$
\end{itemize}
then, if for every leaf node and every partially expanded node $n$ in $G'$ it holds that 
if $n$ is a MAX node or a MIN node, then 
$\beta(n)\leq \alpha(n)+\epsilon$ 
otherwise, if $n$ is a chance node, then 
then 
$MM(root)\in [\opti(root),\pess(root)+\epsilon]$
where $root$ is the root of $G'$. 


\begin{theorem}
\label{theorem_error_bound}
Let $n$ be a chance node in a game tree
having $M$ children $c_1,\ldots c_M$, 
where each child $c_i$ occurs with probability $\pi_i$. 
If .....
then it holds that 
%$MM(n)\leq pess(n)+\epsilon$ and $MM(n)\geq opti(n)+\epsilon$ 
\[opti(n)-\epsilon \leq MM(n)\leq pess(n)+\epsilon\]


$c_i$ of chance node $n$ will occur, and $\epsilon _i$ be an error bound computed in the ComputeChildBound() method by node $n$ and given to $C_i$. Then, by satisfying the following equation, the error bound $\epsilon$ of node $n$ will remain safe.
\begin{equation} 
\label{equ:error}
	\left ( {\sum_{i=1}^{\#Children}{{\epsilon _i}\cdot{\pi _i}}} \right ) \leq {\epsilon } 
\end{equation}
\end{theorem}



\begin{proof}
%Assuming $MM(n)$ is the Minimax value of node $n$ and $MM(c_i)$ is the Minimax value of child $i$ of node $N$ with probability of $\pi_i$ to occur. 
We know that:

$MM(n)={\sum_{i=1}^{\#Children}{{MM(c_i)}\cdot{\pi _i}}}$

from the Expectimax equation. Assuming $\hat{V}$ is the value of node $N$ that satisfying equation \ref{equ:error}. $\hat{V}$ value is:

$\hat{V}={\sum_{i=1}^{\#Children}{({V ^* _i}+{\epsilon _i})\cdot{\pi _i}}}$

$\hat{V}={\sum_{i=1}^{\#Children}{{V ^* _i}\cdot{\pi _i}}}+{\sum_{i=1}^{\#Children}{{\epsilon _i}\cdot{\pi _i}}} \leq V^*+\epsilon$


\end{proof}

As can be inferred from $\mathit{Theorem~\ref{theorem_error_bound}}$, the ComputeChildBound() method can be replaced by a simple function returning $\epsilon$. 
However, there is much room for generality here, as we can set different ``error bounds'' to the different children of a chance node. For example, assuming node $n$ has two children $c_1$ and $c_2$ with probabilities $\pi_1=0.2$ and $\pi_2=0.8$, respectively. There are multiple partitions of the error bound between the two children, for instance, $\epsilon_1=5\epsilon$ and $\epsilon_2=0$. There is an obvious trade-off, while choosing a large error bound to some child can cause many cut-offs, other child must get a smaller bound that might reduce the amount of the cut-offs.


\section{Bounded Monte-Carlo Tree Search}

Based on Abdallah's prior work on Alpha-Beta pruning from MCTS.

\begin{algorithm}
\KwIn{$n$ - a game tree node}
\KwIn{$\alpha_p$ - the $\alpha$ value of the parent node}
\KwIn{$\beta_p$ - the $\alpha$ value of the parent node}

$pess(n)\gets -\infty$\\
$opti(n)\gets \infty$\\
$\alpha = \alpha_p$\\
$\beta = \beta_p$\\
\If{$n$ is a terminal node}{
	$n.pess=eval(n)$\\
    $n.opti=eval(n)$\\
	\Return
}

BestU$\gets -\infty$\\
\ForEach{child $c$ of $n$}{
	\If{$\min(\beta,c.opti)\leq \max(\alpha,c.pess)+\epsilon$}{
   		Continue ($c$ is pruned)\\
   	}
    
    $U_c = \overline(c.value)+C\cdot \sqrt{\frac{\ln (c.playouts)}{p.playouts}}$\\
	\If{BestU$<U_c$}{
    	BestU$\gets U_c$\\
        BestChild$\gets c$\\
    }
        % Update pess, opt, alpha, and beta
    $n.pess\gets \max(c.pess)$\\
    $n.opti\gets \max(c.opti)$\\
    $\alpha\gets \max(\alpha,n.pess)$\\
    $\beta\gets \min(\beta, n.opti)$\\   

}
\caption{BABMCTS: MAX node. The Select-Max procedure}
\label{alg:weightedAlphaBetaMCTS}
\end{algorithm}



\begin{lemma}
If a node $n$ is not pruned by BABMCTS then it must have at least one child that is not pruned either. 
\end{lemma}
\begin{proof}
Let $n$ be a MAX node, and let $c_{max}$ be a child node of $n$ such that 
$c_{max}.opti=n.opti$. Such a child must exist because $n.opti=\max_c (c.opti)$, by definition. 
Since $n.pess=\max_c(c.pess)$, it holds that $n.pess\geq c_{max}.pess$. 
By definition, $c_{max}.opti\geq \beta_n$ and $\alpha_n\geq c_{max}.pess$. 
Therefore, $\beta_{c_{max}}=\min(\beta_n,c_{max}.opti)=\beta_n$, 
and $\alpha_{c_{max}}=\max(\alpha_n, c_{max}.pess)=\alpha_n$. 
Since we assumed that $n$ is not pruned by BABMCTS, then 
$\beta_n> \alpha_n+\epsilon$, and thus we have
$\beta_{c_{max}}>\alpha_{c_{max}}+\epsilon$, and thus $c_{max}$ is not pruned either. 

[[TODO: Prove also for MIN nodes]]

\end{proof}



\begin{theorem}
Running BABMCTS will return a value that is at most $\epsilon$ far from the Minimax value. 
\end{theorem}





\section{Action Items}


\begin{enumerate}
\item Policies for choosing which action to perform after a bounded tree search result
(e.g., optimistic, pessimistic, average).
\item Best-case analysis (or other analyses) of bounded tree search runtime. 
(my hunch: the best-case is the same as that of regular alpha-beta)
\item Compare bounded alpha-beta solver with alpha-beta solver: who will win if they are allowed the same number of nodes to expand
\item Bounded theory for games with chance
\item Compare bounded star-1 to star-1 and the same for star-2
\item Implement bounded MCTS and compare to MCTS
\item Implement bounded proof number search and compare to proof number search
\item More domains!!!
\end{enumerate}

