\relax 
\bibstyle{aaai24}
\citation{he2016deep}
\citation{oord2018parallel}
\citation{kenton2019bert}
\citation{hochreiter1997flat,keskar2016large,dziugaite2017computing,neyshabur2017exploring,Jiang2020Fantastic}
\citation{Jiang2020Fantastic}
\citation{dziugaite2017computing}
\citation{foret2021sharpnessaware}
\citation{liu2022towards,du2022efficient,mi2022make,du2022sharpness,jiang2023an,liu2022random}
\newlabel{sec:Introduction}{{}{1}{}{}{}}
\citation{foret2021sharpnessaware}
\citation{foret2021sharpnessaware,chen2022when}
\citation{bahri2021sharpness}
\citation{liu2022towards}
\citation{du2022efficient}
\citation{mi2022make}
\citation{du2022sharpness}
\citation{jiang2023an}
\citation{liu2022random}
\citation{NEURIPS2020_322f6246}
\citation{szegedy2016rethinking}
\citation{devries2017improved}
\citation{zhang2017mixup}
\citation{cubuk2020randaugment}
\citation{srivastava2014dropout}
\citation{he2016deep}
\citation{ioffe2015batch}
\citation{krogh1991simple}
\citation{drucker1992improving,zhao2022penalizing}
\citation{sokolic2017robust}
\citation{pereyra2017regularizing}
\citation{keskar2016large}
\citation{Jiang2020Fantastic}
\citation{liang2019fisher}
\citation{pereyra2017regularizing,chaudhari2019entropy}
\citation{wu2022alignment,wu2022does}
\citation{dinh2017sharp}
\citation{kaur2023maximum}
\citation{maddox2020rethinking}
\citation{LIU202313}
\citation{LIU202313}
\citation{avron2011randomized}
\citation{LIU202313}
\newlabel{sec:Background}{{}{2}{}{}{}}
\newlabel{subsec:sam}{{}{2}{}{}{}}
\newlabel{eq:sam-approx}{{1}{2}{}{}{}}
\newlabel{subsec:regularization}{{}{2}{}{}{}}
\newlabel{subsec:flat}{{}{2}{}{}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{subfig:cifar10}{{1a}{3}{ResNet-18 on CIFAR10}{}{}}
\newlabel{sub@subfig:cifar10}{{a}{3}{ResNet-18 on CIFAR10}{}{}}
\newlabel{subfig:cifar100}{{1b}{3}{ResNet-18 on CIFAR100}{}{}}
\newlabel{sub@subfig:cifar100}{{b}{3}{ResNet-18 on CIFAR100}{}{}}
\newlabel{fig:AR}{{1}{3}{\relax \fontsize  {9}{10}\selectfont  The evolution of approximation ratio (AR), Hessian trace and top eigenvalue of Hessian (the two Y axes on the right) during SAM training on CIFAR10 and CIFAR100 datasets. The continuously decreasing AR indicates an enlarging curvature whereas both of the Hessian-based curvature metrics (which are expected to continuously increase) fail to capture the true curvature of model loss landscape.}{}{}}
\newlabel{sec:Methodology}{{}{3}{}{}{}}
\newlabel{subsec:empirical}{{}{3}{}{}{}}
\newlabel{eq:taylor}{{3}{3}{}{}{}}
\citation{zhao2022penalizing}
\newlabel{subfig:top_eigen}{{2a}{4}{$\lambda _{top}\left (\nabla ^2 L_{\mathcal  {S}}\left (\boldsymbol  {w}\right )\right )$}{}{}}
\newlabel{sub@subfig:top_eigen}{{a}{4}{$\lambda _{top}\left (\nabla ^2 L_{\mathcal  {S}}\left (\boldsymbol  {w}\right )\right )$}{}{}}
\newlabel{subfig:trace}{{2b}{4}{$\operatorname  {Tr}\left (\nabla ^2 L_{\mathcal  {S}}\left (\boldsymbol  {w}\right )\right )$}{}{}}
\newlabel{sub@subfig:trace}{{b}{4}{$\operatorname  {Tr}\left (\nabla ^2 L_{\mathcal  {S}}\left (\boldsymbol  {w}\right )\right )$}{}{}}
\newlabel{subfig:n_trace}{{2c}{4}{$\mathcal  {C}(\boldsymbol  {w})$}{}{}}
\newlabel{sub@subfig:n_trace}{{c}{4}{$\mathcal  {C}(\boldsymbol  {w})$}{}{}}
\newlabel{subfig:n_trace_vs_ar}{{2d}{4}{$\mathcal  {C}(\boldsymbol  {w}) \: v.s. \: \operatorname  {AR}$}{}{}}
\newlabel{sub@subfig:n_trace_vs_ar}{{d}{4}{$\mathcal  {C}(\boldsymbol  {w}) \: v.s. \: \operatorname  {AR}$}{}{}}
\newlabel{fig:CR}{{2}{4}{Evolution of the three curvature metrics (indicated in the captions of subfig.(a)(b)(c)) during SAM training of ResNet-18 on CIFAR-10 and CIFAR-100. In (a)(b)(c), the left/right Y axes denote the metric values on training/test sets, also corresponding to solid/dashed lines. Subfigs. (a) (b) show that the top Hessian eigenvalue and Hessian trace exhibit large discrepancy on train and test sets where values calculated on test set can be 50x more than those on training set. Subfig (c) shows that our proposed normalized Hessian trace shows consistent trends which implies that it well captures the true model geometry. Finally, subfig (d) illustrates that the normalized Hessian trace also reflects (inversely) the phenomenon of decreasing approximation ratio (AR) since they both indicate a growing curvature throughout training.}{}{}}
\newlabel{eq:normHtr}{{4}{4}{}{}{}}
\newlabel{subsec:crsam}{{}{4}{}{}{}}
\citation{fawzi2018empirical,moosavi2019robustness}
\citation{zhuang2022surrogate}
\citation{krizhevsky2009learning}
\citation{he2016deep}
\citation{he2016deep}
\citation{zagoruyko2016wide}
\citation{han2017deep}
\citation{foret2021sharpnessaware}
\citation{devries2017improved}
\citation{cubuk2018autoaugment}
\newlabel{subsec:fd}{{}{5}{}{}{}}
\newlabel{theorem:fd}{{1}{5}{}{}{}}
\newlabel{eq:R_c_fd}{{7}{5}{}{}{}}
\newlabel{sec:experiments}{{}{5}{}{}{}}
\newlabel{subsec: cifar}{{}{5}{}{}{}}
\citation{liu2022random,du2022sharpness}
\citation{deng2009imagenet}
\citation{he2016deep}
\citation{dosovitskiy2020image}
\citation{hendrycks2018benchmarking}
\citation{hendrycks2021many}
\citation{du2022efficient}
\citation{loshchilov2018decoupled}
\citation{avron2011randomized,yao2020pyhessian}
\citation{li2018visualizing}
\newlabel{fig:computation}{{3}{6}{Computing the gradient of $R_c(\boldsymbol  {w})$. The two gradient steps are independent of each other and can be perfectly parallelized. Hence the training speed is almost the same as SAM.}{}{}}
\newlabel{alg:CR-SAM}{{1}{6}{Training with CR-SAM}{}{}}
\newlabel{subsec:imagenet}{{}{6}{}{}{}}
\newlabel{subsec:eigenvalue}{{}{6}{}{}{}}
\newlabel{subsec:viz}{{}{6}{}{}{}}
\newlabel{tab:cifar}{{1}{7}{Results on CIFAR-10 and CIFAR-100. The base optimizer for SAM and CR-SAM is SGD with Momentum (SGD+M).}{}{}}
\newlabel{tab:imagenet}{{2}{7}{Results on ImageNet-1k/-C/-R, the base optimizer for ResNets and ViTs are SGD+M and AdamW, respectively.}{}{}}
\newlabel{tab:geometry}{{3}{7}{Model geometry of ResNet-18 models trained with SGD, SAM and CR-SAM, values are computed on test set.}{}{}}
\newlabel{sec:conclusion}{{}{7}{}{}{}}
\newlabel{fig:sgd}{{4a}{7}{SGD}{}{}}
\newlabel{sub@fig:sgd}{{a}{7}{SGD}{}{}}
\newlabel{fig:sam}{{4b}{7}{SAM}{}{}}
\newlabel{sub@fig:sam}{{b}{7}{SAM}{}{}}
\newlabel{fig:sam-GAM}{{4c}{7}{CR-SAM}{}{}}
\newlabel{sub@fig:sam-GAM}{{c}{7}{CR-SAM}{}{}}
\newlabel{fig:vis}{{4}{7}{Loss landscapes for SGD, SAM and CR-SAM.}{}{}}
\bibdata{aaai24}
\citation{JMLR:v17:15-290,gat2022importance}
\citation{gat2022importance}
\newlabel{theorem:fd}{{2}{10}{}{}{}}
\newlabel{theorem:bound}{{3}{10}{}{}{}}
\newlabel{fig:loss}{{5a}{11}{Loss vs Epochs of CR-SAM.}{}{}}
\newlabel{sub@fig:loss}{{a}{11}{Loss vs Epochs of CR-SAM.}{}{}}
\newlabel{fig:acc}{{5b}{11}{Accuracy vs epochs of CR-SAM.}{}{}}
\newlabel{sub@fig:acc}{{b}{11}{Accuracy vs epochs of CR-SAM.}{}{}}
\newlabel{fig:convergence}{{5}{11}{\relax \fontsize  {9}{10}\selectfont  The evolution of training and testing loss/accuracy on CIFAR100 trained with ResNet18 by SAM and our proposed CR-SAM. The faster convergence rate of CR-SAM could be explained by the fact that CR-SAM discourages excessive curvature and thus reduces the optimization complexity, thereby making local minimum easier to reach.}{}{}}
\newlabel{tab:hyper_cifar}{{4}{12}{\relax \fontsize  {9}{10}\selectfont  Hyperparameters for training from scratch on CIFAR10 and CIFAR100}{}{}}
\newlabel{tab:hyper_imgnet}{{5}{13}{\relax \fontsize  {9}{10}\selectfont  Hyperparameters for training from scratch on ImageNet}{}{}}
\gdef \@abspage@last{13}
