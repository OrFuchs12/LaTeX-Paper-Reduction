@inproceedings{
foret2021sharpnessaware,
title={Sharpness-aware Minimization for Efficiently Improving Generalization},
author={Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=6Tm1mposlrM}
}

@inproceedings{
Jiang2020Fantastic,
title={Fantastic Generalization Measures and Where to Find Them},
author={Yiding Jiang and Behnam Neyshabur and Hossein Mobahi and Dilip Krishnan and Samy Bengio},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJgIPJBFvH}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@inproceedings{NEURIPS2020_322f6246,
 author = {Wilson, Andrew G and Izmailov, Pavel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {4697--4708},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian Deep Learning and a Probabilistic Perspective of Generalization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/322f62469c5e3c7dc3e58f5a4d1ea399-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{
kukačka2018regularization,
title={Regularization for Deep Learning: A Taxonomy},
author={Jan Kukačka and Vladimir Golkov and Daniel Cremers},
year={2018},
url={https://openreview.net/forum?id=SkHkeixAW},
}

@article{drucker1992improving,
  title={Improving generalization performance using double backpropagation},
  author={Drucker, Harris and Le Cun, Yann},
  journal={IEEE transactions on neural networks},
  volume={3},
  number={6},
  pages={991--997},
  year={1992}
}

@inproceedings{zhao2022penalizing,
  title={Penalizing gradient norm for efficiently improving generalization in deep learning},
  author={Zhao, Yang and Zhang, Hao and Hu, Xiuyuan},
  booktitle={International Conference on Machine Learning},
  pages={26982--26992},
  year={2022},
  organization={PMLR}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={pmlr}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}

@article{krogh1991simple,
  title={A simple weight decay can improve generalization},
  author={Krogh, Anders and Hertz, John},
  journal={Advances in neural information processing systems},
  volume={4},
  year={1991}
}

@misc{
pereyra2017regularizing,
title={Regularizing Neural Networks by Penalizing Confident Output Distributions},
author={Gabriel Pereyra and George Tucker and Jan Chorowski and Lukasz Kaiser and Geoffrey Hinton},
year={2017},
url={https://openreview.net/forum?id=HkCjNI5ex}
}

@article{devries2017improved,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
}

@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={702--703},
  year={2020}
}

@article{sokolic2017robust,
  title={Robust large margin deep neural networks},
  author={Sokoli{\'c}, Jure and Giryes, Raja and Sapiro, Guillermo and Rodrigues, Miguel RD},
  journal={IEEE Transactions on Signal Processing},
  volume={65},
  number={16},
  pages={4265--4280},
  year={2017},
  publisher={IEEE}
}

@article{hochreiter1994simplifying,
  title={Simplifying neural nets by discovering flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Advances in neural information processing systems},
  volume={7},
  year={1994}
}

@inproceedings{mcallester1999pac,
  title={PAC-Bayesian model averaging},
  author={McAllester, David A},
  booktitle={Proceedings of the twelfth annual conference on Computational learning theory},
  pages={164--170},
  year={1999}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{jiang2019fantastic,
  title={Fantastic generalization measures and where to find them},
  author={Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  journal={arXiv preprint arXiv:1912.02178},
  year={2019}
}

@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={arXiv preprint arXiv:1703.11008},
  year={2017}
}

@inproceedings{kenton2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019}
}

@inproceedings{oord2018parallel,
  title={Parallel wavenet: Fast high-fidelity speech synthesis},
  author={Oord, Aaron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and Driessche, George and Lockhart, Edward and Cobo, Luis and Stimberg, Florian and others},
  booktitle={International conference on machine learning},
  pages={3918--3926},
  year={2018},
  organization={PMLR}
}

@article{maddox2020rethinking,
  title={Rethinking parameter counting in deep models: Effective dimensionality revisited},
  author={Maddox, Wesley J and Benton, Gregory and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2003.02139},
  year={2020}
}

@inproceedings{kaur2023maximum,
  title={On the maximum hessian eigenvalue and generalization},
  author={Kaur, Simran and Cohen, Jeremy and Lipton, Zachary Chase},
  booktitle={Proceedings on},
  pages={51--65},
  year={2023},
  organization={PMLR}
}

@article{cubuk2018autoaugment,
  title={Autoaugment: Learning augmentation policies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  journal={arXiv preprint arXiv:1805.09501},
  year={2018}
}

@inproceedings{han2017deep,
  title={Deep pyramidal residual networks},
  author={Han, Dongyoon and Kim, Jiwhan and Kim, Junmo},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5927--5935},
  year={2017}
}

@inproceedings{
chen2022when,
title={When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations},
author={Xiangning Chen and Cho-Jui Hsieh and Boqing Gong},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=LtKcMgGOeLt}
}

@article{bahri2021sharpness,
  title={Sharpness-aware minimization improves language model generalization},
  author={Bahri, Dara and Mobahi, Hossein and Tay, Yi},
  journal={arXiv preprint arXiv:2110.08529},
  year={2021}
}

@inproceedings{liu2022towards,
  title={Towards efficient and scalable sharpness-aware minimization},
  author={Liu, Yong and Mai, Siqi and Chen, Xiangning and Hsieh, Cho-Jui and You, Yang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12360--12370},
  year={2022}
}

@inproceedings{
du2022efficient,
title={Efficient Sharpness-aware Minimization for Improved Training of Neural Networks},
author={Jiawei Du and Hanshu Yan and Jiashi Feng and Joey Tianyi Zhou and Liangli Zhen and Rick Siow Mong Goh and Vincent Tan},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=n0OeTdNRG0Q}
}

@inproceedings{
mi2022make,
title={Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation Approach},
author={Peng Mi and Li Shen and Tianhe Ren and Yiyi Zhou and Xiaoshuai Sun and Rongrong Ji and Dacheng Tao},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=88_wNI6ZBDZ}
}

@article{du2022sharpness,
  title={Sharpness-aware training for free},
  author={Du, Jiawei and Zhou, Daquan and Feng, Jiashi and Tan, Vincent and Zhou, Joey Tianyi},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23439--23451},
  year={2022}
}

@inproceedings{
jiang2023an,
title={An Adaptive Policy to Employ Sharpness-Aware Minimization},
author={Weisen Jiang and Hansi Yang and Yu Zhang and James Kwok},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=6Wl7-M2BC-}
}

@inproceedings{
liu2022random,
title={Random Sharpness-Aware Minimization},
author={Yong Liu and Siqi Mai and Minhao Cheng and Xiangning Chen and Cho-Jui Hsieh and Yang You},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=htUvh7xPoa}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{
hendrycks2018benchmarking,
title={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
author={Dan Hendrycks and Thomas Dietterich},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HJz6tiCqYm},
}

@inproceedings{hendrycks2021many,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8340--8349},
  year={2021}
}

@inproceedings{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1019--1028},
  year={2017},
  organization={PMLR}
}

@article{wu2022alignment,
  title={The alignment property of SGD noise and how it helps select flat minima: A stability analysis},
  author={Wu, Lei and Wang, Mingze and Su, Weijie},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4680--4693},
  year={2022}
}

@article{wu2022does,
  title={When does sgd favor flat minima? a quantitative characterization via linear stability},
  author={Wu, Lei and Wang, Mingze and Su, Weijie},
  journal={arXiv preprint arXiv:2207.02628},
  year={2022}
}

@inproceedings{liang2019fisher,
  title={Fisher-rao metric, geometry, and complexity of neural networks},
  author={Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
  booktitle={The 22nd international conference on artificial intelligence and statistics},
  pages={888--896},
  year={2019},
  organization={PMLR}
}

@article{chaudhari2019entropy,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124018},
  year={2019},
  publisher={IOP Publishing}
}

@article{LIU202313,
title = {Hessian regularization of deep neural networks: A novel approach based on stochastic estimators of Hessian trace},
journal = {Neurocomputing},
volume = {536},
pages = {13-20},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223002515},
author = {Yucong Liu and Shixing Yu and Tong Lin},
keywords = {Hessian regularization, Stochastic algorithm, Dynamical system, Flat minima},
abstract = {In this paper, we develop a novel regularization method for deep neural networks by penalizing the trace of Hessian. This regularizer is motivated by a recent guarantee bound of the generalization error. We explain its benefits in finding flat minima and avoiding Lyapunov stability in dynamical systems. We adopt the Hutchinson method as a classical unbiased estimator for the trace of a matrix and further accelerate its calculation using a Dropout scheme. Experiments demonstrate that our method outperforms existing regularizers and data augmentation methods, such as Jacobian, Confidence Penalty, Label Smoothing, Cutout, and Mixup. The code is available at https://github.com/Dean-lyc/Hessian-Regularization.}
}

@article{avron2011randomized,
  title={Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix},
  author={Avron, Haim and Toledo, Sivan},
  journal={Journal of the ACM (JACM)},
  volume={58},
  number={2},
  pages={1--34},
  year={2011},
  publisher={ACM New York, NY, USA}
}

@inproceedings{fawzi2018empirical,
  title={Empirical study of the topology and geometry of deep networks},
  author={Fawzi, Alhussein and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal and Soatto, Stefano},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3762--3770},
  year={2018}
}

@article{jetley2018friends,
  title={With friends like these, who needs adversaries?},
  author={Jetley, Saumya and Lord, Nicholas and Torr, Philip},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{moosavi2019robustness,
  title={Robustness via curvature regularization, and vice versa},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Uesato, Jonathan and Frossard, Pascal},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9078--9086},
  year={2019}
}

@article{zhuang2022surrogate,
  title={Surrogate gap minimization improves sharpness-aware training},
  author={Zhuang, Juntang and Gong, Boqing and Yuan, Liangzhe and Cui, Yin and Adam, Hartwig and Dvornek, Nicha and Tatikonda, Sekhar and Duncan, James and Liu, Ting},
  journal={arXiv preprint arXiv:2203.08065},
  year={2022}
}

@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@article{yao2018hessian,
  title={Hessian-based analysis of large batch training and robustness to adversaries},
  author={Yao, Zhewei and Gholami, Amir and Lei, Qi and Keutzer, Kurt and Mahoney, Michael W},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{yao2020pyhessian,
  title={Pyhessian: Neural networks through the lens of the hessian},
  author={Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W},
  booktitle={2020 IEEE international conference on big data (Big data)},
  pages={581--590},
  year={2020},
  organization={IEEE}
}

@article{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{srinivas2022efficient,
  title={Efficient training of low-curvature neural networks},
  author={Srinivas, Suraj and Matoba, Kyle and Lakkaraju, Himabindu and Fleuret, Fran{\c{c}}ois},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25951--25964},
  year={2022}
}

@article{pang2020efficient,
  title={Efficient learning of generative models via finite-difference score matching},
  author={Pang, Tianyu and Xu, Kun and Li, Chongxuan and Song, Yang and Ermon, Stefano and Zhu, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19175--19188},
  year={2020}
}

@article{JMLR:v17:15-290,
  author  = {Pierre Alquier and James Ridgway and Nicolas Chopin},
  title   = {On the properties of variational approximations of Gibbs posteriors},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {236},
  pages   = {1--41},
  url     = {http://jmlr.org/papers/v17/15-290.html}
}

@article{gat2022importance,
  title={On the Importance of Gradient Norm in PAC-Bayesian Bounds},
  author={Gat, Itai and Adi, Yossi and Schwing, Alex and Hazan, Tamir},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16068--16081},
  year={2022}
}