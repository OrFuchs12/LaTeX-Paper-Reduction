\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Alquier, Ridgway, and Chopin(2016)}]{JMLR:v17:15-290}
Alquier, P.; Ridgway, J.; and Chopin, N. 2016.
\newblock On the properties of variational approximations of Gibbs posteriors.
\newblock \emph{Journal of Machine Learning Research}, 17(236): 1--41.

\bibitem[{Avron and Toledo(2011)}]{avron2011randomized}
Avron, H.; and Toledo, S. 2011.
\newblock Randomized algorithms for estimating the trace of an implicit
  symmetric positive semi-definite matrix.
\newblock \emph{Journal of the ACM (JACM)}, 58(2): 1--34.

\bibitem[{Bahri, Mobahi, and Tay(2021)}]{bahri2021sharpness}
Bahri, D.; Mobahi, H.; and Tay, Y. 2021.
\newblock Sharpness-aware minimization improves language model generalization.
\newblock \emph{arXiv preprint arXiv:2110.08529}.

\bibitem[{Chaudhari et~al.(2019)Chaudhari, Choromanska, Soatto, LeCun,
  Baldassi, Borgs, Chayes, Sagun, and Zecchina}]{chaudhari2019entropy}
Chaudhari, P.; Choromanska, A.; Soatto, S.; LeCun, Y.; Baldassi, C.; Borgs, C.;
  Chayes, J.; Sagun, L.; and Zecchina, R. 2019.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2019(12): 124018.

\bibitem[{Chen, Hsieh, and Gong(2022)}]{chen2022when}
Chen, X.; Hsieh, C.-J.; and Gong, B. 2022.
\newblock When Vision Transformers Outperform ResNets without Pre-training or
  Strong Data Augmentations.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Cubuk et~al.(2018)Cubuk, Zoph, Mane, Vasudevan, and
  Le}]{cubuk2018autoaugment}
Cubuk, E.~D.; Zoph, B.; Mane, D.; Vasudevan, V.; and Le, Q.~V. 2018.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock \emph{arXiv preprint arXiv:1805.09501}.

\bibitem[{Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le}]{cubuk2020randaugment}
Cubuk, E.~D.; Zoph, B.; Shlens, J.; and Le, Q.~V. 2020.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition workshops}, 702--703.

\bibitem[{Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei}]{deng2009imagenet}
Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei, L. 2009.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, 248--255. Ieee.

\bibitem[{DeVries and Taylor(2017)}]{devries2017improved}
DeVries, T.; and Taylor, G.~W. 2017.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock \emph{arXiv preprint arXiv:1708.04552}.

\bibitem[{Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio}]{dinh2017sharp}
Dinh, L.; Pascanu, R.; Bengio, S.; and Bengio, Y. 2017.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{International Conference on Machine Learning}, 1019--1028.
  PMLR.

\bibitem[{Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly
  et~al.}]{dosovitskiy2020image}
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.;
  Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et~al.
  2020.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}.

\bibitem[{Drucker and Le~Cun(1992)}]{drucker1992improving}
Drucker, H.; and Le~Cun, Y. 1992.
\newblock Improving generalization performance using double backpropagation.
\newblock \emph{IEEE transactions on neural networks}, 3(6): 991--997.

\bibitem[{Du et~al.(2022{\natexlab{a}})Du, Yan, Feng, Zhou, Zhen, Goh, and
  Tan}]{du2022efficient}
Du, J.; Yan, H.; Feng, J.; Zhou, J.~T.; Zhen, L.; Goh, R. S.~M.; and Tan, V.
  2022{\natexlab{a}}.
\newblock Efficient Sharpness-aware Minimization for Improved Training of
  Neural Networks.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Du et~al.(2022{\natexlab{b}})Du, Zhou, Feng, Tan, and
  Zhou}]{du2022sharpness}
Du, J.; Zhou, D.; Feng, J.; Tan, V.; and Zhou, J.~T. 2022{\natexlab{b}}.
\newblock Sharpness-aware training for free.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:
  23439--23451.

\bibitem[{Dziugaite and Roy(2017)}]{dziugaite2017computing}
Dziugaite, G.~K.; and Roy, D.~M. 2017.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}.

\bibitem[{Fawzi et~al.(2018)Fawzi, Moosavi-Dezfooli, Frossard, and
  Soatto}]{fawzi2018empirical}
Fawzi, A.; Moosavi-Dezfooli, S.-M.; Frossard, P.; and Soatto, S. 2018.
\newblock Empirical study of the topology and geometry of deep networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 3762--3770.

\bibitem[{Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur}]{foret2021sharpnessaware}
Foret, P.; Kleiner, A.; Mobahi, H.; and Neyshabur, B. 2021.
\newblock Sharpness-aware Minimization for Efficiently Improving
  Generalization.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Gat et~al.(2022)Gat, Adi, Schwing, and Hazan}]{gat2022importance}
Gat, I.; Adi, Y.; Schwing, A.; and Hazan, T. 2022.
\newblock On the Importance of Gradient Norm in PAC-Bayesian Bounds.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:
  16068--16081.

\bibitem[{Han, Kim, and Kim(2017)}]{han2017deep}
Han, D.; Kim, J.; and Kim, J. 2017.
\newblock Deep pyramidal residual networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 5927--5935.

\bibitem[{He et~al.(2016)He, Zhang, Ren, and Sun}]{he2016deep}
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 770--778.

\bibitem[{Hendrycks et~al.(2021)Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo,
  Desai, Zhu, Parajuli, Guo et~al.}]{hendrycks2021many}
Hendrycks, D.; Basart, S.; Mu, N.; Kadavath, S.; Wang, F.; Dorundo, E.; Desai,
  R.; Zhu, T.; Parajuli, S.; Guo, M.; et~al. 2021.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, 8340--8349.

\bibitem[{Hendrycks and Dietterich(2019)}]{hendrycks2018benchmarking}
Hendrycks, D.; and Dietterich, T. 2019.
\newblock Benchmarking Neural Network Robustness to Common Corruptions and
  Perturbations.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Hochreiter and Schmidhuber(1997)}]{hochreiter1997flat}
Hochreiter, S.; and Schmidhuber, J. 1997.
\newblock Flat minima.
\newblock \emph{Neural computation}, 9(1): 1--42.

\bibitem[{Ioffe and Szegedy(2015)}]{ioffe2015batch}
Ioffe, S.; and Szegedy, C. 2015.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, 448--456.
  pmlr.

\bibitem[{Jiang et~al.(2023)Jiang, Yang, Zhang, and Kwok}]{jiang2023an}
Jiang, W.; Yang, H.; Zhang, Y.; and Kwok, J. 2023.
\newblock An Adaptive Policy to Employ Sharpness-Aware Minimization.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Jiang et~al.(2020)Jiang, Neyshabur, Mobahi, Krishnan, and
  Bengio}]{Jiang2020Fantastic}
Jiang, Y.; Neyshabur, B.; Mobahi, H.; Krishnan, D.; and Bengio, S. 2020.
\newblock Fantastic Generalization Measures and Where to Find Them.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Kaur, Cohen, and Lipton(2023)}]{kaur2023maximum}
Kaur, S.; Cohen, J.; and Lipton, Z.~C. 2023.
\newblock On the maximum hessian eigenvalue and generalization.
\newblock In \emph{Proceedings on}, 51--65. PMLR.

\bibitem[{Kenton and Toutanova(2019)}]{kenton2019bert}
Kenton, J. D. M.-W.~C.; and Toutanova, L.~K. 2019.
\newblock BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding.
\newblock In \emph{Proceedings of NAACL-HLT}, 4171--4186.

\bibitem[{Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang}]{keskar2016large}
Keskar, N.~S.; Mudigere, D.; Nocedal, J.; Smelyanskiy, M.; and Tang, P. T.~P.
  2016.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}.

\bibitem[{Krizhevsky, Hinton et~al.(2009)}]{krizhevsky2009learning}
Krizhevsky, A.; Hinton, G.; et~al. 2009.
\newblock Learning multiple layers of features from tiny images.

\bibitem[{Krogh and Hertz(1991)}]{krogh1991simple}
Krogh, A.; and Hertz, J. 1991.
\newblock A simple weight decay can improve generalization.
\newblock \emph{Advances in neural information processing systems}, 4.

\bibitem[{Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein}]{li2018visualizing}
Li, H.; Xu, Z.; Taylor, G.; Studer, C.; and Goldstein, T. 2018.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{Advances in neural information processing systems}, 31.

\bibitem[{Liang et~al.(2019)Liang, Poggio, Rakhlin, and
  Stokes}]{liang2019fisher}
Liang, T.; Poggio, T.; Rakhlin, A.; and Stokes, J. 2019.
\newblock Fisher-rao metric, geometry, and complexity of neural networks.
\newblock In \emph{The 22nd international conference on artificial intelligence
  and statistics}, 888--896. PMLR.

\bibitem[{Liu et~al.(2022{\natexlab{a}})Liu, Mai, Chen, Hsieh, and
  You}]{liu2022towards}
Liu, Y.; Mai, S.; Chen, X.; Hsieh, C.-J.; and You, Y. 2022{\natexlab{a}}.
\newblock Towards efficient and scalable sharpness-aware minimization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 12360--12370.

\bibitem[{Liu et~al.(2022{\natexlab{b}})Liu, Mai, Cheng, Chen, Hsieh, and
  You}]{liu2022random}
Liu, Y.; Mai, S.; Cheng, M.; Chen, X.; Hsieh, C.-J.; and You, Y.
  2022{\natexlab{b}}.
\newblock Random Sharpness-Aware Minimization.
\newblock In Oh, A.~H.; Agarwal, A.; Belgrave, D.; and Cho, K., eds.,
  \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Liu, Yu, and Lin(2023)}]{LIU202313}
Liu, Y.; Yu, S.; and Lin, T. 2023.
\newblock Hessian regularization of deep neural networks: A novel approach
  based on stochastic estimators of Hessian trace.
\newblock \emph{Neurocomputing}, 536: 13--20.

\bibitem[{Loshchilov and Hutter(2019)}]{loshchilov2018decoupled}
Loshchilov, I.; and Hutter, F. 2019.
\newblock Decoupled Weight Decay Regularization.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Maddox, Benton, and Wilson(2020)}]{maddox2020rethinking}
Maddox, W.~J.; Benton, G.; and Wilson, A.~G. 2020.
\newblock Rethinking parameter counting in deep models: Effective
  dimensionality revisited.
\newblock \emph{arXiv preprint arXiv:2003.02139}.

\bibitem[{Mi et~al.(2022)Mi, Shen, Ren, Zhou, Sun, Ji, and Tao}]{mi2022make}
Mi, P.; Shen, L.; Ren, T.; Zhou, Y.; Sun, X.; Ji, R.; and Tao, D. 2022.
\newblock Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation
  Approach.
\newblock In Oh, A.~H.; Agarwal, A.; Belgrave, D.; and Cho, K., eds.,
  \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Moosavi-Dezfooli et~al.(2019)Moosavi-Dezfooli, Fawzi, Uesato, and
  Frossard}]{moosavi2019robustness}
Moosavi-Dezfooli, S.-M.; Fawzi, A.; Uesato, J.; and Frossard, P. 2019.
\newblock Robustness via curvature regularization, and vice versa.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 9078--9086.

\bibitem[{Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro}]{neyshabur2017exploring}
Neyshabur, B.; Bhojanapalli, S.; McAllester, D.; and Srebro, N. 2017.
\newblock Exploring generalization in deep learning.
\newblock \emph{Advances in neural information processing systems}, 30.

\bibitem[{Oord et~al.(2018)Oord, Li, Babuschkin, Simonyan, Vinyals,
  Kavukcuoglu, Driessche, Lockhart, Cobo, Stimberg et~al.}]{oord2018parallel}
Oord, A.; Li, Y.; Babuschkin, I.; Simonyan, K.; Vinyals, O.; Kavukcuoglu, K.;
  Driessche, G.; Lockhart, E.; Cobo, L.; Stimberg, F.; et~al. 2018.
\newblock Parallel wavenet: Fast high-fidelity speech synthesis.
\newblock In \emph{International conference on machine learning}, 3918--3926.
  PMLR.

\bibitem[{Pereyra et~al.(2017)Pereyra, Tucker, Chorowski, Kaiser, and
  Hinton}]{pereyra2017regularizing}
Pereyra, G.; Tucker, G.; Chorowski, J.; Kaiser, L.; and Hinton, G. 2017.
\newblock Regularizing Neural Networks by Penalizing Confident Output
  Distributions.

\bibitem[{Sokoli{\'c} et~al.(2017)Sokoli{\'c}, Giryes, Sapiro, and
  Rodrigues}]{sokolic2017robust}
Sokoli{\'c}, J.; Giryes, R.; Sapiro, G.; and Rodrigues, M.~R. 2017.
\newblock Robust large margin deep neural networks.
\newblock \emph{IEEE Transactions on Signal Processing}, 65(16): 4265--4280.

\bibitem[{Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov}]{srivastava2014dropout}
Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov,
  R. 2014.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15(1): 1929--1958.

\bibitem[{Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna}]{szegedy2016rethinking}
Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2016.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2818--2826.

\bibitem[{Wilson and Izmailov(2020)}]{NEURIPS2020_322f6246}
Wilson, A.~G.; and Izmailov, P. 2020.
\newblock Bayesian Deep Learning and a Probabilistic Perspective of
  Generalization.
\newblock In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H.,
  eds., \emph{Advances in Neural Information Processing Systems}, volume~33,
  4697--4708. Curran Associates, Inc.

\bibitem[{Wu, Wang, and Su(2022{\natexlab{a}})}]{wu2022alignment}
Wu, L.; Wang, M.; and Su, W. 2022{\natexlab{a}}.
\newblock The alignment property of SGD noise and how it helps select flat
  minima: A stability analysis.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:
  4680--4693.

\bibitem[{Wu, Wang, and Su(2022{\natexlab{b}})}]{wu2022does}
Wu, L.; Wang, M.; and Su, W. 2022{\natexlab{b}}.
\newblock When does sgd favor flat minima? a quantitative characterization via
  linear stability.
\newblock \emph{arXiv preprint arXiv:2207.02628}.

\bibitem[{Yao et~al.(2020)Yao, Gholami, Keutzer, and
  Mahoney}]{yao2020pyhessian}
Yao, Z.; Gholami, A.; Keutzer, K.; and Mahoney, M.~W. 2020.
\newblock Pyhessian: Neural networks through the lens of the hessian.
\newblock In \emph{2020 IEEE international conference on big data (Big data)},
  581--590. IEEE.

\bibitem[{Zagoruyko and Komodakis(2016)}]{zagoruyko2016wide}
Zagoruyko, S.; and Komodakis, N. 2016.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}.

\bibitem[{Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz}]{zhang2017mixup}
Zhang, H.; Cisse, M.; Dauphin, Y.~N.; and Lopez-Paz, D. 2017.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}.

\bibitem[{Zhao, Zhang, and Hu(2022)}]{zhao2022penalizing}
Zhao, Y.; Zhang, H.; and Hu, X. 2022.
\newblock Penalizing gradient norm for efficiently improving generalization in
  deep learning.
\newblock In \emph{International Conference on Machine Learning}, 26982--26992.
  PMLR.

\bibitem[{Zhuang et~al.(2022)Zhuang, Gong, Yuan, Cui, Adam, Dvornek, Tatikonda,
  Duncan, and Liu}]{zhuang2022surrogate}
Zhuang, J.; Gong, B.; Yuan, L.; Cui, Y.; Adam, H.; Dvornek, N.; Tatikonda, S.;
  Duncan, J.; and Liu, T. 2022.
\newblock Surrogate gap minimization improves sharpness-aware training.
\newblock \emph{arXiv preprint arXiv:2203.08065}.

\end{thebibliography}
