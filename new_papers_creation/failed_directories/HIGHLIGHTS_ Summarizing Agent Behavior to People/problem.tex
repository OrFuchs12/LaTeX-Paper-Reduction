\section{Summarizing Agent Behaviors}
\label{sec:problem}
%To help people better understand the capabilities of agents, we propose to generate summaries of an agent's behavior. 
%Because in most settings it will not be feasible to describe the agent's behavior in each of the possible states. 
Our formalization of the summarization problem assumes that the agent uses a Markov Decision Process (MDP), where $A$ is the set of actions available to the agent, $S$ is the set of states, $R$: $S \times A \rightarrow \mathbb{R}$ is the reward function, which maps each state and action to a reward, and $Tr$ is the transition probability function, i.e., $Tr(s', a, s)$ defines the probability of reaching state $s'$, when taking action $a$ in state $s$. The agent has a policy $\pi$ which specifies which action to take in each of the states. 

We formalize the problem of summarizing an agent's behavior as follows: from execution traces of an agent, choose a set $T = \langle t_{1},...,t_{k} \rangle$ of trajectories to include in the summary, where each trajectory is composed of a sequence of $l$ consecutive states and the actions taken in those states $\langle(s_{i},a_{i}),...,(s_{i+l-1},a_{i+l-1})\rangle$. We consider trajectories rather than single states because seeing what action was taken by the agent in a specific state might not be meaningful without a broader context (e.g., watching a self-driving car for one second will not reveal much useful information). Because it is infeasible that people will be able to review the behavior of an agent in all possible states, we assume a limited budget $k$ for the size of the summary, such that $|T| = k$. This budget limits the amount of time and cognitive effort that a person needs to invest in reviewing the agent's behavior. We discuss alternative formulations of the summarization problem in Section~\ref{sec:disc}. 

There are several factors that could be considered when deciding which states to include in a summary, such as the effect of taking a different action in that state, the diversity of the states that are included in the summary and the frequency at which states are likely to be encountered by the agent. In this paper, we focus on the first factor, which we refer to as the ``importance'' of a state. Intuitively, a good summary should provide a person reviewing the summary with a sense of the agent's behavior in states that the person considers important (e.g., when making a mistake would be very costly). The importance of states included in the summary could substantially affect the ability of a person to assess an agent's capabilities. For example, imagine a summary of self-driving car that only shows the car driving on a highway with no interruptions. This summary would provide people with very little understanding of how the car might act in other, more important, scenarios (e.g., when another car drives into its lane, when there is road construction). In contrast, a summary showing the self-driving car in a range on more interesting situations (e.g., overtaking another car, breaking when a person enters the road) would convey more useful information to people reviewing it. 

In Section~\ref{sec:alg} we describe an algorithm that generates summaries based on this state importance criteria. We then extend the algorithm to also take into consideration the diversity of the states included in the summary (described in Section~\ref{sec:algDiv}). That is, instead of considering each state in isolation when deciding whether to include it in the summary, the decision will also depend on the other states that are currently included in the summary. We discuss other possible desired summary properties in Section~\ref{sec:disc}. 

%Intuitively, a good summary should provide a person reviewing the summary with a sense of the agent's behavior in states that the person considers important (e.g., when making a mistake would be very costly). The choice of trajectories to include in the summary could substantially affect the ability of a person to assess an agent's capabilities. For example, imagine a summary of self-driving car that only shows the car driving on a highway with no interruptions. Such summary would provide people with very little understanding of how the car might act in other scenarios (e.g., when another car drives into its lane, when there is road construction). In contrast, a summary showing the self-driving car in a range on more interesting situations would likely convey more useful information to people reviewing it. 

%Second, we might want to produce a summary that is diverse, in the sense that it describes the behavior of the agent in a scenarios that differ from one another. 
