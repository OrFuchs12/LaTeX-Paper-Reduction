\section{Related Work}
\label{sec:related}
Myers~\citep{myers2006metatheoretic} developed methods for summarizing Hierarchical task networks (HTNs) to help people review and compare plans. These summaries showed features such as role allocation and subtasks in the plan. In contrast to this work, our goal is to summarize general agent strategies rather than a hierarchical plan toward completing a specific goal. Other works attempted to explain MDP-based plans~\cite{seegebarth2012making} and recommendations given by MDP-based intelligent assistants~\cite{khan2009minimal,khan2011automatically,dodson2011natural}. The problem we address differs from the problem of generating explanations for specific decisions, as rather than explaining an action taken (or a suggested action), we aim to describe \emph{which} actions will be taken by the agent in \emph{different states}. 

The human-robot interaction literature has developed methods for helping people get insight into a robot's behavior. For example, Lomas et al.~\cite{lomas2012explaining} developed a system that enables a user to query robots about their behavior (e.g., ``why did you turn left here?''). Brooks et al.~\cite{brooks2010towards} developed a system that visualizes and explains past actions of a robot. In other work, animation techniques of anticipation and reaction were used to help people predict what a robot will do next~\cite{takayama2011expressing}. Hayes \& Shah~\cite{hayes2017improving} 
%proposed several methods for explaining robot policies to people using past execution traces. They 
drew an analogy between reviewing a robot's behavior to software debugging and developed methods that enable users to query the agent's behavior in different states and request explanations. 
%This line of work is complementary to our approach, as it provides different ways of examining the behaviors (past or expected) of agents. 
Nikolaidis et al.~\cite{nikolaidis2013human} proposed a cross-training approach where the human and the agent switch roles in simulation to develop a better understanding of their teammate.
Our work introduces a new approach which lets users review automatically generated summaries exemplifying the agent's behavior, without requiring them to manually specify states of interest or work with the agent directly. Our approach is complementary to the above approaches, and could be used in conjunction with them.
%for people to review agents' strategies by automatically generating summaries that exemplify the agent's capabilities. 

%Nikolaidis et al.~\cite{nikolaidis2013human} proposed a cross-training approach where the human and the agent switch roles in simulation to help both parties develop a better understanding of their teammate. However, this approach is not feasible if the person cannot perform the agent's task, and could also require substantial time if the state space is large. Our approach of summarizing the agent's behavior is complementary to the cross-training approach, and provides an alternative way for users to establish a mental model of the agent's behavior.

Last, the growing literature on interpretable machine learning~\cite{doshi2017towards,vellido2012making} introduced methods for algorithms and models more transparent to users. Simlarly to the methods for explaining MDP decisions, these approaches typically explain a one-shot decision (e.g. classification of a particular sample). This is done in different ways, e.g., by showing a simpler model which explains decisions in a particular region of the space~\cite{ribeiro2016model}. Some methods aim to generate a user-understandable decision-making model more generally (e.g., using a prototype-based classification model~\cite{kim2014bayesian}), but these do not address sequential decision-making settings and do not explicitly describe behavior in different scenarios.
%recently many approaches have been proposed for developing interpretable machine learning models, that is, models that are understandable to people~\cite{doshi2017towards,vellido2012making}. These approaches typically seek to explain a decision made by a machine learning model (e.g., by showing a simpler locally correct model~\cite{ribeiro2016model}). As the methods for explaining MDP decisions, these approaches explain a single decision after the fact, rather than provide a description of a strategy or behavior of an agent in different situations. 

