Paper ID1018
Paper Title Algorithm Selection for Optimal Multi-Agent Path Finding via Graph Embedding

Reviewer #1
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
Given a set of source and target locations for a set of agents, the multi-agent path finding problem (MAPF) aims to find paths that lead each agent from its source location to its target location. Many algorithms exist for optimally solving MAPF. This paper suggests MAG, an algorithm selection approach based on graph embedding. Experiments show that MAG usually outperforms other state-of-the-art MAPF algorithm selection methods.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Good: The paper makes non-trivial advances over the current state-of-the-art.
3. {Soundness} Is the paper technically sound?
Good: The paper appears to be technically sound, but I have not carefully checked the details.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have moderate impact within a subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Fair: The paper is somewhat clear, but some important details are missing or unclear.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Fair: The experimental evaluation is weak: important baselines are missing, or the results do not adequately support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Not applicable: For instance, the primary contributions of the paper are theoretical.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Fair: key resources (e.g., proofs, code, data) are unavailable but key details (e.g., proof sketches, experimental setup) are sufficiently well-described for an expert to confidently reproduce the main results
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above)
The paper proposes an interesting new method for MAPF algorithm selection. The experimental section is comprehensive and shows that, in most cases, MAG outperforms other methods.
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
I found two main weaknesses in the paper:
1) Writing - while, in most parts, the paper is well-written, there are many typos, repetitions, and wrong Tables' references (described in more detail below).
2) Experiments - while many maps were tested and MAG performed better than the other methods, I find MAG only slightly better than the state-of-the-art methods. Moreover, some information was missing in this section (all again described below).
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
1) In the experimental results section, how many instances were tested for each map? This information is not presented in the paper.
2) In Table 1, in the \%Rg column of Avg, it seems that KBS+FG2V gets better results than MAG. Why 8.1 is not highlighted? Is it a mistake or is there a specific reason for that? Likewise, in Table 3, "Game" row, "Between-Grid", \%Rg column, why is 122.0 (MAG) highlighted and not 113.1 (G2V)? Is this a mistake?
3) Subsection 4.4 - "the FG2V works quite well on these grids" - Where can I see the trend described in this sentence?

13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
On the one hand, the paper is interesting and enjoyable to read. On the other hand, it is noticeable that it was written just before the deadline and, therefore, there are many typos and wrong sentences. While the paper is not ready for publication as is, I believe that the authors will be able to fix the many writing issues for the final version. Therefore, in my opinion, it is slightly above the acceptance threshold.

Comments and suggestions:
- Abstract - "approach all problems" - "approach for all problems"
- Abstract - "MAPF problem" - "MAPF problem"
- 1 - "Multi-Agent Pathfinding" - "Multi-Agent Path finding"
- 1 - "we propose two contributions" - Three contributions are described below that sentence.
- 1 - "start and goal vertex" - "start and goal vertices"
- 2.1 - "are the sum of costs (SOC) and makespan." - these cost functions are already described one paragraph before that sentence.
- 2.1 - "Makespac" - "Makespan"
- 2.1 - "maximal number of actions each agent makes" - this definition is wrong.
- 2.1 - "goal" - "target"
- 2.1 - "we used considered" - "we used"/"we considered"
- 2.2 - "but their grids are similar topologically" - "but their grids have similar topologically"
- 3.1 - "G2V encode" - "G2V encodes"
- 3.1 - "adds links if any between" - I couldn't understand this sentence.
- 3.1 - "in original MAPF problem" - "in the original MAPF problem"
- Many times throughout the paper, the first letter of the word/dot that came after KBS, G2V or FG2V was missing - e.g., "FG2V ncludes", "G2V For", "to KBS Still", "KBS eatures", "G2V s Maze", "G2V In contrast", "KBS ith", "FG2V ields", "G2V ielded", "FG2V lone".
- 3.1 - "on a simple MAPF problem" - worth to mention that this problem is on a 4-neighbor grid.
- Figure 1 - I couldn't understand the FullG2V illustration. What are the dotted lines? Shortest paths?
- 3.2 - "by firsts" - "by first"
- 3.2 - "This is key" - "This is the key"
- 3.2 - "in FEATHER “mean"" - "in FEATHER is “mean"".
- 3.2 - "of each agents" - "of each agent"
- 4 - "warehouses (warehouse)" - "warehouses ("warehouse")"
- 4 - "1,000 agents where possible" - Couldn't understand the sentence.
- 4 - "10 agents according to their distance" - What distance?
- 4.1.2 - "to solve a single MAPF problems"
- 4.1.2 - I couldn't understand the sentence that starts with "Based on the runtime of Oracle, we.."
- All the tables presented in the paper have bad references - "Table 4.2", "Table 4.3"..
- Footnote 5 - "5 minutes time" - "5-minute time"
- 4.3 - The regret described in the text is written with percentage (e.g., 9.0%) and without (e.g., 180). It should be more consistent.
- 4.4 - The sentence that starts with "A possible explanation for.." should be revised.
- 4.4 - "The poor performance.." - Seems like a repetition of above that sentence.
- 4.5 - "for each of the AS setup" - "for each of the AS setups"
- 4.5 - "that are important, and some features whose importance" - bad sentence
- 4.5 - "This suggests there" - "This suggests that there"
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Borderline accept: Technically solid paper where reasons to accept, e.g., novelty, outweigh reasons to reject, e.g., limited evaluation. Please use sparingly
15. (CONFIDENCE) How confident are you in your evaluation?
Quite confident. I tried to check the important points carefully. It is unlikely, though conceivable, that I missed some aspects that could otherwise have impacted my evaluation.
16. (EXPERTISE) How well does this paper align with your expertise?
Very Knowledgeable: This paper significantly overlaps with my current work and I am very knowledgeable about most of the topics covered by the paper

Reviewer #2
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This paper introduces a new method for Algorithm Selection for MAPF problem, using graph embedding. The introduced method can encode MAPF as a graph, to be able to use it in a graph embedding algorithm. Using these embeddings and combining with previously introduced features, the authors train a model for finding the fastest algorithm to solve a given MAPF instance. The method is evaluated on different types of benchmarks and with three different setups, and compared with some methods from related work, provided as baseline. The evaluations show improvements in comparison to the baseline.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Fair: The paper contributes some new ideas
3. {Soundness} Is the paper technically sound?
Good: The paper appears to be technically sound, but I have not carefully checked the details.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have moderate impact within a subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation could be improved.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Good: The experimental evaluation is adequate, and the results convincingly support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Fair: The shared resources are likely to be moderately useful to other AI researchers
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Fair: key resources (e.g., proofs, code, data) are unavailable but key details (e.g., proof sketches, experimental setup) are sufficiently well-described for an expert to confidently reproduce the main results
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above)
The paper introduces a new method for Algorithm Selection for MAPF and according to the experimental results, improvement in the performance compared to the existing methods is observed. The method incorporates some ideas from the existing methods to obtain better results and it is the first method that can practically use graph embeddings for this problem. Evaluation metrics are explained well and comparisons between the baseline and the presented method are explained clearly, together with an ablation study to identify the importance of different sets of features. Benchmarks contain different types of grids and behaviour of the introduced method for these different types are also shown in experimental results.
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
The biggest contribution seems to be the graph encoding method FG2V. It is used together with the existing method for graph encoding and other features that were defined in another related work, so I am a bit concerned about the significance and impact of the contribution. Since currently the codes are not provided, it can be hard to reproduce the results, but it can still be managed according to the explanations in the paper. Even though the organization of the paper is good, there seem to be some typos in the paper and a few sentences are unclear or some words in them are missing. This causes some problems about the clarity and makes the paper hard to read from time to time.
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
1) For the between-grid-type experiments, are the instances in the training set of a single type of grid or does the training set include different types of grids except the grid type of the test set? Would including a single type or multiple types of grids in the training set affect the results of between-grid-type experiments?
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
In the parts where "a given MAPF problem" is mentioned, I believe it would be more clear to say "a given MAPF instance". Since the MAPF problem has many variations, it created some confusion for me, especially in the beginning of the paper, about whether you mean a different problem or an instance of the classical MAPF problem. It could be better to differentiate between them throughout the paper.

There seem to be some minor typos in the paper and some of the sentences were unclear for me:
- In abstract "MAPF proboem" is written instead of "problem"
- In page 2, section 2.1 MAPF, third paragraph: "Makespac" instead of "Makespan"
- In page 2, section 3.1 Encoding MAPF as a Graph: In the first paragraph, the part "G2V encode Π a the subgraph of G" is a bit unclear.
- In page 2, section 3.1 Encoding MAPF as a Graph: The sentence "adds links if any between adjacent path’s nodes in original MAPF problem" was hard to understand
- In page 3, the first paragraph, "FG2V ncludes". There are some similar errors after abbreviations in the next sections too (mostly on pages 3-4-5). The first letter of the word after an abbreviation or the punctuation after an abbreviation is sometimes missing.
- In page 4, Table 2, "KBS" is written as "KGS" in all rows.
- In page 6, last words of the first sentence seem to be missing.
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Weak Accept: Technically solid, moderate to high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.
15. (CONFIDENCE) How confident are you in your evaluation?
Somewhat confident, but there's a chance I missed some aspects. I did not carefully check some of the details, e.g., novelty, proof of a theorem, experimental design, or statistical validity of conclusions.
16. (EXPERTISE) How well does this paper align with your expertise?
Knowledgeable: This paper has some overlap with my current work. My recent work was focused on closely related topics and I am knowledgeable about most of the topics covered by the paper.




Reviewer #3
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This work seeks to improve the performance of Algorithm Selection (AS) in the context of optimal MAPF solvers.
It is useful to refer to a portfolio of MAPF algorithms, each of which outperforms the others in some subset of problems from a range of common MAPF benchmarks.

The chief contribution is the application of a whole-graph embedding (FEATHER) to generate the features on which to train a classifier to predict the best algorithm to use for a given MAPF instance. The authors point out two further contributions: the concatenation of featuresets, and an empiricial study of the effectiveness of their approach compared to existing algorithm selection literature.

The new representation of MAPF problems in this work encodes the entire graph of the "grid" in which the agents are operating. The graph is augmented with edges representing the shortest route for each agent from its starting position to its goal position. This whole-graph approach differs from the G2V approach which only considered a subgraph comprised of the agents' shortest routes. This representation is then fed to the FEATHER algorithm which produces an embedding of size 500.

In an empirical study the authors test their approach on a number of public benchmarks and compare it to the results of two AS-for-MAPF papers - the papers which give rise to two different featuresets, namely G2V and KBS. The algorithm selection is done using xgboost. It is applied in three settings of increasing difficulty, where the training and testing is done: on identical grids, on the same type of grid (e.g. city, open, maze), or on unseen types of grid. The proposed approach (named MAG) is shown to be more effective in most settings. The authors also provide a breakdown of performance when the consituent featuresets are removed in turn.

Feature importance is also discussed briefly.

2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Fair: The paper contributes some new ideas
3. {Soundness} Is the paper technically sound?
Good: The paper appears to be technically sound, but I have not carefully checked the details.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have moderate impact within a subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Fair: The paper is somewhat clear, but some important details are missing or unclear.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Good: The experimental evaluation is adequate, and the results convincingly support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Not applicable: For instance, the primary contributions of the paper are theoretical.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Poor: key details (e.g., proof sketches, experimental setup) are incomplete/unclear, or key resources (e.g., proofs, code, data) are unavailable.
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above)
The key contribution of representing a MAPF as the entire graph with extra edges for agents' shortest path is novel as far as I can tell. The application of the FEATHER algorithm for graph embedding means that it is possible to train and predict on different graph structures.
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
1. The performance impact seems very limited. For example: by combining the two existing featuresets KBS and G2V, it possible to almost match the performance of the proposed MAG system - in the "between-grid" setting, the regret is 181.8% vs 180% for MAG or 176% for just using FG2V.

2. It is unclear how expensive the feature extraction and prediction apparatus is in relation to the solving time gained. Although the runtime in minutes is reported, it is not clear whether this runtime includes feature generation and prediction.

3. The empirical analysis could be more rigorous and described more clearly. As well as the timing objection raised above, it would be good to know details of how long the training took for the xgboost model, what hardware the experiments were carried out on etc. It's also difficult to see in the paper what the distribution of problem difficulty is, so it could be possible to get good accuracy by winning on lots of "easy" instances and fail on a few hard ones, but in reality these hard ones would be of more interest.

4. There seems to be no commitment to publish the code and/or experimental data - perhaps this would follow once the work was de-anonymised?
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
1. Is there any sense whether per-instance prediction is necessary - would choosing an algorithm per-class be sufficient? In other AS literature a Single-Best algorithm is sometimes used as a benchmark as well as the Oracle (or Virtual Best).

2. How long does the feature generation and prediction take? Does it pay for itself in terms of the MAPF solving time gained?

3. How long does it take to train the classifier?

4. It's hard to get a sense of how damaging a wrong choice is and therefore whether PAR1 is a sensible penalty - do you have any data on this, e.g. what's the distribution of worst-v-best choice regret - this might be a better basis for choosing your penalty, even if PAR1 is used in other literature.

5. How are the feature importances calculated and what exactly do they mean?

6. Whenever you use "average" in your experimental section, is it always the mean? Is this appropriate and justified?
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
Overall it is easy to follow the proposed approach and the motivation for it. The breadth of scenarios (grid types) studied seems comprehensive.

It is helpful that the authors have interpreted the results at length.

However, my major concern is that the empirical study need to be described much more precisely and that certain decisions need to be justified, especially when it comes to the experimental evaluation. I have given more detail in sections 11 (Reasons to Reject) and 12 (Questions for the Authors).


Minor
======

There are several typing errors or other language errors that should be picked up by a good proofreader. Here's just a sample, but there are many more:
- encode Π a the subgraph
- these graphs loose information
- FG2V ncludes
- embedding by firsts
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility, incompletely addressed ethical considerations.
15. (CONFIDENCE) How confident are you in your evaluation?
Quite confident. I tried to check the important points carefully. It is unlikely, though conceivable, that I missed some aspects that could otherwise have impacted my evaluation.
16. (EXPERTISE) How well does this paper align with your expertise?
Mostly Knowledgeable: This paper has little overlap with my current work. My past work was focused on related topics and I am knowledgeable or somewhat knowledgeable about most of the topics covered by the paper.
Go Back