%File: formatting-instructions-latex-2024.tex
%release 2024.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%\graphicspath{{/Figures/}}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{mathtools,amssymb,lipsum}
\usepackage{commath}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepackage{tikz}
\usepgfplotslibrary{groupplots}
\usetikzlibrary{patterns}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Domain Adaptation for Time series Transformers using One-step fine-tuning}
\author{
    %Authors
    % All authors must be in the same font size and format.
   Subina Khanal\textsuperscript{\rm 1},
   Seshu Tirupathi\textsuperscript{\rm 2},
   Giulio Zizzo\textsuperscript{\rm 2},
   Ambrish Rawat\textsuperscript{\rm 2},
   Torben Bach Pedersen\textsuperscript{\rm 1}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Department of Computer Science, Aalborg University\\
    \textsuperscript{\rm 2}IBM Research Europe\\
    subinak@cs.aau.dk, seshutir@ie.ibm.com, giulio.zizzo2@ibm.com, ambrish.rawat@ie.ibm.com,
    tbp@cs.aau.dk
}

\iffalse
\author{
    %Authors
    % All authors must be in the same font size and format.
    Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,\\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz\equalcontrib,
    Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    1900 Embarcadero Road, Suite 101\\
    Palo Alto, California 94303-3310 USA\\
    % email address must be in roman text type, not monospace or sans serif
    proceedings-questions@aaai.org
%
% See more examples next
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
The recent breakthrough of Transformers in deep learning has drawn significant attention of the time series community due to their ability to capture long-range dependencies. However, like other deep learning models, Transformers face limitations in time series prediction, including insufficient temporal understanding, generalization challenges, and data shift issues for the domains with limited data. Additionally, addressing the issue of catastrophic forgetting, where models forget previously learned information when exposed to new data, is another critical aspect that requires attention in enhancing the robustness of Transformers for time series tasks. To address these limitations, in this paper, we pre-train the time series Transformer model on a source domain with sufficient data and fine-tune it on the target domain with limited data. We introduce the {\em One-step fine-tuning} approach, adding some percentage of source domain data to the target domains, providing the model with diverse time series instances. We then fine-tune the pre-trained model using a gradual unfreezing technique. This helps enhance the model's performance in time series prediction for domains with limited data. Extensive experimental results on two real-world datasets show that our approach improves over the state-of-the-art baselines by 4.35\% and 11.54\% for indoor temperature and wind power prediction, respectively.

\end{abstract}

\section{Introduction}
Time series prediction has been a significant subject of academic study with applications in finance, weather, and climate change. Time series prediction approaches have evolved over the past few decades from classical statistical methodologies and machine learning (ML) techniques to deep learning-based solutions. Recently, the breakthrough of Transformers in deep learning has attracted much attention from the time series community owing to its outstanding performance in a variety of computer vision and natural language processing tasks \cite{vaswani2017attention, wen2022transformers}. Transformers have many benefits, but one that makes time series modeling particularly well suited for them is their capacity to capture long-range dependencies and interactions. This has resulted in significant advancements in a variety of time series applications \cite{ahmed2023transformers}. However, Transformers, like other deep learning models such as Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), or Autoencoders,  have several limitations on time series prediction. \emph{First}, \textbf{limited data availability:} deep learning models require a large amount of data for training, which is not always available for some time series prediction in real-world scenarios \cite{sun2017revisiting}. \emph{Second}, \textbf{lack of temporal understanding:} Transformers, in particular, may not accurately capture the temporal dynamics in time series because they are primarily intended for tasks not inherently involving temporal dependencies \cite{zeng2023transformers}. \emph{Third}, the \textbf{problem of data shift:} deep learning models assume that the training and test data are drawn from the same distribution. However, real-world scenarios often involve changes in data distribution, termed data shift. When this assumption is violated, the performance of the model degrades significantly \cite{farahani2021brief}. \emph{Fourth}, \textbf{lack of generalization:} deep learning models frequently have difficulty generalizing well to new, unseen data \cite{ying2019overview}. They may perform well on the training data but fail to accurately predict unseen data. This is the scenario where the unseen data is \textit{not independent and identically distributed (non-i.i.d)} w.r.t. the training data because of the data shift problem. Thus, the model becomes specialized to the training data and cannot perform well for all types of new data.

%This is the scenario of overfitting, where the model becomes specialized to the training data and cannot perform well for all types of new data.

\emph{Domain adaptation (DA)} is a technique used to address the above  mentioned challenges by improving the model's ability to generalize in scenarios with a different data distribution \cite{farahani2021brief, zhuang2020comprehensive}. DA leverages knowledge from the \emph{source domain}, where sufficient data is available, to the \emph{target domain}, where the data is limited, for making accurate predictions even when the two domains have different data distributions. However, models, like Transformers, are usually trained on fixed data, assuming the distribution of the test data remains constant. This is impractical for real-world applications, where data can evolve, and the model might need to adapt to new information without completely retraining from scratch \cite{kirkpatrick2017overcoming}. \emph{Continual learning} \cite{wang2023comprehensive} addresses this by allowing models to learn continuously from a data stream over time. The ML model can retain knowledge from previously learned tasks while learning and adapting continuously as new data becomes available. However, this causes catastrophic forgetting where performance on old tasks degrades over time as new tasks are added \cite{zizzo2022federated}. To address catastrophic forgetting in continual learning, replay methods \cite{bagus2021investigation} reintroduce past data during training that enables the model to balance learning from new tasks and retaining knowledge from earlier ones. It is also seen that catastrophic forgetting can occur during DA when a model is adapted to the target domain \cite{xu2020forget, saunders2022domain}. This is because of two reasons: \emph{First}, when performing DA, we train the prediction model on sufficient data from the source domain. Then, we leverage the knowledge of that model to the target domain with limited data to allow the model to generalize effectively on the target domain. However, during this adaptation process, the model may prioritize the data of the target domain and adjust its parameters in a way that causes it to forget the previously learned knowledge from the source domain. As a result, the performance of the model may degrade in the source domain, leading to generalization issues. \emph{Second}, there is a higher chance of forgetting if the data distributions for the source and target domains differ significantly.

This paper aims to adapt the Transformer model to the target domain while simultaneously addressing the data shift and catastrophic forgetting problems between source and target domains. We pre-train a time series Transformer model on large data of the source domain and fine-tune and adapt the pre-trained model on different target domains using our \emph{One-step fine-tuning} approach. Therein, we specifically involve portion of source domain data to the target domains and fine-tune the pre-trained model using a gradual unfreezing (GU) \cite{howard2018universal} technique, which allows us to adapt and scale the pre-trained Transformer model to different target domains.

In summary, the main contributions of the present paper are as follows.
(1) We propose a \emph{One-step fine-tuning} technique, where we add some percentage of source domain data to the target domains and fine-tune the pre-trained model. Thereby, we show the fine-tuned model is adaptable and performs well on new, unseen data from different target domains. (2) We mitigate the problems of data shift and catastrophic forgetting by fine-tuning the pre-trained time series Transformer model on different target domains with limited data. (3) We conduct an extensive experimental evaluation using real-world datasets, which shows that the our \emph{One-step fine-tuning} approach outperforms the state-of-the-art baselines. We obtain 4.35\% and 11.54\% improvements over the most competitive baseline for indoor temperature and wind power prediction, respectively.

\section{Related Work}
Several Transformer-based time series forecasting techniques have gained significant attention recently, particularly for long-term time series forecasting. The authors in \cite{zhou2021informer} study the long-sequence time series forecasting problem and aim to predict long sequences. The ProbSparse self-attention mechanism and the distilling operation are used to handle the challenges of quadratic time complexity and quadratic memory usage in the vanilla Transformer. Also, this method alleviates the limitation of the traditional encoder-decoder architecture by using a carefully designed generative decoder. Similarly, in \cite{wu2021autoformer}, the authors study the problem of long-term forecasting of time series. They propose a decomposition architecture by embedding the series decomposition block as an inner operator, which can progressively aggregate the long-term trend part from intermediate prediction. This approach also designs an Auto-Correlation mechanism to conduct dependencies discovery and information aggregation at the series level, which differs significantly from the previous self-attention family. Likewise, in \cite{nie2022time}, the authors introduce two key components: 1) patching; segmentation of time series into subseries-level patches, which are served as input tokens to the Transformer; and 2) a channel-independent structure, where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series for long-term multivariate time series forecasting and self-supervised representation learning.

\section{Preliminaries}
This section presents the core concepts of our \emph{One-step fine-tuning} approach for time series prediction that will be used throughout the paper.

\hspace{0.12in}\textbf{Time series:}
A \emph{univariate time series} $X = (x_{1}, x_{2},\ldots, x_n$), where $x_{t} \in \mathbb{R}$, is a sequence of real values that measure the same phenomenon and are chronologically ordered. Each value $x_t$ is recorded at uniformly spaced time instants $t\in \{1,2,\ldots\}$. A time series $X$ has length $n$ if it has $n$ collected samples.

\textbf{Time window of a time series:}
Let $X$ be a time series of length $n$. A time window $X_w = (x_{t-m+1},\ldots, x_{t-1}, x_t)$ consists of historical values of $X$ recorded at time $(t-m+1)$ up to time $t$. Here, $m$ is the \emph{memory}, denoting the size of the time window $X_w$ which defines how many historical values will be used for prediction.

\textbf{Source domain:}  A \emph{source domain} $\mathrm{SD}$ is a set of training data with a given distribution of the input data $P(X_{\mathrm{SD}})$ used to train a prediction model. Here, we consider a training dataset with enough historical data, often collected over more than a year, as the source domain.

\textbf{Target domain:} A \emph{target domain} $\mathrm{TD}$ is a set of data with a given  distribution of the input data $P(X_{\mathrm{TD}})$ that differs from the source domain distribution. Here, we consider target domains to have limited data.

\textbf{Learning task:} Both source and target domains have the same learning task $T$. Here, the learning task is to predict future time series using historical values.

\textbf{Objective of the Source task:} Let us consider $X_{\mathrm{SD}}$ to be the input of the model $\mathcal{M}$, that maps $X_{\mathrm{SD}}$ to the predicted output $\hat{Y}_{\mathrm{SD}}$ in the source domain $\mathrm{SD}$.
\begin{equation}
   \hat{Y}_{\mathrm{SD}} = \mathcal{M}(X_{\mathrm{SD}};\theta),
\end{equation}
where $\theta$ is the learnable parameters, i.e., weights of model $\mathcal{M}$. The objective during source task training is to minimize a source task loss function, denoted as $\mathcal{L}(\hat{Y}_{\mathrm{SD}}, Y_{\mathrm{SD}})$, where $Y_{\mathrm{SD}}$ is the actual output.
\begin{equation}
    \theta^* = \underset{\theta}{\arg\min}\frac{1}{N_{\mathrm{SD}}} \sum_{i=1}^{N_{\mathrm{SD}}} \mathcal{L}(\hat{Y}_{i}, Y_{i}),
\end{equation}
where $N_{\mathrm{SD}}$ is the total number of samples in source domain, $\hat{Y}_{i}$ is the predicted output for the $i^{\text{th}}$ sample, and ${Y}_{i}$ is the actual output for the $i^{\text{th}}$ sample in the source domain.

\textbf{Fine-tuning Objective on the Target task:} Once pre-training of the source task is complete, the knowledge gained by the source model is transferred to the target task. During fine-tuning, we define a target loss function, denoted as $\mathcal{L}(\hat{Y}, Y)$, where $Y$ is the actual output.
\begin{equation}
    \theta_{\text{ft}} = \underset{\theta}{\arg\min}\frac{1}{N} \sum_{i=1}^{N} \mathcal{L}(\hat{Y}_{i}, Y_{i}; \theta^*),
\end{equation}
where $N$ is the total number of samples in target domain, $\hat{Y}_{i}$ is the predicted output for the $i^{\text{th}}$ sample, and $Y_{i}$ is the actual output for the $i^{\text{th}}$ sample in the target domain. Here, the goal is to update the model parameters to minimize the target loss $\mathcal{L}(\hat{Y}, Y)$, thereby improving the performance of the model on the target task in the target domain.

\section{Proposed Approach}
In this section, we present the detailed deployment design of our \emph{One-step fine-tuning} approach. We build on a  time series Transformer model, which is a type of neural network architecture specifically designed to process and model sequential data \cite{wen2022transformers}. The detailed workflow of the model architecture is as follows:

\textbf{Training Dataset and Data Pre-processing:} Data collected from multiple residential buildings and wind turbines are used as the source and target training datasets. Before initiating the model training process, these training datasets are pre-processed to clean and format the data into input vectors.

\textbf{Positional Encoding:} The first layer of the model architecture is positional encoding, which is used to add positional information to the input sequence vectors. In Transformers, positional encoding is essential for informing the model about the relative positions of the data points in the sequence \cite{vaswani2017attention}. For the model to comprehend the temporal relationships between data points, positional encoding is required because the timing and order of observations are critical in time series. The positional encoding is added to the input embeddings, where each input embedding corresponds to a data point in the time series. The positional encoding vector is determined based on each data point's position (timestamp) in the sequence.

We follow the sinusoidal positional encoding technique, initially introduced in the Transformer architecture for natural language processing tasks \cite{vaswani2017attention} defined as:
\begin{equation}
\text{PE}_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i / d_{\text{model}}}} \right),
\end{equation}
\begin{equation}
\text{PE}_{(pos, 2i + 1)} = \cos\left(\frac{pos}{10000^{2i / d_{\text{model}}}} \right),
\end{equation}
where $pos$ is the position of time step in a sequence, $i$ is a index of the dimension, and $d_{\text{model}}$ is the embedding dimension.

\textbf{Encoder Layers:} The encoder layers are the second layer of the model architecture, responsible for processing the input sequence using self-attention mechanisms and feed-forward networks. These layers are stacked on each other to form the encoder \cite{zhou2021informer}. The initial sub-layer of an encoder is the Self-Attention Mechanism, which processes the input sequence vectors and allows the interaction between the values of input vectors. The mechanism learns to weigh the importance of each vector with respect to the others, capturing longer-range temporal dependencies and relationships in the time series. During training, the output of the self-attention part is added to the original inputs for gradient flow, and then layer normalization is applied. Each point in the sequence is individually processed by a feed-forward neural network, adding non-linearity to the encoder's transformations and enabling the learning of higher-level representations of the input data at each layer.

\textbf{Linear Decoder Layer:} The final layer is the linear decoder layer, a fully connected layer used to map the output of the encoder layer to the final prediction \cite{vaswani2017attention}. The input to a linear decoder layer consists of hidden representations, i.e., the output from the encoder layer. The fundamental operation of a linear decoder layer is a linear transformation represented by a fully connected (dense) neural network layer. The linear transformation projects the high-dimensional input representations into a lower-dimensional space, aligning the model's internal representation with the desired output space. Then, it is reshaped to match the target sequence length.

Next, with the model architecture defined, we discuss the two phases of the workflow in our \emph{One-step fine-tuning} approach.

\hspace{0.12in}\textbf{1) Pre-training time series Transformer model in source domain:} In the first phase, we train a time series Transformer model, i.e., the \emph{source model}, using the data from the source domain until it converges to a certain level of prediction accuracy. The trained source model efficiently learns the temporal dependencies and patterns from the large and diverse data of the source domain that we transfer to the target domains with limited data. As the learning task is the same between the source and target domains, we validate the patterns learned by the source model benefits the target domains for such scenarios.

\textbf{2) Fine-tuning on Target Domains:} The second phase is fine-tuning the pre-trained time series Transformer model, i.e., the source model, on the target domains, allowing it to adapt to the target data. This domain adaptation process helps the model perform better in the target domains, even with limited data. For this, we apply our proposed {\em One-step fine-tuning} of the source model in the target domains to obtain a \emph{fine-tuned target model}. This approach includes the following steps:

(2a) We first add some percentage of randomly sampled source domain data to the target domains. Including time series instances of the source domain in the target domains jointly addresses three fundamental issues: (i) the problem of data scarcity, (ii) data distribution mismatch, as data from both domains gets involved during fine tuning, and (iii) catastrophic forgetting, as the source model retains useful data representations through data sharing, enabling better adaptation to the target domain without completely forgetting the knowledge gained from the source domain.

(2b) We then apply gradual unfreezing (GU) technique \cite{howard2018universal}, where each layer of source model is frozen at first and then gradually unfreezed during each training epoch, to obtain a fine-tuned target model. This allows keeping the source model knowledge intact, while the newly added layers of the model are made trainable, and subsequently, tailoring the model for the target domain. As such, this helps stabilize the training process and minimize catastrophic forgetting of pre-trained representations. We consider both source and target domains to have same learning task; hence, new layers are not added to the source model during fine-tuning.

The execution of GU steps for Energy Data (see details on \textbf{Datasets}) is as follows: (2b.1) In the first 10 epochs, the top layers, i.e., the decoder layers (output layer), are unfrozen, (2b.2) From epochs 10 to 20, we gradually unfreeze the layers closer to the input, i.e., we progressively unfreeze the subsequent layers in the Transformer encoder layer and train the model, and finally, (2b.3) after epochs 20 to 35, we unfreeze all the remaining layers of the source model and train the model. Here, we track the training loss over several epochs and implement early stopping to avoid overfitting. If the training loss increases or plateaus, we gradually unfreeze the layers. We fine-tune the source model until it converges to a certain level of prediction accuracy.

\section{Experimental Evaluation}
In this section, we present the experimental evaluation of our \emph{One-step fine-tuning} approach for time series prediction in the source and target domains.

\subsection{Datasets}
We use the following real-world datasets to evaluate the performance of our \emph{One-step fine-tuning} approach:

\textbf{Energy Data:} We use two public energy datasets in our experiments. The first dataset comes from the New York State Energy Research and Development Authority (NYSERDA) \cite{NYSERDA}, which maintain data from residential buildings in New York State. These buildings are 100 to 600 square meters and have 50 geothermal heat pumps. The data includes observations on indoor temperature, outdoor temperature, and power consumption for about 12 months, with readings every 15 minutes.

The second dataset is collected from the Net-Zero Energy Residential Test Facility (NIST) \cite{nist}. This facility tests technologies for meeting residential energy needs with renewable energy. The data, collected over a year, simulates the energy usage of a family of four. The readings are taken every minute.

A 15-minute data granularity is selected to be used across all datasets, as it is usually used in electricity and flexibility markets. Thus, the NIST dataset is down-sampled by averaging power consumption readings to 15-minute intervals and taking the last indoor and outdoor temperature readings.

\textbf{MORE Data:} We use wind park data with 18 months of data from 11 wind turbines (WT2 to WT11) in a wind park provided by Engie as part of the MORE H2020 project \cite{more}. The dataset consists of SCADA data from the sensors on the wind turbines and weather data. Data included in this dataset are: \\
    - Power output: average, minimum, maximum, standard deviation over 10min. \\
    - Ambient temperature: average, minimum, maximum, standard deviation over 10min. \\
    - Wind speed: average, minimum, maximum, standard deviation over 10min.

There are over 840,000 samples from all the wind turbines with 1 hour data granularity. The target variable is minimum power output forecast for a 4 hour horizon at 1 hour intervals.
\begin{table}[t!]
\centering
\caption{Parameters used in source model fine-tuning.}
\label{tab:param}
\begin{tabular}{|l|cc|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Parameters}} & \multicolumn{2}{c|}{Values} \\ \cline{2-3}
\multicolumn{1}{|c|}{} & \multicolumn{1}{l|}{Energy Data} & \multicolumn{1}{l|}{MORE Data} \\ \hline
No. of Input features & \multicolumn{1}{c|}{1} & 18 \\ \hline
Target Variable & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Indoor\\ Temperature\end{tabular}} & \begin{tabular}[c]{@{}c@{}}Minimum Wind\\ Power\end{tabular} \\ \hline
Batch Size & \multicolumn{1}{c|}{8} & 24 \\ \hline
Training Epoch & \multicolumn{1}{c|}{35} & 20 \\ \hline
Historical values & \multicolumn{1}{l|}{96 (i.e. 24 hours)} & 24 (i.e. 24 hours) \\ \hline
Prediction horizon & \multicolumn{1}{l|}{4 (i.e. 1 hour)} & 4 (i.e. 4 hours) \\ \hline
Learning rate & \multicolumn{1}{c|}{0.001} & 0.00001 \\ \hline
Training Set: Test Set & \multicolumn{2}{c|}{70\%:30\%} \\ \hline
Optimizer & \multicolumn{2}{c|}{Adam} \\ \hline
Model Loss Function & \multicolumn{2}{c|}{Mean Absolute Error (MAE)} \\ \hline
\end{tabular}
\end{table}
\begin{table}[t!]
\caption{Prediction error of source model compared to the model baselines.}
\label{tab:ptm}
\centering
\begin{tabular}{|l|ll|ll|}
\hline
\multirow{3}{*}{Methods} & \multicolumn{2}{l|}{Energy Data} & \multicolumn{2}{l|}{MORE Data} \\ \cline{2-5}
 & \multicolumn{2}{l|}{\begin{tabular}[c]{@{}l@{}}Prediction Error \\ in S4\end{tabular}} & \multicolumn{2}{l|}{\begin{tabular}[c]{@{}l@{}}Prediction Error \\ in WT11\end{tabular}} \\ \cline{2-5}
 & \multicolumn{1}{l|}{RMSE} & MAE & \multicolumn{1}{l|}{RMSE} & MAE \\ \hline
\textbf{Informer} & \multicolumn{1}{l|}{0.420} & 0.293 & \multicolumn{1}{l|}{5.389} & 3.251 \\ \hline
\textbf{Autoformer} & \multicolumn{1}{l|}{0.402} & 0.307 & \multicolumn{1}{l|}{4.830} & 3.198 \\ \hline
\textbf{PatchTST} & \multicolumn{1}{l|}{0.383} & 0.296 & \multicolumn{1}{l|}{4.417} & 2.334 \\ \hline
\textbf{LSTM} & \multicolumn{1}{l|}{0.243} & 0.185 & \multicolumn{1}{l|}{0.136} & 0.109 \\ \hline
\textbf{Linear Regression} & \multicolumn{1}{l|}{0.264} & 0.208 & \multicolumn{1}{l|}{0.139} & 0.116 \\ \hline
\textbf{Source Model} & \multicolumn{1}{l|}{\textbf{0.236}} & \textbf{0.180} & \multicolumn{1}{l|}{\textbf{0.118}} & \textbf{0.075} \\ \hline
\end{tabular}
\end{table}
\input{sdavsp}
\input{baselines}
\input{eavsp}
\input{mavsp}
\input{cf}

\subsection{Implementation Details}
We use PyTorch framework for the model implementation and evaluation. The implementation starts by selecting the source and target domains as follows.

\textbf{Selection of source domain for pre-training:} The source domain should have adequate training data with seasonal variations. Therefore, for Energy data, out of $50$ geothermal heat pump sites from the NYSERDA dataset, we have selected deployment site \emph{S4} as the source domain. Similarly, for MORE data, out of $11$ wind turbines, we have selected \emph{WT11} as the source domain.

\textbf{Selection of target domains for fine-tuning:} The target domains should have the higher data dispersion compared to the selected source domain, which leads to better generalization capability of a model, as it can capture the specific patterns and characteristics unique to the target domain. To compute the marginal distribution disparity between the source and target domains, we adopt a widely used loss function, Maximum Mean Discrepancy (MMD) \cite{gretton2012optimal, khanal2023fda}. MMD measures the non-parametric distances between the source and target domains by converting them into a Reproducing Kernel Hilbert Space (RKHS), computed as
\begin{align}
\begin{split}
        {}\mathrm{MMD}[P(X_{\mathrm{SD}}),P(X_{\mathrm{TD}})] = \norm{\mathbb{E}[\varphi(X_{\mathrm{SD}})]- \mathbb{E}[\varphi(X_{\mathrm{TD}})]}_{\mathcal{H}}^2 \\
        = \norm{\frac{1}{N_{\mathrm{SD}}}\sum\nolimits_{p=1}^{N_{\mathrm{SD}}}\varphi(x_p) - \frac{1}{N}\sum\nolimits_{q=1}^{N}\varphi(x_q)}_{\mathcal{H}}^2,
        \label{mmd}
    \end{split}
\end{align}
where $P(X_{\mathrm{SD}})$ and $P(X_{\mathrm{TD}})$ are the marginal data distributions of the source and target domains, respectively, $N_{\mathrm{SD}}$ and $N$ are the number of data samples in the source and target domains, $\varphi(\cdot)$ is the mapping function from the original feature space to the RKHS, and $\mathcal{H}$ is the  RHKS space. The value of MMD starts from $0$, signifying that the compared domains are the same.

The target domains are selected based on MMD values to cover the distribution range of all the target domains. For Energy data, we used the Net-Zero dataset (NIST) and four different deployment sites (S5, S8, S15, and S49) from NYSERDA as the target domains to fine-tune the source model, with MMD values ranging from 0.089 to 0.133. Likewise, for MORE data, we used WT4, WT6, WT7, WT8, and WT10 as the target domains to fine-tune the source model, with MMD values ranging from 0.095 to 0.100.
\begin{table*}[t!]
\caption{Data Shift check on other target domains.}
\centering
\label{tab:ds}
\begin{tabular}{|c|c|cccccccc|llllllll}
\cline{1-10}
\multirow{10}{*}{Energy Data} & \multirow{3}{*}{Domains} & \multicolumn{8}{c|}{Prediction Error on Other Target Domains} &  &  &  &  &  &  &  &  \\ \cline{3-10}
 &  & \multicolumn{2}{c|}{S3} & \multicolumn{2}{c|}{S7} & \multicolumn{2}{c|}{S19} & \multicolumn{2}{c|}{S50} &  &  &  &  &  &  &  &  \\ \cline{3-10}
 &  & \multicolumn{1}{c|}{RMSE} & \multicolumn{1}{c|}{MAE} & \multicolumn{1}{c|}{RMSE} & \multicolumn{1}{c|}{MAE} & \multicolumn{1}{c|}{RMSE} & \multicolumn{1}{c|}{MAE} & \multicolumn{1}{c|}{RMSE} & MAE &  &  &  &  &  &  &  &  \\ \cline{2-10}
 & \begin{tabular}[c]{@{}c@{}}Source Domain (S4)\end{tabular} & \multicolumn{1}{c|}{0.378} & \multicolumn{1}{c|}{0.320} & \multicolumn{1}{c|}{1.09} & \multicolumn{1}{c|}{0.892} & \multicolumn{1}{c|}{0.320} & \multicolumn{1}{c|}{0.260} & \multicolumn{1}{c|}{0.594} & 0.495 &  &  &  &  &  &  &  &  \\ \cline{2-10}
 & \begin{tabular}[c]{@{}c@{}}Target Domains\end{tabular} & \multicolumn{8}{c|}{} &  &  &  &  &  &  &  &  \\ \cline{2-10}
  & S5 & \multicolumn{1}{c|}{0.222} & \multicolumn{1}{c|}{0.161} & \multicolumn{1}{c|}{0.345} & \multicolumn{1}{c|}{0.276} & \multicolumn{1}{c|}{0.249} & \multicolumn{1}{c|}{0.188} & \multicolumn{1}{c|}{0.383} & 0.294 \\ \cline{2-10}
 & S8 & \multicolumn{1}{c|}{0.264} & \multicolumn{1}{c|}{0.202} & \multicolumn{1}{c|}{0.403} & \multicolumn{1}{c|}{0.328} & \multicolumn{1}{c|}{0.227} & \multicolumn{1}{c|}{0.169} & \multicolumn{1}{c|}{0.397} & 0.302 \\ \cline{2-10}
 & S15 & \multicolumn{1}{c|}{0.221} & \multicolumn{1}{c|}{0.165} & \multicolumn{1}{c|}{0.334} & \multicolumn{1}{c|}{0.259} & \multicolumn{1}{c|}{0.251} & \multicolumn{1}{c|}{0.186} & \multicolumn{1}{c|}{0.365} & 0.279 \\ \cline{2-10}
 & S49 & \multicolumn{1}{c|}{0.247} & \multicolumn{1}{c|}{0.197} & \multicolumn{1}{c|}{0.349} & \multicolumn{1}{c|}{0.265} & \multicolumn{1}{c|}{0.233} & \multicolumn{1}{c|}{0.178} & \multicolumn{1}{c|}{0.442} & 0.339 \\ \cline{2-10}
 & NIST & \multicolumn{1}{c|}{0.315} & \multicolumn{1}{c|}{0.239} & \multicolumn{1}{c|}{0.376} & \multicolumn{1}{c|}{0.282} & \multicolumn{1}{c|}{0.232} & \multicolumn{1}{c|}{0.171} & \multicolumn{1}{c|}{0.409} & 0.321 &  &  &  &  &  &  &  &  \\ \cline{1-10}
\multirow{10}{*}{MORE Data} & \multirow{3}{*}{Domains} & \multicolumn{8}{c|}{Prediction Error on Other Target Domains} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} &  &  &  &  &  &  \\ \cline{3-10}
 &  & \multicolumn{2}{c|}{WT2} & \multicolumn{2}{c|}{WT3} & \multicolumn{2}{c|}{WT5} & \multicolumn{2}{c|}{WT9} & \multicolumn{2}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{} & \multicolumn{1}{c}{} &  &  \\ \cline{3-10}
 &  & \multicolumn{1}{c|}{RMSE} & \multicolumn{1}{c|}{MAE} & \multicolumn{1}{c|}{RMSE} & \multicolumn{1}{c|}{MAE} & \multicolumn{1}{c|}{RMSE} & \multicolumn{1}{c|}{MAE} & \multicolumn{1}{c|}{RMSE} & MAE & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} &  &  &  &  &  &  \\ \cline{2-10}
 & \begin{tabular}[c]{@{}c@{}}Source Domain (WT11)\end{tabular} & \multicolumn{1}{c|}{0.133} & \multicolumn{1}{c|}{0.096} & \multicolumn{1}{c|}{0.160} & \multicolumn{1}{c|}{0.118} & \multicolumn{1}{c|}{0.151} & \multicolumn{1}{c|}{0.113} & \multicolumn{1}{c|}{0.152} & 0.112 & \multicolumn{1}{c}{} &  &  &  &  &  &  &  \\ \cline{2-10}
 & \begin{tabular}[c]{@{}c@{}}Target Domains\end{tabular} & \multicolumn{8}{c|}{} &  &  &  &  &  &  &  &  \\ \cline{2-10}
 & WT4 & \multicolumn{1}{c|}{0.121} & \multicolumn{1}{c|}{0.080} & \multicolumn{1}{c|}{0.132} & \multicolumn{1}{c|}{0.087} & \multicolumn{1}{c|}{0.115} & \multicolumn{1}{c|}{0.076} & \multicolumn{1}{c|}{0.121} & 0.078 & \multicolumn{1}{c}{} &  &  &  &  &  &  &  \\ \cline{2-10}
 & WT6 & \multicolumn{1}{c|}{0.120} & \multicolumn{1}{c|}{0.080} & \multicolumn{1}{c|}{0.132} & \multicolumn{1}{c|}{0.085} & \multicolumn{1}{c|}{0.114} & \multicolumn{1}{c|}{0.075} & \multicolumn{1}{c|}{0.120} & 0.078 & \multicolumn{1}{c}{} &  &  &  &  &  &  &  \\ \cline{2-10}
 & WT7 & \multicolumn{1}{c|}{0.118} & \multicolumn{1}{c|}{0.078} & \multicolumn{1}{c|}{0.128} & \multicolumn{1}{c|}{0.084} & \multicolumn{1}{c|}{0.116} & \multicolumn{1}{c|}{0.077} & \multicolumn{1}{c|}{0.119} & 0.077 & \multicolumn{1}{c}{} &  &  &  &  &  &  &  \\ \cline{2-10}
 & WT8 & \multicolumn{1}{c|}{0.124} & \multicolumn{1}{c|}{0.081} & \multicolumn{1}{c|}{0.138} & \multicolumn{1}{c|}{0.089} & \multicolumn{1}{c|}{0.119} & \multicolumn{1}{c|}{0.076} & \multicolumn{1}{c|}{0.126} & 0.081 & \multicolumn{1}{c}{} &  &  &  &  &  &  &  \\ \cline{2-10}
 & WT10 & \multicolumn{1}{c|}{0.131} & \multicolumn{1}{c|}{0.087} & \multicolumn{1}{c|}{0.130} & \multicolumn{1}{c|}{0.086} & \multicolumn{1}{c|}{0.118} & \multicolumn{1}{c|}{0.077} & \multicolumn{1}{c|}{0.122} & 0.078 & \multicolumn{1}{c}{} &  &  &  &  &  &  &  \\ \cline{1-10}
\end{tabular}
\end{table*}

\textbf{Source model fine-tuning parameters:} Table~\ref{tab:param} shows the summary of parameters used during source model fine-tuning in the selected target domains for Energy and MORE data. The hyper-parameters are obtained through grid search.

\subsection{Baselines}
\hspace{0.12in}\textbf{1. Model Baselines:} We have selected five different model baselines to evaluate our source model; including \textbf{Informer} \cite{zhou2021informer}, \textbf{Autoformer} \cite{wu2021autoformer}, \textbf{PatchTST }\cite{nie2022time}, \textbf{LSTM }\cite{staudemeyer2019understanding}, and \textbf{Linear Regression} \cite{liu2021forecast,james2023linear}.

\textbf{2. Fine-tuning Baselines:} We use the following fine-tuning baselines to evaluate our \emph{One-step fine-tuning} approach. \textbf{Gradual unfreezing (GU)} \cite{howard2018universal}: This technique gradually unfreezes the layers during fine-tuning rather than unfreezing all the layers at once, without including data from the source domain.
\textbf{Elastic Weight Consolidation (EWC)} \cite{barone2017regularization, kirkpatrick2017overcoming}: EWC is a regularization technique used to mitigate catastrophic forgetting in neural networks, particularly in the context of continual learning. This technique adds penalty terms or constraints to the model training process. It modifies the model loss/objective function by adding a regularization term.

\textbf{3. Model Training Baselines:} We also compare the performance of our \emph{One-step fine-tuning} approach with other model training baselines as follows. \textbf{Exclusive Training:} Here, we train the time series Transformer model individually in different target domains and evaluate the individual target models on a test set of the target domain data. \textbf{Before fine-tuning:} Here, we directly assess the source model in the target domains without fine-tuning.

\subsection{Time Series Prediction Evaluation}
\hspace{0.12in}\textbf{Evaluation Metrics:}
We use well-known metrics, Root Mean Square Error (RMSE) and Mean Absolute Error (MAE), for the quantitative assessment of source and final global model performance on test target domains. These metrics are calculated as
\begin{equation}
    \mathit{RMSE}(Y_{i}, \hat{Y}_{i}) = \sqrt{\frac{1}{N}\sum_{i=1}^{N}(Y_{i} - \hat{Y}_{i})^2},
\end{equation}
\begin{equation}
   \mathit{MAE}(Y_{i}, \hat{Y}_{i}) = \frac{1}{N}\sum_{i=1}^{N}\abs{(Y_{i} - \hat{Y}_{i})},
\end{equation}
where $Y_{i}$ and $\hat{Y}_{i}$ are the actual and predicted output, and $N$ is the total number of samples in the target domains.

\textbf{Prediction Accuracy Evaluation:} Table~\ref{tab:ptm} shows the quantitative performance of the source model evaluated on the source domains S4 and WT11 based on RMSE and MAE metrics respectively. Here, we observe a significant improvement in the source model's performance against the baselines for S4 and WT11, with up to 44\% and 97.8\% RMSE error reduction, respectively. Likewise, in Fig~\ref{fig:avsp_sd}, we have the prediction performance of the source model on the source domains S4 and WT11, respectively. We can see that the source model obtains improved prediction of the indoor temperature and wind power for both source domains.

Next, Fig~\ref{fig:ftm} shows the performance of the fine-tuned target model. Our \emph{One-step fine-tuning} approach performs better than the baselines \textbf{GU} and \textbf{EWC} on all target domains with limited target data, with a significant improvement of up to 48.5\% after adding a portion of source domain data prior to fine-tuning the source model. For Energy data, we add 5\% (2731 samples) of source domain (S4) data on the data of S5, S49 and NIST as the MMD values between these target domains with S4 are higher. We add 20\% (10927 samples) of S4 data on S8 and S15 as the MMD values between these target domains with S4 are lower (see details on \textbf{Appendix-Ablation Study}). Similarly, for MORE data, we add 5\% (6796 samples) of source domain (WT11) data on the data of all the target domains. In Fig~\ref{fig:eba}, we show the comparison between different model training baselines: \textbf{Exclusive Training} and \textbf{Before fine-tuning}, with our \emph{One-step fine-tuning} approach. Here, we observe that the fine-tuned target model performs up to 52.6\% better than the baselines. This is because the fine-tuned target model can generalize well to new, unseen data through the learned general features and patterns from the source model, mitigating the problem of data shift. The baselines do not consider the data shift problem and perform well only if the test data follows the distribution of the training data.

In Fig~\ref{fig:energy_avsp_td} and Fig~\ref{fig:more_avsp_td}, we illustrate the prediction performance of the fine-tuned target models on the target domains NIST and WT8, respectively. Here, we compare our \emph{One-step fine-tuning} approach with other fine-tuning baselines, where we can see that the fine-tuned target models obtains improved prediction of the indoor temperature and wind power for both target domains as compared to the baselines.

In Table~\ref{tab:ds}, we show evaluation results for our \emph{One-step fine-tuning} approach under data shift scenario. This is done by directly applying both the pre-trained (source) and fine-tuned target models on other new target domains, i.e., without exclusive training and fine-tuning, respectively. We observe that the performance of the fine-tuned target model is better than the source model for all the other target domains, even though having different MMD values (see Table~\ref{tab:mmd} and Table~\ref{tab:mmd_more} in \textbf{Appendix}) that indicates data distribution scenario. This validates our claim to efficiently handle data shift problem; the fine-tuned target model performs better on unseen data by improving generalization, leading to better domain adaptation.

Next, in Fig~\ref{fig:cf}, we show the performance of the fine-tuned target models directly evaluated on the source domains S4 and WT11 for catastrophic forgetting check. Here, we observe the fine-tuned target models using our \emph{One-step fine-tuning} approach outperforms on both S4 and WT11 than the fine-tuning baselines \textbf{GU} and \textbf{EWC}. The \emph{One-step fine-tuning} method achieves RMSE that is better or closer to the performance of the source model on S4, i.e., $0.236$. We obtain similar results for WT11, where the RMSE was $0.118$. This shows the fine-tuned model retains its knowledge learned during pre-training while adapting better to the new target domain data; hence, mitigating the problem of catastrophic forgetting by adding some percentage of data samples of S4 and WT11 to the respective target domains during model training.

\section{Conclusion}
Transformers improve time series prediction by effectively capturing long-term patterns and dependencies, improving the model's capability to understand and predict complex temporal relationships. However, their performance suffers severely due to limitations like insufficient temporal understanding, generalization challenges, data shift issues, and the critical concern of catastrophic forgetting. Therefore, in this paper, we propose a new \emph{One-step fine-tuning} approach where we pre-train the time series Transformer model on a source domain with sufficient data and fine-tune it on the target domain with limited data. Our approach builds on a gradual unfreezing technique, however, with addition of source domain data in the target domains to fine-tune the pre-trained model. This helps enhance the model's performance in time series prediction for domains with limited data. In the future, we will explore privacy aspects of our \emph{One-step fine-tuning} when sharing source domain data.

\section{Acknowledgments}
This work has been partially supported by the MORE project (grant agreement 957345), funded by the EU Horizon 2020 program.

\clearpage
\bibliography{aaai24}

\section{Appendix}
\begin{table*}[t!]
\caption{Overview of dataset used during pre-training and \emph{One-step fine-tuning}.}
\centering
\label{tab:dataset}
\begin{tabular}{|ccc|ccc|}
\hline
\multicolumn{3}{|c|}{Energy Data} & \multicolumn{3}{c|}{MORE Data} \\ \hline
\multicolumn{1}{|c|}{Domain} & \multicolumn{1}{c|}{Training Data} & Test Data & \multicolumn{1}{c|}{Domain} & \multicolumn{1}{c|}{Training Data} & Test Data \\ \hline
\multicolumn{1}{|c|}{Source Domain (S4)} & \multicolumn{1}{c|}{38,245} & 16,390 & \multicolumn{1}{c|}{Source Domain (WT11)} & \multicolumn{1}{c|}{95,148} & 40,777 \\ \hline
\multicolumn{1}{|c|}{S5} & \multicolumn{1}{c|}{4097} & 1755 & \multicolumn{1}{c|}{WT4} & \multicolumn{1}{c|}{10,850} & 4650 \\ \hline
\multicolumn{1}{|c|}{S8} & \multicolumn{1}{c|}{10,070} & 4316 & \multicolumn{1}{c|}{WT6} & \multicolumn{1}{c|}{10,794} & 2587 \\ \hline
\multicolumn{1}{|c|}{S15} & \multicolumn{1}{c|}{8830} & 3784 & \multicolumn{1}{c|}{WT7} & \multicolumn{1}{c|}{9994} & 2569 \\ \hline
\multicolumn{1}{|c|}{S49} & \multicolumn{1}{c|}{5700} & 2442 & \multicolumn{1}{c|}{WT8} & \multicolumn{1}{c|}{8128} & 2626 \\ \hline
\multicolumn{1}{|c|}{NIST} & \multicolumn{1}{c|}{5439} & 2331 & \multicolumn{1}{c|}{WT9} & \multicolumn{1}{c|}{9014} & 2577 \\ \hline
\end{tabular}
\end{table*}
\begin{table}[t!]
\caption{Parameters used in source model pre-training.}
\centering
\label{tab:param_source}
\begin{tabular}{|c|cc|}
\hline
\multirow{2}{*}{Parameters} & \multicolumn{2}{c|}{Values} \\ \cline{2-3}
 & \multicolumn{1}{c|}{Energy Data} & MORE Data \\ \hline
No. of Input features & \multicolumn{1}{c|}{1} & 18 \\ \hline
Target Variable & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Indoor\\ Temperature\end{tabular}} & \begin{tabular}[c]{@{}c@{}}Minimum Wind\\  Power\end{tabular} \\ \hline
Batch Size & \multicolumn{1}{c|}{8} & 64 \\ \hline
Training Epoch & \multicolumn{1}{c|}{50} & 100 \\ \hline
Historical values & \multicolumn{1}{c|}{96 (i.e. 24 hours)} & 24 (i.e. 24 hours) \\ \hline
Prediction horizon & \multicolumn{1}{c|}{4 (i.e. 1 hour)} & 4 (i.e. 4 hours) \\ \hline
Learning rate & \multicolumn{1}{c|}{0.001} & 0.0001 \\ \hline
Training Set: Test Set & \multicolumn{2}{c|}{70\%:30\%} \\ \hline
Optimizer & \multicolumn{2}{c|}{Adam} \\ \hline
Model Loss Function & \multicolumn{2}{c|}{Mean Absolute Error (MAE)} \\ \hline
\end{tabular}
\end{table}
\input{vp}
\input{ft}
\begin{table}[t!]
\caption{Calculation of MMD values between source and target domains for Energy data.}
\centering
\label{tab:mmd1}
\begin{tabular}{|c|c|c|}
\hline
Source Domain & Target Domain & MMD values \\ \hline
\multirow{5}{*}{S4} & S5 & 0.114 \\ \cline{2-3}
 & S8 & 0.097 \\ \cline{2-3}
 & S15 & 0.089 \\ \cline{2-3}
 & S49 & 0.104 \\ \cline{2-3}
 & NIST & 0.133 \\ \hline
\end{tabular}
\end{table}
\begin{table*}[t!]
\caption{Calculation of MMD values between source and target domains for Energy data.}
\centering
\label{tab:mmd}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Source Domain       & Other Target Domains & MMD values & Target Domain         & Other Target Domains & MMD values \\ \hline
\multirow{4}{*}{S4} & S3                   & 0.127      & \multirow{4}{*}{S15}  & S3                   & 0.058      \\ \cline{2-3} \cline{5-6}
                    & S7                   & 0.153      &                       & S7                   & 0.157      \\ \cline{2-3} \cline{5-6}
                    & S19                  & 0.131      &                       & S19                  & 0.107      \\ \cline{2-3} \cline{5-6}
                    & S50                  & 0.167        &                       & S50                  & 0.100      \\ \hline
Target Domain       & Other Target Domains & MMD values & Target Domain         & Other Target Domains & MMD values \\ \hline
\multirow{4}{*}{S5} & S3                   & 0.110      & \multirow{4}{*}{S49}  & S3                   & 0.117      \\ \cline{2-3} \cline{5-6}
                    & S7                   & 0.164      &                       & S7                   & 0.216      \\ \cline{2-3} \cline{5-6}
                    & S19                  & 0.125      &                       & S19                  & 0.170      \\ \cline{2-3} \cline{5-6}
                    & S50                  & 0.150      &                       & S50                  & 0.158      \\ \hline
Target Domain       & Other Target Domains & MMD values & Target Domain         & Other Target Domains & MMD values \\ \hline
\multirow{4}{*}{S8} & S3                   & 0.230      & \multirow{4}{*}{NIST} & S3                   & 0.069      \\ \cline{2-3} \cline{5-6}
                    & S7                   & 0.083      &                       & S7                   & 0.170      \\ \cline{2-3} \cline{5-6}
                    & S19                  & 0.088      &                       & S19                  & 0.123      \\ \cline{2-3} \cline{5-6}
                    & S50                  & 0.240      &                       & S50                  & 0.110      \\ \hline
\end{tabular}
\end{table*}
\begin{table*}[t!]
\caption{Calculation of MMD values between source and target domains for MORE data.}
\centering
\label{tab:mmd_more}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Source Domain & Other Target Domains & MMD values & Target Domain & Other Target Domains & MMD values \\ \hline
\multirow{4}{*}{WT11} & WT2 & 0.079 & \multirow{4}{*}{WT4} & WT2 & 0.077 \\ \cline{2-3} \cline{5-6}
 & WT3 & 0.084 &  & WT3 & 0.084 \\ \cline{2-3} \cline{5-6}
 & WT5 & 0.095 &  & WT5 & 0.092 \\ \cline{2-3} \cline{5-6}
 & WT9 & 0.095 &  & WT9 & 0.093 \\ \hline
Target Domain & Other Target Domains & MMD values & Target Domain & Other Target Domains & MMD values \\ \hline
\multirow{4}{*}{WT6} & WT2 & 0.080 & \multirow{4}{*}{WT7} & WT2 & 0.079 \\ \cline{2-3} \cline{5-6}
 & WT3 & 0.081 &  & WT3 & 0.093 \\ \cline{2-3} \cline{5-6}
 & WT5 & 0.095 &  & WT5 & 0.097 \\ \cline{2-3} \cline{5-6}
 & WT9 & 0.100 &  & WT9 & 0.095 \\ \hline
Target Domain & Other Target Domains & MMD values & Target Domain & Other Target Domains & MMD values \\ \hline
\multirow{4}{*}{WT8} & WT2 & 0.085 & \multirow{4}{*}{WT9} & WT2 & 0.080 \\ \cline{2-3} \cline{5-6}
 & WT3 & 0.085 &  & WT3 & 0.083 \\ \cline{2-3} \cline{5-6}
 & WT5 & 0.100 &  & WT5 & 0.098 \\ \cline{2-3} \cline{5-6}
 & WT9 & 0.099 &  & WT9 & 0.100 \\ \hline
\end{tabular}
\end{table*}
\subsection{Dataset Overview}
This section provides an overview of the training and evaluation datasets used during the pre-training and fine-tuning phases for Energy and MORE data. Table~\ref{tab:dataset} shows the amount of data used for training and testing the source model and fine-tuned target models. Here, we can observe that the source domains S4 and WT11 have sufficient model training data compared to the target domains, even after we add the certain amount of data from source domain to the target domains.

\subsection{Hyper-parameters used during pre-training}
Table~\ref{tab:param_source} shows the summary of parameters used during pre-training the source model in the selected source domains S4 and WT11 for Energy and MORE data, respectively. The hyper-parameters are obtained through grid search.

\subsection{Ablation Study}\label{ablation}
We also performed additional experiments on Energy data for an ablation study of our design decisions. In Fig~\ref{fig:varyingpercent}, we show how adding a certain amount of data samples from the source domain affects the performance of target domains during source model fine-tuning. As can be seen, for the target domains S5, S49, and NIST, adding 5\% of data from the source domain performs better compared to other percentages of added data, as the MMD values between these target domains with S4 are higher (see Table~\ref{tab:mmd1}). Likewise, for the target domains S8 and S15, adding 20\% of data from the source domain works best compared to other percentages of added data, as the MMD values between these target domains with S4 are lower (see Table~\ref{tab:mmd1}).

Next, in Fig~\ref{fig:freezing_tech}, we experimented with various techniques for freezing pre-trained layers. We compared our \emph{One-step fine-tuning} approach with two different methods: \textbf{Top layer unfreeze} and \textbf{w/o GU}. For the \textbf{Top Layer Unfreeze approach}, we only unfreeze the top layer- specifically, the decoder layer of the source model. We then fine-tune the model using the GU technique. In the \textbf{w/o GU approach}, we unfreeze all the layers at once at the initial stage of fine-tuning rather than gradually unfreezing the source model layers. Our \emph{One-step fine-tuning} approach, where we gradually unfreeze all the pre-trained layers after adding some percentage of source domain data to the target domain, performs better than the other two approaches. Here, we can see an improvement of up to 40\%.

\subsection{Distribution Alignment Analysis}
In this section, we delve into a thorough analysis of the alignment between the data distributions of the source and different target domains. Table~\ref{tab:mmd1}, Table~\ref{tab:mmd}, and Table~\ref{tab:mmd_more} present the MMD values, which serve as a crucial metric for quantifying the dissimilarity or similarity between these domains.

Table~\ref{tab:mmd1} shows the calculation of MMD values between the source and target domains for Energy data. These target domains were selected for source model fine-tuning. Here, the varying MMD values among the target domains, ranging from 0.089 to 0.133, show the discrepancies in the alignment between the source and individual target domains. Notably, the lower MMD values, such as 0.089, indicate a more effective alignment, indicating a closer resemblance in the feature distributions between the source and those specific target domains. This potentially signifies more robust adaptability of the model to those domains. On the other hand, higher MMD values, like 0.133, imply a more significant dissimilarity, highlighting challenges in aligning certain target domains with the source.

Next, Table~\ref{tab:mmd} and Table ~\ref{tab:mmd_more} shows the calculation of MMD values between the source and various target domains for Energy and MORE data, respectively. Here, the other target domains were not involved during source model fine-tuning. The tables not only provide the perspective of the alignment between the source and individual target domains but also of the disparities among the various target domains themselves. These MMD values help provide insight into examining data shifts between the source and different target domains in the domain adaptation process. This analysis allows us to see how well the domain adaptation strategy works and how the fine-tuned target model works well across different domains.

\end{document}
