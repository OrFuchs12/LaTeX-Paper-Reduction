\relax 
\bibstyle{aaai24}
\citation{lee2018simple,tack2020csi,2021ssd,sun2022knn}
\citation{sun2022knn}
\citation{beyer1999nearest}
\citation{beyer1999nearest}
\citation{beyer1999nearest,hinneburg2000nearest,aggarwal2001surprising,houle2010can,kriegel2009outlier}
\citation{houle2010can}
\citation{yu2015lsun}
\citation{sun2022knn}
\newlabel{sec:intro}{{1}{1}{}{}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}}
\citation{sun2022knn}
\citation{2021ssd,sun2022knn,lee2018simple}
\newlabel{sec:background}{{2}{2}{}{}{}}
\newlabel{sec:background@cref}{{[section][2][]2}{[1][2][]2}}
\newlabel{equ:ood_score}{{2}{2}{}{}{}}
\newlabel{equ:ood_score@cref}{{[equation][2][]2}{[1][2][]2}}
\newlabel{sec:method}{{3}{2}{}{}{}}
\newlabel{sec:method@cref}{{[section][3][]3}{[1][2][]2}}
\newlabel{sec:train}{{3.1}{2}{}{}{}}
\newlabel{sec:train@cref}{{[subsection][1][3]3.1}{[1][2][]2}}
\newlabel{eq:orig}{{4}{2}{}{}{}}
\newlabel{eq:orig@cref}{{[equation][4][]4}{[1][2][]2}}
\citation{sun2022knn}
\citation{zhao2022analysis}
\newlabel{eq:relevance}{{6}{3}{}{}{}}
\newlabel{eq:relevance@cref}{{[equation][6][]6}{[1][3][]3}}
\newlabel{sec:test}{{3.2}{3}{}{}{}}
\newlabel{sec:test@cref}{{[subsection][2][3]3.2}{[1][3][]3}}
\newlabel{sec:theory}{{4}{3}{}{}{}}
\newlabel{sec:theory@cref}{{[section][4][]4}{[1][3][]3}}
\newlabel{th:main}{{4.1}{3}{}{}{}}
\newlabel{th:main@cref}{{[theorem][1][4]4.1}{[1][3][]3}}
\citation{hendrycks2016baseline}
\citation{liang2018enhancing}
\citation{hsu2020generalized}
\citation{liu2020energy}
\citation{sun2021react}
\citation{huang2021importance}
\citation{wei2022mitigating}
\citation{sun2022dice}
\citation{lee2018simple}
\citation{sun2022knn}
\citation{hendrycks2016baseline}
\citation{liang2018enhancing}
\citation{hsu2020generalized}
\citation{liu2020energy}
\citation{sun2021react}
\citation{huang2021importance}
\citation{wei2022mitigating}
\citation{sun2022dice}
\citation{lee2018simple}
\citation{sun2022knn}
\citation{krizhevsky2009learning}
\citation{cimpoi2014describing}
\citation{svhn}
\citation{yu2015lsun}
\citation{yu2015lsun}
\citation{xu2015turkergaze}
\citation{zhou2017places}
\citation{sun2021react}
\citation{sun2021react}
\citation{sun2022dice}
\citation{lee2018simple}
\citation{lee2018simple}
\citation{lee2018simple}
\citation{lee2018simple}
\citation{sun2022knn}
\citation{lee2018simple}
\citation{lee2018simple}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:hard_ood}{{1}{4}{\relax \fontsize  {9}{10}\selectfont  Performance comparison on near-OOD and far-OOD detection task. Architecture used is DenseNet-101 and ID data is CIFAR-10. We report the mean and variance across 3 training runs.}{}{}}
\newlabel{tab:hard_ood@cref}{{[table][1][]1}{[1][4][]4}}
\newlabel{tab:cifar-100}{{2}{4}{\relax \fontsize  {9}{10}\selectfont  Performance comparison on CIFAR-100 dataset. We use DenseNet-101 for all baselines. Best results are in \textbf  {bold}. We report the mean and variance across 3 different training runs.}{}{}}
\newlabel{tab:cifar-100@cref}{{[table][2][]2}{[1][4][]4}}
\newlabel{sec:experiment}{{5}{4}{}{}{}}
\newlabel{sec:experiment@cref}{{[section][5][]5}{[1][4][]4}}
\newlabel{subsec:common_benchmark}{{5.1}{4}{}{}{}}
\newlabel{subsec:common_benchmark@cref}{{[subsection][1][5]5.1}{[1][4][]4}}
\citation{huang2018densely}
\citation{sun2022knn}
\citation{ho1998nearest}
\citation{ho1998nearest}
\newlabel{tab:compatibility}{{3}{5}{\relax \fontsize  {9}{10}\selectfont  $\mathcal  {S}$NN\xspace  is also compatible with parametric approaches such as Mahalanobis distance\nobreakspace  {}\citep  {lee2018simple}. The model is DenseNet. All values are averaged over six OOD test datasets.}{}{}}
\newlabel{tab:compatibility@cref}{{[table][3][]3}{[1][4][]5}}
\newlabel{tab:train_time}{{4}{5}{\relax \fontsize  {9}{10}\selectfont  \textbf  {Computational cost for training}. trained using ResNet-101. Model used is DenseNet-101. For the comparison, we used the software configuration as reported in Appendix C.2.}{}{}}
\newlabel{tab:train_time@cref}{{[table][4][]4}{[1][5][]5}}
\newlabel{sec:ablations}{{6}{5}{}{}{}}
\newlabel{sec:ablations@cref}{{[section][6][]6}{[1][5][]5}}
\newlabel{tab:subspace-selection}{{5}{5}{\relax \fontsize  {9}{10}\selectfont  Ablation on subspace selection methods. Best performing results are marked in \textbf  {bold}. Model is DenseNet. All values are averaged over multiple OOD test datasets.}{}{}}
\newlabel{tab:subspace-selection@cref}{{[table][5][]5}{[1][5][]5}}
\newlabel{eq:least_relevance}{{11}{5}{}{}{}}
\newlabel{eq:least_relevance@cref}{{[equation][11][]11}{[1][5][]5}}
\citation{deng2009imagenet}
\citation{huang2021mos}
\citation{sun2022dice}
\citation{sun2022dice}
\newlabel{fig:ablations}{{1}{6}{\relax \fontsize  {9}{10}\selectfont  \textbf  {Left}: Effect of varying the relevance ratio ($r$) on OOD detection performance when $k=20$. \textbf  {Right}: Effect of varying the number of neighbors ($k$) when $r=0.25$. }{}{}}
\newlabel{fig:ablations@cref}{{[figure][1][]1}{[1][5][]6}}
\newlabel{fig:subspace}{{2}{6}{ \relax \fontsize  {9}{10}\selectfont  Visualization of learned final-layer weight matrix for $r \in \{1, 0.75, 0.25, 0.05\}$. For each $r$, we visualize a $342$-dimensional weight vector corresponding to each class in CIFAR-100. }{}{}}
\newlabel{fig:subspace@cref}{{[figure][2][]2}{[1][5][]6}}
\newlabel{fig:imagenet}{{3}{6}{\relax \fontsize  {9}{10}\selectfont  OOD detection performance comparison on ImageNet dataset (ID). $\mathcal  {S}$NN\xspace  consistently outperforms KNN across all OOD test datasets on the same architecture. }{}{}}
\newlabel{fig:imagenet@cref}{{[figure][3][]3}{[1][6][]6}}
\newlabel{sec:discussion}{{7}{6}{}{}{}}
\newlabel{sec:discussion@cref}{{[section][7][]7}{[1][6][]6}}
\citation{hinton2012improving,srivastava2014dropout}
\citation{ho1998nearest}
\citation{ba2013adaptive}
\citation{gomez2019learning}
\citation{gomez2019learning}
\citation{ba2013adaptive}
\citation{wong2021leveraging}
\citation{wong2021leveraging}
\citation{devries2017improved}
\citation{huang2021mos}
\citation{lee2017training,bevandic2018discriminative,hendrycks2018deep,geifman2019selective,hein2019why,meinke2019towards,liu2020energy,wei2022mitigating,KatzSamuels2022,Du2022,Du2022a,ming2022posterior,hebbalaguppenovel,ming2023how,huang2023harnessing,wang2023outofdistribution,bai2023feed,wang2023learning,du2023dream,zheng2023detection}
\citation{lee2018simple,hendrycks2018deep,wang2022outofdistribution}
\citation{liu2020energy,Du2022,song2022rankfeat}
\citation{hendrycks2016baseline,liang2018enhancing}
\citation{liu2020energy,morteza2022provable}
\citation{huang2021importance}
\citation{Wilson_2023_ICCV}
\citation{lee2018simple,2021ssd}
\citation{sun2022knn}
\citation{ho1998nearest}
\citation{hund2015subspace}
\citation{liu2007computational}
\citation{kriegel2009clustering}
\citation{houle2014efficient}
\newlabel{sec:related works}{{8}{7}{}{}{}}
\newlabel{sec:related works@cref}{{[section][8][]8}{[1][7][]7}}
\newlabel{sec:conclusion}{{9}{7}{}{}{}}
\newlabel{sec:conclusion@cref}{{[section][9][]9}{[1][7][]7}}
\bibdata{egbib}
\citation{zhao2022analysis}
\citation{huang2018densely}
\citation{he2016deep}
\citation{huang2018densely}
\citation{he2016deep}
\citation{hendrycks2016baseline}
\citation{liang2018enhancing}
\citation{liu2020energy}
\citation{hsu2020generalized}
\citation{liang2018enhancing}
\newlabel{sec:impact}{{A}{9}{}{}{}}
\newlabel{sec:impact@cref}{{[appendix][1][2147483647]A}{[1][9][]9}}
\newlabel{app:proof}{{B}{9}{}{}{}}
\newlabel{app:proof@cref}{{[appendix][2][2147483647]B}{[1][9][]9}}
\newlabel{th:sup_main}{{B.1}{9}{}{}{}}
\newlabel{th:sup_main@cref}{{[theorem][1][2147483647,2]B.1}{[1][9][]9}}
\newlabel{lemma:est_err}{{B.2}{9}{}{}{}}
\newlabel{lemma:est_err@cref}{{[theorem][2][2147483647,2]B.2}{[1][9][]9}}
\newlabel{app:experimental_details}{{C}{9}{}{}{}}
\newlabel{app:experimental_details@cref}{{[appendix][3][2147483647]C}{[1][9][]9}}
\newlabel{app:train_details}{{C.1}{9}{}{}{}}
\newlabel{app:train_details@cref}{{[subappendix][1][2147483647,3]C.1}{[1][9][]9}}
\newlabel{app:hardware}{{C.2}{9}{}{}{}}
\newlabel{app:hardware@cref}{{[subappendix][2][2147483647,3]C.2}{[1][9][]9}}
\newlabel{app:ood_description}{{C.3}{9}{}{}{}}
\newlabel{app:ood_description@cref}{{[subappendix][3][2147483647,3]C.3}{[1][9][]9}}
\citation{sun2021react}
\citation{huang2021importance}
\citation{wei2022mitigating}
\citation{sun2022dice}
\citation{lee2018simple}
\citation{mahalanobis1936generalized}
\citation{sun2022knn,2021ssd}
\citation{sun2022knn}
\citation{lee2018simple}
\citation{huang2018densely}
\citation{he2016deep}
\citation{huang2018densely}
\citation{he2016deep}
\citation{sun2022knn}
\newlabel{app:validation}{{C.4}{10}{}{}{}}
\newlabel{app:validation@cref}{{[subappendix][4][2147483647,3]C.4}{[1][10][]10}}
\newlabel{app:pseudo}{{C.5}{10}{}{}{}}
\newlabel{app:pseudo@cref}{{[subappendix][5][2147483647,3]C.5}{[1][10][]10}}
\newlabel{app:diff_arch}{{D.1}{10}{}{}{}}
\newlabel{app:diff_arch@cref}{{[subappendix][1][2147483647,4]D.1}{[1][10][]10}}
\citation{sun2022knn}
\citation{sun2022knn}
\citation{wong2021leveraging}
\citation{srivastava2014dropout}
\citation{ba2013adaptive}
\citation{gomez2019learning}
\citation{sun2022knn}
\citation{sun2022knn}
\citation{naeini2015obtaining}
\citation{nixon2019measuring}
\citation{muller2019does}
\citation{lin2017focal,mukhoti2020calibrating}
\citation{muller2019does}
\citation{lin2017focal}
\citation{huang2018densely}
\citation{wong2021leveraging}
\citation{wong2021leveraging}
\citation{wong2021leveraging}
\citation{wong2021leveraging}
\citation{wong2021leveraging}
\citation{hendrycks2016baseline}
\citation{liang2018enhancing}
\citation{hsu2020generalized}
\citation{liu2020energy}
\citation{sun2021react}
\citation{huang2021importance}
\citation{wei2022mitigating}
\citation{sun2022dice}
\citation{lee2018simple}
\citation{sun2022knn}
\citation{cimpoi2014describing}
\citation{svhn}
\citation{yu2015lsun}
\citation{yu2015lsun}
\citation{xu2015turkergaze}
\citation{zhou2017places}
\citation{hendrycks2016baseline}
\citation{liang2018enhancing}
\citation{hsu2020generalized}
\citation{liu2020energy}
\citation{sun2021react}
\citation{huang2021importance}
\citation{wei2022mitigating}
\citation{sun2022dice}
\citation{lee2018simple}
\citation{sun2022knn}
\citation{cimpoi2014describing}
\citation{svhn}
\citation{yu2015lsun}
\citation{yu2015lsun}
\citation{xu2015turkergaze}
\citation{zhou2017places}
\newlabel{tab:arch}{{6}{11}{\relax \fontsize  {9}{10}\selectfont  Performance comparison on CIFAR-100 dataset for various network architectures. All values are averaged over multiple OOD test datasets. The best results are in \textbf  {bold}.}{}{}}
\newlabel{tab:arch@cref}{{[table][6][2147483647]6}{[1][11][]11}}
\newlabel{tab:sparsification-method}{{7}{11}{\relax \fontsize  {9}{10}\selectfont  Ablation on training-time regularization methods. For OOD detection using Dropout algorithms, we calculate KNN score\nobreakspace  {}\citep  {sun2022knn} using feature vector $h(\mathbf  {x})$. }{}{}}
\newlabel{tab:sparsification-method@cref}{{[table][7][2147483647]7}{[1][11][]11}}
\newlabel{app:rel}{{D.2}{11}{}{}{}}
\newlabel{app:rel@cref}{{[subappendix][2][2147483647,4]D.2}{[1][11][]11}}
\newlabel{app:calibration}{{D.3}{11}{}{}{}}
\newlabel{app:calibration@cref}{{[subappendix][3][2147483647,4]D.3}{[1][11][]11}}
\newlabel{app:results}{{D.4}{11}{}{}{}}
\newlabel{app:results@cref}{{[subappendix][4][2147483647,4]D.4}{[1][11][]11}}
\newlabel{app:add_discuss}{{D.5}{11}{}{}{}}
\newlabel{app:add_discuss@cref}{{[subappendix][5][2147483647,4]D.5}{[1][11][]11}}
\newlabel{fig:fpr}{{4a}{12}{\relax }{}{}}
\newlabel{fig:fpr@cref}{{[subfigure][1][2147483647,4]4a}{[1][11][]12}}
\newlabel{sub@fig:fpr}{{a}{12}{\relax }{}{}}
\newlabel{sub@fig:fpr@cref}{{[subfigure][1][2147483647,4]4a}{[1][11][]12}}
\newlabel{fig:auroc}{{4b}{12}{\relax }{}{}}
\newlabel{fig:auroc@cref}{{[subfigure][2][2147483647,4]4b}{[1][11][]12}}
\newlabel{sub@fig:auroc}{{b}{12}{\relax }{}{}}
\newlabel{sub@fig:auroc@cref}{{[subfigure][2][2147483647,4]4b}{[1][11][]12}}
\newlabel{fig:relationship}{{4}{12}{\relax \fontsize  {9}{10}\selectfont  Visualization of the relationship between the hyper-parameters $r$ and $k$ through OOD detection performance. The model is DenseNet-