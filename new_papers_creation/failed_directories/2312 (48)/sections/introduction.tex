\vspace{-0.3cm}
\section{Introduction}
\label{sec:intro}



Modern machine learning models deployed in the wild face risks from out-of-distribution (OOD) data -- samples from novel contexts and classes that were not taught to the model during training. Identifying OOD samples is paramount for safely operating machine learning models in uncertain environments. A safe system should be able to identify the unknown data as OOD so that the control can be passed to humans. This illuminates the importance of OOD detection, which allows the learner to express ignorance and take further precautions.

Recent solutions for OOD detection tasks often rely on distance measures to distinguish samples that are conspicuously dissimilar from the in-distribution (ID) data~\cite{lee2018simple, tack2020csi, 
 2021ssd, sun2022knn}. These methods leverage embeddings extracted from a well-trained classifier and operate under the assumption that test OOD samples are far away from the ID data in the feature space. For example, \citet{sun2022knn} recently showed that non-parametric nearest neighbor distance is a strong indicator function for OOD likelihood. Despite the promise, distance measures can be sensitive to the dimensionality of the feature space. Those research questions have been raised in the 90s,
including \citet{beyer1999nearest}, as to whether the nearest neighbor
is meaningful in high dimensions. The key result of ~\cite{beyer1999nearest} states that in high dimensional spaces, the difference between the minimum and the maximum distance between a random reference point and a list of $n$ random data points $\*x_1$,...,$\*x_n$ become indiscernible compared to the minimum distance:
\begin{equation}
    \lim_{d \rightarrow \infty} E\bigg( \frac{D_\text{max} - D_\text{min}}{D_\text{min}}\bigg) \rightarrow 0.
\end{equation}

This phenomenon---recognized as one aspect of the ``\textit{curse of dimensionality}''~\cite{beyer1999nearest, hinneburg2000nearest, aggarwal2001surprising, houle2010can, kriegel2009outlier}---limits the efficacy of distance-based OOD detection method for modern neural networks. For example, let's consider 50000 points sampled randomly from a 2048 dimensional space bounded by $[0,1]$. Given a test sample, to find the 10-th nearest neighbor, we need a hyper-cube with a length of at least 0.9958\footnote{The calculation is done by $0.9958^{2048} \approx \frac{10}{50000}$. }, which covers the entire feature space. In other words, the high-dimensional spaces can lead to instability of nearest neighbor queries. Despite its importance, the problem has received little 
attention in the literature on OOD detection. This begs the following question:

\begin{center}
    \textit{How do we combat the curse-of-dimensionality for OOD detection?}
\end{center}
Targeting this important problem, we propose a new framework called \emph{Subspace Nearest Neighbor} (\textbf{SNN}) for OOD detection. Our key idea is to learn a feature subspace---a subset of relevant features---for distance-based detection. Our method is motivated by the observation drawn in \citet{houle2010can}: irrelevant attributes in a
feature space may impede the separability of different distributions and thus have the
potential for rendering the nearest neighbor less meaningful.
In light of this,
\name regularizes the model and its feature representation by leveraging the most relevant subset of dimensions (\emph{i.e.} subspace) for the class prediction. Geometrically, this is equivalent to projecting the embedding to the selected subspace, and the output for
each class is derived using the projected features. The subspaces are learned in a way that maintains the discriminative power for classifying ID data. During testing, distance-based OOD detection is performed by leveraging the learned features. 


We show that \name is both theoretically grounded (Section~\ref{sec:theory}) and empirically effective (Section~\ref{sec:experiment}) to combat the curse-of-dimensionality for OOD detection. 
Extensive experiments show that \name substantially outperforms the competitive methods in the literature.  For example,
using the CIFAR-100 dataset as ID and LSUN~\cite{yu2015lsun} as OOD data,
our approach reduces the FPR95 from {66.09}\% to {24.43}\%---a \textbf{41.66}\% direct improvement over the current best distance-based  method KNN~\cite{sun2022knn}. Beyond the OOD detection task, our subspace learning algorithm also leads to improved calibration performance of the ID data itself\footnote{Code is available at \url{https://github.com/deeplearning-wisc/SNN}}. We summarize our contributions below:

\begin{enumerate}
    \item We propose a new framework, \emph{subspace nearest neighbor} (\name),  for distance-based OOD detection. \name effectively alleviates the curse-of-dimensionality suffered by the vanilla KNN approach operating on the full feature space.
    
    \item We demonstrate the strong performance of \name on an extensive collection of OOD detection tasks. On CIFAR-100, \name substantially reduces the average FPR95 by \textbf{15.96}\% compared to the current best distance-based approach~\cite{sun2022knn}. Further, we provide in-depth analysis and ablations to understand the effect of each component in our method (Section~\ref{sec:ablations}). 
    
    \item We provide theoretical analysis, showing that ID and OOD data become more distinguishable in a subspace with reduced dimensions. We hope our work draws attention to the strong promise of subspace learning for OOD detection.

\end{enumerate}





