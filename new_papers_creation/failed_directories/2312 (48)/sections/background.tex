\section{Preliminaries}
\label{sec:background}
\paragraph{Setup.} In this paper, we consider a supervised multi-class classification problem, where $\mathcal{X}$ denotes the input space and $\mathcal{Y}=\{1,2,...,C\}$ denotes the label space. The training set $\mathbb{D}_\text{in} = \{(\*x_i, y_i)\}_{i=1}^N$ is drawn \emph{i.i.d.} from the joint data distribution $P_{\mathcal{X}\mathcal{Y}}$.
Let $\mathcal{P}_\text{in}$ denote the marginal distribution on $\mathcal{X}$. Let $f: \mathcal{X} \mapsto \mathbb{R}^{|\mathcal{Y}|}$ be a neural network trained on samples drawn from $P_{\mathcal{X}\mathcal{Y}}$ to output a logit vector, which is used to predict the label of an input sample. 

\vspace{0.05cm}
\noindent\textbf{Out-of-distribution detection.} 
Our framework concerns a common real-world scenario in which the algorithm is trained on the ID data but will then be deployed in environments containing \emph{out-of-distribution} samples from {unknown} class $y\notin \mathcal{Y}$ and therefore should not be predicted by $f$. Formally, OOD detection can be formulated as a level-set estimation problem. At test time, one can perform the  following test to determine whether a sample $\*x \in \mathcal{X}$ is from $\mathcal{P}_{\text{in}}$ (ID) or not (OOD):
\begin{equation}
	G_{\lambda}(\*x) =
	\begin{cases}
		\id & \text{if}\ S(\*x) \geq \lambda, \\
		\ood & \text{if}\ S(\*x) < \lambda \\
	\end{cases}
	\label{equ:ood_score}
\end{equation}
where $S(\*x$) denotes a scoring function and $\lambda$ is a chosen threshold such that a high fraction ($95\%$) of \id data is correctly classified. Test samples with higher values of S($\*x$) are classified as \id and vice-versa. 