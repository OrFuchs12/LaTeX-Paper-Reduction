\section{Theoretical Insights}
\label{sec:theory}
In this section, we provide theoretical insights into why using a feature subspace is critical for $k$-NN distance-based OOD detection. 


\noindent \textbf{Setup.}
We consider the OOD detection task as a special binary classification task, where the negative samples (OOD) are only available in the testing stage. A key challenge in OOD detection (and theoretical analysis) is the lack of knowledge on OOD distribution. Common algorithms detect OOD samples when ID density  $p(\*z_i|\*z_i \in  \text{ID})$ is low. For simplicity, we let $p_{in}(\*z_i) = p(\*z_i|\*z_i \in \text{ID})$. We thus divide the bounded-set $\mathcal{Z}$ into two disjoint set $\mathcal{Z}_{in}$ and $\mathcal{Z}_{out}$ with:
    \begin{align}
        \mathcal{Z}_{in} = \{\*z | p_{in}(\*z) \geq \lambda\},  \mathcal{Z}_{out} = \{\*z | p_{in}(\*z) < \lambda\}, 
    \end{align}
where $\lambda$ is a certain real number.

\noindent \textbf{Main result. }   In our analysis, we aim to investigate the impact of embedding dimension $m$ on the following gap:
\begin{align}
    \hat{\Delta}(m) = \mathbb{E}[\hat{p}_{in}(\*z)|{\*z \in \mathcal{Z}_{in}}] - \mathbb{E}[\hat{p}_{in}(\*z)|{\*z \in \mathcal{Z}_{out}}] ,
\end{align}
which is the difference of average estimated density between $\mathcal{Z}_{in}$ and $\mathcal{Z}_{out}$. This gap can be translated into OOD detection performance with the $k$-NN distance. To see this, a function of the $k$-NN distance $r_k(\*z)$ can be regarded as an estimator which approximates $p_{in}(\*z)$ \cite{zhao2022analysis}:
\begin{equation}
  \hat{p}_{in}(\*z) = \frac{k-1}{N V(B(\mathbf{z}, r_k(\*z))},
\end{equation}
where $V(B(\mathbf{z}, r_k(\mathbf{z})))$ denotes the volume of the ball $B(\mathbf{z}, r_k(\mathbf{z}))$ centered at $\mathbf{z}$ with radius $r_k(\mathbf{z})$ and $N$ represents number of training samples. To investigate how OOD detection performance with $k$-NN distance is impacted by the {curse-of-dimensionality}, we derive the following theorem showing how the lower bound of $\hat{\Delta}(m)$ changes when $m$ becomes large. 

\begin{theorem} Let $\mathbb{E}[p_{in}(\*z)|{\*z \in \mathcal{Z}_{in}}] - \mathbb{E}[p_{in}(\*z)|{\*z \in \mathcal{Z}_{out}}] = \Delta(m)$ be a function of the feature dimensionality $m$. We have the following bound (proof is in Appendix B): 
\begin{align}
            \hat{\Delta}(m) & \geq \Delta(m) - 
            O\left((\frac{k}{N})^{\frac{1}{m}} + k^{-\frac{1}{2}}\right).
    \end{align}

\label{th:main}
\end{theorem}


\noindent \textbf{Implications of theory.} 
In Theorem~\ref{th:main}, we see that $\hat{\Delta}(m)$ is lower-bounded by two terms: $\Delta(m)$ and $-O\left((\frac{k}{N})^{\frac{1}{m}} + k^{-\frac{1}{2}}\right)$. Specifically, $\Delta(m)$ represents the true density gap between the ID set and OOD set, which generally sets a ground-truth oracle on how well we can perform OOD detection.  The second term $-O\left((\frac{k}{N})^{\frac{1}{m}} + k^{-\frac{1}{2}}\right)$ measures the approximation error 
when using $k$-NN distance to estimate the true density gap. The approximation error worsens when $m$ gets larger,  which is { exactly where the ``curse of dimensionality'' comes from}.  Consider the following case: when $\Delta(m)$ is nearly constant, $-O\left((\frac{k}{N})^{\frac{1}{m}} + k^{-\frac{1}{2}}\right)$  monotonically decreases when $m$ gets larger, then it is likely for $\hat{\Delta}(m)$ goes to 0 (indistinguishable between ID \& OOD). This theoretical finding further validates the necessity of using a subspace of embedding, which is exactly the solution provided by $\mathcal{S}$NN.

\vspace{0.2cm}
\noindent \textbf{Remark on a small $\mathbf{m}$.} If we only focus on the second term $-O\left((\frac{k}{N})^{\frac{1}{m}} + k^{-\frac{1}{2}}\right)$, it seems like we need the dimension $m$ to be as small as possible. However, in practice, a very small dimension will lose the necessary information needed to separate ID and OOD. Such implications are captured by $\Delta(m)$ which measures the true density gap between ID and OOD. If $\Delta(m)$ is an increasing function over $m$, there is a trade-off in choosing $m$ with the second term that inversely encourages a smaller $m$. Such insight is indeed verified empirically in Section~\ref{sec:ablations} that there exists an optimal $m$--- neither too small nor too large.  