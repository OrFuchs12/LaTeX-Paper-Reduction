

\section{Methodology}
\label{sec:method}

A common design of $S(\*x)$ often relies on distance measures~\cite{2021ssd,sun2022knn,lee2018simple} to distinguish OOD samples that are far away from the ID data in the feature space. Despite the promise, they operate on the full feature space with a large dimension, which is known to be susceptible to the ``{curse-of-the dimensionality}''. Intuitively, irrelevant attributes in a
feature space may impede the separability of different distributions and thus have the
potential to render the distance measure less meaningful. Rather than relying on the full feature space, our key idea is to use a subset of relevant dimensions for distance-based OOD detection.

In what follows, we formally introduce our framework, \emph{Subspace Nearest Neighbor} (dubbed {\name}) for OOD detection. \name consists of two components. First, during training, the model learns the relevant subset of dimensions
(\emph{i.e.} subspace) for each class. The subspaces are learned in a way that maintains the discriminative power for classifying ID data (Section~\ref{sec:train}). Second, during testing, distance-based OOD detection is performed by using the learned features (Section~\ref{sec:test}).  

\subsection{Learning the Subspaces}
\label{sec:train}

We start with the first challenge: {how to learn the relevant subset of dimensions (a.k.a. subspace)?} Our idea is driven by the fact that the class prediction of a given example depends only on a subset of relevant features. Moreover, the relevance of any particular feature dimension may vary across different classes and instances. For example, for a \textsc{Dog} class instance, features such as tail, nose, etc are relevant. However, these feature dimensions can be irrelevant for an instance from \textsc{Bird} class. This necessitates training the model using \emph{class-dependent subspaces} conditioned on the given instance. We formally describe the learning procedure in the sequel.

\paragraph{Defining the subspace.} We consider a deep neural network parameterized by $\theta$, which encodes an input $\*x \in \mathcal{X}$ to a feature space with dimension $m$. We denote by $ h(\*x) = [h^{(1)}(\*x), h^{(2)}(\*x),...,h^{(m)}(\*x) ]$ the feature vector from the penultimate layer of the network, which is a $m$-dimensional vector. For a class $c$, we select the subspace by masking the original feature:
\begin{equation}
\small
    h_{\text{masked}}(\*x) = h(\*x) \odot R_c(\*x), \textit{s.t.}, R_c(\*x)\in \{0,1\}^m, \|R_c(\*x)\|_1 = s,
\end{equation}
where $\odot$ represents the element-wise multiplication and  $R_c(\*x)$  is a binary mask with $s$ non-zero elements encoding the subset of dimensions (\emph{i.e.} subspace) for class $c$. This way, the high-dimensional features can be projected onto the corresponding subspace defined by $R_c(\*x)$, which is class-dependent for a given input $\*x$. The model output $f_c(\*x;\theta, R_c(\*x))$ is further obtained by passing the projected feature through a linear transformation with weight vector $\*w_c \in \mathbb{R}^m$:
\begin{equation}
\label{eq:orig}
    f_c(\*x; \theta, R_c(\*x)) = \langle \*w_c, h(\*x) \odot R_c(\*x) \rangle .
\end{equation}

\paragraph{Learning subspace via relevance.}
Given an input $\*x$, we now describe how to identify $R_c(\*x)$, the subset of most \emph{relevant} dimensions for a class $c$. Our key idea is to formulate the following  optimization objective:
\begin{equation}
    \min_\theta ~~\mathbb{E}_{(\*x,y)\sim\mathcal{P}_{\mathcal{X}\mathcal{Y}}} \left[\mathcal{L}_{\text{CE}}\left({{f}}(\*x;\theta, R_c(\*x)), y\right)\right],
\end{equation}
\begin{multline}
     \text{where ~~~}f_c(\*x; \theta, R_c(\*x)) \\
    = \max_{R_c(\*x)\in \{0,1\}^m, \|R_c(\*x)\|_1 = s} \langle \*w_c, h(\*x) \odot R_c(\*x) \rangle. 
    \label{eq:relevance}
\end{multline}
      

Equation~\ref{eq:relevance} indicates that the subspace is chosen based on the features that are most responsible for the class prediction. 
Geometrically, this is equivalent to projecting 
the point to the selected subspace, and the output for each class is derived using
the projected features. The search of subspace can be computed efficiently. One can easily prove that $f_c(\*x;\theta, R_c(\*x))$ in Equation~\ref{eq:relevance} is equivalent to the summation of the top $s$ entries in 
$\*w_c \odot h(\*x)$ with the highest values. In Section~\ref{subsec:common_benchmark}, we show that our subspace learning objective incurs similar (and in some cases faster) training time compared to the standard cross-entropy loss. During inference, given a test sample $\mathbf{x}^{\prime}$, the class prediction $\hat{y}$ is made by $
    \hat{y}  = \argmax_{c \in \mathcal{Y}} \langle \*w_c, h(\*x^{\prime}) \odot R_c(\*x^{\prime}) \rangle $.


Our formulation also flexibly allows each feature dimension to be used for multiple class predictions. As an example, a class \textsc{laptop} might depend on the two most relevant feature dimensions: \textsc{Keyboard}  and \textsc{screen}. Similarly, the class prediction for \textsc{TV} might be relying on two features \textsc{Telecontrol}  and \textsc{screen}. In both cases, the feature of \textsc{screen} is
responsible for both TV and Laptop. 

\subsection{Out-of-Distribution Detection with Subspace Nearest Neighbor}
\label{sec:test}

We now describe how to perform test-time OOD detection leveraging the learned features.
In particular, given a test sample's embedding $\*z'$,  we determine its OOD-ness by computing the  $k$-th nearest neighbor distance \textit{w.r.t.} the training embeddings.  Here $\*z'= h(\*x') / \lVert h(\*x') \rVert_2$ is the $L_2$-normalized feature embedding. The decision function for OOD detection is given by: 
\begin{equation*}
    G(\*x';k, \mathbb{D}_\text{in}) = \mathbf{1}\{-r_k(\*z') \ge \lambda\},
\end{equation*} 
where $r_k(\*z') = ||\*z' - \*z_{k}||_2$ is the  distance to the $k$-th nearest neighbor ($k$-NN) training embedding ($\*z_k$) and $\mathbf{1}\{\cdot\}$ is the indicator function. The threshold $\lambda$ is typically chosen so that a high fraction of ID data (\emph{e.g.,} 95\%) is correctly classified. Different from \citet{sun2022knn}, we calculate the nearest neighbor distance based on the subspace-regularized feature space, rather than the vanilla ERM-trained model. Next, we  show that our subspace nearest neighbor approach is both theoretically grounded (Section~\ref{sec:theory}) and empirically effective (Section~\ref{sec:experiment}) to combat the curse-of-dimensionality for OOD detection.

\input{sections/new_theory_draft}
