
\section{Related Works}
\label{sec:related works}

\vspace{0.1cm}
\noindent\textbf{Training-based OOD detection methods.} Commonly used approaches in this direction design training-time loss functions and regularization for better ID/OOD separability. Initial work by \cite{devries2017improved} proposed to augment the network with a confidence estimation branch. MOS~\cite{huang2021mos} improves OOD detection by modifying the loss to use a pre-defined group structure. Another branch of studies based on train-time regularization have also shown promising and significant improvement in OOD detection performance~\cite{lee2017training, bevandic2018discriminative, hendrycks2018deep, geifman2019selective, hein2019why, meinke2019towards, liu2020energy, wei2022mitigating,  KatzSamuels2022, Du2022, Du2022a, ming2022posterior, hebbalaguppenovel, ming2023how, huang2023harnessing, wang2023outofdistribution, bai2023feed, wang2023learning, du2023dream, zheng2023detection}. Common approaches include training models to give predictions with lower confidences~\cite{lee2018simple, hendrycks2018deep, wang2022outofdistribution} or higher energies~\cite{liu2020energy, Du2022, song2022rankfeat} for outlier samples. 
    Most regularization methods assume the availability of a large-scale and diverse outlier dataset which is not always realizable. 
    Different from these methods, our proposed method does not require any additional outlier data. Rather, in this work, we formulate a novel train-time regularization approach based on learning feature subspaces. Our empirical analysis highlights the benefits of subspace learning for OOD detection.

\vspace{0.1cm}
 \noindent\textbf{Inference-based OOD detection methods.} Studies in this domain mainly focus on designing scoring functions for OOD detection. These approaches include for example: (1) confidence-based methods~\cite{hendrycks2016baseline, liang2018enhancing}, (2) energy-based methods~\cite{liu2020energy, morteza2022provable}, (3) gradient-based method~\cite{huang2021importance}, and (4) {feature-based methods}~\cite{Wilson_2023_ICCV}. Some representative works include Mahalanobis distance~\cite{lee2018simple, 2021ssd} and non-parametric KNN distance~\cite{sun2022knn}. However, the efficacy of these metrics is often limited in higher dimensions due to the ``curse-of-dimensionality'' (\textit{c.f.} Section~\ref{sec:theory}). Our paper targets precisely this critical yet underexplored problem. 
    We show new insights that learning feature subspaces effectively alleviate this problem. 

\vspace{0.1cm}
\noindent\textbf{Subspace learning.} To overcome limitations of ``curse-of-dimensionality" in high dimensions, \citet{ho1998nearest} explored the idea of random subspace
for nearest neighbor. Subspace learning has also been used to improve search queries in high dimensions~\cite{hund2015subspace}, feature selection~\cite{liu2007computational}, finding clusters in arbitrarily oriented spaces~\cite{kriegel2009clustering}, and intrinsic dimensionality estimation~\cite{houle2014efficient}.  Different from these previous works, we explore class-relevant feature subspace learning for OOD detection.



