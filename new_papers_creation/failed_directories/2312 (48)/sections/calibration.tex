\subsection{Model Calibration}
\label{subsec:calibration}

\input{tables/calibration_main}

Beyond\ood detection tasks, in this section we show that our subspace training algorithm also leads to a significant improvement in model calibration. 

\paragraph{Datasets.} We train on three common benchmark datasets: CIFAR-10/100~\cite{krizhevsky2009learning} and Tiny-ImageNet\cite{deng2009imagenet} (200 classes). For each dataset, we have a separate train and test set. We further split the train set into two mutually exclusive sets: (1) $90\%$ of the train samples are used for training the model, and (2) the remaining $10\%$ of samples are used for validation and hyper-parameter adjustment for post-hoc calibration.

\paragraph{Evaluation Metrics.} To measure model calibration we report: (1) \textbf{Expected Calibration Error (ECE)}~\cite{naeini2015obtaining}: is the weighted average of differences in confidence of predicted class and accuracy of samples predicted with particular confidence. (2) \textbf{Static Calibration Error (SCE)}~\cite{nixon2019measuring}: is a simple class-wise extension of ECE. Refer Appendix for a detailed overview of the metrics used. 


\paragraph{Learning subspace improves calibration.} In Table~\ref{tab:calibration_1}, we observe that our subspace-based training algorithm improves calibration performance. In particular, we consider three losses that are commonly studied for calibration: (1) Cross Entropy Loss (NLL), (2) Label Smoothing (LS)~\cite{muller2019does}, and (3) Focal Loss (FL)~\cite{lin2017focal}. We train the model with each of these losses and measure calibration performance with and without subspace regularization. For clarity, we refer to the technique using our subspace-based learning as ``\textbf{ * + Subspace }'', where * refers to NLL/FL/LS. We see from Table~\ref{tab:calibration_1} that across multiple datasets, using subspace improves the calibration performance of all the loss functions. Specifically, we note that FL+Subspace gives the best performance on multiple datasets. In the Appendix\SL{???}, we further study the effect of combining different post-hoc calibration methods, namely Temperature Scaling and Dirichlet Calibration, over various calibration strategies studied in Table~\ref{tab:calibration_1}.
