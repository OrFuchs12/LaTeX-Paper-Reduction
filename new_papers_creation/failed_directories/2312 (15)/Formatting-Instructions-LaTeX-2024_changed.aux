\relax 
\bibstyle{aaai24}
\citation{schick-schutze-2021a,schick-schutze-2021b,poesia2022,hu-etal-2022}
\citation{qin2021,lester2021}
\citation{wei2022,wang2022,wang2023,aiyappa2023}
\citation{Timo2023,yang2023}
\citation{brown2020,jacovi2023}
\citation{magar2022,jacovi2023}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:baseline-percentage}{{1}{1}{Percentage of datasets with accuracy higher than the majority baseline for datasets released prior and post LLM training data collection date, for both zero-shot (blue, left) and few-shot (green, right). Results are across all models and all datasets. On datasets released post training data collection date for the LLM, the LLM is much less likely to improve upon the simple majority baseline. \textit  {Stat. sig.} (darker) is the percent of datasets for which the performance above majority baseline is significant at the 99\% confidence level.}{}{}}
\citation{openai2023a}
\citation{artetxe2022}
\citation{gpt-j}
\citation{scao2022}
\citation{zhang2022}
\citation{touvron2023}
\citation{alpaca}
\citation{vicuna2023}
\citation{lm-contamination}
\citation{hu2022}
\citation{Mann1947}
\citation{student1908}
\citation{Cobbe2021}
\citation{Srivastava2023}
\newlabel{tab:data-time}{{1}{3}{Dates for the training data creation and model release. davinci-XXX refers to \texttt  {text-davinci-XXX}. GPT-3.5-T refers to \texttt  {GPT-3.5-turbo-0301}.}{}{}}
\newlabel{sec:chronological}{{4}{3}{}{}{}}
\newlabel{table:datasets}{{2}{3}{Dataset release year for each dataset, split into pre-2021 datasets and post-2021 datasets. }{}{}}
\citation{shah2023}
\citation{Dixon1946}
\newlabel{fig:Zero shot performance for old datasets1}{{2a}{4}{}{}{}}
\newlabel{sub@fig:Zero shot performance for old datasets1}{{a}{4}{}{}{}}
\newlabel{fig:Few shot performance for old datasets1}{{2b}{4}{}{}{}}
\newlabel{sub@fig:Few shot performance for old datasets1}{{b}{4}{}{}{}}
\newlabel{fig:Zero shot performance for old datasets2}{{2c}{4}{}{}{}}
\newlabel{sub@fig:Zero shot performance for old datasets2}{{c}{4}{}{}{}}
\newlabel{fig:Few shot performance for old datasets2}{{2d}{4}{}{}{}}
\newlabel{sub@fig:Few shot performance for old datasets2}{{d}{4}{}{}{}}
\newlabel{fig:experiments-majority}{{2}{4}{}{}{}}
\newlabel{fig:GPT-old-datasets}{{3a}{5}{}{}{}}
\newlabel{sub@fig:GPT-old-datasets}{{a}{5}{}{}{}}
\newlabel{fig:GPT-new-datasets}{{3b}{5}{}{}{}}
\newlabel{sub@fig:GPT-new-datasets}{{b}{5}{}{}{}}
\newlabel{fig:Open-old-datasets}{{3c}{5}{}{}{}}
\newlabel{sub@fig:Open-old-datasets}{{c}{5}{}{}{}}
\newlabel{fig:Open-newdatasets}{{3d}{5}{}{}{}}
\newlabel{sub@fig:Open-newdatasets}{{d}{5}{}{}{}}
\newlabel{fig:experiments-average}{{3}{5}{}{}{}}
\newlabel{fig:fine-tune-on-llm}{{4}{5}{}{}{}}
\citation{ouyang2022}
\citation{lm-contamination}
\citation{shah2023}
\newlabel{sec:trainingdatainspection}{{5}{6}{}{}{}}
\newlabel{sec:trainingdataextraction}{{6}{6}{}{}{}}
\newlabel{tab:open-source-llm-contam}{{3}{6}{}{}{}}
\citation{hu2022}
\citation{yu2018}
\newlabel{tab:extract}{{4}{7}{Task example extraction results on all tasks (tasks ordered top to bottom by release date). A line separates those datasets released before the LLM's training data collection date (pre-collection, top) and those after (post-collection, bottom) for each LLM. $\color  {red}\textbf  {X}$\nobreakspace  {}indicates the model can generate training examples for the task. We indicate models with instruction tuning and those without using $\color  {ao(english)}\blacksquare $\nobreakspace  {}and $\color  {gray}\blacksquare $, respectively. $\color  {ao(english)}\blacksquare $\nobreakspace  {}indicates a model with instruction tuning cannot generate task examples, while $\color  {gray}\blacksquare $\nobreakspace  {}indicates a model without instruction tuning cannot generate task examples. Models without instruction tuning cannot follow the instructions directing them to generate task examples. }{}{}}
\newlabel{sec:no_contamination}{{7}{7}{}{}{}}
\newlabel{sec:membershipinference}{{8}{7}{}{}{}}
\citation{brown2020}
\citation{magar2022}
\citation{chang2023}
\citation{aiyappa2023}
\citation{jacovi2023}
\citation{lm-contamination,zhou2023,golchin2023,sainz2023,deng2023investigating,oren2023proving,li2023}
\citation{blevins2023,briakou2023}
\newlabel{fig:spider}{{5a}{8}{}{}{}}
\newlabel{sub@fig:spider}{{a}{8}{}{}{}}
\newlabel{fig:spider-2}{{5b}{8}{}{}{}}
\newlabel{sub@fig:spider-2}{{b}{8}{}{}{}}
\newlabel{fig:spider-both}{{5}{8}{}{}{}}
\newlabel{fig:dev-acc-em}{{6}{8}{}{}{}}
\bibdata{aaai24}
\citation{wang2018}
\citation{Wang2019}
\citation{dolan-brockett-2005}
\citation{clark2019}
\citation{socher2013}
\citation{Demszky2018}
\citation{levesque2012}
\citation{Giampiccolo2008}
\citation{Marneffe2019}
\citation{roemmele2011}
\citation{pilehvar2019}
\citation{geva2021}
\citation{kwak2022}
\citation{hamborg2021}
\citation{yu2023}
\citation{shah2023}
\citation{joseph2023}
\citation{Bang2023,Qin2023}
\citation{openai2023b}
\citation{Bang2023}
\citation{Bang2023}
\citation{Bang2023}
\citation{openai2023b}
\citation{openai2023b}
\citation{Bang2023}
\citation{Bang2023}
\citation{openai2023b}
\citation{Qin2023}
\citation{Qin2023}
\citation{openai2023b}
\citation{openai2023b}
\citation{Bang2023}
\citation{openai2023b}
\citation{shah2023}
\citation{Bang2023}
\newlabel{app:hyperparameters}{{A}{14}{}{}{}}
\newlabel{app:datasets}{{B}{14}{}{}{}}
\newlabel{tab-size-year-llm}{{5}{14}{}{}{}}
\newlabel{app:inspection}{{D}{14}{}{}{}}
\newlabel{fig-prompt-source-llm}{{6}{14}{}{}{}}
\newlabel{tab-re-patterns}{{7}{14}{}{}{}}
\newlabel{app:detailed_results}{{E}{15}{}{}{}}
\newlabel{tab:zero-shot}{{8}{15}{Zero-shot performances on experimented LLMs and datasets. Datasets above the single line are pre- LMM training data collection datasets. Confidence intervals are computed using a t-distribution. Bold text indicates significantly larger than the majority baseline using a t-test with $p=.99$. A graphical representation of this data is in Figs.\nobreakspace  {}\ref {fig:experiments-zero-shot} and \ref {fig:experiments-few-shot}.}{}{}}
\newlabel{tab:few-shot}{{9}{15}{Few-shot performances on GPT-series models. Datasets above the single line are pre- LMM training data collection datasets. Confidence intervals are computed using a t-distribution. Bold text indicates significantly larger than the majority baseline using a t-test with $p=.99$. A graphical representation of this data is in Figs.\nobreakspace  {}\ref {fig:experiments-zero-shot} and \ref {fig:experiments-few-shot}.}{}{}}
\newlabel{fig:GPT-datasets}{{7a}{15}{}{}{}}
\newlabel{sub@fig:GPT-datasets}{{a}{15}{}{}{}}
\newlabel{fig:Open-datasets}{{7b}{15}{}{}{}}
\newlabel{sub@fig:Open-datasets}{{b}{15}{}{}{}}
\newlabel{fig:experiments-average-all}{{7}{15}{}{}{}}
\newlabel{fig:Zero shot performance for old datasets}{{8a}{16}{}{}{}}
\newlabel{sub@fig:Zero shot performance for old datasets}{{a}{16}{}{}{}}
\newlabel{fig:Few shot performance for old datasets}{{8b}{16}{}{}{}}
\newlabel{sub@fig:Few shot performance for old datasets}{{b}{16}{}{}{}}
\newlabel{fig:Zero shot performance for old datasets}{{8c}{16}{}{}{}}
\newlabel{sub@fig:Zero shot performance for old datasets}{{c}{16}{}{}{}}
\newlabel{fig:Few shot performance for old datasets}{{8d}{16}{}{}{}}
\newlabel{sub@fig:Few shot performance for old datasets}{{d}{16}{}{}{}}
\newlabel{fig:experiments-zero-shot}{{8}{16}{}{}{}}
\newlabel{fig:Zero shot performance for new datasets}{{9a}{16}{}{}{}}
\newlabel{sub@fig:Zero shot performance for new datasets}{{a}{16}{}{}{}}
\newlabel{fig:Few shot performance for new datasets}{{9b}{16}{}{}{}}
\newlabel{sub@fig:Few shot performance for new datasets}{{b}{16}{}{}{}}
\newlabel{fig:Zero shot performance for new datasets}{{9c}{16}{}{}{}}
\newlabel{sub@fig:Zero shot performance for new datasets}{{c}{16}{}{}{}}
\newlabel{fig:Few shot performance for new datasets}{{9d}{16}{}{}{}}
\newlabel{sub@fig:Few shot performance for new datasets}{{d}{16}{}{}{}}
\newlabel{fig:experiments-few-shot}{{9}{16}{}{}{}}
\newlabel{app:prompt_examples}{{G}{17}{}{}{}}
\newlabel{app:prompt_extraction}{{H}{21}{}{}{}}
\newlabel{tab:task-extraction-prompts}{{10}{21}{Prompts used for each task for task example extraction.}{}{}}
\gdef \@abspage@last{21}
