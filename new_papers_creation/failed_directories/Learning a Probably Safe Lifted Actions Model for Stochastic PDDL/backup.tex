\def\year{2022}\relax
%File: formatting-instructions-latex-2022.tex
%release 2022.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
%\nocopyright
%
% PDF Info Is REQUIRED.
% For /Title, write your title in Mixed Case.
% Don't use accents or commands. Retain the parentheses.
% For /Author, add all authors within the parentheses,
% separated by commas. No accents, special characters
% or commands are allowed.
% Keep the /TemplateVersion tag as is
\pdfinfo{
/Title (Learning Probably Safe Action Models in Stochastic Worlds)
/Author (Us)
/TemplateVersion (2022.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{paralist}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}


\newcommand{\eff}{\textit{eff}}
\newcommand{\pre}{\textit{pre}}
\newcommand{\tuple}[1]{\ensuremath{\left \langle #1 \right \rangle }}
\newcommand{\goal}{\textit{goal}}
\newcommand{\sam}{\textit{SAM}}



\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai22.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Learning Probably  Approximately Complete and Correct Action Models for Stochastic Worlds}
\author{Submission 6194}
% \affiliations{
%     %Afiliations
%     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
%     % If you have multiple authors and multiple affiliations
%     % use superscripts in text and roman font to identify them.
%     % For example,

%     % Sunil Issar, \textsuperscript{\rm 2}
%     % J. Scott Penberthy, \textsuperscript{\rm 3}
%     % George Ferguson,\textsuperscript{\rm 4}
%     % Hans Guesgen, \textsuperscript{\rm 5}.
%     % Note that the comma should be placed BEFORE the superscript for optimum readability

%     2275 East Bayshore Road, Suite 160\\
%     Palo Alto, California 94303\\
%     % email address must be in roman text type, not monospace or sans serif
%     publications22@aaai.org
% %
% % See more examples next
% }

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name,\textsuperscript{\rm 1}
    Second Author Name, \textsuperscript{\rm 2}
    Third Author Name \textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1} Affiliation 1\\
    \textsuperscript{\rm 2} Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
We consider the problem of learning models of stochastic environments for planning, assuming that the transitions on the various state factors are independent. We suppose we have example trajectories of policies for random problems executing in an unknown environment, and we would like to match the rate of success of those policies. We present a polynomial-time algorithm that, given a polynomial number of trajectories, produces a PPDDL model such that optimal policies under the PPDDL model will provably achieve a success rate at future goals that is almost as good or better than the policies that produced the training trajectories. Moreover, we also consider a variant of PPDDL in which there is uncertainty about the transition probabilities, specified by an interval for each factor that contains the respective true transition probabilities. We also give a polynomial-time algorithm that, when given the example trajectories, produces such an imprecise-PPDDL environment model and guarantees that with high probability, the true environment is indeed captured by the uncertain parameters, while also guaranteeing that the optimal bound on the success probability in this model is almost as good or better than the success rate of the policies that produced the training examples.  
\end{abstract}

\section{Introduction}

% Domain independent planning high-level, action models, STRIPS, hard to model
Domain-independent planning is a long-standing goal of Artificial Intelligence (AI) research. 
The input to a domain-independent planning algorithm traditionally includes a description of the domain in which we wish to plan.
This domain description is usually specified in a formal language and includes an \emph{action model}, which specifies which actions can be in a plan and how they work. 
In a simple planning languages such as STRIPS \cite{fikes1971strips}, this action model consists of the set of effects of the actions on the world state, and the set of preconditions that must hold in order for each action to be taken. 
Action models are notoriously hard to formally specify, even in such a simple planning language. 
This has motivated work on a number of methods for automatically learning these action models from examples~\cite{yang2007learning,cresswell2011generalised,cresswell2013acquiring,zhuo2013action,stern2017efficientAndSafe,aineto19,juba2021kr}. 

% We focus on stochastic effects. This is so hard :)
The challenge of specifying a domain model is only more acute in richer classes of action models. 
In this work, we consider domains in which the effects of actions are randomly determined each time an action is taken. 
Therefore, action model models in such domains specify a distribution on effects for each action. 
Specifying such an action model by hand is extremely difficult: small errors in the probabilities may accumulate over the course of an execution, leading to wildly inaccurate estimates of the effects of a plan in the real world. 
It is therefore essential to use data about the real world to inform the model.


% Our objective: learn action models for such domains, but preserve guarantees on safety and completeness


% We are not doing RL. But we have similar challenges. In particular, getting guarantees is hard. 
This problem of learning an action model is related to Reinforcement Learning (RL), with the crucial difference that RL typically assumes that we only wish to design a policy for a single, fixed, learned reward function, whereas here we wish to use the same domain model for various goals given as input. % Hmm, not sure
Nevertheless, the central challenges limiting the state-of-the-art in RL apply here as well.
In particular, we would like methods for learning domain models with guarantees of efficiency and correctness. Ideally, we would of course like an algorithm that finds a domain model that is close to the true domain and, if states are described by a set of (Boolean) fluents $|F|$, has both time complexity and sample complexity bounded by a polynomial in $|F|$. Unfortunately, in general this is  too much to hope for, since there are $2^{|F|}$ states. Concretely, along the lines of Kakade \shortcite{kakade2003thesis}, domains that capture a simple ``combination lock'' require $\Omega(2^{|F|}|A|)$ trials to solve, if the combination is described by the fluents and $A$ is the set of actions. Consequently, algorithms with theoretical guarantees for learning optimal policies in a general RL setting seek to observe every action in every state, and this is optimal \cite{strehl2006pac}. 
%[[Roni: mayb e reph]
%In general, in a general RL setting one must try to take every action in every state multiple times in order to guarantee that an optimal policy has been found \cite{strehl2006pac}. 

% Our approach
We also follow an offline approach to permit safe learning. But, in order to establish that our model generalizes across goals, we consider a different learning paradigm. 
Following a recent approach to learning deterministic action models \cite{stern2017efficientAndSafe,juba2021kr}, we suppose that problems for a domain are sampled from a fixed distribution, and we are given a training set of trajectories executing policies aiming to solve those problems in the domain.
We seek an action model that captures enough of the domain faithfully to ensure that
\begin{compactenum}
    \item[(i)] policies that can be executed in the model behave similarly in the real world (solution correctness) and 
    \item[(ii)] the model contains policies that attain a similar rate of success on the problem distribution as the training policy distribution (solution completeness).
\end{compactenum}
Such a guarantee is similar to that provided by imitation learning \cite{osa2018algorithmic} (see also \cite{khardon1999l2act}), with the difference again (similar to the distinction with RL) that rather than seeking to match a human teacher's performance at a single objective, we would like our model to generalize across many possible goals.

We give algorithms with polynomial time and sample complexity guarantees for learning models of domains, represented in Probabilistic Planning Domain Description Language (PPDDL)~\cite{younes2004ppddl1}, that satisfy the above conditions, given that the transition distribution is independent for each fluent, in line with the MBRL works. Note that without such an assumption, in general each action on each of the $2^{|F|}$ states yields a distribution over the $2^{|F|}$ possible states, where such a model requires $|A|2^{|F|}(2^{|F|}-1)$ parameters to specify, which is clearly infeasible.

We also introduce an extension of PPDDL that captures a relaxed class of domain models in which there is uncertainty about the effects. Instead of giving a probability of an effect occurring, an interval is specified such that the actual probability lies somewhere in the interval---that is, specifying a MDP with Imprecise Probabilities \cite{satia1973markovian}. We also give an algorithm for learning such uncertain PPDDL models that guarantees with high probability that the true transition distribution is consistent with the learned model. The learned model also satisfies an analogous completeness property, that ensures that there is a policy that the model guarantees to have a success rate that is similar to the policies used to generate the examples. Because probability bounds obtained in the model are guaranteed to hold for the real domain, such a model can be used for \emph{safe} planning.


 


\section{Background and Problem Definition}



% PPDDL
In this work, we assume the domain can be described using the Probabilistic Planning Domain Description (PPDDL) language. 
PPDDL is a formal language for specifying factored stochastic shortest path problems. We provide a brief desription of PPDDL below. 
For simplicity, we focus on the grounded version of PDDL, and consider action costs to be unit for all actions. 

% PPDDL states, fluents, literal, and actions. 
A fluent $f$ is a fact that may or may not be true in the domain. 
A state $s$ in PPDDL is an assignment of values --- true or false --- to the set of fluents $F$ in the domain, specifying which fluents are true in $s$. 
We refer to a state as a set, each element of which is an assignment to a single fluent.
Let $s(f)$ be the value assigned to a fluent $f\in F$ in a state $s$. 
A partial state is an assignment of values to only a subset of the fluents in $F$. 
A partial state $s'$ is consistent with a state $s$ 
if the fluents specified by $s'$ take the same values in $s$, i.e., $s'\subseteq s$. 
Let $A$ be the set of actions that may be in a plan. 
An action model $M$ for $A$ specifies preconditions and effects for each action $a\in A$, 
denoted $\pre_M(a)$ and $\eff_M(a)$, respectively. 
The preconditions of an action is a partial state. 
An action $a$ is applicable in a state $s$ iff its preconditions are consistent with $s$. 
The effects of an action is a set of the form $\{ \tuple{e_i, p_i} \}_i$, 
where $e_i$ is a partial state and $p_i$ is the probability that it will \emph{occur}. 
An effect $\tuple{e_i, p_i}$ \emph{occurring} means that after applying an action $a$ at a state $s$ we reach a state $s'$ that is consistent with $e_i$ even if $e_i$ was not consistent with $s$. We denote by $\Pr_M[s'|a,s]$ the probability, according to the action model $M$, 
that applying $a$ in state $s$ will result in reaching $s'$. 

% Simplifying assumption: distributions of fluents in the effects are independent
PPDDL supports advanced features such as recursive probabilistic effects and conditional effects, which we do not consider here. In addition, for most of this paper we limit the scope of our discussion to action models in which the effects are partial assignments of a set of literals whose distribution is independent. 
Formally, for every action $a$ there is a set of fluents $E_a$, 
and each fluent $f\in E_a$ is associated with a marginal probability (factor) $\Pr[s'(f)|a,s(\neg f)]$. 
Every subset of fluents $E'_a\subset E_a$ is an effect of $a$ that occurs with probability $(\prod_{f\in E'_a} \Pr[s'(f)|a,s(\neg f)])\cdot(\prod_{f\in E_a\setminus E'_a} 1-\Pr[s'(f)|a,s(\neg f)])$. 
In the future work section we discuss how to relax this requirement. 


% Domains and problems 
A PPDDL planning \emph{domain} is defined by a tuple 
$D=\tuple{F, A, M}$
where $F$ is the set of fluents, 
$A$ is the set of actions,
and $M$ is the action model for these actions.  
% Objective
A PPDDL planning \emph{problem} is defined by a tuple 
$\tuple{D, s_I, G}$ where $D$ is a PPDDL domain;
$s_I$ is the start state, i.e., the state of the world before planning;  
and $G$ is a partial state that define when a goal state has been found. 
A \emph{solution} to a PPDDL problem $\Pi=\tuple{D, s_I, G}$ is a \emph{policy} $\pi:2^F\rightarrow A$,  
mapping a state that may be encountered to an action. 
% Evaluation: we aim to maximize goal prob.
Executing a policy $\pi$ on a problem $\Pi$ means starting at $s_I$, applying the action $\pi(s_I)$, 
sampling a new state $s'$, and continuing to apply actions according to $\pi$ and the current state until some pre-defined stopping condition is met.\footnote{This stopping condition can be any property of the trajectory to that point, e.g., either reaching a goal state or performing a fixed number of actions, after which the agent gives up.} 
The resulting sequence of states and actions is called a \emph{trajectory}. 

\begin{definition}[Trajectory]
A trajectory is an alternating sequence of states and actions of the form $(s_0, a_0, s_1, a_1,\ldots, a_n, s_n)$. 
% A trajectory $T=\tuple{s_0, a_1, s_1, \ldots a_n, s_n}$ is an alternating sequence of states and actions of the form $(s_0, a_0, s_1, a_1,\ldots, a_n, s_n)$. 
%$(s_0,\ldots,s_n)$ and actions $(a_1,\ldots,a_n)$ that starts and ends with a state.
\end{definition}
A single execution of a policy yields a trajectory where $s_0=s_I$ and $s_n$ is the last state reached in that execution. 
The length of a trajectory $T$, denoted $|T|$, is the number of actions in it.  
In the literature on learning action models~[inter alia]\cite{wang1994learning,wang1995learning,walsh2008efficient,stern2017efficientAndSafe,arora2018review}, 
it is common to represent a trajectory $T=\tuple{s_0, a_1, \ldots, a_{|T|}, s_{|T|}}$
as a set of triples 
$\big\{\tuple{s_{i-1},a_i,s_i}\big\}_{i=1}^{|T|}$. 
Each triple $\tuple{s_{i-1},a_i,s_i}$ is called an \emph{action triplet}. 
The probability of observing $T$ assuming an action model $M$, denoted $\Pr_{M,\pi}[T]$, is the product of the probabilities of $T$'s constituent action triplets, i.e., $\Pr_{M,\pi}[T]=\prod_{(s,\pi(s),s')\in T} \Pr_M[s'|\pi(s),s]$. 


% in a domain $D$ when executing a policy $\pi$, denoted $P(T,D,\pi)$, is the product of the probabilities of $T$'s constituent action triplet, i.e., $P(T,D,\pi)=\prod_{(s,\pi(s),s')\in T} p_{D.M}$, where $D.M$ is our notation for the action model of the domain $D$. 

% We denote by $\mathcal{T}(a)$ the set of all action triplets in the trajectories in $\mathcal{T}$ that include the grounded action $a$. 




% Goal achieving prob. 
To evaluate a policy for a given problem, we consider the set of trajectories that can be reached when following that policy, starting from the initial state. We denote this set of trajectories by $T(\pi,s)$, and denote by $T_G(\pi,s)$ its subset that includes only trajectories that reach the goal $G$. 
Common metrics include the expected number of actions in performed until a goal state is reached, and the \emph{goal-achieving probability}. 
We mainly consider the latter option, denote by $\Pr_{M,\pi}[G|s]$ and given by $\sum_{T\in T_G(\pi,s)} \Pr_{M,\pi}[T]$. 
%$\Pr_{M,\pi}[goal|s]$ can be computed via dynamic programming.
% Only efficient if the state space is small....







% There is more than one way to evaluate a given policy $\pi$ in a given PPDDL problem $\Pi$. 
% One option is to compute the expected number of actions performed until a goal state is reached, when executing $\pi$ in $\Pi$.  
% Another option is to compute the \emph{goal-achieving probability} of achieving the goal, i.e., reaching a goal state. 
% In this work, we mainly consider the second option, and denote by $P_\goal(s, \pi, D)$ 
% the probability that a goal state will eventually be reached when executing $\pi$ in a domain $D$. 
% Computing $P_\goal$ can be done via dynamic programming. 







% starting from $s$. 
% To compute the probability goal-achieving p
% V^\pi(s) = expected number of states observed until the goal is rech
% P(s, \pi) = the probab



\subsection{Safe Planning without an Action Model}

% This is good text but should go to the intro
% There are many planning algorithms for solving PPDDL problems and factored stochastic shortest path problems in general, such as RTDP~\cite{barto1995learning}. 
% % TODO: Add examples with references
% Many of these algorithms assume knowledge of the action model $M$. 


% No action model, yes trajectories. 
We consider the setting where the planning agent is tasked to find a policy for a PPDDL problem
$\Pi=\tuple{D, s_I, G}$ but it has only a partial knowledge of the domain $D$.  %$\Pi=\tuple{D(\Pi), s_I(\Pi), G(\Pi)}$ but it has only a partial knowledge of the domain $D$. 
Specifically, the planner does not know the action model of the domain $D$. 
Instead, it is given a set of trajectories $\mathcal{T}=\{T_1,\ldots, T_m\}$ created by executing policies designed to solve problems 
$\{ \Pi_1,\ldots,\Pi_m \}$ in the same PPDDL domain, and in particular, all having the same action model. 
%, i.e.,  with $D(\Pi)=D(\Pi_1)=\cdots=D(\Pi_m)$. 


Standard online RL setups approach this model-free setting by allowing the agent to perform exploratory actions, learning over time which policies are more effective than others. 
We focus on \emph{offline learning},  where the 
objective is to learn an action model $M'$ with which we can generate policies for a range of problems in the domain $D$. 
% that are likely to be applicable in the domain $D$, and reach a goal state with high probability. 
%agent's \emph{safety} is a top priority, and thus we aim to learn an action model with which we can generate a policy that will achieve the goal with high probability. 
% The first problem we address is therefore how to learn an action model $M'$ such that policies that achieve a goal according to $M'$ with some probability 
% it will be applicable 
The main problem we address in this paper is to learn an action model that satisfies the following desirable properties:
\begin{compactenum}
    \item \textbf{Safety.} Policies created using $M'$ will also be applicable and effective for problems in the domain $D$.
    \item \textbf{Completeness.} Policies that are effective in $D$ will also be applicable and effective in the learned model $M'$.
\end{compactenum}
We refer to $D$ and its action model as the \emph{real domain} and the \emph{real action model}, respectively, and denote the latter by $M^*$. 

% agent's \emph{safety} is a top priority, and thus we aim to learn an action model with which we can generate a policy that will achieve the goal with high probability. 
% The term \emph{completeness} here refers to the ability to recognize and find a policy with a goal achievement probability that close to the highest goal achievement probability possible for policies in the real action model. 
% The terms \emph{safety} here refers to having a guarantee about the probability that the goal will be achieved. 
% some lower bound on the probability that a goal is achieved --- in the real PPDDL problem --- by policies created with the learned action model.






\subsection{SAM Learning}

The Safe Action Model (SAM) learning algorithm~\cite{juba2021kr,stern2017efficientAndSafe} has safety and completeness guarantees similar to those specified above. 
However, it is designed for classical planning, where states are fully observable and actions have deterministic effects. 
Classical planning is significantly simpler than our setting, where actions can have stochastic effects, 
Nevertheless, our work builds on SAM learning, so we describe it here briefly for completeness. 


SAM learning, when applied to grounded domains, is based on the following simple rules, which apply for every observed action triplet $(s,a,s')\in \mathcal{T}$.
\begin{compactenum}
    \item Rule 1 [not a precondition].  $\forall l \notin s: l \notin \pre(a)$
    \item Rule 2 [not an effect].  $\forall l \notin s': l \notin \eff(a)$
    \item Rule 3 [must be an effect].  $\forall l \in s'\setminus s: l \in \eff(a)$
\end{compactenum}
where $\pre(a)$ and $\eff(a)$ are the preconditions and effects of $a$ according to real action model $M^*$. 
We refer to these rules as the SAM learning rules. 
SAM learning works by initially assuming every action has all the literals as preconditions and none of the literals as effects, and then applying rules 1 and 3 above to remove preconditions and add effects as needed. 
In a classical planning setting where all actions are grounded, Juba et al.~\shortcite{juba2021kr} proved that the action model $M_{\sam}$ created by SAM learning is: (1) \emph{safe}, in the sense that every plan consistent with $M_{\sam}$ is also consistent in the real action model, (2) \emph{probably complete}, in the sense that with high probability, 
for most solvable problems there exists a plan that solves them and is consistent with $M_{SAM}$, given a number of trajectories that is polynomial in the number of fluents and actions. 
They also extended SAM learning to learn lifted action models and provided similar safety and completeness guarantees. 




\section{SAM+: SAM Learning for stochastic domains}


% In the literature on learning action models~\cite{wang1994learning,wang1995learning,walsh2008efficient,stern2017efficientAndSafe,arora2018review}, 
% it is common to represent a trajectory 
% $\tuple{s_0, a_1, \ldots, a_{|\pi|}, s_{|\pi|}}$
% as a set of triples 
% $\big\{\tuple{s_{i-1},a_i,s_i}\big\}_{i=1}^{|\pi|}$.
% Each triple $\tuple{s_{i-1},a_i,s_i}$ is called an \emph{action triplet},  and the states $s_{i-1}$ and $s_i$ are referred to as the pre- and post- state of action $a_i$. 
% We denote by $\mathcal{T}(a)$ the set of all action triplets in the trajectories in $\mathcal{T}$ that include the action $a$. 

The first SAM learning rule (``not a precondition'') applies in our setting, since for every $\ell\in\pre(a)$, we must have $\ell\in s$ for every $s$ in which $a$ is applied. 
The last SAM learning rule (``must be an effect'') also apply to our setting, due to the standard frame axioms (i.e., state only changes due to actions of the agent). 
The second rule (``not an effect''), however, does not: a literal $\ell$ may be an effect of an action $a$ even if there exists an action triplet $(s,a,s')$ where $\ell\notin s'$. 
This may occur if $\ell$ is an effect of $a$ but the probability of this effect occurring is not 1.0. 
This rule is crucial for SAM learning's safety guarantee, since it asserts that no unexpected effects will occur.\footnote{See proof by Stern et al.\shortcite{stern2017efficientAndSafe}.} 
Thus, a strict form of safety as achieved in the deterministic domains cannot be achieved in our setting. 
Instead, the SAM+ algorithm described below outputs an action model that
yields a probabilistic form of safety, using a novel extension of PPDDL that we call \emph{PPDDL with Impercise Probabilities}. 
We describe all this below. 



\subsection{PPDDL with Imprecise Probabilities}
Markov Decision Processes with Impercise Probabilities (MDP-IP) \cite{satia1973markovian}
are a representation specifying a \emph{set} of MDPs as follows. 
Instead of a transition function, a \emph{credal set} of constraints over the the probability of the values of the state variables is given. 
In detail, the MDP-IP definition specifies a \emph{transition credal set}, often denoted by $K(s'|s,a)$, which maps state-action pairs to a set of constraints that the actual transition probabilities $\Pr_{M}[s'|s,a]$ satisfy; that is, it specifies a set of MDPs $M$ for which $\Pr_{M}[s'|s,a]$ is consistent with $K(s'|s,a)$ for all pairs $(s,a)$. This representation  captures the uncertainty about the real transition probabilities that  arises when models are estimated from empirical observations.
We note that planners for MDP-IPs have been proposed, e.g., by \citet{delgado2011efficient}.

We propose to extend the PPDDL model to an \emph{imprecise probabiltiy} PPDDL model by replacing the probabilities $p_i$ of each effect $e_i$ with an interval $[l_i,u_i]\subseteq [0,1]$, with the property that for an effect $\{\tuple{e_i,[l_i,u_i]}\}_{i=1}^k$, there exists $(p_1,\ldots,p_k)\in [l_1,u_i]\times \cdots \times [l_k,u_k]$ such that $\sum_{i=1}^kp_i = 1$, i.e., there must exist at least one consistent probability assignment in the intervals. Indeed, analogously to the relationship between MDP and MDP-IP, this model specifies a set of PPDDL representations, for each assignment of probabilities satisfying the given interval constraints. The preconditions are specified identically to standard PPDDL models. For independent effects, we will simply specify an interval for each factor $\Pr_M[s'(f)|a,s(\neg f)]$ of the effect.



\subsection{The SAM+ Action Model}


% Intro to SAM+: we capture the approxiamtely using credal sets
The SAM+ algorithm accepts a set of trajectories $\mathcal{T}$ and outputs an action model $M_{SAM+}$, using our imprecise probability PPDDL formalism. 
% NOTE: I moved discussion of the formalism, credal sets, etc. earlier.
%This action model specified the preconditions of every action like a standard PPDDL action model. However, it defines the effects of actions in a different way, specifying for every literal $\ell$ a lower and upper bound on the probability that it will occur. Similar types of effects have been studied in the context of factored MDPs with Imprecise Probabilities (MDP-IP)~\cite{delgado2011efficient}. 

\noindent \textbf{Preconditions.} 
Let $\mathcal{T}(a)$ be all the action triplets for action $a$. 
States $s$ and $s'$ are said to be a \emph{pre-} and \emph{post-state} of $a$, respectively, if there is an action triplet $\tuple{s,a,s'}\in \mathcal{T}(a)$. 
Since the first SAM learning rule holds here, we have that:
        %infer the following:
\begin{equation}
\small
        \pre(a) \subseteq  \bigcap_{\tuple{s, a, s'}\in \mathcal{T}(a)} s \label{eq:pre} 
\end{equation}        
%Indeed, since for every $\ell\in\pre(a)$, we must have $\ell\in s$ for every $s$ such that $\tuple{s,a,s'}\in\mathcal{T}(a)$, the bound holds. 
SAM+ sets this upper bound to be the preconditions for every action observed in the given set of trajectories ($\mathcal{T}$). %in the observed in the SAM+ action model.

% Actual effect learning
\noindent \textbf{Effects.} 
Since SAM learning rule 3 holds here, we have:
\begin{equation}
\small
        E_a \subseteq  \bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'\setminus s \label{eq:eff}
\end{equation} 
However, we cannot fully identify which literals are not in $E_a$. 
Therefore, SAM+ assumes that all literals are in $E_a$, but differentiates between literals in how it sets the probability that they will occur --- $\Pr[s'(f)|a,s(f)]$. 
To estimate $\Pr[s'(f)|a,s(f)]$, SAM+ counts the number of action triplets in $\mathcal{T}(a)$ in which $\ell$ was in the post-state and not the pre-state 
($\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\in s'\setminus s\}$), 
and the number of action triplet in which $\ell$ was not in the pre-state 
($\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\notin s\}$). 
We denote these numbers by $\#_a(\ell\in s'\setminus s)$ and $\#_a(\ell\notin s)$, respectively. 

% \begin{equation}
% K_\delta(\ell|s,a) =     
% \begin{cases}
% \ell\in \bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'\setminus s 
% & \frac{\#_a(\ell\in s'\setminus s)}{\#_a(\ell\notin s)} \pm\sqrt{\frac{\ln(2/\delta)}{2\#_a(\ell\notin s)}} \\
% \text{Otherwise} 
% & \left[0,\frac{\ln(1/\delta)}{\#_a(\ell\notin s)}\right] \\
% \end{cases}
% \end{equation}

SAM+ defines the range of values for $\Pr[s'(f)|a,s(f)]$ as follows.
For an action $a$, fluent $f$, and constant $0<\delta\leq 1$, we define the range $K_\delta(s'(f)|a,s(f))$ as follows. 
\begin{equation}\label{eq:present-effects}
\small
K_\delta(s'(f)|a,s(\neg f)) = \frac{\#_a(f\in s'\setminus s)}{\#_a(f\notin s)} \pm\sqrt{\frac{\ln(2/\delta)}{2\#_a(f\notin s)}} 
\end{equation}
if $f,\neg f\in \bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'\setminus s$, 
and otherwise, for the $\ell\notin\bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'\setminus s $,
\begin{equation}\label{eq:missing-effects}
\small
K_\delta(s'(\ell)|a,s(\neg\ell)) = \left[0,\frac{\ln(1/\delta)}{\#_a(\ell\notin s)}\right] 
\end{equation}
i.e., if $\ell=f$, this is $K_\delta(s'(f)|a,s(\neg f))$, and otherwise it is $1-K_\delta(s'(f)|a,s(\neg f))$; we generally let $K_\delta(s'(\neg f)|a,s(f))=1-K_\delta(s'(f)|a,s(\neg f))$.
For a selected value of $\delta$, let $M_\delta$ be the action model returned by SAM+. We omit $\delta$ and just denote the learned model by $M$ when it is clear from context. 
This $\delta$ parameter represents the confidence we that the learned model $M_\delta$ captures the effect probabilities of the real action model,  i.e., that the range $K_\delta(s'(\ell)|a,s(\neg\ell))$ indeed includes the probability that $\ell$ will occur ($\Pr[s'(\ell)|a,s(\neg\ell)]$). 
Thus, increasing $\delta$ increases the range $K_\delta(s'(\ell)|a,s(\neg\ell))$. 
On the other hand, having more trajectories to learn from should yield a smaller range $K_\delta(s'(\ell)|a,s(\neg\ell))$ for a fixed $\delta$. %, ideally converging to $pp(\ell,a)$. 
We now formalize this relation between $K_\delta(s'(\ell)|a,s(\neg\ell))$ and $\Pr[s'(\ell)|a,s(\ell)]$, and more generally $M_\delta$ and $M^*$. 


% We estimate the effects with the following credal sets:
% For $\ell\in\bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'\setminus s $, $K_\delta(\ell|s,a)$ is
% \begin{equation}
% \small
% \frac{\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\in s'\setminus s\}}{\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell \notin s\}}\pm\sqrt{\frac{\ln(2/\delta)}{2\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\notin s\}}} 
% \end{equation}
% and otherwise (i.e., for $\ell\notin\bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'\setminus s $),
% \begin{equation}
% \small
% K_\delta(\ell|s,a) = \left[0,\frac{\ln(1/\delta)}{\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\notin s\}}\right] 
% \end{equation}


\begin{definition}
We say that an action model $M$ with credal set function $K_M$ is \emph{correct} for an MDP $M^*$ if for all possible triplets $(s,a,s')$, $\Pr_{M^*}[s'|s,a]\in K_M(s'|s,a)$.
\end{definition}


\begin{theorem}[Safety]\label{thm:sam-safe}
For $\delta'=\frac{\delta}{2|F||A|}$, the SAM+ action model given by credal set function $K_{\delta'}$ for trajectories drawn from $M^*$ is correct for $M^*$ with probability $1-\delta$. Moreover, any action that is applicable in the SAM+ action model is applicable in $M^*$.
\end{theorem}
\begin{proof}
First, consider any $\ell\in\bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'\setminus s $; as trajectories are sampled from $M^*$, when $a$ is taken in state $s$, $\ell\in s'$ with probability $\Pr_{M^*}[s'(\ell)|a,s]$. Consider the indicator random variables for $\ell\in s'$ for each of these events; note that if we subtract off their expectation (equal to $\Pr_{M^*}[s'(\ell)|a,s]$), the sum of these differences is a martingale sequence with differences bounded by 1, and the total number of such events in each trajectory, being determined by the policy in $M^*$, is a valid stopping time. So, for 
$
\gamma = \sqrt{\frac{\ln(2/\delta)}{2\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\notin s\}}}
$
the Azuma-Hoeffding inequality gives that for our $\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\notin s\}$ such random variables, the fraction that take value 1, i.e., the empirical fraction
$
\frac{\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\in s'\setminus s\}}{\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell \notin s\}}
$
is within $\gamma$ of its expectation, $\Pr_{M^*}[s'(\ell)|a,s]$, with probability at least $1-2e^{-2\gamma^2\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell \notin s\}}=1-\delta'$. Thus, with probability $1-\delta'$, $\Pr_{M^*}[s'(\ell)|a,s]\in K_{\delta'}(s'(\ell)|a,s(\neg\ell))$. 

Similarly, for $\ell\notin\bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'\setminus s$, we observe that if 
$\Pr_{M^*}[s'(\ell)|a,s]\geq \epsilon$
then since each $s'$ is generated independently conditioned on $s$, the probability that $\ell\notin\bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'\setminus s$ is at most
$
(1-\epsilon)^{\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\notin s\}}.
$
So, taking
$
\epsilon = \frac{\ln(1/\delta')}{\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\notin s\}}
$
we find that the probability is at most
$
(1-\epsilon)^{\ln(1/\delta')/\epsilon}
\leq e^{-\epsilon \ln(1/\delta')/\epsilon}
=\delta'.
$

By a union bound over all $2|F|$ literals $\ell$ and $|A|$ $a\in A$, we find that all $\Pr_{M^*}[s'(\ell)|a,s]\in K_{\delta'}(s'(\ell)|a,s(\neg\ell))$ (so the action model is correct) with probability at least $1-\delta$ overall.

For the final part of the claim, we simply note that by Equation~\ref{eq:pre}, since we have taken the upper bound as the precondition for SAM+, if $\ell\in\pre(a)$, $\ell$ is also in the precondition for SAM+. Thus, if $s$ satisfies the SAM+ precondition, $s$ also satisfies $\pre(a)$.
\end{proof}

\noindent
Also, the SAM+ action model can be computed efficiently:
\begin{theorem}[Efficiency]
The SAM+ action model for $m$ examples can be computed in time $O((m+|A|)|F|)$.
\end{theorem}
\begin{proof}
We can compute the SAM+ action model in a single pass over the $m$ triplets, taking time $O(m|F|)$: as in the deterministic SAM learning algorithms, we apply rule 1 to each observed triplet to obtain the preconditions. We also compute counts $\#_a(\ell\in s'\setminus s)$ and $\#_a(\ell\notin s)$. These counts then may be used to produce the intervals given by equations \ref{eq:present-effects} and \ref{eq:missing-effects} for the effects in time $O(|A||F|)$.
\end{proof}

\subsection{SAM+ is approximately complete}

We suppose that the trajectories are obtained by first sampling a planning problem $\Pi$ from a fixed distribution $D$, sampling a policy for $\Pi$ by executing a (possibly probabilistic) planner, and then executing that policy on $\Pi$ in $M^*$.
Approximate completeness means that if the policy used in the sampling distribution achieves the goal of the sampled problem with probability $p$, then the SAM+ model produces policies that solve the sampled problems with probability at least $p-\epsilon$ in $M^*$.

\begin{theorem}[Approximate completeness]
\label{thm:sam-ac}
Fix a distribution on poilicies such that for the distribution $D$ over problems and MDP $M^*$, the policies solve the problems with probability $p$ and runs for $L$ steps in expectation over $M^*$, the draw from $D$, and draw of the policy.
Given 
$
m\geq \frac{4096|A|^2|F|^3L^2}{(1-\epsilon)^4\epsilon^4}\ln\frac{4|F||A|}{\delta}
$
trajectories independently drawn from the policies for problems from $D$, with probability $1-2\delta$, the SAM+ action model satisfies the following:  when a problem $\Pi$ is sampled from $D$ and we execute a policy that has a maximum lower bound of solving $\Pi$ in the SAM+ model, $\Pi$ is solved with probability at least $p-\epsilon$ (over both the draw of $\Pi$ and execution in $M^*$).
\end{theorem}

%%
%% Remarks for us:
%% (1) the dependence on L is suboptimal. It stems from (a) not acknowledging that we get more samples from the effects of actions on literals that are used more frequently and therefore (b) can obtain tighter credal sets for these literals, which requires using a different epsilon for every literal-action pair. We should get a width of the confidence interval that is proportional to the number of times the action-literal pair is used. Summing these over all transitions in the plan then gives us a constant error.
%%
%% (2) the rest of the terms are probably necessary. We do need to observe all of the actions (one factor of |A|) frequently enough that for all fluents, we can estimate the effects of the actions over L steps, thus to an accuracy of epsilon*freq/L|F|, which requires (L/epsilon*freq)^2|F|^2 samples of that fluent-action pair. Note that we get freq*L samples of that fluent-action pair per trajectory, so that should correspond to sampling (L/freq)*(1/epsilon)^2|F|^2 trajectories. Now, one of the actions might be way more common than the rest, which only occur an epsilon/|A| fraction of the time (giving an extra 1/epsilon factor) and there might be settings of fluents that are rarely observed with each action, e.g., with probablity merely ~ epsilon/|A||F| each, giving factors of |A||F|/epsilon. SO, in total, we got our four factors of epsilon, two factors of |A|, and three factors of |F|. All of this is similar to the hard distribution giving the lower bound for VC-dimension. The only part that doesn't quite work out is the dependence on L, which we noted above is suboptimal.
%%

To prove Theorem~\ref{thm:sam-ac}, we first observe that we can afford to ignore actions that are seldom used by the planner to solve problems drawn from $D$; that is, we can restrict our attention to \emph{useful} actions in the following sense:

\begin{definition}
An action $a$ is said to be \emph{$\epsilon$-useful} with respect to an MDP $M^*$, given planner, and a distribution on problems $D$ if with probability at least $1-\epsilon$, $a$ appears in a trajectory sampled by executing the policy in $M^*$ obtained by giving the planner a problem drawn from $D$. 
\end{definition}

\noindent
Indeed, with high probability, problems drawn from $D$ can be solved by policies that only use useful actions.

\begin{lemma}\label{lem:only-useful}
Suppose trajectories sampled as in Theorem~\ref{thm:sam-ac} solve the sampled problem $\Pi$ with probablity $p$.
Then problems sampled from $D$ can be solved by policies that only use $\epsilon/|A|$-useful actions with probability at least $p-\epsilon$.
\end{lemma}
\begin{proof}
Suppose we sample $\Pi$ from $D$ and run the planner on $\Pi$ to obtain a policy $\pi$. Now, we consider the policy $\tilde{\pi}$ obtained by modifying $\pi$ by replacing all actions that are not $\epsilon/|A|$-useful with termination. We observe that conditioned on $\pi$ only producing useful actions, $\tilde{\pi}$ is identical to $\pi$. By a union bound over the inadequate actions, the probability that $\pi$ uses any action that is not $\epsilon/|A|$-useful is at most $\epsilon$.  Thus, $\tilde{\pi}$ solves the random problem $\Pi$ with probability at least $p-\epsilon$, and only uses $\epsilon/|A|$-useful actions, as needed.
\end{proof}

\noindent
Moreover, 
%for the given number of trajectories, 
we can guarantee that we will observe each of the useful actions many times:

\begin{lemma}\label{lem:many-useful}
Among  $m \geq 8|A|\ln\frac{2|A|}{\delta}\frac{1}{\epsilon}$
trajectories, each $\epsilon/|A|$-useful action occurs in at least
$m' \geq \frac{\epsilon}{2|A|}m$
of the trajectories with probability at least $1-\delta/2$
\end{lemma}
\begin{proof}
Consider any $\epsilon/|A|$-useful action $a$. For each of the $m$ trajectories, we define an indicator random variable indicating whether or not $a$ is used in that trajectory. Since $a$ is $\epsilon/|A|$-useful, these random variables all have expected value at least $\epsilon/|A|$. Since each of the trajectories are sampled independently, these random variables are mutually independent, and by a Chernoff bound, the probability that $a$ appears in fewer than $\frac{\epsilon}{2|A|}m$ of the trajectories is at most
$
e^{-\frac{1}{2\cdot 2^2}m\frac{\epsilon}{|A|}}\leq\frac{\delta}{2|A|}
$
Now, taking a union bound over the $\epsilon/|A|$-useful actions, we find that they all appear at least $\frac{\epsilon}{2|A|}m$ times with probability at least $1-\delta/2$, as needed.
\end{proof}

In particular, the widths of the credal sets we create for the SAM+ model shrink as we observe the actions more frequently; this enables us to ensure that the SAM+ model is \emph{adequate} for accurate planning in the following sense:

\begin{definition}
We say that the the SAM+ action model  is \emph{$(\epsilon_1,\epsilon_2)$-adequate} for action $a$ if with probability $1-\epsilon_1$ over trajectories sampled as in Theorem~\ref{thm:sam-ac},
\begin{compactenum}
\item  the SAM+ precondition for $a$ is satisfied for all states $s$ where $a$ is used in the trajectory
\item The width of the credal sets $K_{\delta'}(\ell,a)$ is at most $\epsilon_2$ for all $\ell$ where $a$ is used in the trajectory.
\end{compactenum}
\end{definition}

\begin{lemma}\label{lem:sam-adequate}
If $\mathcal{T}(a)$ contains at least
$
m' \geq \frac{16|A||F|}{\epsilon_1\epsilon_2^2}\ln\frac{2}{\delta'}
$
triplets, then the SAM+ action model for $a$ is
$(\frac{\epsilon_1}{|A|},\epsilon_2)$-adequate with probability $1-\delta/2|A|$
\end{lemma}
\begin{proof}
For the first condition, observe that if a literal $\ell$ is not in $\pre(a)$ and occurs in some state $s$ of a sampled trajectory with probability greater than $\frac{\epsilon_1}{4|A||F|}$, then the probability that $\ell$ is in the SAM+ precondition for $a$ is at most
$
(1-\frac{\epsilon_1}{4|A||F|})^{m'}\leq \frac{\delta}{8|A||F|}
$
Thus, taking a union bound over all $2|F|$ literals, we find that with probability $1-\delta/8|A|$, no such literals are present in the SAM+ precondition for $a$. Now, moreover, also by a union bound over the literals not in $\pre(a)$, the probability that any of the literals $\ell$ not in $\pre(a)$ that occur in some state $s$ of a sampled trajectory with probability greater than $\frac{\epsilon_1}{4|A||F|}$ actually occur in a state $s$ where $a$ is used in a sampled trajectory is at most $\epsilon_1/2|A|$.

For the second condition, we observe that for each literal $\ell$, the width of the interval for $a$ is at most $\sqrt{\frac{2\ln(2/\delta')}{\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\notin s\}}}$. In particular, consider any $\ell$ that is is not in $\pre(a)$ such that $\ell$ does not occur in some state $s$ of a sampled trajectory with probability greater than $\frac{\epsilon_1}{4|A||F|}$. If we consider the indicator random variable for whether a sampled trajectory contains a triplet in which $a$ is taken in some state $s$ such that $\ell\notin s$, we see that these are independent random variables with expected value at least $\frac{\epsilon_1}{4|A||F|}$. So, by a Chernoff bound, with probability $1-\frac{\delta}{8|A||F|}$, at least $\frac{\epsilon_1}{8|A||F|}m'=\frac{2}{\epsilon_2^2}\ln\frac{2}{\delta'}$ out of the $m'$ triplets have $s$ such that $\ell\notin s$. Hence, the width of the credal set for $\ell$ and $a$ is at most
$
\sqrt{\frac{2\ln(2/\delta')}{2\ln(2/\delta')/\epsilon_2^2}}=\epsilon_2
$
as needed. By a union bound over all literals now, with probability $1-\delta/4|A|$ the probability that any of the credal sets is wider than $\epsilon_2$ for any of the literals that occur in any of the states in any of the trajectories a sampled trajectory with probability greater than $\frac{\epsilon_1}{4|A||F|}$. In turn, by a union bound over the literals that occur with probability less than $\frac{\epsilon_1}{4|A||F|}$ when $a$ is taken, the probability that any of these occur in any state of a sampled trajectory where $a$ is taken is at most $\frac{\epsilon_1}{2|A|}$. Thus, with probability $1-\frac{\epsilon_1}{2|A|}$, all of the credal sets have width at most $\epsilon_2$.

By union bounds over the two cases, with probability $1-\frac{\delta}{2|A|}$, we obtain a SAM+ action model such that for a sampled trajectory, each condition holds with probability $\frac{\epsilon_1}{2|A|}$; by another union bound over the conditions, both simultaneously hold in the trajectory with probability $\frac{\epsilon_1}{|A|}$.
\end{proof}

\noindent
Adequate action models include solutions to problems from $D$ with success rates similar to the training distribution:

\begin{lemma}\label{lem:sam-success}
Suppose that the SAM+ action model is $(\frac{\epsilon_1}{|A|},\epsilon_2)$-adequate for all $\frac{\epsilon_3}{|A|}$-useful actions. Then with probability $1-\delta$, if the sampling distribution solves the problems from $D$ with probability $p$ while taking $L$ steps in expectation, executing policies obtained from a planner using the SAM+ action model yields a distribution on trajectories in which the problems are solved with probability at least $p-(\epsilon_1+\frac{\epsilon_2\cdot L\cdot |F|}{(1-\epsilon_1)(1-\epsilon_3)}+\epsilon_3)$.
Moreover, these policies may only take actions for which the width of the credal set for all literals is at most $\epsilon_2$ for the states in which they are invoked.
\end{lemma}
\begin{proof}
We start by extending Lemma~\ref{lem:only-useful}: for the policies $\pi$ produced by the planner for problems sampled from $D$, we consider $\tilde{\pi}$ that executes $\pi$ until it would either take an action that is not $\frac{\epsilon_3}{|A|}$-useful, or would invoke an action in a state for which some literal has a credal set of width greater than $\epsilon_2$, and terminates in either of these conditions. We observe that $\tilde{\pi}$ has trajectories that
are no longer than $\pi$ in any sample. Moreover, by a union bound over the actions that are not $\frac{\epsilon_3}{|A|}$-useful, the original distribution over trajectories only produces a trajectory that uses any of these actions with probability at most $\epsilon_3$. Since by hypothesis, the action model is $(\frac{\epsilon_1}{|A|},\epsilon_2)$-adequate for all the useful actions, the probability that $\pi$ would take one of these actions in a state where the credal sets are wider than $\epsilon_2$ for any literal is at most $\frac{\epsilon_1}{|A|}$; by a union bound over the useful actions, we find thus that with probability $1-(\epsilon_1+\epsilon_3)$, neither of these events occurs and $\pi$ produces an execution that is identical to that produced by $\tilde{\pi}$. In particular, with probability at least $p-(\epsilon_1+\epsilon_3)$, $\tilde{\pi}$ must also solve the problem $\Pi$ sampled from $D$.

Since by construction, $\tilde{\pi}$ only takes actions for which the width of the credal set is at most $\epsilon_2$, we can bound the expected lower probability guarantee $\tilde{p}$ obtained by the SAM+ action model for $\tilde{\pi}$ as follows.
\begin{small}
\begin{align*}
\tilde{p} &\geq \sum_{\Pi,\pi}\Pr_{D,P}[\Pi,\pi]\sum_{\substack{\text{trajectories }T\\\text{of }\tilde{\pi}\text{ solving }\Pi}}\Pr_{\tilde{\pi},M}[T|\Pi]\\
&=\sum_{\Pi,\pi}\Pr_{D,P}[\Pi,\pi]\sum_{k=0}^\infty\sum_{\substack{\text{trajectories }T\\\text{of }\tilde{\pi}\text{ solving}\\
\Pi\text{ in }k\text{ steps}}}\prod_{i=1}^k\Pr_{\tilde{\pi},M}[T_i|T_{i-1}]\\
%&\geq \sum_{\Pi,\pi}\Pr_{D,P}[\Pi,\pi]\sum_{k=0}^\infty\sum_{\substack{\text{trajectories }T\\\text{of }\tilde{\pi}\text{ solving}\\\Pi\text{ in }k\text{ steps}}}\prod_{i=1}^k(\Pr_{\tilde{\pi},M^*}[T_i|T_{i-1}]-\epsilon_2|F|)\\
%&\geq \sum_{\Pi,\pi}\Pr_{D,P}[\Pi,\pi]\sum_{k=0}^\infty\sum_{\substack{\text{trajectories }T\\\text{of }\tilde{\pi}\text{ solving}\\\Pi\text{ in }k\text{ steps}}}(1-k\epsilon_2|F|)\prod_{i=1}^k\Pr_{\tilde{\pi},M^*}[T_i|T_{i-1}]\\
&\geq \sum_{\Pi,\pi}\Pr_{D,P}[\Pi,\pi]\sum_{k=0}^\infty\sum_{\substack{\text{trajectories }T\\\text{of }\tilde{\pi}\text{ solving}\\
\Pi\text{ in }k\text{ steps}}}(1-k\epsilon_2|F|)\Pr_{\tilde{\pi},M^*}[T|\Pi]\\
&\geq p-\frac{L}{(1-\epsilon_1)(1-\epsilon_3)}\epsilon_2|F|-(\epsilon_1+\epsilon_3)
\end{align*}
\end{small}

Since the planner chooses a policy that obtains the largest lower probability bound, in particular it obtains a lower bound that is at least that of $\tilde{\pi}$ in all executions. Since by Theorem~\ref{thm:sam-safe} the SAM+ action model is correct with probability $1-\delta$, the policies obtained from executing the planer on the SAM+ action model indeed obtain a probability of success that is at least the expected lower bound on $\tilde{\pi}$, $p-(\epsilon_1+\frac{\epsilon_2\cdot L\cdot |F|}{(1-\epsilon_1)(1-\epsilon_3)}+\epsilon_3)$ with probability $1-\delta$.
\end{proof}

\noindent
By the above reasoning, we obtain Theorem~\ref{thm:sam-ac}. 
A formal version is included in the supplemental material.

%Formally:

%\begin{proof} (of Theorem \ref{thm:sam-ac})
%We first note that the quoted number of actions is sufficient to obtain that by Lemma~\ref{lem:many-useful}, for $\epsilon_3 = \epsilon/4$, each of the $\epsilon_3/|A|$-useful actions occurs in at least $m'\geq \frac{512 |A||F|^2L^2}{(1-\epsilon)^2\epsilon^3}\log\frac{4|F||A|}{\delta}$ of the traces with probability at least $1-\delta/2$. In turn, by Lemma~\ref{lem:sam-adequate}, for each of these actions, for $\epsilon_1 = \epsilon/4$ and $\epsilon_2 = \frac{\epsilon(1-\epsilon)^2}{2|F|L}$, the $\epsilon_3/|A|$-useful actions have $(\epsilon_1/|A|,\epsilon_2)$-adequate action models with probability $1-\delta/2|A|$ each; a union bound over these actions gives that overall, they are simultaneously adequate with probability $1-\delta/2$, and hence with probability $1-\delta$ overall, we have an adequate action model for all of the $\epsilon_3/|A|$-useful actions. So finally, by Lemma~\ref{lem:sam-success}, with probability $1-2\delta$ overall, problems drawn from $D$ are solved with probability at least $p-\epsilon$ by policies that maximize the lower bound on success probability under the SAM+ action model, as claimed.
%\end{proof}

\subsection{SAM+ with PPDDL Planners}
Actually, Lemma~\ref{lem:sam-success} gives us slightly more than needed for Theorem~\ref{thm:sam-ac}: the policy only needs to consider actions for which the credal sets are narrow. So, we can take the midpoint of the credal set as an estimate of the probability in a standard PPDDL encoding, if we include 
preconditions
%guard conditional effects 
that prevent a solution from executing actions that would lead to wide credal sets for any of the factors. In more detail: First, let us suppose $L'$ is an {\em upper bound} on the lengths of plans we consider. For $\ell\in\bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'\setminus s $, $\ell\in\pre(a)$ iff 
\begin{small}
\begin{equation*}
\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\notin s\}< \frac{8|F|^2L'^2}{(1-\epsilon)^4\epsilon^2}\ln\frac{4|F||A|}{\delta},
\end{equation*}
\end{small}
and otherwise (for $\ell\notin\bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'\setminus s $),  $\ell\in\pre(a)$ iff 
\begin{small}
\begin{equation*}
\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\notin s\}<\frac{2|F|L'}{\epsilon(1-\epsilon)^2}\ln\frac{2|F||A|}{\delta}.
\end{equation*}
\end{small}
We note that the original preconditions correspond to the set of literals $\ell$ such that $\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\notin s\}=0$, so this is a superset, i.e., a stricter precondition. Moreover, for these literals, we indeed have respectively that the widths of the confidence intervals for literals {\em not} included in the precondition are at most $\frac{\epsilon(1-\epsilon)^2}{2|F|L'}=\epsilon_2$.

Now, instead of the credal sets, we use the following factors for the transition probabilities: For $\ell\in\bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'\setminus s $, the transition probability factor for $\ell$ given $\ell\notin s$ and $a$ is an empirical estimate of the probability:
\begin{equation}
\small
\Pr[s'(\ell)|a,s(\neg\ell)]=\frac{\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\in s'\setminus s\}}{\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell \notin s\}}
\end{equation}
and otherwise (i.e., for $\ell\notin\bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'\setminus s $),
\begin{equation}
\small
\Pr[s'(\ell)|a,s(\neg\ell)]=\frac{\ln(2|F||A|/\delta)}{2\#\{\tuple{s, a, s'}\in \mathcal{T}(a):\ell\notin s\}}
\end{equation}
i.e., the midpoints of our previous credal sets.

Whereas the credal sets contained the true transition probabilities with high probability, we now only have that our point estimates of these transition probabilities are $\epsilon_2/2$-close to the true probabilities on each transition. We can thus (only) guarantee an approximate form of safety:
\begin{theorem}[Approximate safety---PPDDL]
\label{thm:ppddl-sam-safe}
With probability $1-\delta$, the SAM+ PPDDL action model satisfies the following:
the probability any plan of length at most $L'$ succeeds in the SAM+ PPDDL model is at most $(1+\epsilon)$ times greater than under the true model $M^*$. In particular, all actions that are applicable in a plan under the SAM+ PPDDL model are applicable in $M^*$.
\end{theorem}
\begin{proof}
We recall that by Theorem~\ref{thm:sam-safe}, $M^*$ is contained in the credal sets $K_\delta$ used by our imprecise MDP model. The midpoints of the credal sets are, by construction, at most $\epsilon_2/2$-far from the true probability for each factor. Thus, our estimated probability of success $\hat{p}$ for the policy $\pi$ for the problem $\Pi$ is at most
\begin{small}
\begin{align*}
\hat{p} &\leq 
%\sum_{\substack{\text{trajectories }T\\\text{of }\pi\text{ solving }\Pi}}\Pr_{\pi,M}[T|\Pi]\\
%&=
\sum_{k=0}^{L'}\sum_{\substack{\text{trajectories }T\\\text{of }\pi\text{ solving}\\
\Pi\text{ in }k\text{ steps}}}\prod_{i=1}^k\Pr_{\pi,M}[T_i|T_{i-1}]\\
%&\leq \sum_{k=0}^{L'}\sum_{\substack{\text{trajectories }T\\\text{of }\pi\text{ solving}\\\Pi\text{ in }k\text{ steps}}}\prod_{i=1}^k(\Pr_{\pi,M^*}[T_i|T_{i-1}]+|F|\epsilon_2/2)\\
&\leq \sum_{k=0}^{L'}\sum_{\substack{\text{trajectories }T\\\text{of }\pi\text{ solving}\\\Pi\text{ in }k\text{ steps}}}(1+|F|\epsilon_2/2)^k\prod_{i=1}^k\Pr_{\pi,M^*}[T_i|T_{i-1}]\\
&\leq \sum_{k=0}^{L'}\sum_{\substack{\text{trajectories }T\\\text{of }\pi\text{ solving}\\
\Pi\text{ in }k\text{ steps}}}(1+k|F|\epsilon_2/2+\left(\frac{k|F|\epsilon_2}{2}\right)^2)\Pr_{\pi,M^*}[T|\Pi]\\
&\leq (1+\epsilon)p
\end{align*}
\end{small}
where the second to last line used
$
1+x\leq e^x\leq 1+x+x^2
$
for all $x\leq 1$; note here that we have chosen $\epsilon_2=\frac{\epsilon}{L'|F|}$ so that $k|F|\epsilon_2\leq \epsilon < 1$ for all $k\leq L'$, so we can invoke this inequality, and $(\epsilon/2)^2<\epsilon/2$. The moreover part follows directly from Theorem~\ref{thm:sam-safe} and the fact that the PPDDL preconditions are only stronger than the original.
\end{proof}

In turn, we obtain the following guarantee for completeness, essentially analogously to the original argument:
\begin{theorem}[Approximate completeness---PPDDL]
\label{thm:ppddl-sam-ac}
Fix a planner, and suppose that for the distribution $D$ over problems and MDP $M^*$, the planner produces a policy that solves the problems with probability $p$ and runs for $L$ steps in expectation over $M^*$, the draw from $D$, and the planner itself.
Given 
$
m\geq \frac{4096|A|^2|F|^3L^2}{(1-\epsilon)^4\epsilon^4}\ln\frac{4|F||A|}{\delta}
$
trajectories independently drawn from the planner on problems from $D$, with probability $1-2\delta$, the SAM+ action model satisfies the following:  when a problem $\Pi$ is sampled from $D$ and we execute a policy of length at most $\frac{1}{\epsilon}L$ that maximizes the probability of solving $\Pi$ in the SAM+ PPDDL model with $L'=\frac{1}{\epsilon}L$, $\Pi$ is solved with probability at least $p-3\epsilon$ (over both the draw of $\Pi$ and execution in $M^*$).
\end{theorem}
%\begin{proof}
%We first note that by Markov's inequality, that if $\tilde{\pi}$ runs for $L$ steps in expectation, then with probability $1-\epsilon$ it runs for at most $\frac{1}{\epsilon}L$ steps. Theorem~\ref{thm:ppddl-sam-safe} guarantees that $p$ is overestimated by at most $\epsilon$ on all policies that execute for at most $\frac{1}{\epsilon}L$ steps. In turn, the success probability of an optimal policy is underestimated by at most $\epsilon$, following the original proof of Theorem~\ref{thm:sam-ac}. All together, we find that with probability $p-\epsilon$, the optimal policy solves the problem $\Pi$ in at most $\frac{1}{\epsilon}L$ steps, and achieves an estimated probability under $M$ that is at most $\epsilon$ smaller. Hence, the optimal policy under $M$ that overestimates by at most $\epsilon$ is succeeding with probability at least $p-3\epsilon$, as claimed. 
%\end{proof}


% I created a brief add'l related work section at the end of the intro. The intro already discusses quite a bit of related work, and I don't feel like we can afford to repeat ourselves.
%
%\section{Related Work}
% 
%
%A Review of Learning Planning Action Models
%
%Learning action models in general ...
%
%
%Learning STRIPS Operators from Noisy and Incomplete Observations
%- not like us -> we have complete obs but world is stochastic
%
%
%TODO: Learning action models is an noisy STRIPS rules ...
%TODO: Learning action models in RDDL ...
%
%
%TODO: Learning dynamic bayesian network ...


\section{Related Work}




% Existing approaches in the RL world are not applicable for symbolic planning. 
This may be viewed as a difficulty with ``exploring'' the environment (the state with the open lock is hard to reach) or a difficulty with ``generalizing'' across states (an action suddenly opens the lock in the state where the correct combination is entered). The methods for circumventing such problems in RL frequently involve assuming some kind of linear structure on the reward or value function \cite{osband2014,osband2016,jin2020,yang2019}, which prevents them from capturing STRIPS-style goals, which are given by a possibly large conjunction of the fluent settings. 
By contrast, work on Model-Based RL (MBRL)~\cite{kearns1999efficient,koller2000policy,strehl2007efficient,diuk2009adaptive}, attempts to fit the environment dynamics to a specific model class; classically, these were based on Dynamic Bayesian Network models~\cite{dean1989model} of the transition distribution, which may be much more compact, by assuming that the various factors are generated independently. 

A problem with even MBRL algorithms -- and in particular, for the aforementioned methods for MBRL -- is that they still incentivize an agent to visit unexplored portions of the state space (w.r.t.\ the conditional probability tables), which may be \emph{unsafe}. 
This has posed an obstacle to the adoption of RL in practice; in turn, it has motivated recent work on \emph{offline RL}~\cite{levine2020offline,kidambi2020morel,yu2020mopo}, which seeks to learn policies, in particular \emph{safe} policies, using a training set of trajectories collected in the domain. These trajectories might, for example, be provided by a human demonstrating good behavior. 
Safety here generally means that we guarantee with high probability that the policy will remain in some set of ``safe'' states \cite{thomas2015safe,thomas2019preventing}. These methods are cast in a traditional RL, discounted-reward setting, and simply charge a penalty for uncertainty, which is not a good match to the reachability goals we consider.
%But, a difficulty with offline approaches to safe learning is that they necessarily must sometimes abstain from proposing a policy, if the data is not sufficient to guarantee a goal can be safely reached. 
Moreover, how and when can we hope to guarantee that the model transfers to other goals? 



In addition to the works discussed above, there has been work on learning noisy STRIPS operators from incomplete observations~\cite{pasula2007learning,mourao2012learning,rodrigues2011incremental,ng2019incremental}. These works propose methods for learning PPDDL representations from incomplete observations, a more demanding setting than ours. But, they do not provide any theoretical guarantees. Similarly, \citet{martinez2017relational} consider learning richer probabilistic domain models that also incorporate exogenous effects; but, they do not have time or sample complexity guarantees, nor do they connect the objective they optimize to the quality of the model.

\section{Conclusion and Future Work}
We have shown how to safely and efficiently acquire models of stochastic domains that nevertheless generalize to goals beyond those for which solutions were previously demonstrated. Although the style of domain model we used here is a relatively limited fragment of PPDDL that uses propositional fluents and assumes all fluents transition independently, the technique is not inherently limited to such models. It is straightforward to extend this algorithm using the approach of \citet{juba2021kr} to learn lifted domain models, at least when the ``injective binding assumption'' holds, i.e., when two parameters of the same type are not bound to the same object in the example trajectories. We believe it should be possible to extend this further to handle the kind of ``deictic'' references considered by \citet{pasula2007learning}. It is also straightforward to extend to domains in which the transitions depend on a constant-size set of attributes in the previous state, using the technique of \citet{strehl2007efficient}. This captures a family of conditional effects, in which any given fluent only has conditional effects depending on a small family of other fluents. Or, indeed, it may similarly be extended to domains in which the fluents take values from a larger discrete set. In turn, this allows us to consider domains in which there are dependencies among small sets of fluents, by treating the entire block as a vector-valued fluent. 

There are, of course, limits to how far we may extend the approach: learning arbitrary graphical model representations with unknown structures is known to be hard in various senses~\cite{chickering1996learning,chickering2004large}. But, concretely, it is still not clear how rich a fragment of PPDDL we can learn efficiently: Can we learn products of arbitrary small-support distributions? Can we learn exogenous effects?  What about models of domains with continuous state spaces? We leave these to future work.

\bibliography{library}

\newpage\ 
\newpage

\noindent
For completeness, we include here full proofs of Theorems \ref{thm:sam-ac} and \ref{thm:ppddl-sam-ac}:

\begin{proof} (of Theorem \ref{thm:sam-ac})
We first note that the quoted number of actions is sufficient to obtain that by Lemma~\ref{lem:many-useful}, for $\epsilon_3 = \epsilon/4$, each of the $\epsilon_3/|A|$-useful actions occurs in at least
$
m'\geq \frac{512 |A||F|^2L^2}{(1-\epsilon)^2\epsilon^3}\log\frac{4|F||A|}{\delta}
$
of the traces with probability at least $1-\delta/2$. In turn, by Lemma~\ref{lem:sam-adequate}, for each of these actions, for $\epsilon_1 = \epsilon/4$ and $\epsilon_2 = \frac{\epsilon(1-\epsilon)^2}{2|F|L}$, the
$\epsilon_3/|A|$-useful actions have $(\epsilon_1/|A|,\epsilon_2)$-adequate action models with probability $1-\delta/2|A|$ each; a union bound over these actions gives that overall, they are simultaneously adequate with probability $1-\delta/2$, and hence with probability $1-\delta$ overall, we have an adequate action model for all of the $\epsilon_3/|A|$-useful actions. So finally, by Lemma~\ref{lem:sam-success}, with probability $1-2\delta$ overall, problems drawn from $D$ are solved with probability at least $p-\epsilon$ by policies that maximize the lower bound on success probability under the SAM+ action model, as claimed.
\end{proof}

\begin{proof} (of Theorem~\ref{thm:ppddl-sam-ac})
We first note that by Markov's inequality, that if $\tilde{\pi}$ runs for $L$ steps in expectation, then with probability $1-\epsilon$ it runs for at most $\frac{1}{\epsilon}L$ steps. Theorem~\ref{thm:ppddl-sam-safe} guarantees that $p$ is overestimated by at most $\epsilon$ on all policies that execute for at most $\frac{1}{\epsilon}L$ steps. In turn, the success probability of an optimal policy is underestimated by at most $\epsilon$, following the original proof of Theorem~\ref{thm:sam-ac}. All together, we find that with probability $p-\epsilon$, the optimal policy solves the problem $\Pi$ in at most $\frac{1}{\epsilon}L$ steps, and achieves an estimated probability under $M$ that is at most $\epsilon$ smaller. Hence, the optimal policy under $M$ that overestimates by at most $\epsilon$ is succeeding with probability at least $p-3\epsilon$, as claimed. 
\end{proof}

\end{document}



 %nature of the effects we conisder.  an effect  



% Juba et al.~\cite{juba2021kr,stern2017efficientAndSafe} proposed an algorithm called Safe Action Model (SAM) learning that outputs a PDDL action model that is complete safe .... TODO BACKGROUND
% But, they are more challenging in our domain. 
% We provide here a brief 








% What are trajectories, action triplets


% % [good text, but should move to the intro ] A common approach in such a model-free setting is to learn the value of performing different actions in a trial-and-error fashion, performing exploratory action in the domain that may or may not fail. Our work is primarily motivated by domains in which performing actions in the domain is extremely expensive. 
% Instead, it is given a set of \emph{trajectories} $\mathcal{T}=\{\mathcal{T}_1,\ldots, \mathcal{T}_m\}$ created by executing policies designed to solve problems 
% $\{ \Pi_1,\ldots,\Pi_m \}$ in the same PPDDL domain, i.e., 
% the agent knows that $D(\Pi)=D(\Pi_1)=\cdots=D(\Pi_m)$. 
% What are trajectories, action triplets
% Safety as a primary goal, we want to avoid a trial-and-error
% A common approach in 

% \begin{definition}[$\epsilon$-Safe Action Model]

% \end{definition}



% a sequence of grounded actions that can be applied to $s_I$ and if applied to $s_I$ results in a state $s_G$ that contains all the grounded literals in $G$. 
% Such a sequence of grounded actions is called a \emph{plan}. 
% The trajectory of a plan starts with $s_I$ and ends with a goal state $s_G$ (where $G\subseteq s_G$). 
% The \emph{safe model-free planning} problem~\cite{stern2017efficientAndSafe} is defined as follows. 
% \begin{definition}[Safe model-free planning]
% Let $\Pi=\tuple{\tuple{T, \mathcal{F}, \mathcal{A}, \realm}, O, s_I, G}$ be a classical planning problem and let $\mathcal{T}=\{\mathcal{T}_1,\ldots, \mathcal{T}_m\}$ be a set of trajectories %of plans 
% for other planning problems in the same domain. 
% The input to a safe model-free planning algorithm is the tuple $\tuple{T,O, s_I, G, \mathcal{T}}$ and the desired output is a plan $\pi$ that is a solution to $\Pi$. We denote this safe model-free planning problem as $\Pi_{\mathcal{T}}$. 
% \label{def:safe-model-free-planning}
% \end{definition}
% We refer to the action model $\realm$ as the real action model. 
% The trajectories in $\mathcal{T}$ share the same domain as $\Pi$, 
% and thus they have been generated by applying actions from $\mathcal{A}$ 
% and following the action model specified in $\realm$. 
% However, these trajectories may start in states that are not from $s_I$, 
% may end in states that do not satisfy $G$, 
% and may consider a set of objects that is different from $O$.  
% Safety is captured in Definition~\ref{def:safe-model-free-planning} by requiring that the output plan $\pi$ is a \textbf{sound plan} for $\Pi$. That is, $\pi$ is applicable and ends up reaching a state that satisfies the goal.  
% The main challenge is that the problem-solver -- the agent -- needs to find a sound plan to $\Pi$ but it is not given the set of fluents, actions, and action model of the domain ($\mathcal{F}$, $\mathcal{A}$, and \realm, respectively). 




% % Objects and types
% Let $O$ be a set of objects and let $T$ be a set of types. 
% Every object $o\in O$ is associated with a type $t\in T$ denoted $\type(o)$. 
% For example, in the logistics domain from the International Planning Competition (IPC)~\cite{ipc} there are types \emph{truck} and \emph{location} and there may be objects $t_1$ and $t_2$ that represent two different trucks and two objects $l_1$ and $l_2$ that represent two different locations. 

% \subsection{Lifted and Grounded Literals}
% % Lifted and grounded fluent
% A \emph{lifted fluent} $\liftf$ is a pair $\tuple{\name, \params}$ representing a relation over typed objects, where  $\name$ is a symbol and $\params$ is a list of types. 
% We denote the name of $\liftf$ and its parameters by $\name(\liftf)$ and $\params(\liftf)$ respectively, and $arity(\liftf,t)$ denotes the number of type-$t$ parameters. 
% % relation over a list of  \emph{types}.  
% % These types are called the \emph{parameters} of $\liftf$ and denoted by $\params(\liftf)$. 
% For example, in the logistics domain $at(?truck, ?location)$ is a lifted fluent that represents which trucks ($?truck$) are at which locations ($?location$). 
% A \emph{binding} of a lifted fluent $\liftf$ is a function $b: \params(\liftf)\rightarrow O$ 
% mapping every parameter of $\liftf$ to an object in $O$ of the indicated type. 
% A \emph{grounded fluent} $f$ is a pair $\tuple{\liftf, b}$ where $\liftf$ is a lifted fluent 
% and $b$ is a binding for \liftf. 
% To \emph{ground} a lifted fluent $\liftf$ with a binding $b$ means to 
% create a (Boolean-valued) fluent with a value determined by whether or not the objects in the image of $b$ satisfy the relation associated with the lifted fluent. 
% In our logistics example, for $\liftf=at(?truck, ?location)$ and $b=\{?truck: truck1, ?location: loc1\}$ 
% the corresponding grounded fluent $f$ is $at(truck1, loc1)$, indicating whether $truck1$ is at $loc1$.
% The term \emph{literal} refers to either a fluent or its negation. 
% The definitions of binding, lifted, and grounded fluents transfer naturally to literals. 
% A \emph{state} of the world is a set of grounded literals that, for every grounded fluent, either includes that fluent or its negation. 


% % Actions, parameter binding, grounded action
% \subsection{Lifted and Grounded Actions}
% A lifted action $\lifta\in \mathcal{A}$ is a pair $\tuple{\name, \params}$ 
% where $\name$ is a symbol and $\params$ is a list of types, 
% denoted $\name(\lifta)$ and $\params(\lifta)$, respectively, and $arity(\lifta,t)$ denotes the number of type-$t$ parameters. 
% The action model $M$ for a set of actions $\mathcal{A}$ 
% is a pair of functions $\pre_M$ and $\eff_M$ that map every action in $\mathcal{A}$ to its preconditions and effects. 
% To define the preconditions and effects of a lifted action, 
% we first define the notion of a \emph{parameter-bound literal}. 
% A \emph{parameter binding} of a lifted literal $\liftl$ and an action $\lifta$ is a function $b_{\liftl,\lifta}: \params(\liftl)\rightarrow \params(\lifta)$ that maps every parameter of $\liftl$ to a parameter in $\lifta$. 
% A \emph{parameter-bound literal} $l$ for the lifted action $\lifta$ is a 
% pair of the form $\tuple{\liftl,b_{\liftl,\lifta}}$ where $b_{\liftl,\lifta}$ is a parameter binding of $\liftl$ and $\lifta$. 
% $\pre_M(\lifta)$ and $\eff_M(\lifta)$ are sets of parameter-bound literals for $\lifta$. 



% A \emph{binding} of a lifted action $\lifta$ is defined like a binding of a lifted fluent, i.e., a function $b:\params(\lifta)\rightarrow O$. 
% A \emph{grounded action} $a$ is a tuple $\tuple{\lifta, b_\lifta}$ where $\lifta$ is a lifted action and $b_\lifta$ is a binding of $\lifta$. 
% The preconditions of a grounded action $a$ according to the action model $M$, denoted $\pre_M(a)$, is the set of grounded literals created by taking every parameter-bound literal $\tuple{\liftl, b_{\liftl,\lifta}}\in \pre_M(\lifta)$ and grounding $\liftl$ with the binding $b_\lifta\circ b_{\liftl,\lifta}$. 
% The effects of a grounded action $a$, denoted $\eff_M(a)$, are defined in a similar manner. 
% The grounded action $a$ can be applied in a state $s$ iff $\pre_M(a)\subseteq s$. 
% The outcome of applying $a$ to a state $s$ according to action model $M$, denoted $a_M(s)$, is a new state that contains all literals in $\eff_M(a)$ and all the literals in $s$ such that their negation is not in $\eff_M(a)$. 
% Formally:
% \begin{equation}\small
%     a_M(s)=\{ l | (l\in s \wedge \neg l\notin \eff_M(a)) \vee l\in \eff_M(a) \} 
% \end{equation}
% We omit $M$ from $a_M(s)$ when it is clear from the context.
% The outcome of applying a sequence of grounded actions $\pi=(a_1,\ldots a_n)$ to a state $s$ is the state $s'=a_n(\cdots a_1(s)\cdots)$. 
% A sequence of actions $a_1,\ldots, a_n$ can be applied to a state $s$ 
% if for every $i\in 1,\ldots,n$ the action $a_i$ is applicable in the state 
% $a_{i-1}(\cdots a_1(s)\cdots)$. 

% \begin{definition}[Trajectory]
% A trajectory $T=\tuple{s_0, a_1, s_1, \ldots a_n, s_n}$ is an alternating sequence of states $(s_0,\ldots,s_n)$ and actions $(a_1,\ldots,a_n)$ that starts and ends with a state.
% \end{definition}
% The trajectory created by applying $\pi$ to a state $s$ is 
% the sequence $\tuple{s_0, a_1, \ldots, a_{|\pi|}, s_{|\pi|}}$ such that 
% $s_0=s$ and for all $0<i\leq |\pi|$, $s_i=a_i(s_{i-1})$. 
% In the literature on learning action models~\cite{wang1994learning,wang1995learning,walsh2008efficient,stern2017efficientAndSafe,arora2018review}, 
% it is common to represent a trajectory 
% $\tuple{s_0, a_1, \ldots, a_{|\pi|}, s_{|\pi|}}$
% as a set of triples 
% $\big\{\tuple{s_{i-1},a_i,s_i}\big\}_{i=1}^{|\pi|}$.
% Each triple $\tuple{s_{i-1},a_i,s_i}$ is called an \emph{action triplet},  and the states $s_{i-1}$ and $s_i$ are referred to as the pre- and post- state of action $a_i$. 
% We denote by $\mathcal{T}(a)$ the set of all action triplets in the trajectories in $\mathcal{T}$ that include the grounded action $a$. $\mathcal{T}(\lifta)$ is defined for all action triplets that contain actions that are groundings of the lifted action $\lifta$.  

% \subsection{Domains and Problems}

% % Planning domain
% A classical planning \textbf{domain} is defined by a tuple 
% $\tuple{T, \mathcal{F}, \mathcal{A}, M}$
% where $T$ is a set of types, 
% $\mathcal{F}$ is a set of lifted fluents, 
% $\mathcal{A}$ is a set of lifted actions, 
% and $M$ is an action model for $\mathcal{A}$.
% % Objective
% A classical planning \textbf{problem} is defined by a tuple $\tuple{D, O,  s_I, G}$ where $D$ is a classical planning domain;  
% $O$ is a set of objects; 
% $s_I$ is the start state, i.e., the state of the world before planning;  
% and $G$ is a set of grounded literals that define when the goal has been found. 
% A \textbf{solution} to a planning problem is a sequence of grounded actions that can be applied to $s_I$ and if applied to $s_I$ results in a state $s_G$ that contains all the grounded literals in $G$. 
% Such a sequence of grounded actions is called a \emph{plan}. 
% The trajectory of a plan starts with $s_I$ and ends with a goal state $s_G$ (where $G\subseteq s_G$). 
% The \emph{safe model-free planning} problem~\cite{stern2017efficientAndSafe} is defined as follows. 
% \begin{definition}[Safe model-free planning]
% Let $\Pi=\tuple{\tuple{T, \mathcal{F}, \mathcal{A}, \realm}, O, s_I, G}$ be a classical planning problem and let $\mathcal{T}=\{\mathcal{T}_1,\ldots, \mathcal{T}_m\}$ be a set of trajectories %of plans 
% for other planning problems in the same domain. 
% The input to a safe model-free planning algorithm is the tuple $\tuple{T,O, s_I, G, \mathcal{T}}$ and the desired output is a plan $\pi$ that is a solution to $\Pi$. We denote this safe model-free planning problem as $\Pi_{\mathcal{T}}$. 
% \label{def:safe-model-free-planning}
% \end{definition}
% We refer to the action model $\realm$ as the real action model. 
% The trajectories in $\mathcal{T}$ share the same domain as $\Pi$, 
% and thus they have been generated by applying actions from $\mathcal{A}$ 
% and following the action model specified in $\realm$. 
% However, these trajectories may start in states that are not from $s_I$, 
% may end in states that do not satisfy $G$, 
% and may consider a set of objects that is different from $O$.  
% Safety is captured in Definition~\ref{def:safe-model-free-planning} by requiring that the output plan $\pi$ is a \textbf{sound plan} for $\Pi$. That is, $\pi$ is applicable and ends up reaching a state that satisfies the goal.  
% The main challenge is that the problem-solver -- the agent -- needs to find a sound plan to $\Pi$ but it is not given the set of fluents, actions, and action model of the domain ($\mathcal{F}$, $\mathcal{A}$, and \realm, respectively). 


% % Scope
% In this work, we make the following simplifying assumptions. 
% Actions have deterministic effects, 
% the agent has complete observability, 
% and when the agent observes a grounded action $a=\tuple{\lifta, b_a}$, it is able to discern that $a$ is the result of grounding $\lifta$ with $b_a$. 
% Similarly, if it observes a state with a grounded fluent $f=\tuple{\liftf, b_f}$, it is able to discern that $f$ is the result of grounding $\liftf$ with $b_f$. Also, we assume that actions' preconditions and effects are conjunctions of literals, as opposed to more complex logical statements, and we do not currently consider conditional effects of actions. 
% These assumptions are reasonable when planning in digital/virtual environments, such as video games, or environments that have been instrumented with reliable sensors, such as warehouses designed to be navigated by robots~\cite{li2020lifelong}. 
% Later, we will discuss approaches to relax these assumptions and apply our work to a broader range of environments. 
% % Here we limit the scope





