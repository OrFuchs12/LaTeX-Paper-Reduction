\def\year{2022}\relax
%File: formatting-instructions-latex-2022.tex
%release 2022.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage{enumitem}


%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
%\nocopyright
%
% PDF Info Is REQUIRED.
% For /Title, write your title in Mixed Case.
% Don't use accents or commands. Retain the parentheses.
% For /Author, add all authors within the parentheses,
% separated by commas. No accents, special characters
% or commands are allowed.
% Keep the /TemplateVersion tag as is
\pdfinfo{
/Title (Learning Probably Safe Action Models in Stochastic Worlds)
/Author (Us)
/TemplateVersion (2022.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{paralist}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}


\newcommand{\eff}{\textit{eff}}
\newcommand{\pre}{\textit{pre}}
\newcommand{\tuple}[1]{\ensuremath{\left \langle #1 \right \rangle }}
\newcommand{\goal}{\textit{goal}}
\newcommand{\sam}{\textit{SAM}}
\newcommand{\ip}{\textit{IP}}



\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai22.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
% \title{Learning Probably  Approximately Complete and Correct Action Models for Stochastic Worlds}
%\author{Author Response for Submission 6194}



% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\section{General Response}
We thank the reviewers for their helpful comments and suggestions, and will incorporate them into the paper. 
As several reviewers noted, we have used the property "Safety" and "Correctness" exchangeably. We will revise the paper to use consistently only one of these terms. 
In addition, we will make a special effort to improve readability to make the paper more accessible. 

\section{Response to Reviewer 1}

\begin{compactenum}

\item We will remove "-1" in the introduction to include the option for actions without effects (no-ops). 

\item Since we want to estimate the probability of $\ell$ occurring, we restrict to transitions on which we can observe whether or not it occurred. 
If literal $\ell$ is in a pre-state $s$ of an action $a$, then even if $\ell$ is an effect of $a$ an action it will not be observable. 

\item Indeed, requiring a high confidence of the learned model ($\delta$ and $\epsilon$=0.1) entails needing more trajectories. But, $|A|$=1000 is very large for a planning domain there, especially if these actions are lifted (i.e., parameterized) and not grounded. 
Most planning domains in the IPC have significantly fewer ($<$50) lifted actions. 
Note that there exists already a lifted version of SAM learning (see ICAPS 2021 PRL workshop). 

\item We will clarify that states only change due to effects, i.e., applying an action only changes the literals of a state as described by the effects. 

\end{compactenum}


\section{Response to Reviewer 2}

\noindent We answer the reviewer's rebuttal questions as follows:
\begin{compactenum}

% 1. To what extent do you agree with the following statement? Given the assumptions necessary to learn a safe action model, one could instead learn a safe goal-conditioned policy and get the same guarantees, but without needing to plan.
    \item In principle a safe goal-conditioned policy could achieve the same thing: the policy could itself run a planner. But, the existing methods for learning goal-conditioned policies don't seem to work well for long horizon problems in practice. Planning seems to help significantly.
% 2. In the case where probabilities are degenerate (all 0 or 1) in the ground truth model, what is the relationship between SAM+ and SAM?
    \item Since in SAM learning the real probabilities are 0 or 1, it can be much more aggressive than SAM+. Specifically, even though the real probabilities are 0 or 1, SAM+ maintains a confidence interval of width that gradually shrinks with the number of observations, effectively treating the probabilities as being $\epsilon$ or $1-\epsilon$ for $\epsilon > 0$.

% 3. I was surprised that the conversion from SAM+ to PPDDL, as in Theorem 4, did not just take the most conservative PPDDL model that is consistent with the SAM+ model, i.e., just take all of the lower bounds from all of the intervals. What would be the trade of between this conservative approach and the one described in the paper?
    \item The lower bound is not necessarily ``conservative,'' since sometimes success may rely on a stochastic effect failing to occur at some point, for example. In such a case, the lower bound overstates the probability rather than understating it, so it would incur more error than the midpoint.
    
% 4. Are the following statements correct? Consider a PPDDL-IP model where all possible effects are given the interval [0, 1]. So, all possible PPDDL models are contained in this set. Intuitively, this model would be correct, as in Theorem 2, but it would not be complete, as in Theorem 3.
    \item The reviewer is correct that such a model with degenrate intervals would be correct but incomplete. 
    
    %5. Conversely, can you give an example of a PPDDL-IP model that would be complete but not correct? Maybe something where all of the intervals are degenerate?
    \item One example would be a nearly deterministic environment, where the effects all have probability $\epsilon$ or 1-$\epsilon$ for an $\epsilon$ that is sufficiently small that $L\cdot\epsilon$ is still essentially negligible (within our tolerance for approximate completeness, e.g.). The model that actually rounds these probabilities to 0 or 1 is not correct, but it is complete, since dropping the $\epsilon$ on each transition only alters the probability of reaching the goal by at most $L\cdot\epsilon$ over the trajectory.

\end{compactenum}

\noindent With respect to the other questions and suggestions:
\begin{compactenum}
    \item We appreciate the reviewer's suggestion to provide a walkthrough example of SAM+ and are very happy to add such an example either directly in the paper or in a tech report and provide a link to it in the paper. 

    \item Theorem 3 does not require optimal policies to generate the trajectories, only that the goal achievement probability of these policy will be some value $p$. The approximate completenss guarantees are given wrt that $p$. 
    
    \item We will cleanup the related work and conclusion section as suggested, and rephrase the sentence in the introduction to avoid misleading the reader. 
    
    \item By “a policy that has a maximum lower bound of solving $\Pi$ in the SAM+ model.” we mean what the reviewer wrote. We will clarify this.  
\end{compactenum}


\section{Response to Reviewer 3}


\noindent We answer the reviewer's rebuttal questions as follows:
\begin{compactenum}
% 1) Eq (4) is supposed to represent the case when l \not\in s as explained in the prior paragraph ‘Effects’. However, the condition expressed in (4) is the complement of the condition of Eq (3). Is this an error? I notice the effects of the SAM+ algorithm revolve around the literals that do not belong to state s of triplet <s,a,s´>. The first case covers the newly generated effects of action a which are included in state s’ but not in s. The second case seems to me it covers the literals that are outside state s by the application of the closed world assumption. Is that correct? If this is so then it means SAM+ accounts for the probability that l appears in state s’ given that l is not in s. Can authors give an intuitive explanation of why the effects of SAM+ are classified into these two categories to create the intervals? Shouldn’t it also be considered the case when l is not in s’? (Minor: I think the symbol ‘f’ in Eq (3) must be replaced by ‘l’). (1)
    \item The two categories are reflecting that when an effect has never been observed to occur, we can obtain a confidence interval that is much shorter -- the upper bound scales with $1/\# observations$, whereas in Eq.~3, the width of the interval scales with $1/\sqrt{\# observations}$, which is thus much wider. Eq.\ 3 is still valid, but for effects that appear to be absent based on the data, we clearly prefer the tighter estimate of Eq.~4.


% 2) Many approaches to learning action models in classical planning accept trajectories with partially observed states. The present approach defines complete <state,action,state> trajectories. Can authors give some intuition on the guarantee of the properties of safety and completeness in the case of partially observable states?

\item Preserving safety and approximate completeness in partially observed settings is a challenge, e.g., when we observe an effect in a post state it then requires a method to  disambiguate which prior action yielded that effect. Exploring the range of partial observability settings and how to learn safe action models for them is a topic for future work.

% 3) I don’t quite grasp how the training policy distribution gives rise to a distribution over problems in the PPDDL domain D. How can you guarantee the problems are solvable? Any further elaboration on this aspect will be appreciated. 
\item The assumption is that the trajectories in training were obtained rather by first sampling a problem, then obtaining a policy to solve the problem. We do not actually require that the problems are solvable; but we only guarantee that we can obtain a rate of success similar to what was achieved on the training trajectories. In particular, if the problems are frequently unsolvable, then this rate will necessarily be low.


%4) Let’s suppose I have a stochastic problem where the probability distribution over states is known. I would like to know if the SAM+ algorithm would apply to my problem as it is presented in the paper or any further consideration would be needed. 
\item In general, the probability distribution over states depends on the chosen policy. If the reviewer is referring to a probability over the start and goal states for future problems, then yes, SAM+ would be a good fit. The input given to it should be trajectories created by executing policies created for problems (start state and goal) drawn from the same distribution.   

%5) My final remark is about providing a neat take-home message for researchers who wish to produce a PPDDL model. I think that authors could try to reflect what is needed from a practical standpoint in order to get the formal guarantees of correctness and completeness. Let’s say D=blocks-world domain. I know I have to generate a set of trajectories, and in order to get the trajectories I need to define some policies. Can I assume any distribution of the policies? Do I need to consider any other requirements in the domain/policies/trajectories? A practical guide on how to produce a PPDDL model would make the work more valuable and appealing.

 \item We appreciate the reviewer's suggestion to add a ``take home'' message that explain how to apply SAM+, and will be happy to incorporate it in the paper. Briefly, to learn a PPDDL model all that is needed is (1) define the fluents of the domain, (2) generate a set of trajectories in some way that a sufficiently high probability of these trajectories reach the goal. One way to get about doing (2) is to define some baseline, possible inefficient policies that achieve the goal with high probability, and using them to generate trajectories.
   
\end{compactenum}


\end{document}
