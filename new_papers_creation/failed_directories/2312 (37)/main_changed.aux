\relax 
\bibstyle{aaai24}
\citation{lee2022threshold,yoon2022adversarial}
\citation{cordts2016cityscapes}
\citation{bearman2016s}
\citation{akiva2023single}
\citation{akiva2023single}
\citation{zhou2016learning}
\citation{jiang2019integral,wei2018revisiting,choe2019attention}
\citation{jiang2022l2g}
\citation{lee2021railroad,lee2022weakly,Xie_2022_CVPR}
\citation{everingham2015pascal}
\citation{lin2014microsoft}
\citation{akiva2023single}
\citation{akiva2023single}
\citation{cordts2016cityscapes}
\citation{zhou2019semantic}
\citation{wang2020deep}
\citation{wang2020deep}
\citation{radford2021learning}
\citation{ding2022decoupling,Wang_2022_CVPR}
\citation{Xie_2022_CVPR}
\citation{Lin_2023_CVPR}
\citation{selvaraju2017grad}
\citation{li2022languagedriven,xu2021}
\citation{zhou2022extract}
\citation{DBLP:conf/nips/KendallG17}
\citation{feng2022dmt}
\citation{oh2021background,DBLP:conf/aaai/ZhangXWSH20}
\citation{yang2022st++}
\citation{NEURIPS2020_f73b76ce,Huynh:CVPR22,li2022uncertainty,wang2022semi}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dataset}{{1}{2}{Dataset statistics for Cityscapes, CamVid, MS COCO, and PASCAL VOC. (a) Counting the number of images given by the number of classes in a single image. (b) Histogram of co-occurrence ratio between classes. (c) The number of positive and negative images for each class.}{}{}}
\newlabel{fig:framework}{{2}{3}{Overall framework of proposed method. (Global-local View Training) CLIP gives different pseudo masks for cropping and resizing. (CARB) The pseudo-mask is divided into the consistent / inconsistent regions and the high loss of inconsistent regions is suppressed via adaptive region balancing.}{}{}}
\newlabel{fig:mask_resizencrop}{{3}{3}{Pseudo-masks after resizing and cropping. (a) The original CLIP mask. (b) CLIP mask with resize ratio 2. (c) The concatenation of quarter-size cropped CLIP masks (d) The mask applying both operations. For visual clarity, we modify color palette of motorcycle to \textit  {cyan} in this figure.}{}{}}
\newlabel{fig:mask_character}{{4}{4}{The characteristics of two different masks. (a) The mask from CLIP contains small and blob-like noisy regions. (b) The output mask from the segmentation network is more systematic. We identify reliable regions (c) based on prediction consistency between (a) and (b).}{}{}}
\newlabel{fig:fig_loss1}{{5}{4}{Changes in (a) loss and (b) area of consistent/inconsistent regions during training. Adaptive region balancing is applied from 16K iteration, affecting the training dynamics.}{}{}}
\newlabel{eq_local_mask}{{1}{4}{}{}{}}
\newlabel{eq_loss_loc}{{2}{4}{}{}{}}
\newlabel{eq_loss_glo}{{3}{4}{}{}{}}
\newlabel{sec:separation}{{}{4}{}{}{}}
\newlabel{eq_region_con}{{4}{4}{}{}{}}
\newlabel{eq_region_inc}{{5}{4}{}{}{}}
\citation{cordts2016cityscapes}
\citation{brostow2009semantic}
\citation{Zendel_2022_CVPR}
\citation{wang2020deep}
\citation{dosovitskiyimage}
\citation{he2016deep}
\citation{chen2017deeplab}
\citation{mmseg2020}
\citation{zhou2022extract}
\newlabel{eq_loss_con}{{6}{5}{}{}{}}
\newlabel{eq_loss_inc}{{7}{5}{}{}{}}
\newlabel{tab:split}{{1}{5}{Ablation study of the proposed modules. The accuracy (mIoU) is evaluated on the Cityscapes validation set. The best score is in \textbf  {bold} throughout all experiments.}{}{}}
\newlabel{sec:ablation}{{}{5}{}{}{}}
\citation{wang2020deep}
\citation{ahn2018learning}
\citation{wang2020self}
\citation{araslanov2020single}
\citation{kolesnikov2016seed}
\citation{wang2020deep}
\citation{zhou2016learning}
\citation{lee2022threshold}
\citation{Xie_2022_CVPR}
\citation{Lin_2023_CVPR}
\citation{wang2020deep}
\citation{wang2020deep}
\citation{wang2020deep}
\newlabel{fig:ablation}{{6}{6}{Segmentation results (mIoU) on Cityscapes validation set depending on (a) the crop size (b) the weight. We set the length of one side to 512 and varied the length of the other side between 128 and 512. Yellow star indicates the experiment using $256 \times 256$ patch.}{}{}}
\newlabel{tab:seg_cityscapes}{{2}{6}{Segmentation results (mIoU) on Cityscapes.}{}{}}
\newlabel{fig:qualitative_city}{{7}{7}{Qualitative results on Cityscapes validation set. (a) Input image, (b) Ground-truth, (c) CLIP-ES, and (d) Our method.}{}{}}
\newlabel{tab:seg_camvid}{{3}{7}{Segmentation results (mIoU) on CamVid.}{}{}}
\newlabel{tab:seg_coarse}{{4}{7}{Segmentation results (mIoU) on WildDash2 \textit  {val} set.}{}{}}
\bibcite{ahn2018learning}{{1}{2018}{{Ahn and Kwak}}{{}}}
\bibcite{akiva2023single}{{2}{2023}{{Akiva and Dana}}{{}}}
\bibcite{araslanov2020single}{{3}{2020}{{Araslanov and Roth}}{{}}}
\bibcite{bearman2016s}{{4}{2016}{{Bearman et~al.}}{{Bearman, Russakovsky, Ferrari, and Fei-Fei}}}
\bibcite{brostow2009semantic}{{5}{2009}{{Brostow, Fauqueur, and Cipolla}}{{}}}
\bibcite{chen2017deeplab}{{6}{2017}{{Chen et~al.}}{{Chen, Papandreou, Kokkinos, Murphy, and Yuille}}}
\bibcite{choe2019attention}{{7}{2019}{{Choe and Shim}}{{}}}
\bibcite{mmseg2020}{{8}{2020}{{Contributors}}{{}}}
\bibcite{cordts2016cityscapes}{{9}{2016}{{Cordts et~al.}}{{Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson, Franke, Roth, and Schiele}}}
\bibcite{ding2022decoupling}{{10}{2022}{{Ding et~al.}}{{Ding, Xue, Xia, and Dai}}}
\bibcite{dosovitskiyimage}{{11}{2021}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly et~al.}}}
\bibcite{everingham2015pascal}{{12}{2015}{{Everingham et~al.}}{{Everingham, Eslami, Van~Gool, Williams, Winn, and Zisserman}}}
\bibcite{feng2022dmt}{{13}{2022}{{Feng et~al.}}{{Feng, Zhou, Gu, Tan, Cheng, Lu, Shi, and Ma}}}
\bibcite{he2016deep}{{14}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{NEURIPS2020_f73b76ce}{{15}{2020}{{Hu, Sclaroff, and Saenko}}{{}}}
\bibcite{Huynh:CVPR22}{{16}{2022}{{Huynh et~al.}}{{Huynh, Kuen, Lin, Gu, and Elhamifar}}}
\bibcite{jiang2019integral}{{17}{2019}{{Jiang et~al.}}{{Jiang, Hou, Cao, Cheng, Wei, and Xiong}}}
\bibcite{jiang2022l2g}{{18}{2022}{{Jiang et~al.}}{{Jiang, Yang, Hou, and Wei}}}
\bibcite{DBLP:conf/nips/KendallG17}{{19}{2017}{{Kendall and Gal}}{{}}}
\bibcite{kolesnikov2016seed}{{20}{2016}{{Kolesnikov and Lampert}}{{}}}
\bibcite{lee2022weakly}{{21}{2022}{{Lee et~al.}}{{Lee, Oh, Yun, Choe, Kim, and Yoon}}}
\bibcite{lee2022threshold}{{22}{2022}{{Lee, Kim, and Shim}}{{}}}
\bibcite{lee2021railroad}{{23}{2021}{{Lee et~al.}}{{Lee, Lee, Lee, and Shim}}}
\bibcite{li2022languagedriven}{{24}{2022{a}}{{Li et~al.}}{{Li, Weinberger, Belongie, Koltun, and Ranftl}}}
\bibcite{li2022uncertainty}{{25}{2022{b}}{{Li et~al.}}{{Li, Duan, Kuang, Chen, Zhang, and Li}}}
\bibcite{lin2014microsoft}{{26}{2014}{{Lin et~al.}}{{Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{\'a}r, and Zitnick}}}
\bibcite{Lin_2023_CVPR}{{27}{2023}{{Lin et~al.}}{{Lin, Chen, Wang, Wu, Li, Lin, Liu, and He}}}
\bibcite{oh2021background}{{28}{2021}{{Oh, Kim, and Ham}}{{}}}
\bibcite{radford2021learning}{{29}{2021}{{Radford et~al.}}{{Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark et~al.}}}
\bibcite{selvaraju2017grad}{{30}{2017}{{Selvaraju et~al.}}{{Selvaraju, Cogswell, Das, Vedantam, Parikh, and Batra}}}
\bibcite{wang2020deep}{{31}{2020}{{Wang, Ma, and You}}{{}}}
\bibcite{wang2022semi}{{32}{2022{a}}{{Wang et~al.}}{{Wang, Wang, Shen, Fei, Li, Jin, Wu, Zhao, and Le}}}
\bibcite{wang2020self}{{33}{2020}{{Wang et~al.}}{{Wang, Zhang, Kan, Shan, and Chen}}}
\bibcite{Wang_2022_CVPR}{{34}{2022{b}}{{Wang et~al.}}{{Wang, Lu, Li, Tao, Guo, Gong, and Liu}}}
\bibcite{wei2018revisiting}{{35}{2018}{{Wei et~al.}}{{Wei, Xiao, Shi, Jie, Feng, and Huang}}}
\bibcite{Xie_2022_CVPR}{{36}{2022}{{Xie et~al.}}{{Xie, Hou, Ye, and Shen}}}
\bibcite{xu2021}{{37}{2022}{{Xu et~al.}}{{Xu, Zhang, Wei, Lin, Cao, Hu, and Bai}}}
\bibcite{yang2022st++}{{38}{2022}{{Yang et~al.}}{{Yang, Zhuo, Qi, Shi, and Gao}}}
\bibcite{yoon2022adversarial}{{39}{2022}{{Yoon et~al.}}{{Yoon, Kweon, Cho, Kim, and Yoon}}}
\bibcite{Zendel_2022_CVPR}{{40}{2022}{{Zendel et~al.}}{{Zendel, Sch\"orghuber, Rainer, Murschitz, and Beleznai}}}
\bibcite{DBLP:conf/aaai/ZhangXWSH20}{{41}{2020}{{Zhang et~al.}}{{Zhang, Xiao, Wei, Sun, and Huang}}}
\bibcite{zhou2016learning}{{42}{2016}{{Zhou et~al.}}{{Zhou, Khosla, Lapedriza, Oliva, and Torralba}}}
\bibcite{zhou2019semantic}{{43}{2019}{{Zhou et~al.}}{{Zhou, Zhao, Puig, Xiao, Fidler, Barriuso, and Torralba}}}
\bibcite{zhou2022extract}{{44}{2022}{{Zhou, Loy, and Dai}}{{}}}
\gdef \@abspage@last{9}
