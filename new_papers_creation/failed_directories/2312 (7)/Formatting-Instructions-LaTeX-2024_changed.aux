\relax 
\bibstyle{aaai24}
\citation{Ben-DavidRR20}
\citation{ZiserR18,ZhangHPJ18,Ben-DavidRR20}
\citation{ZhouTWWXH20}
\citation{WuS22}
\citation{YeTHLNB20,LuoGL022}
\citation{KarouzosPP21}
\citation{MoonMLLS21}
\citation{GeirhosJMZBBW20,Yunlong2023}
\citation{HuangZCWY21}
\citation{BaoZHLMVDC19}
\citation{BlitzerDP07,YuJ16,RamponiP20}
\citation{LiWZY18,ZiserR18,ZhangHPJ18,Ben-DavidRR20}
\citation{GaninUAGLLML16,QuZC0Z19,WuS22}
\citation{ZhouTWWXH20}
\citation{LiWJZ22}
\citation{YeTHLNB20,LuoGL022}
\citation{DuSWQL20,KarouzosPP21}
\citation{ChenCDHSSA16}
\citation{JohnMBV19}
\citation{BaoZHLMVDC19}
\citation{PergolaGH21}
\citation{ColomboSNP22}
\citation{KingmaW13}
\citation{JohnMBV19,HuangZCWY21}
\citation{WuS22}
\citation{BaoZHLMVDC19,JohnMBV19}
\citation{KingmaW13}
\newlabel{sec:work}{{}{2}{}{}{}}
\citation{kawata1949characterisation,Fotopoulos07}
\citation{JohnMBV19}
\citation{JohnMBV19,HuangZCWY21}
\citation{UtamaMG20,UtamaMG20b}
\citation{LaiZFHZ21}
\citation{DuMJDDGSH21}
\citation{Yixuan2022}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:model}{{1}{3}{TACIT's overall architecture and processing flow. It consists of two main steps and three tasks. In Step 1, an underfitting model selects a subset of easy samples from the source domain based on the confidence. Subsequently, such samples are used to train a teacher model. In Step 2, the output features of the base model are fed into VAE for disntanglement. The robust feature $z_\mu $ is used to predict the sample labels. Then, the unrobust feature $z_\sigma $ is scheduled to be learned from the teacher's output $\hat  {z}$ through feature distillation. Finally, cross-entropy loss, VAE loss and distillation loss are used to co-optimize the model. {{\FAtwo  \symbol  {181}}}\nobreakspace  {}indicates that model parameters are not updated during training.}{}{}}
\citation{BlitzerDP07}
\citation{Ben-DavidRR20}
\citation{WuS22}
\citation{ZiserR18}
\citation{DuSWQL20}
\citation{Ben-DavidRR20}
\citation{YeTHLNB20}
\citation{KarouzosPP21}
\citation{LuoGL022}
\citation{WuS22}
\citation{DevlinCLT19}
\citation{RoBERTa}
\citation{KarouzosPP21,WuS22}
\citation{DistilBERT}
\citation{LoshchilovH19}
\newlabel{sec:exp}{{}{4}{}{}{}}
\newlabel{sec:data}{{}{4}{}{}{}}
\newlabel{sec:baseline}{{}{4}{}{}{}}
\newlabel{sec:details}{{}{4}{}{}{}}
\citation{WuS22}
\newlabel{sec:results}{{}{5}{}{}{}}
\newlabel{fig:multi}{{2}{5}{Comparison of single-source and multi-source experimental results on similar data sets K and E.}{}{}}
\newlabel{sec:params}{{}{5}{}{}{}}
\newlabel{sec:abl}{{}{5}{}{}{}}
\citation{vandermaaten08a}
\newlabel{tab:results}{{1}{6}{Single source cross-domain generalization performance for TACIT and baselines. The boldface indicates the optimal results. For each model we report the average results across the five folds. `Vanilla' denotes fine-tuning on the source domain labeled data. `Source' denotes training on the source and `Target' means testing on the target dataset. `Avg' represents the average of all cross-domain generalization tasks.}{}{}}
\newlabel{tab:multi-source}{{2}{6}{Cross-domain generalization results of multiple training sources. The boldface indicates the optimal results. \textbf  {AdSPT} is the only method reporting multiple sources in the baselines, so we use RoBERTa$_{base}$ as the backbone for comparison.}{}{}}
\newlabel{fig:loss}{{3}{6}{The changes of loss on fold-1 with Books and DVDs as source domains during the model training process. Different styles of lines represent different datasets as well as loss values. }{}{}}
\newlabel{sec:vis}{{}{6}{}{}{}}
\newlabel{fig:abl}{{4}{7}{Comparison of ablation results of different cross-domain generalization tasks, where different colors and styles of bars indicate different TACIT variants. }{}{}}
\newlabel{fig:sub1}{{5a}{7}{TACIT}{}{}}
\newlabel{sub@fig:sub1}{{a}{7}{TACIT}{}{}}
\newlabel{fig:sub2}{{5b}{7}{TACIT$_{-distill}$}{}{}}
\newlabel{sub@fig:sub2}{{b}{7}{TACIT$_{-distill}$}{}{}}
\newlabel{fig:sub3}{{5c}{7}{TACIT$_{-vae}$}{}{}}
\newlabel{sub@fig:sub3}{{c}{7}{TACIT$_{-vae}$}{}{}}
\newlabel{fig:vis}{{5}{7}{Feature visualisation results of $z_\mu $ and $z_\sigma $ for TACIT and the two corresponding variants TACIT$_{-distill}$ and TACIT$_{-vae}$ on B$\to $D, where the green nodes indicate $z_\sigma $ and the purple nodes indicate $z_\mu $. }{}{}}
\bibdata{aaai24}
\citation{HeLGC21}
\citation{OPT}
\newlabel{app:a}{{}{13}{}{}{}}
\newlabel{eq:p}{{7}{13}{}{}{}}
\newlabel{eq:p2}{{8}{13}{}{}{}}
\newlabel{eq:p3}{{9}{13}{}{}{}}
\newlabel{app:b}{{}{13}{}{}{}}
\newlabel{tab:a1}{{3}{13}{Results based on OPT-1.3b and DeBERTa. The boldface indicates the optimal results.}{}{}}
\newlabel{tab:a2}{{4}{14}{Spam and multi-class classification results. }{}{}}
\gdef \@abspage@last{14}
