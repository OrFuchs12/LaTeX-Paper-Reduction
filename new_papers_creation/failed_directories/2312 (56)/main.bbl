\begin{thebibliography}{81}
\providecommand{\natexlab}[1]{#1}

\bibitem[{{An} et~al.(2018){An}, {Wang}, {Sun}, {Xu}, {Dai}, and {Zhang}}]{An18PID}
{An}, W.; {Wang}, H.; {Sun}, Q.; {Xu}, J.; {Dai}, Q.; and {Zhang}, L. 2018.
\newblock A PID Controller Approach for Stochastic Optimization of Deep Networks.
\newblock In \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 8522--8531.

\bibitem[{Avdiukhin and Kasiviswanathan(2021)}]{Avdiukhin21arbitrarycommunication}
Avdiukhin, D.; and Kasiviswanathan, S. 2021.
\newblock Federated Learning under Arbitrary Communication Patterns.
\newblock In Meila, M.; and Zhang, T., eds., \emph{Proceedings of the 38th International Conference on Machine Learning}, volume 139 of \emph{Proceedings of Machine Learning Research}, 425--435. PMLR.

\bibitem[{Bao, Gu, and Huang(2020)}]{bao2020fast}
Bao, R.; Gu, B.; and Huang, H. 2020.
\newblock Fast OSCAR and OWL regression via safe screening rules.
\newblock In \emph{International Conference on Machine Learning}, 653--663. PMLR.

\bibitem[{Bao et~al.(2022)Bao, Wu, Xian, and Huang}]{bao2022doubly}
Bao, R.; Wu, X.; Xian, W.; and Huang, H. 2022.
\newblock Doubly sparse asynchronous learning for stochastic composite optimization.
\newblock In \emph{Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI}, 1916--1922.

\bibitem[{Bao et~al.(2023)Bao, Wei, Wang, and He}]{bao2023adaptive}
Bao, W.; Wei, T.; Wang, H.; and He, J. 2023.
\newblock Adaptive Test-Time Personalization for Federated Learning.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}.

\bibitem[{Basu et~al.(2019)Basu, Data, Karakus, and Diggavi}]{Basu19Qsparse-Local-SGD}
Basu, D.; Data, D.; Karakus, C.; and Diggavi, S. 2019.
\newblock \emph{Qsparse-Local-SGD: Distributed SGD with Quantization, Sparsification, and Local Computations}.
\newblock Red Hook, NY, USA: Curran Associates Inc.

\bibitem[{BUKATY(2019)}]{California_Consumer_Privacy_Act_CCPA}
BUKATY, P. 2019.
\newblock \emph{The California Consumer Privacy Act (CCPA): An implementation guide}.
\newblock IT Governance Publishing.
\newblock ISBN 9781787781320.

\bibitem[{Chen, Horv{\'a}th, and Richt{\'a}rik(2020)}]{Chen2020ClientSampling}
Chen, W.; Horv{\'a}th, S.; and Richt{\'a}rik, P. 2020.
\newblock Optimal Client Sampling for Federated Learning.
\newblock \emph{ArXiv}, abs/2010.13723.

\bibitem[{Cheng et~al.(2016)Cheng, Koc, Harmsen, Shaked, Chandra, Aradhye, Anderson, Corrado, Chai, Ispir, Anil, Haque, Hong, Jain, Liu, and Shah}]{google16deep&wide}
Cheng, H.-T.; Koc, L.; Harmsen, J.; Shaked, T.; Chandra, T.; Aradhye, H.; Anderson, G.; Corrado, G.; Chai, W.; Ispir, M.; Anil, R.; Haque, Z.; Hong, L.; Jain, V.; Liu, X.; and Shah, H. 2016.
\newblock Wide \& Deep Learning for Recommender Systems.
\newblock In \emph{Proceedings of the 1st Workshop on Deep Learning for Recommender Systems}, DLRS 2016, 7–10. New York, NY, USA: Association for Computing Machinery.
\newblock ISBN 9781450347952.

\bibitem[{Cutkosky and Mehta(2020)}]{Cutkosky2020MomentumIN}
Cutkosky, A.; and Mehta, H. 2020.
\newblock Momentum Improves Normalized SGD.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova}]{devlin2018bert}
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova}]{Devlin2019BERT}
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
\newblock BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
\newblock \emph{ArXiv}, abs/1810.04805.

\bibitem[{Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby}]{dosovitskiy2021VIT}
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.
\newblock An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{{European Commission}(2016)}]{european_commission_regulation_2016}
{European Commission}. 2016.
\newblock Regulation ({EU}) 2016/679 of the {European} {Parliament} and of the {Council} of 27 {April} 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing {Directive} 95/46/{EC} ({General} {Data} {Protection} {Regulation}) ({Text} with {EEA} relevance).

\bibitem[{Fallah, Mokhtari, and Ozdaglar(2020)}]{fallah2020personalized}
Fallah, A.; Mokhtari, A.; and Ozdaglar, A. 2020.
\newblock Personalized federated learning: A meta-learning approach.
\newblock \emph{arXiv preprint arXiv:2002.07948}.

\bibitem[{Ge et~al.(2019{\natexlab{a}})Ge, Kakade, Kidambi, and Netrapalli}]{Ge2019TheSD}
Ge, R.; Kakade, S.~M.; Kidambi, R.; and Netrapalli, P. 2019{\natexlab{a}}.
\newblock The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure.
\newblock In \emph{NeurIPS}.

\bibitem[{Ge et~al.(2019{\natexlab{b}})Ge, Kakade, Kidambi, and Netrapalli}]{ge19stepdecay}
Ge, R.; Kakade, S.~M.; Kidambi, R.; and Netrapalli, P. 2019{\natexlab{b}}.
\newblock \emph{The Step Decay Schedule: A near Optimal, Geometrically Decaying Learning Rate Procedure for Least Squares}.
\newblock Red Hook, NY, USA: Curran Associates Inc.

\bibitem[{Goetz et~al.(2019)Goetz, Malik, Bui, Moon, Liu, and Kumar}]{Goetz2019ActiveFL}
Goetz, J.; Malik, K.; Bui, D.~V.; Moon, S.; Liu, H.; and Kumar, A. 2019.
\newblock Active Federated Learning.
\newblock \emph{ArXiv}, abs/1909.12641.

\bibitem[{Goyal et~al.(2017)Goyal, Doll{\'{a}}r, Girshick, Noordhuis, Wesolowski, Kyrola, Tulloch, Jia, and He}]{GoyalDGNWKTJH17LargeMinibatch}
Goyal, P.; Doll{\'{a}}r, P.; Girshick, R.~B.; Noordhuis, P.; Wesolowski, L.; Kyrola, A.; Tulloch, A.; Jia, Y.; and He, K. 2017.
\newblock Accurate, Large Minibatch {SGD:} Training ImageNet in 1 Hour.
\newblock \emph{CoRR}, abs/1706.02677.

\bibitem[{Gu et~al.(2021)Gu, Huang, Zhang, and Huang}]{gu2021arbitraryunavailable}
Gu, X.; Huang, K.; Zhang, J.; and Huang, L. 2021.
\newblock Fast Federated Learning in the Presence of Arbitrary Device Unavailability.
\newblock In Beygelzimer, A.; Dauphin, Y.; Liang, P.; and Vaughan, J.~W., eds., \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Hamilton, Ying, and Leskovec(2017)}]{Jure2017GNN}
Hamilton, W.; Ying, Z.; and Leskovec, J. 2017.
\newblock Inductive Representation Learning on Large Graphs.
\newblock In Guyon, I.; Luxburg, U.~V.; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and Garnett, R., eds., \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc.

\bibitem[{He, Liu, and Tao(2019)}]{He19ControlBatch}
He, F.; Liu, T.; and Tao, D. 2019.
\newblock Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, 1143--1152. Curran Associates, Inc.

\bibitem[{{He} et~al.(2016){He}, {Zhang}, {Ren}, and {Sun}}]{He16Res}
{He}, K.; {Zhang}, X.; {Ren}, S.; and {Sun}, J. 2016.
\newblock Deep Residual Learning for Image Recognition.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 770--778.

\bibitem[{He et~al.(2016)He, Zhang, Ren, and Sun}]{He2016DeepResNet}
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016.
\newblock Deep Residual Learning for Image Recognition.
\newblock \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 770--778.

\bibitem[{Hsu, Qi, and Brown(2019)}]{Hsu2019MeasuringTE}
Hsu, T.-M.~H.; Qi; and Brown, M. 2019.
\newblock Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification.
\newblock \emph{ArXiv}, abs/1909.06335.

\bibitem[{Hu, Wu, and Huang(2023)}]{hu2023beyond}
Hu, Z.; Wu, X.; and Huang, H. 2023.
\newblock Beyond Lipschitz smoothness: a tighter analysis for nonconvex optimization.
\newblock In \emph{International Conference on Machine Learning}, 13652--13678. PMLR.

\bibitem[{{Huang} et~al.(2017){Huang}, {Liu}, {Van Der Maaten}, and {Weinberger}}]{Huang2017DenseNet}
{Huang}, G.; {Liu}, Z.; {Van Der Maaten}, L.; and {Weinberger}, K.~Q. 2017.
\newblock Densely Connected Convolutional Networks.
\newblock In \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2261--2269.

\bibitem[{Jee~Cho, Wang, and Joshi(2022)}]{cho22biased_selection}
Jee~Cho, Y.; Wang, J.; and Joshi, G. 2022.
\newblock Towards Understanding Biased Client Selection in Federated Learning.
\newblock In Camps-Valls, G.; Ruiz, F. J.~R.; and Valera, I., eds., \emph{Proceedings of The 25th International Conference on Artificial Intelligence and Statistics}, volume 151 of \emph{Proceedings of Machine Learning Research}, 10351--10375. PMLR.

\bibitem[{Kairouz et~al.(2021)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji, Bonawitz, Charles, Cormode, Cummings, D'Oliveira, Eichner, Rouayheb, Evans, Gardner, Garrett, Gascón, Ghazi, Gibbons, Gruteser, Harchaoui, He, He, Huo, Hutchinson, Hsu, Jaggi, Javidi, Joshi, Khodak, Konecný, Korolova, Koushanfar, Koyejo, Lepoint, Liu, Mittal, Mohri, Nock, Özgür, Pagh, Qi, Ramage, Raskar, Raykova, Song, Song, Stich, Sun, Suresh, Tramèr, Vepakomma, Wang, Xiong, Xu, Yang, Yu, Yu, and Zhao}]{Kairouz21AdvancesProblems}
Kairouz, P.; McMahan, H.~B.; Avent, B.; Bellet, A.; Bennis, M.; Bhagoji, A.~N.; Bonawitz, K.~A.; Charles, Z.; Cormode, G.; Cummings, R.; D'Oliveira, R. G.~L.; Eichner, H.; Rouayheb, S.~E.; Evans, D.; Gardner, J.; Garrett, Z.; Gascón, A.; Ghazi, B.; Gibbons, P.~B.; Gruteser, M.; Harchaoui, Z.; He, C.; He, L.; Huo, Z.; Hutchinson, B.; Hsu, J.; Jaggi, M.; Javidi, T.; Joshi, G.; Khodak, M.; Konecný, J.; Korolova, A.; Koushanfar, F.; Koyejo, S.; Lepoint, T.; Liu, Y.; Mittal, P.; Mohri, M.; Nock, R.; Özgür, A.; Pagh, R.; Qi, H.; Ramage, D.; Raskar, R.; Raykova, M.; Song, D.; Song, W.; Stich, S.~U.; Sun, Z.; Suresh, A.~T.; Tramèr, F.; Vepakomma, P.; Wang, J.; Xiong, L.; Xu, Z.; Yang, Q.; Yu, F.~X.; Yu, H.; and Zhao, S. 2021.
\newblock Advances and Open Problems in Federated Learning.
\newblock \emph{Found. Trends Mach. Learn.}, 14(1-2): 1--210.

\bibitem[{Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and Suresh}]{karimireddy2020scaffold}
Karimireddy, S.~P.; Kale, S.; Mohri, M.; Reddi, S.; Stich, S.; and Suresh, A.~T. 2020.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, 5132--5143. PMLR.

\bibitem[{Khanduri et~al.(2021)Khanduri, Sharma, Yang, Hong, Liu, Rajawat, and Varshney}]{khanduri2021stem}
Khanduri, P.; Sharma, P.; Yang, H.; Hong, M.; Liu, J.; Rajawat, K.; and Varshney, P. 2021.
\newblock Stem: A stochastic two-sided momentum algorithm achieving near-optimal sample and communication complexities for federated learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34.

\bibitem[{Kidambi et~al.(2018)Kidambi, Netrapalli, Jain, and Kakade}]{Kidambi18Insufficiency}
Kidambi, R.; Netrapalli, P.; Jain, P.; and Kakade, S.~M. 2018.
\newblock On the insufficiency of existing momentum schemes for Stochastic Optimization.
\newblock \emph{CoRR}, abs/1803.05591.

\bibitem[{Krizhevsky(2009)}]{Krizhevsky2009CIFAR}
Krizhevsky, A. 2009.
\newblock Learning Multiple Layers of Features from Tiny Images.

\bibitem[{Krizhevsky, Sutskever, and Hinton(2012)}]{Krizhevsky12ImageNet}
Krizhevsky, A.; Sutskever, I.; and Hinton, G.~E. 2012.
\newblock ImageNet Classification with Deep Convolutional Neural Networks.
\newblock 1097--1105.

\bibitem[{Lessard, Recht, and Packard(2014)}]{Lessard14SNV}
Lessard, L.; Recht, B.; and Packard, A. 2014.
\newblock Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints.
\newblock \emph{SIAM Journal on Optimization}, 26.

\bibitem[{Li et~al.(2022)Li, Diao, Chen, and He}]{li2022NIIDBenchmark}
Li, Q.; Diao, Y.; Chen, Q.; and He, B. 2022.
\newblock Federated Learning on Non-IID Data Silos: An Experimental Study.
\newblock In \emph{IEEE International Conference on Data Engineering}.

\bibitem[{Li, He, and Song(2021)}]{li2021model}
Li, Q.; He, B.; and Song, D. 2021.
\newblock Model-Contrastive Federated Learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}.

\bibitem[{Li et~al.(2020{\natexlab{a}})Li, Sahu, Zaheer, Sanjabi, Talwalkar, and Smith}]{Li20FedProx}
Li, T.; Sahu, A.~K.; Zaheer, M.; Sanjabi, M.; Talwalkar, A.; and Smith, V. 2020{\natexlab{a}}.
\newblock Federated Optimization in Heterogeneous Networks.
\newblock In Dhillon, I.; Papailiopoulos, D.; and Sze, V., eds., \emph{Proceedings of Machine Learning and Systems}, volume~2, 429--450.

\bibitem[{Li et~al.(2020{\natexlab{b}})Li, Huang, Yang, Wang, and Zhang}]{Li2020Fed-Non-IID}
Li, X.; Huang, K.; Yang, W.; Wang, S.; and Zhang, Z. 2020{\natexlab{b}}.
\newblock On the Convergence of FedAvg on Non-IID Data.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Lian et~al.(2015)Lian, Huang, Li, and Liu}]{Lian15Asynchronous}
Lian, X.; Huang, Y.; Li, Y.; and Liu, J. 2015.
\newblock Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization.
\newblock In \emph{Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2}, NIPS'15, 2737–2745. Cambridge, MA, USA: MIT Press.

\bibitem[{Lin et~al.(2020)Lin, Stich, Patel, and Jaggi}]{lin2020dont}
Lin, T.; Stich, S.~U.; Patel, K.~K.; and Jaggi, M. 2020.
\newblock Don't Use Large Mini-batches, Use Local {SGD}.
\newblock In \emph{ICLR - International Conference on Learning Representations}.

\bibitem[{Liu, Gao, and Yin(2020)}]{liu2020improved}
Liu, Y.; Gao, Y.; and Yin, W. 2020.
\newblock An Improved Analysis of Stochastic Gradient Descent with Momentum.
\newblock arXiv:2007.07989.

\bibitem[{Ma and Yarats(2019)}]{ma2018quasihyperbolic}
Ma, J.; and Yarats, D. 2019.
\newblock Quasi-hyperbolic momentum and Adam for deep learning.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and y~Arcas}]{McMahan2017FedAvg}
McMahan, H.~B.; Moore, E.; Ramage, D.; Hampson, S.; and y~Arcas, B.~A. 2017.
\newblock Communication-Efficient Learning of Deep Networks from Decentralized Data.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}.

\bibitem[{Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou, Wierstra, and Riedmiller}]{Mnih2013PlayingAW}
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.; Antonoglou, I.; Wierstra, D.; and Riedmiller, M.~A. 2013.
\newblock Playing Atari with Deep Reinforcement Learning.
\newblock \emph{ArXiv}, abs/1312.5602.

\bibitem[{Nguyen et~al.(2021)Nguyen, Malik, Zhan, Yousefpour, Rabbat, Malek, and Huba}]{Nguyen2021FedBuff}
Nguyen, J.; Malik, K.; Zhan, H.; Yousefpour, A.; Rabbat, M.~G.; Malek, M.; and Huba, D. 2021.
\newblock Federated Learning with Buffered Asynchronous Aggregation.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}.

\bibitem[{Nishio and Yonetani(2018)}]{Nishio2018ClientSelection}
Nishio, T.; and Yonetani, R. 2018.
\newblock Client Selection for Federated Learning with Heterogeneous Resources in Mobile Edge.
\newblock \emph{ICC 2019 - 2019 IEEE International Conference on Communications (ICC)}, 1--7.

\bibitem[{Reddi et~al.(2020)Reddi, Charles, Zaheer, Garrett, Rush, Kone{\v{c}}n{\`y}, Kumar, and McMahan}]{reddi2020adaptive}
Reddi, S.; Charles, Z.; Zaheer, M.; Garrett, Z.; Rush, K.; Kone{\v{c}}n{\`y}, J.; Kumar, S.; and McMahan, H.~B. 2020.
\newblock Adaptive federated optimization.
\newblock \emph{arXiv preprint arXiv:2003.00295}.

\bibitem[{Reddi, Kale, and Kumar(2018)}]{reddi18adam_convergence}
Reddi, S.~J.; Kale, S.; and Kumar, S. 2018.
\newblock On the Convergence of Adam and Beyond.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Ribero and Vikalo(2020)}]{Ribero2020clientsampling}
Ribero, M.; and Vikalo, H. 2020.
\newblock Communication-Efficient Federated Learning via Optimal Client Sampling.
\newblock \emph{ArXiv}, abs/2007.15197.

\bibitem[{Rothchild et~al.(2020)Rothchild, Panda, Ullah, Ivkin, Stoica, Braverman, Gonzalez, and Arora}]{rothchild20fetchsgd}
Rothchild, D.; Panda, A.; Ullah, E.; Ivkin, N.; Stoica, I.; Braverman, V.; Gonzalez, J.; and Arora, R. 2020.
\newblock FetchSGD: Communication-Efficient Federated Learning with Sketching.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning}, ICML'20. JMLR.org.

\bibitem[{Simonyan and Zisserman(2015)}]{Simonyan14VGG}
Simonyan, K.; and Zisserman, A. 2015.
\newblock Very Deep Convolutional Networks for Large-Scale Image Recognition.
\newblock In \emph{3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings}.

\bibitem[{{Smith}(2017)}]{Smith17Cyclic}
{Smith}, L.~N. 2017.
\newblock Cyclical Learning Rates for Training Neural Networks.
\newblock In \emph{2017 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 464--472.

\bibitem[{Smith and Le(2018)}]{Smith18Bayesian}
Smith, S.; and Le, Q.~V. 2018.
\newblock A Bayesian Perspective on Generalization and Stochastic Gradient Descent.

\bibitem[{Smith, Kindermans, and Le(2018)}]{Smith18DontDecay}
Smith, S.~L.; Kindermans, P.-J.; and Le, Q.~V. 2018.
\newblock Don't Decay the Learning Rate, Increase the Batch Size.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Sun et~al.(2022)Sun, Huai, Jha, and Zhang}]{Sun22Hyperparameters}
Sun, J.; Huai, M.; Jha, K.; and Zhang, A. 2022.
\newblock Demystify Hyperparameters for Stochastic Optimization with Transferable Representations.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, KDD '22, 1706–1716. New York, NY, USA: Association for Computing Machinery.
\newblock ISBN 9781450393850.

\bibitem[{Sun, Sinha, and Zhang(2023)}]{Sun23Diffusion}
Sun, J.; Sinha, S.; and Zhang, A. 2023.
\newblock Enhance Diffusion to Improve Robust Generalization.
\newblock In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, KDD '23, 2083–2095. New York, NY, USA: Association for Computing Machinery.
\newblock ISBN 9798400701030.

\bibitem[{Sun et~al.(2021)Sun, Yang, Xun, and Zhang}]{sun21stagewise}
Sun, J.; Yang, Y.; Xun, G.; and Zhang, A. 2021.
\newblock A Stagewise Hyperparameter Scheduler to Improve Generalization.
\newblock In \emph{Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining}, KDD '21, 1530–1540. New York, NY, USA: Association for Computing Machinery.
\newblock ISBN 9781450383325.

\bibitem[{Sun et~al.(2023)Sun, Yang, Xun, and Zhang}]{Sun23TKDD}
Sun, J.; Yang, Y.; Xun, G.; and Zhang, A. 2023.
\newblock Scheduling Hyperparameters to Improve Generalization: From Centralized SGD to Asynchronous SGD.
\newblock \emph{ACM Trans. Knowl. Discov. Data}, 17(2).

\bibitem[{Suo et~al.(2019)Suo, Yao, Xun, Sun, and Zhang}]{SuoICHI19}
Suo, Q.; Yao, L.; Xun, G.; Sun, J.; and Zhang, A. 2019.
\newblock Recurrent Imputation for Multivariate Time Series with Missing Values.
\newblock In \emph{2019 {IEEE} International Conference on Healthcare Informatics, {ICHI} 2019, Xi'an, China, June 10-13, 2019}, 1--3. {IEEE}.

\bibitem[{Sutskever et~al.(2013)Sutskever, Martens, Dahl, and Hinton}]{Sutskever13Init}
Sutskever, I.; Martens, J.; Dahl, G.; and Hinton, G. 2013.
\newblock On the Importance of Initialization and Momentum in Deep Learning.
\newblock In \emph{Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28}, ICML'13, III–1139–III–1147.

\bibitem[{{Van Scoy}, {Freeman}, and {Lynch}(2018)}]{VanScoy18Triple}
{Van Scoy}, B.; {Freeman}, R.~A.; and {Lynch}, K.~M. 2018.
\newblock The Fastest Known Globally Convergent First-Order Method for Minimizing Strongly Convex Functions.
\newblock \emph{IEEE Control Systems Letters}, 2(1): 49--54.

\bibitem[{Wang et~al.(2020)Wang, Liu, Liang, Joshi, and Poor}]{Wang20FedNova}
Wang, J.; Liu, Q.; Liang, H.; Joshi, G.; and Poor, H.~V. 2020.
\newblock Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization.
\newblock In \emph{Proceedings of the 34th International Conference on Neural Information Processing Systems}, NIPS'20. Red Hook, NY, USA: Curran Associates Inc.
\newblock ISBN 9781713829546.

\bibitem[{Wang and Ji(2022)}]{wang2022arbitraryparticipation}
Wang, S.; and Ji, M. 2022.
\newblock A Unified Analysis of Federated Learning with Arbitrary Client Participation.
\newblock In Oh, A.~H.; Agarwal, A.; Belgrave, D.; and Cho, K., eds., \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Wang, Magn{\'u}sson, and Johansson(2021)}]{wang21stepdecay}
Wang, X.; Magn{\'u}sson, S.; and Johansson, M. 2021.
\newblock On the Convergence of Step Decay Step-Size for Stochastic Optimization.
\newblock In Beygelzimer, A.; Dauphin, Y.; Liang, P.; and Vaughan, J.~W., eds., \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Wang, Lin, and Chen(2022)}]{wang22adaptive}
Wang, Y.; Lin, L.; and Chen, J. 2022.
\newblock Communication-Efficient Adaptive Federated Learning.
\newblock In \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning Research}, 22802--22838. PMLR.

\bibitem[{Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and Recht}]{Wilson2017Generalization}
Wilson, A.~C.; Roelofs, R.; Stern, M.; Srebro, N.; and Recht, B. 2017.
\newblock The Marginal Value of Adaptive Gradient Methods in Machine Learning.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, 4148--4158. Curran Associates, Inc.

\bibitem[{Wu et~al.(2023{\natexlab{a}})Wu, Huang, Hu, and Huang}]{wu2023faster}
Wu, X.; Huang, F.; Hu, Z.; and Huang, H. 2023{\natexlab{a}}.
\newblock Faster adaptive federated learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, 10379--10387.

\bibitem[{Wu et~al.(2023{\natexlab{b}})Wu, Sun, Hu, Li, Zhang, and Huang}]{wu2023federated}
Wu, X.; Sun, J.; Hu, Z.; Li, J.; Zhang, A.; and Huang, H. 2023{\natexlab{b}}.
\newblock Federated Conditional Stochastic Optimization.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}.

\bibitem[{Wu et~al.(2023{\natexlab{c}})Wu, Sun, Hu, Zhang, and Huang}]{wu2023solving}
Wu, X.; Sun, J.; Hu, Z.; Zhang, A.; and Huang, H. 2023{\natexlab{c}}.
\newblock Solving a Class of Non-Convex Minimax Optimization in Federated Learning.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}.

\bibitem[{Wu et~al.(2023{\natexlab{d}})Wu, Hu, Zhang, and Huang}]{Wu2023DiPmarkAS}
Wu, Y.; Hu, Z.; Zhang, H.; and Huang, H. 2023{\natexlab{d}}.
\newblock DiPmark: A Stealthy, Efficient and Resilient Watermark for Large Language Models.
\newblock \emph{ArXiv}, abs/2310.07710.

\bibitem[{Xie, Koyejo, and Gupta(2019)}]{Xie2019AsynchronousFO}
Xie, C.; Koyejo, O.; and Gupta, I. 2019.
\newblock Asynchronous Federated Optimization.
\newblock \emph{ArXiv}, abs/1903.03934.

\bibitem[{Xun et~al.(2020)Xun, Jha, Sun, and Zhang}]{Xun2020CorrelationNF}
Xun, G.; Jha, K.; Sun, J.; and Zhang, A. 2020.
\newblock Correlation Networks for Extreme Multi-label Text Classification.
\newblock \emph{Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}.

\bibitem[{Yan et~al.(2020)Yan, Niu, Ding, Zheng, Wu, Chen, Tang, and Wu}]{Yan2020DistributedClient}
Yan, Y.; Niu, C.; Ding, Y.; Zheng, Z.; Wu, F.; Chen, G.; Tang, S.; and Wu, Z. 2020.
\newblock Distributed Non-Convex Optimization with Sublinear Speedup under Intermittent Client Availability.
\newblock \emph{ArXiv}, abs/2002.07399.

\bibitem[{Yang, Fang, and Liu(2021)}]{yang2021achieving}
Yang, H.; Fang, M.; and Liu, J. 2021.
\newblock Achieving Linear Speedup with Partial Worker Participation in Non-{IID} Federated Learning.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Yang et~al.(2021)Yang, Zhang, Khanduri, and Liu}]{Yang2021AnarchicFL}
Yang, H.; Zhang, X.; Khanduri, P.; and Liu, J. 2021.
\newblock Anarchic Federated Learning.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Yurochkin et~al.(2019)Yurochkin, Agarwal, Ghosh, Greenewald, Hoang, and Khazaeni}]{Yurochkin2019BayesianNF}
Yurochkin, M.; Agarwal, M.; Ghosh, S.~S.; Greenewald, K.~H.; Hoang, T.~N.; and Khazaeni, Y. 2019.
\newblock Bayesian Nonparametric Federated Learning of Neural Networks.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and Sra}]{Zhang20Adam_Attention}
Zhang, J.; Karimireddy, S.~P.; Veit, A.; Kim, S.; Reddi, S.; Kumar, S.; and Sra, S. 2020.
\newblock Why are Adaptive Methods Good for Attention Models?
\newblock In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., \emph{Advances in Neural Information Processing Systems}, volume~33, 15383--15393. Curran Associates, Inc.

\bibitem[{Zhang, Choromańska, and LeCun(2014)}]{Zhang2014DeepElasticAvg}
Zhang, S.; Choromańska, A.; and LeCun, Y. 2014.
\newblock Deep learning with Elastic Averaging SGD.
\newblock In \emph{NIPS}.

\bibitem[{Zhao et~al.(2018)Zhao, Li, Lai, Suda, Civin, and Chandra}]{zhao2018federated-noniid}
Zhao, Y.; Li, M.; Lai, L.; Suda, N.; Civin, D.; and Chandra, V. 2018.
\newblock Federated learning with non-iid data.
\newblock \emph{arXiv preprint arXiv:1806.00582}.

\bibitem[{Zheng et~al.(2017)Zheng, Meng, Wang, Chen, Yu, Ma, and Liu}]{Zheng17ASGD}
Zheng, S.; Meng, Q.; Wang, T.; Chen, W.; Yu, N.; Ma, Z.-M.; and Liu, T.-Y. 2017.
\newblock Asynchronous Stochastic Gradient Descent with Delay Compensation.
\newblock In \emph{Proceedings of the 34th International Conference on Machine Learning - Volume 70}, ICML'17, 4120–4129. JMLR.org.

\end{thebibliography}
