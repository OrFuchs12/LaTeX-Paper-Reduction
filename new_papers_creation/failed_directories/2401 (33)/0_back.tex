\section{Background}
\subsection{Problem Setup}
In the context of our study, we consider a regression task derived from a dataset $\mathcal{D}=\left\{\left(X_i, y_i\right)\right\}_{i=1}^N$, where $X_i \in \mathbb{R}^d$ denotes an independently and identically distributed (i.i.d.) input vector with $d$ dimensions. Corresponding to each input $X_i$, we have a real-valued target $y_i \in \mathbb{R}$. Our dataset comprises $N$ samples and the task is to predict the targets based on the input data points. We tackle the regression task by modeling the probabilistic distribution of the target variable $y$, which is formulated as $p\left(y \mid f_{\boldsymbol{\theta}}(X)\right)$, where $f$ refers to a neural network, and $\boldsymbol{\theta}$ denotes its parameters. For simplicity, we omit the subscript $i$.






\subsection{Evidential Regression Network}
As is illustrated in Figure~\ref{uncertainty_framework}, Evidential Regression Network (ERN)~\cite{NEURIPS2020_aab08546} introduces a Gaussian distribution $\mathcal{N}\left(\mu, \sigma^2\right)$ with unknown mean $\mu$ and variance $\sigma$ for modeling the regression problem. It is generally assumed that a target value $y$ is drawn i.i.d. from the Gaussian distribution, and that the unknown parameters $\mu$ and $\sigma$ follow a Normal Inverse-Gamma (NIG) distribution:
\begin{equation} \label{eq:1}
\begin{gathered}
y \sim \mathcal{N}\left(\mu, \sigma^2\right) \\
\mu \sim \mathcal{N}\left(\gamma, \sigma^2 v^{-1}\right) \quad \sigma^2 \sim \Gamma^{-1}(\alpha, \beta) \\
(\mu, \sigma^2) \sim \operatorname{NIG}(\gamma, v, \alpha, \beta)
\end{gathered}
\end{equation}
where $\Gamma(\cdot)$ is the gamma function, parameters $\boldsymbol{m}=(\gamma, v, \alpha, \beta)$, and $\gamma \in \mathbb{R}$, $v>0$, $\alpha>1$, $\beta>0$. The parameters of NIG distribution $\boldsymbol{m}$ is modeled by the output of a neural network $f_{\boldsymbol{\theta}}(\cdot)$, where $\boldsymbol{\theta}$ is the trainable parameters of such neural network. To enforce constraints on $(v, \alpha, \beta)$, a $\operatorname{SoftPlus}$ activation is applied (additional +1 added to $\alpha$). Linear activation is used for $\gamma \in \mathbb{R}$.
Considering the NIG distribution in Eq~\ref{eq:1}, the prediction, aleatoric uncertainty, and epistemic uncertainty can be calculated as the following:
\begin{equation}
\begin{gathered}
\underbrace{\mathbb{E}[\mu]=\gamma}_{\text {prediction }} 
\quad \underbrace{\mathbb{E}\left[\sigma^2\right]=\frac{\beta}{\alpha-1}}_{\text {aleatoric }} \quad  \underbrace{\operatorname{Var}[\mu]=\frac{\beta}{v(\alpha-1)}}_{\text {epistemic }} 
\end{gathered}
\end{equation}
Therefore, we can use $\mathbb{E}[\mu]=\gamma$ as the prediction of ERN, $\mathbb{E}\left[\sigma^2\right]=\frac{\beta}{\alpha-1}$ and $\operatorname{Var}[\mu]=\frac{\beta}{v(\alpha-1)}$ as the uncertainty estimation of ERN.

The likelihood of an observation $y$ given $\boldsymbol{m}$ is computed by marginalizing over $\mu$ and $\sigma^2$:
\begin{equation}
\label{eq:ern-prob}
%\begin{aligned}
p\left(y \mid \boldsymbol{m}\right) 
% &=\int_{\sigma^2=0}^{\infty} \int_{\mu=-\infty}^{\infty} p\left(y \mid \mu, \sigma^2\right) p\left(\mu, \sigma^2 \mid \boldsymbol{m}\right) \mathrm{d} \mu \mathrm{d} \sigma^2 \\
=\mathrm{St}\left(y ; \gamma, \frac{\beta(1+v)}{v \alpha}, 2 \alpha\right)
%\end{aligned}
\end{equation}
where $\mathrm{St}\left(y ; \mu_{\mathrm{St}}, \sigma_{\mathrm{St}}^2, v_{S t}\right)$ is the Student-t distribution with location $\mu_{\mathrm{St}}$, scale $\sigma_{\mathrm{St}}^2$ and degrees of freedom $v_{S t}$.

\subsubsection{Training Objective of ERN}
The parameters $\boldsymbol{\theta}$ of ERN are trained by maximizing the marginal likelihood in Eq.~\ref{eq:ern-prob}. The training objective is to minimize the negative logarithm of $p\left(y \mid \boldsymbol{m}\right)$, therefore the negative log likelihood (NLL) loss function is formulated as:
\begin{equation}
\begin{aligned}
\mathcal{L}_{{\theta}}^{\mathrm{NLL}}&=\frac{1}{2} \log \left(\frac{\pi}{v}\right)-\alpha \log (\Omega) \\
&+\left(\alpha+\frac{1}{2}\right) \log \left(\left(y-\gamma\right)^2 v +\Omega\right) \\
&+\log \left(\frac{\Gamma(\alpha)}{\Gamma\left(\alpha+\frac{1}{2}\right)}\right)
\end{aligned}
\end{equation}
where $\Omega=2 \beta(1+v)$.

To minimize the evidence on errors, the regularization term $\mathcal{L}_{\theta}^{\mathrm{R}} = \left|y-\gamma\right| \cdot(2 v+\alpha)$ is proposed to minimize evidence on incorrect predictions. Therefore, the loss function of ERN is:
\begin{equation}
\label{eq:ern-loss}
    \mathcal{L}_{\theta}^{\mathrm{ERN}} = \mathcal{L}_{{\theta}}^{\mathrm{NLL}} + \lambda \mathcal{L}_{\theta}^{\mathrm{R}}
\end{equation}
where $\lambda$ is a settable hyperparameter. For simplicity, we omit $\theta$ in the following sections.


\subsection{Variants of ERN}
\label{sec:back:variant}
ERN is for univariate regression and has been extended to multivariate regression with a different prior distribution normal-inverse-Wishart (NIW) distribution~\cite{meinert2021multivariate}. Multivariate ERN employs an NIW distribution and, similar to ERN, formulates the loss function as (see~\citeauthor{meinert2021multivariate} for details):
\begin{equation}
\begin{aligned}
\mathcal{L}^{\mathrm{MERN}} & \equiv  \mathcal{L}^{\mathrm{NLL}}=  \log \Gamma\left(\frac{\nu-n+1}{2}\right)-\log \Gamma\left(\frac{\nu+1}{2}\right) \\
& +\frac{n}{2} \log \left(r+\nu\right)-\nu \sum_j \ell_j \\
& +\frac{\nu+1}{2} \log \left|\boldsymbol{L} \boldsymbol{L}^{\top}+\frac{1}{r+\nu}\left(\vec{y}-\vec{\mu}_{0}\right)\left(\vec{y}-\vec{\mu}_{0}\right)^{\top}\right| \\
& +\text { const. }
\end{aligned}
\end{equation}

And estimation of the prediction as well as uncertainties as:
\begin{equation}
\begin{gathered}
\underbrace{\mathbb{E}[\mu]=\vec{\mu}_{0}}_{\text {prediction }} \quad 
\underbrace{\mathbb{E}[\boldsymbol{\Sigma}] \propto \frac{\nu}{\nu-n-1} \boldsymbol{L} \boldsymbol{L}^{\top}}_{\text {aleatoric }}  \\
\underbrace{\operatorname{var}[\vec{\mu}] \propto \mathbb{E}[\boldsymbol{\Sigma}] / \nu}_{\text {epistemic }}
\end{gathered}
\end{equation}

To learn the parameters $\boldsymbol{m}=\left(\vec{\mu}_{0}, \vec{\ell}, \nu\right)$, a NN has to have  $n(n+3) / 2+1$ outputs ($p_{1} \cdots p_{m}$). Also, activation functions have to be applied to the outputs of NN to ensure the following:
\begin{equation}
    \nu = n(n+5)/2+\tanh p_{\nu} \cdot  n(n+3)/2  + 1 > n+1
\end{equation}
where $p_{\nu} \in (p_{1} \cdots p_{m})$. And
\begin{equation}
\left(\boldsymbol{L}\right)_{j k}= \begin{cases}\exp \left\{\ell_j\right\} & \text { if } j=k \\ \ell_{j k} & \text { if } j>k \\ 0 & \text { else. }\end{cases}
\end{equation}
where $\ell_j, \ell_{j k} \in (p_{1} \cdots p_{m})$.

