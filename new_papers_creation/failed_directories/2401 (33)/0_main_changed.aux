\relax 
\bibstyle{aaai23}
\citation{godard2017unsupervised,he2016deep}
\citation{zhao2023evaluating,devlin2018bert,vaswani2017attention}
\citation{ye2023bidirectional}
\citation{gal2016dropout,guo2017calibration}
\citation{pearce2020uncertainty,lakshminarayanan2017simple}
\citation{gal2016dropout,wilson2020bayesian,blundell2015weight}
\citation{sensoy2018evidential,NEURIPS2020_aab08546,malinin2018predictive}
\citation{NEURIPS2020_aab08546}
\citation{shafer1976mathematical}
\citation{malinin2018predictive,malinin2019reverse,bilovs2019uncertainty,haussmann2019bayesian,malinin2019ensemble}
\citation{meinert2021multivariate}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{uncertainty_framework}{{1}{1}{ An overview of the Evidential Regression Network (ERN) architecture with illustrations on the final distributions of the prediction. ERN outputs four predictions as distribution parameters, with activation functions like Relu or Softplus to constrain the output to meet the requirements of distribution parameters.}{}{}}
\citation{NEURIPS2020_aab08546,malinin2020regression,charpentier2021natural,oh2022improving,feng2023deep,mei2023uncertainty}
\citation{pandey2023learn}
\citation{pandey2023learn}
\citation{NEURIPS2020_aab08546}
\newlabel{eq:1}{{1}{2}{}{}{}}
\newlabel{eq:ern-prob}{{3}{2}{}{}{}}
\citation{meinert2021multivariate}
\citation{meinert2021multivariate}
\newlabel{eq:ern-loss}{{5}{3}{}{}{}}
\newlabel{sec:back:variant}{{2.3}{3}{}{}{}}
\newlabel{proof1}{{1}{3}{}{}{}}
\citation{NEURIPS2020_aab08546}
\citation{meinert2021multivariate}
\citation{meinert2021multivariate}
\newlabel{uncertainty_visualization}{{2}{4}{$\mathcal  {L}^{\mathrm  {ERN}}$ in Equation\nobreakspace  {}\eqref  {eq:ern-loss} cannot help the model get out of high uncertainty area while our proposed $\mathcal  {L}^{\mathrm  {U}}$ can still learn from samples in the grey area.}{}{}}
\citation{NEURIPS2020_aab08546}
\citation{meinert2021multivariate}
\citation{NEURIPS2020_aab08546}
\citation{NEURIPS2020_aab08546}
\citation{NEURIPS2020_aab08546}
\citation{NEURIPS2020_aab08546}
\citation{kuleshov2018accurate}
\citation{kuleshov2018accurate}
\newlabel{fig:toy_dataset}{{3}{5}{Uncertainty estimation on Cubic Regression. The blue shade represents prediction uncertainty. An effective evidential model would cause the blue shade to cover the distance between the predicted value and the ground truth precisely. Up: Comparison of model performance within HUA. Down: Comparison of model performance outside HUA. \text  {UR-ERN}\xspace  can cover the ground truth precisely under both within HUA and outside HUA.}{}{}}
\citation{silberman2012indoor}
\citation{NEURIPS2020_aab08546}
\citation{ronneberger2015u}
\citation{NEURIPS2020_aab08546,kuleshov2018accurate}
\citation{NEURIPS2020_aab08546}
\citation{NEURIPS2020_aab08546}
\citation{kuleshov2018accurate}
\citation{meinert2021multivariate}
\citation{meinert2021multivariate}
\citation{pearce2020uncertainty,lakshminarayanan2017simple}
\citation{gal2016dropout,wilson2020bayesian,blundell2015weight}
\citation{gal2016dropout}
\citation{sensoy2018evidential,NEURIPS2020_aab08546,malinin2018predictive}
\citation{sensoy2018evidential}
\citation{NEURIPS2020_aab08546}
\citation{meinert2021multivariate}
\citation{pandey2023learn}
\newlabel{fig:depth_HUA}{{4}{7}{Uncertainty prediction of Depth Estimation within HUA. (a) The blue shade represents prediction uncertainty. A good estimation of uncertainty should cover the gap between prediction and ground truth exactly. (b) Root Mean Square Error (RMSE) at various confidence levels. The evidential model with a larger confidence level should have a lower RMSE. (c) Uncertainty calibration calculated following\nobreakspace  {}\citeauthor  {kuleshov2018accurate}, the ideal curve is $y=x$. The calibration errors are 0.2261, 0.2250, and 0.0243 for ERN, NLL-ERN and UR-ERN, respectively.}{}{}}
\newlabel{fig:depth_outside_HUA}{{5}{7}{Uncertainty prediction of Depth Estimation outside HUA. (a) RMSE at various confidence levels. (b) Uncertainty calibration (ideal: $y=x$). The calib