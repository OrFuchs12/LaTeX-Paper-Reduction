\section{The ``Highlights'' Algorithm}
\label{sec:alg}
We developed HIGHLIGHTS, an algorithm that generates a summary of an agent's behavior from simulations of the agent in an online manner. HIGHLIGHTS uses the notion of state \emph{importance}~\cite{torrey2013teaching} to decide which states to include in the summary.  Intuitively, a state is considered important if taking a wrong action in that state can lead to a significant decrease in future rewards, as determined by the agent's Q-values. Formally, the importance of a state, denoted $I(s)$, is defined as: 
\begin{equation}
\label{eq:importance}
I(s)=\max\limits_{a}Q^{\pi}_{(s,a)}-\min\limits_{a}Q^{\pi}_{(s,a)}
\vspace{-0.1cm}
\end{equation}
 This measure has been shown to be useful for choosing teaching opportunities in the context of student-teacher reinforcement learning~\cite{torrey2013teaching,amirIjcaiRL}. We note, however, that this measure has significant limitations (e.g., sensitivity to the number of possible actions) which we discuss in Section~\ref{sec:disc}.

Before providing a detailed pseudo-code of the algorithm, we describe its operation at a high-level. HIGHLIGHTS generates a summary that includes trajectories that captures the most important states that an agent encountered in a given number of simulations. To do so, at each step it evaluates the importance of the state and adds it to the summary if its importance value is greater than the minimal value currently represented in the summary (replacing the minimal importance state). To provide more context to the user,  for each such state HIGHLIGHTS also extracts a trajectory of states neighboring it and the actions taken in those states.

%To do so, the algorithm runs simulations of the agent and maintains a priority queue of trajectories  to include in the summary. For each state $s$ the agent encounters, HIGHLIGHTS evaluates the state importance as defined in Equation~\ref{eq:importance}. If the importance of the state is greater than the minimal importance of a state that is currently included in the summary, that state will be removed and replaced with the current state $s$. Because a single state is non-informative on its own in many domains, rather than only including $s$ in the summary, HIGHLIGHTS considering a trajectory of length $l$ that includes $s$ (showing both states preceding $s$ and subsequent states). At the end of this process, the summary will include trajectories corresponding to the most important states the agent encountered in the simulation\footnote{In this work we choose trajectories for the summary based on the importance of a single state in the trajectory. We discuss possible extensions that assess the importance of a complete trajectory in Section~\ref{sec:disc}.}). To avoid including overlapping trajectories, we enforce an interval between trajectories (i.e., we do not consider states that appear immediately after a trajectory that was added to the summary). 

%The algorithm runs simulations of the agent and extracts states that it considers important, using the basic q-value-based importance defined in Equation~\ref{eq:importance}. It maintains a priority queue of size $k$ and populates it online while running the agent simulation. Specifically, when a new state $s$ is encountered and the queue is not yet full, the state is added to the queue. Alternatively, if the queue is full, the importance value of the state ($I(s)$) is compared with the lowest importance value currently in the queue. If $I(s)$ is greater than $I(s_{min})$, state $s$ will be added to the queue in place of $s_{min}$. To provide context in the summary, for each state we also show a trajectory of $\frac{l}{2}$ states which preceded $s$ in the execution and the $\frac{l}{2}$ states encountered following $s$. To ensure that trajectories do not overlap, after adding a trajectory to the summary we assess the state immediately following the last state in the trajectory.

A pseudo-code of the HIGHLIGHTS algorithm is  given in Algorithm~\ref{alg:highlights}. 
Table~\ref{tb:parameters} summarizes the parameters of the algorithm. 
HIGHLIGHTS takes as input the policy of the agent $\pi$ which is used to determine the agent's actions in the simulation and state importance values, the budget for the number of trajectories to include in the summary ($k$) and the length of each trajectory surrounding a state ($l$). Each such trajectory includes both states preceding  the important state and states that were encountered immediately after it. The number of subsequent states to include is determined by the $statesAfter$ parameter (the number of preceding states can be derived from this parameter and $l$). We also specify the number of simulations that can be run ($numSimulations$), and the minimal ``break'' interval between trajectories ($intervalSize$) which is used to prevent overlaps between trajectories. HIGHLIGHTS outputs a summary of the agent's behavior, which is a set of trajectories ($T$). 


\begin{table}[h]
\centering
\small
\resizebox{0.85\columnwidth}{!}{%
\begin{tabular}{|p{2.2cm}|p{6cm}|}
\hline
\textbf{Parameter} & \textbf{Description (value used in experiments)}                      \\ \hline
$k$                & Summary budget, i.e., number of trajectories (5)                                  \\ \hline
$l$                & Length of each trajectory (40)                                        \\ \hline
$numSimulations$   & The number of simulations run by HIGHLIGHTS (50)                      \\ \hline
$intervalSize$     & Minimal number of states between two trajectories in the summary (50) \\ \hline
$statesAfter$      & Number of states following $s$ to include in the trajectory (10)      \\ \hline
\end{tabular}}
\caption{Parameters of the HIGHLIGHTS algorithm and the values assigned to them in the experiments (in parentheses).}
\label{tb:parameters}
\vspace{-0.7cm}
\end{table}

% \begin{table}[h]
% \centering
% \small
% \begin{tabular}{|p{2.1cm}|p{3.8cm}|p{1.7cm}|}
% \hline
% \textbf{Parameter} & \textbf{Description}                                         & \textbf{Value in \hspace{2em} Experiments} \\ \hline
% $k$                & Summary budget (\# trajectories)                             & 5                             \\ \hline
% $l$                & Length of each trajectory                                    & 40                            \\ \hline
% $numSimulations$   & The number of simulations run by HIGHLIGHTS                  & 50                            \\ \hline
% $intervalSize$     & Minimal number of states between two trajectories & 50                            \\ \hline
% $statesAfter$      & Number of states following $s$ to include in the trajectory      & 10                            \\ \hline
% \end{tabular}
% \caption{Parameters of the HIGHLIGHTS algorithm and the values assigned to them in the experiments.}
% \label{tb:parameters}
% \vspace{-0.3cm}
% \end{table}

The algorithm maintains two data structures: $T$ is a priority queue (line 2), which will eventually hold the trajectories chosen for the summary; $t$ is a list of state-action pairs (line 3), which holds the current trajectory the agent encounters. The procedure runs simulations of the agent acting in the domain. At each step of the simulation, the agent takes an action based on its policy and advances to a new state (line 8). That state-action pair is added to the current trajectory (line 11). If the current trajectory reached its maximal length, the oldest state in the trajectory is removed (lines 9-10).  HIGHLIGHTS computes the importance of $s$ based on the Q-values of the agent itself, as defined in Equation~\ref{eq:importance} (line 14). 

If a sufficient number of states were encountered since the last trajectory was added to the summary, state $s$ will be considered for the summary (the $c==0$ condition in line 17). $s$ will be added to the summary if one of two conditions hold: either the size of the current summary is smaller than the summary size budget, or the importance of $s$ is greater than the minimal importance value of a state currently represented in the summary (line 17). If one of these conditions holds, a trajectory corresponding to $s$ will be added to the summary. The representation of a trajectory in the summary (a $summaryTrajectory$ object) consists of the set of state-action pairs in the trajectory (which will be presented in the summary), and the importance value $I_{s}$ based on which the trajectory was added (such that it could be compared with the importance of states encountered later). This object ($st$) is initialized with the importance value (line 20) and is added to the summary (line 21), replacing the trajectory with minimal importance if the summary reached the budget limit (lines 18-19).
%For each such state, we store the importance value based on which it was added to the summary and the trajectory of states surrounding it. 
%replacing the trajectory with minimal importance if the summary already reached the budget limit (lines 18-19). For each trajectory we store the state based on which it was added (line 20), such that we can compare its importance to new states as they are encountered. 
%If one of these conditions holds, a trajectory corresponding to $s$ will be added to the summary (line 21), replacing the trajectory with minimal importance if the summary already reached the budget limit (lines 18-19). For each trajectory we store the state based on which it was added (line 20), such that we can compare its importance to new states as they are encountered. 
Because the trajectory will also include states that follow $s$, the final set of state-action pairs in the trajectory is updated later (lines 15-16). Last, we set the state counter $c$ to the interval size, such that the immediate states following $s$ will not be considered for the summary. At the end of each simulation, the number of runs is incremented (line 24). The algorithm terminates when it reaches the specified number of simulations. 

% HIGHLIGHTS considers adding the trajectory corresponding to the current state $s$ unless f a sufficient number of states were encountered since the 

% , adding each state $s$ it encounters to the current trajectory. If the trajectory is at the maximal length, the new state replaces the oldest state in $t$ (lines 6--9). 

% If the summary size has not yet reached the budget, the current trajectory $t$ will be added to the summary (lines 11 --13). Otherwise, the trajectory will be added only if the importance of the last state encountered ($I(s)$) is greater than the minimal importance value currently in the summary (lines 13--15). We note that in our implementation of HIGHLIGHTS, to provide more context surrounding the important state, we added to the trajectory the 10 states that followed $s$ (removing the first 10 states in the trajectory to maintain a trajectory length $l$). We did not include this in the pseudo-code for clarity of presentation. 
% =
\begin{algorithm}
%\vspace{-0.2cm}
\SetAlFnt{\small\sf} 
\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
\KwIn{$\pi, k, l, numSimulations, intervalSize, statesAfter$}
\KwOut{$T$}
$runs = 0$ \\
$T \leftarrow PriorityQueue(k, importanceComparator)$ \\
$t \leftarrow$ empty list \\
$c = 0$ \\
\While {$(runs < numSimulations)$} {
$sim = InitializeSimulation()$ \\
\While {$(!sim.ended())$} {
$(s,a) \leftarrow sim.advanceState(\pi)$ \\
\If{$(|t| == l)$} {
$t.remove()$ 
}
$t.add((s,a))$ \\
\If {$(c>0)$} {
$c = c-1$
}
$I_{s} \leftarrow computeImportance(\pi,s)$ \\
\If{$(IntervalSize - c == statesAfter)$} {
lastSummaryTrajectory.setTrajectory(t) \\
}
% $I_{s} \leftarrow computeImportance(\pi,s)$ \\
% \uIf{$|T|<k$} {
% $T.add(t)$ 
% }
% \ElseIf{$I_{s} > minImportance(T)$} {
% T.pop() \\
% $T.add(t)$ 
% }
\If{($(|T|<k)$ or $(I_{s} > minImportance(T)))$ and $(c==0))$ } {
\If{$|T|==k$} {
T.pop()
}
$st\leftarrow$ new $summaryTrajectory(I_{s})$ \\
$T.add(st)$ \\
$lastSummaryTrajectory \leftarrow st$ \\
$c = intervalSize$ \\
}
}
runs = runs+1
}
\caption{The HIGHLIGHTS algorithm. }
\label{alg:highlights}
\end{algorithm}

%\vspace{-0.5cm}


 We chose to implement HIGHLIGHTS as an online algorithm  because it is less costly, both in terms of runtime and in terms of memory usage. 
 %For example, if important states are rare, it might be too costly to store all the states encountered and analyze them retrospectively. 
 In addition, such an algorithm can be incorporated into the agent's own learning process without additional cost. The algorithm can be easily generalized to work offline. 
 
\subsection{Considering State Diversity}
\label{sec:algDiv}
Because HIGHLIGHTS  considers the importance of states in isolation when deciding whether to add them to the summary, the produced summary might include trajectories that are similar to each other. This could happen in domains in which the most important scenarios tend to be similar to each other. To mitigate this problem, we developed a simple extension to the HIGHLIGHTS algorithm, which we call HIGHLIGHTS-DIV. Similarly to HIGHLIGHTS, this algorithm also determines which states to include in the summary based on their importance. However, it also attempts to avoid including a very similar set of states in the summary, thus potentially utilizing the summary budget more effectively. 

HIGHLIGHTS-DIV takes into consideration the diversity of states in the following way: when evaluating a state $s$, it first identifies the state most similar to $s$ that is currently included in the summary\footnote{We assume that distance metric to compare states can be defined. This can be done in many domains,  e.g., by computing Euclidean distance if states are represented by feature vectors.}, denoted $s'$. Then, instead of comparing the importance of a state to the minimal importance value that is currently included in the summary, HIGHLIGHTS-DIV compares $I_{s}$ to $I_{s'}$. If $I_{s}$ is greater than $I_{s'}$, the trajectory which includes $s'$ in the summary will be replaced with the current trajectory (which includes $s$). This approach allows less important states to remain represented in the summary (because they will not be compared to some of the more important states that differ from them), potentially increasing the diversity of trajectories in the summary and thus conveying more information to users. 





