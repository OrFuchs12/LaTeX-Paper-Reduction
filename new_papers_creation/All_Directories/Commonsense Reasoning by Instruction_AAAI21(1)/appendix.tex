% \hrule height 4pt
% \vskip 0.25in
% {\centering \Large\bf Conversational Neuro-Symbolic Commonsense Reasoning}
% \vskip 0.25in
% \hrule height 1pt
\section*{Appendix}
\subsection*{Data Collection}
Data collection was done in two stages. In the first stage, we collected if-then-because commands from humans subjects. In the second stage, a team of annotators annotated the data with commonsense presumptions. Below we explain the details of the data collection and annotation process.

In the data collection stage, we asked a pool of human subjects to write commands that follow the general format: if $\langle$ state holds $\rangle$ then $\langle$ perform action $\rangle$ because $\langle$ i want to achieve goal $\rangle$. The subjects were given the following instructions at the time of data collection:

``
% \vskip -0.5em
Imagine the two following scenarios:

Scenario 1: Imagine you had a personal assistant that has access to your email, calendar, alarm, weather and navigation apps, what are the tasks you would like the assistant to perform for your day-to-day life? And why?

Scenario 2: Now imagine you have an assistant/friend that can understand anything. What would you like that assistant/friend to do for you?

Our goal is to collect data in the format ``If …. then …. because ….''
% \vskip -0.5em
''


After the data was collected, a team of annotators annotated the commands with additional presumptions that the human subjects have left unspoken. These presumptions were either in the \emph{if}-clause and/or the \emph{then}-clause and examples of them are shown in Tables \ref{tab:statement_stats} and \ref{tab:data_examples}
\input{data_examples.tex}

\subsection*{Logic Templates}
As explained in the main text, we uncovered 5 different logic templates, that reflect humans' reasoning, from the data after data collection. The templates are listed in Table \ref{tab:logic_templates}. In what follows, we will explain each template in detail using the examples of each template listed in Tab.~\ref{tab:logic_templates}.

In the blue template (Template 1), the \textState results in a ``bad state'' that causes the \emph{not} of the goal. The speaker asks for the \textAction in order to avoid the bad state and achieve the \textGoal. For instance, consider the example for the blue template in Table \ref{tab:logic_templates}. The \textState of snowing a lot at night, will result in a bad state of traffic slowdowns which in turn causes the speaker to be late for work. In order to overcome this bad state. The speaker would like to take the \textAction, waking up earlier, to account for the possible slowdowns cause by snow and get to work on time.

In the orange template (Template 2), performing the \textAction when the \textState holds allows the speaker to achieve the \textGoal and not performing the \textAction when the \textState holds prevents the speaker from achieving the \textGoal. For instance, in the example for the orange template in Table \ref{tab:logic_templates} the speaker would like to know who the attendees of a meeting are when the speaker is walking to that meeting so that the speaker is prepared for the meeting and that if the speaker is not reminded of this, he/she will not be able to properly prepare for the meeting. 

In the green template (Template 3), performing the \textAction when the \textState holds allows the speaker to take a hidden \textAction that enables him/her to achieve the desired \textGoal. For example, if the speaker is reminded to buy flower bulbs close to the Fall season, he/she will buy and plant the flowers (hidden \textAction s) that allows the speaker to have a pretty spring garden.

In the purple template (Template 4), the \textGoal that the speaker has stated is actually a goal that they want to \emph{avoid}. In this case, the \textState causes the speaker's \textGoal, but the speaker would like to take the \textAction when the \textState holds to achieve the opposite of the \textGoal. For the example in Tab.~\ref{tab:statement_stats}, if the speaker has a trip coming up and he/she buys perishables the perishables would go bad. In order for this not to happen, the speaker would like to be reminded not to buy perishables to avoid them going bad while he/she is away. 

The rest of the statements are categorized under the ``other'' category. The majority of these statements contain conjunction in their \textState and are a mix of the above templates. 
% In the pink template (Template 5), the \textGoal is an action that the speaker will do if the \textAction (in the if-then-because statement) is performed as a result of the \textState. For the example in Tab.~\ref{tab:logic_templates}, if the speaker is reminded of a schedule conflict, then he/she will take action to resolve it by contacting his/her colleagues.
A reasoning engine could potentially benefit from these logic templates when performing reasoning. We provide more detail about this in the Extended Discussion section in the Appendix. 

\input{logic_templates.tex}

\subsection*{Prolog Background}
% In Prolog, all the data including a Prolog program are represented as Prolog terms. \amoscomment{can you please rephrase the previous sentence? Or maybe remove it.}
Prolog \cite{colmerauer1990introduction} is a declarative logic programming language. A Prolog program consists of a set of predicates. A predicate has a name (functor) and $N\geq0$ arguments. $N$ is referred to as the arity of the predicate. A predicate with functor name $F$ and arity $N$ is represented as $F(T_1, \dots, T_N)$ where $T_i$'s, for $i\in [1,N]$, are the arguments that are arbitrary Prolog terms. A Prolog term is either an atom, a variable or a compound term (a predicate with arguments). A variable starts with a capital letter (e.g., \prologTerm{Time}) and atoms start with small letters (e.g. \prologTerm{monday}). 
% We use the notation $\state$, $\action$ and $\goal$ to represent the logical form of the \textState, \textAction and \textGoal, respectively where $S$, $A$ and $G$ are predicate names and $X, Y$ and $Z$ indicate the \emph{list} of arguments of each predicate. We refrained from expanding the argument list for these predicates to make the notation more compact.
% The predicate name is an atom and each argument is an arbitrary Prolog term.
A predicate defines a relationship between its arguments. For example, \prologTerm{isBefore(monday, tuesday)} indicates that the relationship between Monday and Tuesday is that, the former is before the latter.

A predicate is defined by a set of clauses. A clause is either a Prolog~~\emph{fact} or a Prolog \emph{rule}. A Prolog rule is denoted with \prologTerm{$\text{Head} \coloneq \text{Body}.$},
% \begin{equation}
%     \text{Head} \coloneq \text{Body}.
% \end{equation}
where the \prologTerm{Head} is a predicate, the \prologTerm{Body} is a conjunction (\prologTerm{$\wedge$}) of predicates, \prologTerm{$\coloneq$} is logical implication, and period indicates the end of the clause. The previous rule is an if-then statement that reads ``\emph{if} the Body holds \emph{then} the Head holds''.
%The Body of the rule is also referred to as the rule goal. %If the body of the rule holds then the head of the rule holds as well.
A fact is a rule whose body always holds, and is indicated by \prologTerm{Head.} 
% \begin{equation}
%     Head.
% \end{equation}
, which is equivalent to \prologTerm{Head $\coloneq$  true}. Rows 1-4 in Table \ref{tab:kb_examples} are rules and rows 5-8 are facts.  
% \begin{equation}
%     \text{Head} \coloneq true.
% \end{equation}

Prolog can be used to logically ``prove'' whether a specific query holds or not (For example, to prove that \prologTerm{isAfter(wednesday,thursday)?} is \prologTerm{false} or that \prologTerm{status(i, dry, tuesday)?} is \prologTerm{true} using the Program in Table \ref{tab:kb_examples}). The proof is performed through \emph{backward chaining}, which is a backtracking algorithm that usually employs a depth-first search strategy implemented recursively. 
% Proof is performed using backward chaining, a backtracking algorithm implemented recursively.
In each step of the recursion, the input is a query (goal) to prove and the output is the proof's success/failure. in order to prove a query, a rule or fact whose head \emph{unifies} with the query is retrieved from the Prolog program. The proof continues recursively for each predicate in the body of the retrieved rule and succeeds if all the statements in the body of a rule are \prologTerm{true}. The base case (leaf) is when a fact is retrieved from the program.

At the heart of backward chaining is the \emph{unification} operator, which matches the query with a rule's head. Unification first checks if the functor of the query is the same as the functor of the rule head. If they are the same, unification checks the arguments. If the number of arguments or the arity of the predicates do not match unification fails. Otherwise it iterates through the arguments. For each argument pair, if both are grounded atoms unification succeeds if they are exactly the same grounded atoms. If one is a variable and the other is a grounded atom, unification grounds the variable to the atom and succeeds. If both are variables unification succeeds without any variable grounding. The backwards chaining algorithm and the unification operator is depicted in Figure \ref{fig:prooftree_simple}.

% Unification succeeds if a match is found. Given a query or a goal to prove, backward chaining considers all the goals and attempts to unify the head of each rule with the goal. If unification succeeds, the rule will be added to a proof stack and the proof proceeds with the body of the rule as the new goal. If a successful proof is found, the variable groundings will be returned in the output. We refer to the rules in the proof stack as the proof trace or the proof tree (Fig \ref{fig:prooftree}).

% Proof is performed using backward chaining, a backtracking algorithm implemented recursively. In each step of the recursion, the input is a goal to prove and the output is the proof's success/failure. in order to prove a query, a rule or fact whose head unifies with the query is retrieved from \KB. The proof continues recursively for each predicate in the body and succeeds if all the statements in the body of a rule are \prologTerm{true}. The base case (leaf) is when a fact is retrieved from \KB

\input{proof_trace_simple.tex}
\input{kb_examples.tex}

\subsection*{Parsing}
The goal of our parser is to extract the \textState, \textAction and \textGoal from the input utterance and convert them to their logical forms $\state$, $\action$, and $\goal$, respectively. The parser is built using Spacy \cite{honnibal2017spacy}. We implement a relation extraction method that uses Spacy's built-in dependency parser. The language model that we used is the en$\_$coref$\_$lg$-3.0.0$ released by Hugging face\footnote{\url{https://github.com/huggingface/neuralcoref-models/releases/download/en_coref_lg-3.0.0/en_coref_lg-3.0.0.tar.gz}}. %Amos: I think that the previous sentence and footnote can be removed to save space.
The predicate name is typically the sentence verb or the sentence root. The predicate's arguments are the subject, objects, named entities and noun chunks extracted by Spacy. The output of the relation extractor is matched against the knowledge base through rule-based mechanisms including string matching to decide weather the parsed logical form exists in the knowledge base. 
% in the future we will use the learned rule embeddings to match the parsed predicate against the knowledge base to make it more robust. \amoscomment{Why do you talk about future work here? This is confusing.}
If a match is found, the parser re-orders the arguments to match the order of the arguments of the predicate retrieved from the knowledge base. This re-ordering is done through a type coercion method. In order to do type coercion, we use the types released by Allen AI in the Aristo tuple KB v1.03 Mar 2017 Release \cite{dalvi2017domain} and have added more entries to it to cover more nouns. %Amos: again, the exact specifics can be removed for now to save space.
The released types file is a dictionary that maps different nouns to their types. For example, doctor is of type person and Tuesday is of type date. If no match is found, the parsed predicate will be kept as is and CORGI tries to evoke relevant rules conversationally from humans in the \emph{user feedback loop} in Figure \ref{fig:model}.
% between the new predicate and its existing knowledge base through conversation.

We would like to note that we refrained from using a grammar parser, particularly because we want to enable open-domain discussions with the users and save the time required for them to learn the system's language. As a result, the system will learn to adapt to the user's language over time since the background knowledge will be accumulated through user interactions, therefore it will be adapted to that user. A negative effect, however, is that if the parser makes a mistake, error will propagate onto the system's future knowledge. This is an interesting future direction that we are planning to address. %Amos: can this be moved to the discussion section?

% Natural language statements and instructions are converted to logical statements through a relation extraction strategy implemented using Spacy \citep{spacy2}. \amoscomment{I think that giving the details of the library used should be done later when describing the engine (not in the introduction).}
% The alternative is to use a grammar based parser which will limit the user to the grammar that the agent understands. However, our relation extraction strategy will allow the users to have open domain dialogs with the engine rather than restricting them to the language that the agent understands.

% We have used the Aristo Tuples KB v1.03 - Mar 2017 Release \cite{dalvi2017domain} for automated typing of the added queries.

\subsection*{Inference} The inference algorithm for our proposed neuro-symbolic theorem prover is given in Alg.~\ref{alg:inference}.
% We use the learned rule and variable embeddings and use cosine similarity to decide if variable unification succeeds or fails in each step. If the cosine similarity is above a certain threshold then unification succeeds, otherwise it fails. 
% Our inference algorithm is based on backward chaining. 
In each step $t$ of the proof, given a query $Q$, we calculate $\mathbf{q}_t$
% using the trained character RNN 
and $\mathbf{r}_t$ from the trained model to compute $\mathbf{r}_{t+1}$. Next, we choose $k$ entries of $\mathbf{M}^{rule}$ corresponding to the top $k$ entries of $\mathbf{r}_{t+1}$ as candidates for the next proof trace. $k$ is set to $5$ and is a tuning parameter. For each rule in the top $k$ rules, we attempt to do variable/argument unification by computing the cosine similarity between the arguments of $Q$ and the arguments of the rule's head. If all the corresponding pair of arguments in $Q$ and the rule's head have a similarity higher than threshold, $T_1=0.9$, unification succeeds, otherwise it fails. %Amos: this repeats what is written in the previous paragraph. Can be shortened to save space. Forough: done
% Note that this allows us to unify atoms such as \prologTerm{me} and \prologTerm{i} %given good embeddings, %Amos: removed this, Forough: done
% which is not possible in Prolog. 
% Threshold $T_1=0.9$ is a tuning parameter, and in this paper we set it to $0.9$. %Amos: just mention its value when you describe T_1. Forough: done
If unification succeeds, we move to prove the body of that rule. If not, we move to the next rule. %Refer to Algorithm \ref{alg:inference} for more details.
% Inference is a three step process. Given each goal G to prove, using the rule embeddings we compute the best next rule $R$ to choose according to a similarity metric. 
% \amoscomment{Which similarity metric? Was this something mentioned earlier?
% In the algorithm it seems that the next rule chosen randomly, so I think that you should add a few words there. Maybe a for loop on the list of rules sorted according to their the similarity metric would better convey the idea of what is going on.}
% In the next step we unify the head of the rule with the goal. Unification is performed in a soft manner by comparing the embedding of the variables and atoms and the predicate name. If unification succeeds we attempt to prove the body of the selected rule. Otherwise we try to prove the goal using another rule $R'\neq R$. The inference algorithm is given in Algorithm~\ref{alg:inference}. 
% \facomment{Mikayla: can you look at the algorithm and modify it according to what you do?}\mgcomment{Modified}
\input{ns_theorem_prover.tex}

\subsection*{Extended Discussion}
% The last 4 columns in 
Table \ref{tab:user_study_lt} shows the performance breakdown with respect to the logic templates in Table \ref{tab:logic_templates}. %Amos: I don't understand the LTi in the table, I think that it is only discussed in the appendix, this is problematic.
Currently, CORGI uses a general theorem prover that can prove all the templates. The large variation in performance indicates that taking into account the different templates would improve the performance. For example, the low performance on the green template is expected, since CORGI currently does not support the extraction of a hidden \textAction from the user, and interactions only support extraction of missing \textGoal s. This interesting observation indicates that, even within the same benchmark, we might need to develop several reasoning strategies to solve reasoning problems. Therefore, even if CORGI adapts a general theorem prover, accounting for logic templates in the conversational knowledge extraction component would allow it to achieve better performance on other templates.

\input{results_table.tex}


% overview of how i conducted user study
% In the user study of 14 participants, the participants were informed that they would be given 10 if-then-because statements where they would engage with an open-dialog AI system that would prompt them with up to three clarification questions. The users were also informed that the AI system would attempt to prove the goal with access to the knowledge base and the user's dialogue. \\
% % observation
% There were 210 utterances in this user study and 259 utterances in the original user study.

% Overall, we made several observations about the user's interactions with CORGI. First, long or wordy responses to the system's questions made the dialog a lot more difficult to handle two-fold. First, long responses is more difficult for the AI system to parse. Because natural language is inherently fluid whereas Prolog, which our system uses, pattern matches the responses to rules or facts, providing wordier responses decreases the system's probability of grounding 
% % maybe define grounding?
% the goal. As a result, the system is more likely to go back to the user with ``How do I know if I $\langle Y \rangle$ where $Y$ is the user's initial response. The phrasing of this question is too ambiguous or syntactically incorrect because user's responses have high variance. For example, asking a user ``How do I know if I \textit{If I maintain a healthy diet and exercise daily then I will stay fit}?'' cannot warrant a valid response that provides the system with information it needs to complete the proof. Even if the goal is grounded after the user's response, it is possible that the system will choose a more complex goal that will be more difficult to prove. Secondly, the system's dialog sometimes frustrated or confused the user. If the user's response is $Y$ and $Y$ cannot be grounded, the system prompts the user with ``How do I know if I $Y$''. However, sometimes $Y$ is true because of common sense. For example, asking the user, ``How do I know if \textit{I check my email}?'' proves difficult to answer because from a human's perspective, there's no indication that you checked your email except for remembering that you did.

% In addition, some of the users also gave feedback about the wording of the system's questions. In particular, one critique was the tenses of the questions. 
% % Our system, when interacting with the user, only asks, ``How do I know if I $\langle X \rangle$?'' where $X$ is either the goal or the user's response to a previous question. When $X$ is the goal, the phrasing sometimes confused the users since the goals across the statements are syntactically varied. For example, one of the experiment statements was, ``If I haven't been to the gym for more than 3 days then remind me to go to the gym because I want to stay fit.'' As a result, the system prompts the user with, ``How do I know if I stay fit?'' The implication here is that the user has

% We also observed that background did not have a correlation with performance. Out of the 12 users involved in the study, eight were of computer science or engineering background and the other four were of business administration or psychology background. Three of the technical students achieved 0 statements whereas only one non-technical student achieved 0 statements. This means 62.5\% of the engineering undergraduates achieved a non-zero result. However, of the technical students who did achieve non-zero results (excluding myself because I am not novice), they achieved on average 2.25 statements. For the non-technical undergraduates who had non-zero results, they achieved 1.33 statements on average. While our sample size is too small to make any conclusions, the results imply that there does not exist a correlation between background or major and performance. 

% \input{COMET_generations.tex}