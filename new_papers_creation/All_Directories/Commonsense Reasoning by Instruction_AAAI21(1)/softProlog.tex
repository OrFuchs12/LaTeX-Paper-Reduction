% \vspace{-1em}
\section{Neuro-Symbolic Theorem Proving}
\label{sec:softProlog}
% \vspace{-1em}
% As stated earlier, given the state $\state$, action $\action$ and goal $\goal$, we intend to prove the goal against a knowledge base $K$ such that the state and action appear in the proof path. This is how we filter-out irrelevant proofs. The variable groundings and the proof trace itself allows us to make informed decisions about the under specified user statements. For example, Figure \ref{fig:prooftree} shows the proof trace for ``if it snows at night then wake me up early because I want to get to work on time''. Traversing the proof trace allows CORGI to indicate the amount of precipitation to search for when checking the state $\state$ and also the meaning of ``early'' in the action $\action$ according to the user's commute time and traffic time.
Our Neuro-Symbolic theorem prover is a neural modification of \emph{backward chaining} and uses the vector similarity between rule and variable embeddings for unification.
In order to learn these embeddings, our theorem prover learns a general proving strategy by training on proof traces of successful proofs.
% The proving strategy that the model learns to mimic is \emph{backward chaining}. 
From a high level, for a given query our model maximizes the probability of choosing the correct rule to pick in each step of the backward chaining algorithm.
% Our system uses an embeddings-based approach to enable it to handle the variations typical to natural language settings. \textState $\state$, \textAction $\action$ and \textGoal $\goal$ are all extracted directly from user utterances, and may not exactly match anything in the knowledge base.
% The inference algorithm we use for extracting a relevant proof of $\goal$ is backward chaining. In what follows we explain our soft unification algorithm that learns
% Given a \textGoal to prove, backward chaining searches all the rules and facts in the knowledge base to find the ones whose head unify with the \textGoal. If unification succeeds for a rule, the algorithm moves on to prove the statements in the body of the rule in the same manner. If unification with a fact succeeds the proof terminates with a success. Since our rules and facts are extracted from natural language statements, there is a high chance that there is variation in the arguments and predicate names. For example, unification of \prologTerm{awake(i,early$\_$morning)} and \prologTerm{wake(me, early$\_$morning)} will fail using traditional backward chaining. Therefore, we propose a ``soft unification'' strategy to deal with these variations. \amoscomment{The information in this paragraph has appeared twice earlier, you can save some space here.}
% Our soft unification algorithm learns 
% embeddings for the prolog terms and uses vector similarity to make a decision about the success or failure of unification. In order to learn these embeddings, we collect proof traces and maximize the probability of extracting the correct proof for each query by walking down the proof path and maximizing the probability of choosing the correct rule next and making the correct variable groundings if any.
% \amoscomment{All these citations don't appear correctly in pdf.}
% \cite{rocktaschel2017end} and \cite{weber2019nlprolog} also propose a soft unification method, but they present soft ``AND'' and ``OR'' operations and propose a differentiable unification operator. 
This proposal is an adaptation of Reed et al.'s Neural Programmer-Interpreter \cite{reed2015neural} that learns to execute algorithms such as addition and sort, by training on their execution trace.

In what follows, we represent scalars with lowercase letters, vectors with bold lowercase letters and matrices with bold uppercase letters. %Amos: I don't really like the future tense, but not sure if this is really a problem. Forough: fixed it to present tense
% Our main observation is that we want to use the chain of reasoning -- proof trace in Prolog -- for a given statement to learn continuous representations for the facts and rules in the knowledge base.
% Given a state $\state$, and action $\action$ and a goal $\goal$ we want to indicate the $\theta$ for which a proof path in the knowledge base of facts $K$ exists. Formally:
% \amoscomment{Something is obviously missing here. Where is $\theta$ defined? I don't understand what it stands for.}
% \begin{equation}
%     S(X) \cap A_\theta(X) \cap K \rightarrow G_\theta{X}
% \end{equation}
% Our goal is to represent the backward chaining algorithm through a differentiable process similar to \cite{rocktaschel2017end}. Our approach is inspired by the neural program interpreter \citep{reed2015neural} where the authors use the program traces to execute a given program. Our goal here is to prove a statement given the complete proof trace.
% \facomment{consider using an RNN for embedding the rules}
$\mathbf{M}^{\text{rule}} \in \mathbb{R}^{n_1 \times m_1}$ denotes the embedding matrix for the rules and facts, where $n_1$ is the number of rules and facts and $m_1$ is the embedding dimension. 
% Row $i$ in $\mathbf{M}^{\text{rule}}$ corresponds to the $i^{\text{th}}$ rule/fact in the knowledge base $K$. 
$\mathbf{M}^{\text{var}} \in \mathbb{R}^{n_2 \times m_2}$ denotes the variable embedding matrix, where $n_2$ is the number of all the atoms and variables in the knowledge base and $m_2$ is the variable embedding dimension. 
% Each variable and atom has a corresponding row in the variable embedding matrix. 
Our knowledge base is type coerced, therefore the variable names are associated with their types (e.g., \prologTerm{alarm(Person,Time)})
% In the next subsection we present the learning algorithm for learning these embeddings and present the details of the model.
% We will represent the rules with two key-value matrices $M^{\text{key}} \in \mathbb{R}^{N \times K}$ and  $M^{\text{rule}} \in \mathbb{R}^{N \times M_1}$.  Variables are represented with an embedding matrix $V^{\text{var}} \in \mathbb{R}^{N \times M_2}$ and an attention over the embedding space  $V^{\text{key}} \in \mathbb{R}^{N \times K}$. 
% \mgcomment{Just an embedding matrix for both rules and variables, variable embedding matrix is initialized with gLoVe embeddings} Row $i$ in the $M$ matrices corresponds to rule $i$ in the knowledgebase, and row $i$ in the $V$ matrices corresponds to variable $i$ in the knowledgebase.
%\mgcomment{Could mention that all facts with a certain predicate are mapped to a common $\langle predicate\rangle/\langle arity\rangle$ embedding but rules have their own embedding} \facomment{debating whether we want to give this much detail}. 
%% We assume that we maintain and update a list of variables and predicates from the knowledgebase.
%% \amoscomment{I think that I undetsand this, but I thought that you are using Prolog. The standard Prolog obviously doesn't support all this, right? Or is all this only for selecting the rules to be used, but then the proof uses standard Prolog.} \facomment{that would be the inference algorithm. No we are not using standard prolog anywhere}
%\amoscomment{Can you please give an example here? What might be the an entry for a specific case, what does that entry actually mean?}\facomment{Is this still needed?}
% \vspace{-0.5em}
\paragraph{Learning}
% In this section, we present the model that takes as input a query to prove and returns a proof that consists of a sequence of rules and variable bindings. This model is adapted from \cite{reed2015neural}'s neural program interpreter that learns to execute programs and is trained on program execution traces. In this paper, we learn to complete proofs by training a model using proof traces.% \amoscomment{Again citations don't appear correctly. I think that citet doesn't work.}
The model's core consists of an LSTM network whose hidden state indicates the next rule in the proof trace and a proof termination probability, given a query as input. The model has a feed forward network that makes variable binding decisions. The model's training is fully supervised by the proof trace of a query given in a depth-first-traversal order from left to right (Fig.~\ref{fig:prooftree}). The trace is sequentially input to the model in the traversal order as explained in what follows. In step $t \in [0,T]$ of the proof, the model's input is $\epsilon^{inp}_t = \big(\mathbf{q}_t, \mathbf{r}_t, (\mathbf{v}^1_t, \dots, \mathbf{v}^\ell_t) \big)$ and $T$ is the total number of proof steps. $\mathbf{q}_t$ is the query's embedding and is computed by feeding the predicate name of the query into a character RNN. $\mathbf{r}_t$ is the concatenated embeddings of the rules in the parent and the left sister nodes in the proof trace, looked up from $\mathbf{M}^{rule}$. For example in Fig.\ref{fig:prooftree}, $\mathbf{q}_3$ represents the node at proof step $t=3$, $\mathbf{r}_3$ represents the rule highlighted in green (parent rule), and $\mathbf{r}_4$ represents the fact \prologTerm{alarm(i, 8)}. 
% We concluded empirically that it is critical to use both the parent and the left sister nodes in $r_t$. 
The reason for including the left sister node in $\mathbf{r}_t$ is that the proof is conducted in a left-to-right depth first order. Therefore, the decision of what next rule to choose in each node is dependent on both the left sisters and the parent (e.g. the parent and the left sisters of the node at step $t=8$ in Fig.~\ref{fig:prooftree} are the rules at nodes $t=1$, $t=2$, and $t=6$, respectively). 
% since both the sister nodes and the parent node are needed to make an informed decision about the next rule $r_{t+1}$ that should be chosen for the proof. 
% In order to compute $\mathbf{r}_t$, we concatenate the embeddings of the parent and sister nodes, which are looked up from $\mathbf{M}^{rule}$. 
The arguments of the query are presented in $(\mathbf{v}^1_t, \dots, \mathbf{v}^\ell_t)$ where $\ell$ is the arity of the query predicate. For example, $\mathbf{v}_3^1$ in Fig \ref{fig:prooftree} is the embedding of the variable \prologTerm{Person}. Each $\mathbf{v}^i_t$ for $i \in [0,\ell]$, is looked up from the embedding matrix $\mathbf{M}^{var}$. The output of the model in step $t$ is $\epsilon^{out}_t = \big(c_t, \mathbf{r}_{t+1}, (\mathbf{v}^1_{t+1}, \dots, \mathbf{v}^\ell_{t+1}))$ and is computed through the following equations
\begin{align}
    \mathbf{s}_t &= f_{enc}(\mathbf{q}_t,), ~~
    \mathbf{h}_t = f_{lstm}(\mathbf{s}_t, \mathbf{r}_t ,\mathbf{h}_{t-1}), \\
    c_t &= f_{end}(\mathbf{h}_t), ~~
    \mathbf{r}_{t+1} = f_{rule}(\mathbf{h}_t), ~~ %\label{eq:rt}
    \mathbf{v}^i_{t+1} = f_{var}(\mathbf{v}^i_t), \label{eq:ct} %\label{eq:vt},
\end{align}
where $\mathbf{v}^i_{t+1}$ is a probability vector over all the variables and atoms for the $i^{\text{th}}$ argument, $\mathbf{r}_{t+1}$ is a probability vector over all the rules and facts and $c_t$ is a scalar probability of terminating the proof at step t.
$f_{enc}$, $f_{end}$, $f_{rule}$ and $f_{var}$ are feed forward networks with two fully connected layers, and $f_{lstm}$ is an LSTM network. The trainable parameters of the model are the parameters of the feed forward neural networks, the LSTM network, the character RNN that embeds $\mathbf{q}_t$ and the rule and variable embedding matrices $\mathbf{M}^{rule}$ and $\mathbf{M}^{var}$.

Our model is trained end-to-end. In order to train the model parameters and the embeddings, we maximize the log likelihood probability given below
\begin{equation}
    {\bm \theta}^* = argmax~_{{\bm \theta}} \sum_{\epsilon^{out}, \epsilon^{in}} \log(P(\epsilon^{out} \vert \epsilon^{in};{\bm \theta})),
\end{equation}
where the summation is over all the proof traces in the training set and ${\bm \theta}$ is the trainable parameters of the model. We have
\begin{equation}
    \log(P(\epsilon^{out} \vert \epsilon^{in};{\bm \theta})) = \sum_{t=1}^T \log P(\epsilon^{out}_t \vert \epsilon^{in}_1 \dots \epsilon^{in}_{t-1}; {\bm \theta}), \\
\end{equation}
\begin{align}
    \log P(\epsilon^{out}_t \vert \epsilon^{in}_1 \dots \epsilon^{in}_{t-1}; {\bm \theta}) = & \log P(\epsilon^{out}_t \vert \epsilon^{in}_{t-1}; {\bm \theta}) \nonumber \\ 
     = & \log P(c_{t}\vert \mathbf{h}_t) +  \label{eq:probs}  \nonumber \\
     & \log P(\mathbf{r}_{t+1}\vert \mathbf{h}_t) + \nonumber \\
     & \log \sum_{i} P(\mathbf{v}^i_{t+1}\vert \mathbf{v}^i_t).
\end{align} 
% \end{equation}
% \begin{align}
%     \log P(\epsilon^{out}_t \vert \epsilon^{in}_1 \dots \epsilon^{in}_{t-1}; {\bm \theta}) & = \log P(\epsilon^{out}_t \vert \epsilon^{in}_{t-1}; {\bm \theta}) \\
%     & = \log P(c_{t}\vert \mathbf{h}_t) \label{eq:probs} + \nonumber \\
%     & ~~~~ \log P(\mathbf{r}_{t+1}\vert \mathbf{h}_t) +  \nonumber \\
%     & ~~~~  \log \sum_{i} P(\mathbf{v}^i_{t+1}\vert \mathbf{v}^i_t) 
% \end{align}
% \begin{align}
Where the probabilities in Equation \eqref{eq:probs} are given in Equations  \eqref{eq:ct}. The inference algorithm for porving is given in the Appendix, section Inference. %, \eqref{eq:rt} and \eqref{eq:vt}.
% Our learning algorithm uses the proof trace of different goals to learn the rule embedding and the variable embedding matrices. An example of the proof trace for a given goal is depicted in Figure \ref{fig:prooftree}. We optimize the log likelihood function 
% We present the input and output at each step of the proof with $\epsilon^{inp}_t = (q_t, r_t, v_t)$ where $q_t$ is the embedding of the query of the head of the rule \mgcomment{embedding of predicate of query} that is obtained through a character RNN, \amoscomment{Can you please provide more information on this character RNN? what was it trained to do (predict the next character?) what corpus was it trained on? (If it was taken from somewhere reference it).}
% $r_t$ is the embedding of the rule in this proof step \amoscomment{Where is this embedding obtained from?} 
%\mgcomment{$r\_t$ is a tuple of the embeddings of the parent rule and left sister rule in the proof tree or zero vector if not present for e.g. current_query: $food(X)$. parent_rule}
% and $v_t$ is the representation of the variables of the rule, which is obtained by a weighted average of the attention vector over the variable matrix. The output is $\epsilon^{out}_t = (v_{t+1}, r_{t+1}, c_t)$ where $c_t$ is the probability of proof completion. \amoscomment{I think that all this would be much easier to follow if reversed. First define the smaller pieces, then combine them.}
% We train the system such that we observe the output given the input for all the steps of the proof. More specifically:
% \facomment{no longer using $h_t$ for the variables}
% for inference and how to compute $h_t$ we have the following:
% where $q_0=[G(X), S(X), A(X)]$ and $q_t$ is the head of the rule at time $t$.
% \begin{align}
%     i^*_{rule} &= argmax~_{i=1\dots N} (M^{key}_{i,:})^T k_t \\
%     r_{t+1} &= M_{i*,:}^{rule} \\
%     v_{t+1} &= k_{t+1}^v V^{var} \label{eq:change}
% \end{align}
% \facomment{change \ref{eq:change} $k_{t+i} V_{t+1}$}
% \facomment{mention using GLOVE for initializing the variable embeddings}
% \facomment{predicates initialized randomly}
% \resizebox{0.7\textwidth}{!}{

% }