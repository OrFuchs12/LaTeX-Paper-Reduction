\vspace{-0.5em}
\subsection{Discussion}
In this section, we analyze the results from the study and provide examples of the 4 scenarios in Section \ref{sec:corgi} that we encountered. As hypothesized, scenario $\mathfrak{A}$ 
% (users not knowing the answer) 
hardly occurred as the commands are about day-to-day activities that all users are familiar with.
% Since our domain is concerned with commonsense knowledge about day-to-day activities, scenario $\mathfrak{A}$ (users not knowing the answer) hardly occurred. \amoscomment{This exact sentence appeared earlier (in the method section). Either remove or at least add "as mentioned / stated" or maybe "as hypothesized" (assuming the next sentence strengthens this)?}
% We did not observe any meaningful correlation between the student's background and their success rate, which is expected since the reasoning tasks are in the context of day-to-day activities. \amoscomment{What do you mean by "student's background", did you measure Pearson's correlation? What exact values did you get? I would recommend just removing this sentence due to lack of space.}
% Let us now discuss why CORGI is not able to achieve
% our observations of the study's dialog transcript and explain why there is a 
% ideal performance in the oracle scenario. This is mainly caused by the inability to extract the relevant knowledge from the human subjects, and partly caused by the limitations of the parser. 
% Scenario $\mathfrak{B}$ refers to cases where users misunderstood the question.
We did encounter scenario $\mathfrak{B}$, however. The study's dialogs show that some users provided means of \emph{sensing} the \textGoal rather than the \emph{cause} of the \textGoal.
% provided the meaning of certain queries rather than commonsense knowledge.
For example, for the reasoning task \emph{``If there are thunderstorms in the forecast within a few hours then remind me to close the windows because I want to keep my home dry''}, in response to the system's prompt \emph{``How do I know if `I keep my home dry'?''} a user responded \emph{``if the floor is not wet''} as opposed to an answer such as \emph{``if the windows are closed''}. Moreover, some users did not pay attention to the context of the reasoning task. For example, another user responded to the above prompt (same reasoning task) %Amos: I recommend removing "same reasoning task"
with \emph{``if the temperature is above 80''}! %Amos: This is an excellent paragraph!
% which is not even correct common sense (except perhaps in certain climates). 
Overall, we noticed that CORGI's ability to successfully reason about an if-then-because statement was heavily dependent on whether the user knew how to give the system what it needed, and not necessarily what it asked for; see Table \ref{tab:dialog} for an example. As it can be seen in Table \ref{tab:user_study}, expert users are able to more effectively provide answers that complete CORGI's reasoning chain, likely because they know that regardless of what CORGI asks, the object of the dialog is to connect the because \textGoal back to the knowledge base in some series of if-then rules (\textGoal/sub-\textGoal path in Sec.\ref{sec:corgi}). Therefore, one interesting future direction is to develop a dynamic context-dependent Natural Language Generation method for asking more effective questions.
% In contrast, CORGI's current method of asking, ``How do I know if \textGoal?'' often mis-states the immediate information need of the system. If users are not prompted with a good question, they cannot be expected to provide useful answers to the system. Therefore, one interesting future direction is to develop a dynamic context-dependent Natural Language Generation method for asking more effective questions. 
%This can be addressed by asking more informative questions that help users to consider the context in their answers. In the future, we will develop dynamic context-dependent Natural Language Generation pipelines to address this. % For example, instead of asking ``how do I know if $\textGoal$'', CORGI could ask, ``what does the $\textAction$ cause that allows you to achieve the $\textGoal$ ?''.

We would like to emphasize that although it seems to us, humans, that the previous example requires very simple background knowledge that likely exists in SOTA large commonsense knowledge graphs such as ConcepNet\footnote{\url{http://conceptnet.io/}}, ATOMIC\footnote{\url{https://mosaickg.apps.allenai.org/kg_atomic}} or COMET \cite{bosselut2019comet}, this is not the case (verifiable by querying them online). 
% Even the generative COMET model \cite{bosselut2019comet} is not able to generate commonsense knowledge relevant to this reasoning task. 
For example, for queries such as \emph{``the windows are closed''}, COMET-ConceptNet generative model\footnote{\url{https://mosaickg.apps.allenai.org/comet_conceptnet}} returns knowledge about blocking the sun, and COMET-ATOMIC generative model\footnote{\url{https://mosaickg.apps.allenai.org/comet_atomic}} returns knowledge about keeping the house warm or avoiding to get hot; which while being correct, is not applicable in this context. For \emph{``my home is dry''}, both COMET-ConceptNet and COMET-ATOMIC generative models return knowledge about house cleaning or house comfort. On the other hand, the fact that 40\% of the novice users in our study were able to help CORGI reason about this example with responses such as \emph{``If I close the windows''} to CORGI's prompt, is an interesting result. This tells us that conversational interactions with humans could pave the way for commonsense reasoning and enable computers to extract just-in-time commonsense knowledge, which would likely either not exist in large knowledge bases or be irrelevant in the context of the particular reasoning task.  
% Overall, the success rate of reasoning for this statement was $40\%$ i.e. 18 out of the 45 user interactions for this statement succeeded. %\facomment{make it more precise}.
% \tmcomment{maybe delete next two sentences to save space?} We would like to add that a comprehensive analysis of the performance of these large knowledge bases requires developing models targeted at solving our proposed reasoning task and is out of the scope of our current study. We hope instead, that this discussion reveals gaps in the commonsense reasoning literature to be addressed by the reasoning community's future endeavours.
Lastly, we re-iterate that as conversational agents (such as Siri and Alexa) enter people's lives, leveraging conversational interactions for learning has become a more realistic opportunity than ever before.
% \input{sample_dialogs.tex}

% \amoscomment{In the previous two scenarios you stated what they were. Either add it here as well, or if there isn't enough space, just remove and let the reader go back to section 3.1.} \facomment{sure, removed the previous ones, thanks}
In order to address scenario $\mathfrak{C}.1$, the conversational prompts of CORGI 
% Another point of failure for CORGI is caused by the fact that conversational prompts of the computer 
ask for specific small pieces of knowledge that can be easily parsed into a predicate and a set of arguments. However, some users in our study tried to provide additional details, which challenged CORGI's natural language understanding. 
% that some users gave all the reasoning steps in answer to the first question, which confused the parser. 
For example, for the reasoning task \emph{``If I receive an email about water shut off then remind me about it a day before because I want to make sure I have access to water when I need it.''}, in response to the system's prompt \emph{``How do I know if `I have access to water when I need it.'?''} one user responded \emph{``If I am reminded about a water shut off I can fill bottles''}. This is a successful knowledge transfer. However, the parser expected this to be broken down into two steps. If this user responded to the prompt with \emph{``If I fill bottles''} first, CORGI would have asked \emph{``How do I know if `I fill bottles'?''} and if the user then responded \emph{``if I am reminded about a water shut off''} CORGI would have succeeded. The success from such conversational interactions are not reflected in the overall performance mainly due to the limitations of natural language understanding.%, which we are planning to address in the future.

% The effectiveness of interactive reasoning and well as the learned rule embeddings is shown in Table \ref{tab:user_study}. 
Table \ref{tab:user_study} evaluates the effectiveness of conversational interactions for proving compared to the no-feedback model. The 0\% success rate there reflects the incompleteness of \KB. The improvement in task success rate between the no-feedback case and the other rows indicates that when it is possible for users to contribute useful common-sense %Amos: commonsense is usually written as one word in this paper.
knowledge to the system, performance improves. The users contributed a total number of 96 rules to our knowledge base, 31 of which were unique rules. 
Scenario $\mathfrak{C}.2$ occurs when there is variation in the user's natural language statement and is addressed with our neuro-symbolic theorem prover. Rows 2-3 in Table \ref{tab:user_study} evaluate our theorem prover (\emph{soft unification}). 
% and replaces the Neuro-Symbolic theorem prover in Fig \ref{fig:model} with different variations.
% in parallel with soft proving.
% CORGI's performance is a combination of both successful knowledge acquisition and successful soft logical proving. But  CORGI has a serial design, such that it can only succeed when both components succeed.
% The no-feedback scenario, in which we do not engage in a conversation with the user, compared with all the other rows reflect the incompleteness of the knowledge base and the need for missing knowledge acquisition. We address it here by conversing with the users. The other 3 rows evaluate the effectiveness of the learned rule embeddings.
% Hard unification 
% (row 2) uses vanilla Prolog for proving and depends on hard unification (i.e. exact matches) for predicates. Therefore, it 
% is not capable of supporting the variations of natural language.
% HARD HERE: Soft unification improves the performance of hard unification significantly, since the latter is not capable of supporting the variations of natural language. Oracle unification (row 4) results show that 
% is the performance of our inference algorithm (Alg.\ref{alg:inference}) when oracle rule and variable embeddings are available. 
Having access to the optimal rule for unification 
% at each step of the proof 
does still better, but the task success rate is not 100\%, mainly due to the limitations of natural language understanding explained earlier.  %However, this usually did not occur. %Amos: I removed this, I think that it is confusing. You can write "However, even when using oracle unification the majority of rules cannot be proven".

% %Some of the performance loss in Table~\ref{tab:user_study}, is due to an imperfect parser. Better parsing would let us extract predicate arguments in a more robust manner. % \kmcomment{identify types? extract phrases? something specific}.
% There are several other opportunities for exploring improvements to the system. 
% Extending CORGI to handle conjunction statements would let us cover more of the if-then-because scenarios from Table \ref{tab:statement_stats}. Furthermore, the rule embeddings are not currently updated as the user interacts with the model. While a successful dialog with the system does add new facts to the knowledge base as shown in Figure \ref{fig:model} (\emph{knowledge update loop}), a neural model that could adaptively update and add embeddings for new facts and rules on the fly would support handling \emph{future} statements that are semantically similar to the added rule. 

% \kmcomment{run the experiment for no-feedback so we can include denominators} \kmcomment{run an experiment without soft-unification} \amoscomment{Table 4 is in the Appendix, don't reference it here.}
%
%
%\begin{table}[t]
%    \centering
%    \begin{tabular}{ccc}
%    Statement & Novice & Expert \\\hline
%    CORGI total & 16/88 & 8/24  \\\hline
%%If I have an early morning meeting then wake me up early because I want to be ontime. 
%1 & 1 & 1 \\
%%If there are thunderstorms in the forecast within a few hours then remind me to close the windows because I want to keep my home dry. 
%2 & 3& 2  \\
%%If I schedule an appointment that overlaps with another appointment then notify me immediately because I want to let my colleagues know of the conflict. 
%3* & 0& 0 \\
%%If I search for a gas station in the navigation app and there is a cheaper gas station that is not too much further away then ask me immediately whether I want to switch the destination to the new gas station because I want to save money. 
%4 & 0& 0 \\
%%If I search for a restaurant in the navigation app and there is a cheaper restaurant with a similar rating then notify me immediately whether I want to switch the destination to the new restaurant because I want to save money. 
%5 & 0& 0 \\
%% If I receive an email about a critical software update then notify me immediately because I want to keep my computers safe from malware. 
%6 & 0& 0 \\
%%If it's going to rain in the afternoon then remind me to bring an umbrella because I want to remain dry.
%7* & 0& 0\\
%%If I haven't been to the gym for more than 3 days then remind me to go to the gym because I want to stay fit. 
%8 & 6 & 3  \\
%%If I receive an email about water shut off then remind me about it a day before because I want to make sure I have access to water when I need it. 
%9 & 0 & 0 \\
%%If my calender is clear today, then remind me to go to gym in the afternoon, because I want to keep myself healthy. 
%10 & 6 &2 \\
%    \end{tabular}
%    \caption{Percentage of users who were able to prove each statement in the target study set. *: These statements saw fewer proof attempts due to a bug in the initial round of tests.}
%    \label{tab:user_study}
%\end{table}