\section{Related Work}
In recent years, explainable AI has regained interest, initially focusing mainly
on explaining supervised models. More recently, research has begun
exploring explanations of reinforcement learning agents~\cite{XRL_survey,
heuillet2021explainability}. 
% Some explanation methods attempt to convey the agents' reasoning when
% selecting actions, e.g. using decision-trees~\cite{liu2018toward} or causal
% models~\cite{madumal2020explainable}.
In this work, we focus on global explanations that aim to describe the policy of
the agent rather than explain a particular action. Specifically, we develop a
new method for strategy summarization. In this section, we describe in
more depth strategy summary methods~\cite{amir2019summarizing}. 


% % decision trees Decision trees are popularly known for their high
% interpretability, \citet{liu2018toward} described a novel method for mimicking
% a Deep Reinforcement Learning (DRL) model by using a continuous tree
% representation (LMUTs) and interpreting it by analyzing the knowledge stored
% in the tree structure.\\
% % causal graphs Harnessing the inherent explainability of causality methods,
% \citet{madumal2020explainable} introduce and formalize an action influence
% model for model-free RL based on a structural causal model received as input
% (e.g. a DAG). The action influence model provides the ability to approximate
% the causal model of the environment relative to actions taken by the agent,
% enabling the generation of contrastive explanations for questions sch as
% ``why?'' as well as ``why not?''.\\

% In this work we develop a new method for policy summaries, therefore, in this
% section we describe in more depth policy summary methods. \oa{start with a
% more general paragraph about explainable RL (can steal from the intro and
% elaborate a bit. So it will be something like "In recent years, explainable AI
% has regained interest, initially focusing mainly on explaining supervised
% models. More recently, a line of research has begun exploring explanations of
% reinforcement learning agents. [[describe some methods]]. In this work we
% develop a new method for policy summaries, therefore, in this section we
% describe in more depth policy summary methods. Then move to your next
% paragraph}

Strategy summarization techniques convey agent behavior by demonstrating the
actions taken by the agent in a selected set of world states. The key question
in this approach is then how to recognize meaningful agent situations.

One such approach, called HIGHLIGHTS~\cite{amir18highlights}, extracts
\emph{important} states from execution traces of the agent. Intuitively, a state
is considered important if the decision made in that state has a substantial
impact on the agent's utility. To illustrate, a car reaching an intended highway
exit would be an important state, as choosing a different action (continuing on
the highway) will cause a significant detour. 
% The algorithm also considers the diversity of states in the summary to avoid
% showing redundant information. 
HIGHLIGHTS has been shown to support people's ability to understand the
capabilities of agents and develop mental models of their behavior
~\cite{amir18highlights,Tobias}. 

%OFra: removed Tobi since it's not super related here. Only added him to the
%studies showing highlights results.

% %% Tobias work \citet{Tobias} combined global and local explanation methods by
% integrating HIGHLIGHTS summaries with saliency maps depicting the attention of
% the agent. They found mixed results with respect to the contributions of
% saliency maps, but reinforced earlier results regarding the effectiveness of
% HIGHLIGHTS in conveying agent behavior to people.\\

% \citet{Sequeira2020} extended HIGHLIGHTS by suggesting additional importance
% criteria for the summary state selection. These are referred to as
% \emph{Interestingness Elements} --- domain independent criteria obtained through
% statistical analysis of an agent's interaction history with the environment over
% the training sessions. Their user study did not reach clear conclusions as to
% which criteria is optimal, rather they suggest that a combination of elements
% enables the most accurate understanding of an agentâ€™s aptitude in a task.

% Other works have proposed a different approach for generating summaries, based
% on machine teaching approaches~\cite{huang2017enabling,lage2019exploring}. These
% methods' underlying idea is to select a set of states that is optimized to allow
% the reconstruction of the agents' policy using imitation learning or inverse
% reinforcement learning methods. While such approaches are computationally
% appealing, as they rely on computational principles rather than heuristics, in
% practice they were shown to be limited. Specifically, participants' performance
% in predicting agent behavior was inferior to that of computational approaches.
% This is largely associated to the use of different state representations and
% computational models from those assumed by summary generation algorithms
% \cite{lage2019exploring}.


\citet{Sequeira2020} extended HIGHLIGHTS by suggesting additional importance
criteria for the summary state selection referred to as \emph{Interestingness
Elements}. \citet{huang2017enabling} and \citet{lage2019exploring} proposed a different
approach for generating summaries based on machine teaching methods. The key idea underlying this approach is to select a set of states that is optimized to allow
the reconstruction of the agent's policy using imitation learning or inverse
reinforcement learning methods.

%Ofra: removed for space reasons, and because we do not address these
% limitations in this work Prior work \cite{amir18highlights, Tobias} showcased
% that policy summaries can help users assess agents' capabilities, but also
% revealed some limitations. People's performance in predicting agent behavior
% was lower than that of computational approaches, because they often used
% different state representations and computational models from those assumed by
% summary generation algorithms \cite{lage2019exploring}. In addition, showing
% unrepresentative demonstrations of agent behavior can lead users to make wrong
% inferences, emphasizing the need to design methods that allow people to
% proactively explore the agent's policy. \\

Common to all previous policy summarization approaches is that each summary is
generated specifically for a single agent policy, independent of other agents.
This can hinder users' ability to compare agents, as the
summaries might show regions of the state-space where the agents act similarly,
failing to capture useful information with respect to where the agent policies
diverge. For example, HIGHLIGHTS focuses on ``important'' states and it is
likely that the states found to be most the important to one agent will be
considered important by another agent as well. These could be inherently
important stages of the domain such as reaching the goal or evading a dangerous
state. If the agents act similarly in these important states, the HIGHLIGHTS
summaries of the agents might portray similar behavior, even for agents whose
global aptitude varies greatly. In contrast, if the summaries do differ from
one another and portray different regions of the state-space, they do not convey how
the alternative agent would have acted had it been tasked with the same
scenario.
% Additionally, when the summaries of different agents cover different regions
% of the state-space, they do not convey to the user what an agent would have
% done had it encountered situations shown in the other agent's summary. 

% if different behavior is attained in the summaries, it remains unclear how the
% other agent would have acted in a scenario encountered by the other agent.
% summaries of different agents demonstrate their behavior in different states,
% they still do not convey to the user what the other agent would have done had
% it encountered that particular state. 
To address these limitations, we propose a new approach that is specifically
optimized for supporting users' ability to distinguish between policies.  

% While shown to be effective in conveying agent aptitude to users, the
% HIGHLIGHTS algorithm \cite{amir18highlights} has several caveats when applied
% as a method for comparing different agents. First, HIGHLIGHTS generates an
% \emph{independent} summary of important states in a given domain for each of
% the agents. Therefore, it is likely that the states found to be of most
% importance to one agent may be considered important by another agent as well.
% These could be inherently important stages of the domain such as reaching the
% goal or bypassing a dangerous state. If the agents act similarly in these
% important state, the HIGHLIGHTS summaries of the agents could portray similar
% behavior, even for agents that have different policies in many other parts of
% the state-space. Furthermore, even if the HIGHLIGHTS summaries of different
% agents demonstrate their behavior in different states, they still do not
% convey to the user what the other agent would have done had it encountered
% that particular state. 

% These can be thought of as \emph{contrastive} summaries, as they show where
% the agents' policies diverge. This approach is in line with the literature on
% explanations from the social sciences, which shows that people prefer
% contrastive explanations \cite{miller2018explanation}. We note that while
% typically contrastive explanations refer to ``why not?'' questions and
% consider counterfactuals, in our case the contrast is between the decisions
% made by two different policies.

\section{Background}
%%% RL and MDP %%%
For the purpose of this work, we assume a Markov Decision Process (MDP) setting.
Formally, an MDP is a tuple $\langle S,A,R,Tr \rangle$, where S is the set of
world states; {A} is the set of possible actions available to the agent; $R:S
\times A \rightarrow \mathbb{R}$ is a reward function mapping each state, and
$Tr(s,a,s') \rightarrow [0,1] \; s.t. \; s,s' \in S, \; a\in A$ is the
transition function.
% \begin{itemize} \item \textbf{S:} The finite set of world states. \item
%     \textbf{A:} The finite set of possible actions available to the agent.
%     \item \textbf{R:} A reward function $R:S \times A \rightarrow \mathbb{R}$,
%     mapping each state and action to a reward. \item \textbf{Tr:} A transition
%     probability function $Tr(s,a,s') \rightarrow [0,1] \; s.t. \; s,s' \in S,
%     \; a\in A$, defining the probability of reaching state $s'$ when
%     initiating action $a$ in state $s$. \end{itemize}
A solution to an MDP is a \emph{policy} denoted $\pi$. 
% An agent's policy is a
% mapping from states to actions such that for each state, a designated action is assigned.


%%% summaries %%%
\paragraph{Summaries}
%Ofra: we said this in the related work section

% Recent work introduced a new paradigm for conveying agent behavior ---
% \emph{``agent strategy summarization''}
% \cite{amir2018agent,amir2019summarizing}. In this paradigm, the strategy of an
% agent is described by demonstrating its actions in a carefully selected set of
% world states. 

% Such summaries can reduce the human effort required to review the agent's
% behavior, while still providing sufficient information about its capabilities.
% The key question in strategy summarization is then selecting the subset of
% situations, i.e. state-action pairs, to display. In other words which action
% sequences are most crucial in order to best portray the agent behaviour.

A summary, denoted $\mathbb{S}$, is a set of trajectories $T = \langle t_1,\dots t_n \rangle$.
% allowing for context surrounding the important state.  as opposed to singular
% states, in order to supply the user with richer context for the agent's
% behaviour in that state. 
Each trajectory $t$ is a sequence of $l$ consecutive states $t =
% \langle (s_i,a_i), \dots, (s_D,a_D), \dots, (s_{i+l},a_{i+l}) \rangle$
\langle s_i, \dots, s_D, \dots, s_{i+l} \rangle$
surrounding the disagreement state $s_D$ and extracted from the agent's
execution traces. 

We formally define a summary extraction process of an agent's policy given an
arbitrary importance function $Im$, mapping state-action pairs to numerical
scores.


\begin{definition}[Summary Extraction]
Given an agent's execution traces, a summary trajectory budget $k$, and an
importance function $Im$. \\
The agent's summary $\mathbb{S}$ is then the set of trajectories $T = \{
t_1,...,t_k \}$ that maximizes the importance function.
\begin{align}
    \mathbb{S} = \max_T \; Im(T)
\end{align}
\end{definition}
In this paper, our baseline is the HIGHLIGHTS algorithm, which computes
importance as a function of the $Q$-values in a given state. Specifically, we
implement the importance function from \citet{Tobias}, an extension to
HIGHLIGHTS, which suggests determining the importance of a state based on the
difference between the maximal and second $Q$-values.
% as opposed to the original HIGHLIGHTS importance function that was calculated
% between the maximal and the minimal values. 
Formally:
\begin{align}
   Im(s) = \max_a\; Q^\pi(a,s) - second\underset{a}highest\; Q^\pi(a,s)
\end{align}
The trajectory is then the sequence of states preceding and succeeding the
important state. 
