
\clearpage

\section{General Response}
We thank the reviewers for their feedback. We first address concerns that appeared in multiple reviews, then reply to each reviewer's specific comments separately.
\\
\textbf{Choice of HIGHLIGHTS as a baseline}
Prior work has shown that HIGHLIGHTS outperformed several baselines (Amir \& Amir 2019, Huber et al. 2021). Another approach for generating summaries is through machine teaching (Lage et al., 2019, Huang et al., 2019), but it requires making assumptions about  users’ generalization, and does not scale computationally so could not be applied to these domains. Other summary criteria were proposed by Sequeria \& Gervasio (2020) but they were not evaluated in the context of comparing agents and there was also no clear approach that outperformed others. As such, and since the main goal of the paper was to demonstrate the potential benefit of moving from independent summaries to summaries that contrast two agents, HIGHLIGHTS was the most prominent baseline.

\textbf{Simulator requirement}
% While our approach assumes access to a simulator, this does not have to be the case.
% Following the Leader’s execution plan while simultaneously querying the Disageer to identify disagreements is applicable in real-world scenarios by determining importance based solely on the disagreement state. Using trajectory-based disagreements could prove to be more difficult, as allowing the Disagreer to branch out and revert back to the disagreement state might not be feasible in some real-world domains.
We agree that the need for a simulator limits the ability to use the approach on any provided agent. However, as long as there's a way to query the agent for its actions given a state, the agents do not have to be trained using the same simulator. This is a first step toward enabling agent comparison, a problem which has not been directly addressed by prior work. The current approach could support developers and allow them to share summaries with domain experts that collaborate with them and end users who wish to compare different policies. We note that all existing approaches require a simulator, and compared to these, we require fewer assumptions (e.g., an additional causal model (Miller 2020) or access to the agent's training as required for reward decomposition and saliency maps). 


\paragraph{Response to R1:}
\textbf{Contribution and advantage over multiple HIGHLIGHTS}
The underlying research question is supporting comparison of alternative agents. The proposed solution provides a conceptual contribution rather than an increment to  HIGHLIGHTS,  motivated by literature in cognitive science which suggests that contrastive explanations are typically more helpful to people than only showing a chosen action. We followed this approach to compare two policies.
% The comparisons generated by our approach are contrastive, compare the agents in equivalent scenarios, algorithmically locate and display only divergence in agent policies and do not require the user to watch numerous different test cases which might be  time-consuming and tedious.
The user study demonstrates the benefit over using separate HIGHLIGHTS summaries as DISAGREEMENTS summaries led to higher correctness rates in users' evaluations of agents.
Computational efficiency-wise, the  approaches are similar, as both require running simulations of the agent and ranking of states. 

\textbf{Motivation example}
Safety over speed is an example for cases where agents may prioritize different outcomes.


\textbf{Claim that other approaches are not optimized for the task}
We were referring to other global explanations, claiming they are not tailored to support a particular user need, e.g., debugging an agent, collaboration, or the need we address here - comparing alternative policies.

\paragraph{Response to R2:}
\textbf{Supported MDPs}
Any MDP can be used. In particular, the highway domain has a continuous state space. Currently, the approach only supports discrete actions, but it could be extended to a continuous action space by clustering actions, or defining distances between actions.


\paragraph{Response to R3:}
\textbf{Works on reconciling a user's expected policy}
We are not sure which works the reviewer refers to, we would be happy to add discussion of any additional related works we've missed. 

\textbf{Success rate using HIGHLIGHTS}
We explain this in the results section: \textit{``We hypothesize that these responses are a consequence of a single trajectory in agent E’s summary [...] These results emphasize the limitations of independent comparisons.''}

\textbf{Rescaling Q-values}
While the q-values may differ due to the rescaling, the selected action of each agent is unaffected, therefore the disagreements themselves are still viable.
We agree that the scaling might need to be fitted for particular domains. In the domains we used we conducted sensitivity analyses to make sure their effect is not substantial.

\textbf{Last state importance}
The chosen criteria for disagreement importance identifies trajectories where the agents (while possibly prioritizing alternative outcomes) agree that one of them has reached a poor state while the other a desirable one. The reason it was chosen was to provides a shared ground truth regarding the states the agents arrive at.
We considered other importance functions which were left out due to space limitations, we will add those to the Appendix.

\textbf{Proxy metrics vs. human studies}
Our goal is to support users and it is unclear what computational metric could serve as a proxy for this outcome. We did conduct sensitivity analyses computationally to test the affects of parameters on the output summaries. 


\paragraph{Response to R4:}
\textbf{soundness of user studies.}
To be as confident as possible in our findings, we followed key principles of experimental design (e.g., choice of evaluation metrics that answer the research question, random assignment to conditions, testing of participants’ attention and understanding of the task, use of multiple domains, comparison to a meaningful baseline). 

\textbf{Addressing continuous or vast state space}
We overcame this challenge by observing all disagreements in a sampled set of trajectories. The number of samples is configurable.

\textbf{Filtered participants}
The reported number of participants is after filtering. We excluded 3 participants.

\textbf{Participant proficiency}
No significant differences were uncovered by these questions, we will add this to the Appendix.

\textbf{No statistical differences in confidence and satisfaction}
Our null findings with respect to confidence and satisfaction are in line with some other prior studies that showed that even when significant performance differences were found, this was not always reflected in participants’ confidence in their answers (Amir \& Amir 2019). Since these are subjective measures, it is also possible that the between-subject design reduces our ability to observe such differences (if variance between participants in these aspects is greater than the effect of the intervention).

\textbf{Requirement of access to the policy}
It is true we need such access. One idea to overcome this would be to use Inverse RL methods to retrieve an estimate of an agent's policy and then apply our approach to it. 


