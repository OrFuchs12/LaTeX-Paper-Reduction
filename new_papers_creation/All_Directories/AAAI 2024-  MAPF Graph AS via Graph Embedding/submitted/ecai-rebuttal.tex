We are fully committed to making the source code, data sets, and results publicly available. 

To all reviewers:
We deeply apologize for the many typos and English mistakes. These errors are easily corrected. 

To Reviewer 1:
Re. the advantage of MAG over the state-of-the-art, please note that it is almost always better than KBS and G2V across the different experimental setups and metrics. In some cases, the advantage is small but in others it is more significant. 

Answers to Questions:

1. We used on average 3,000 instances for training for each map. 
2. In Tables 1 & 2 we highlighted in bold the best AS method among MAG and the two baselines, KBS and G2V (this is stated in page 4). The last 4 rows in these tables are not baselines for comparison but our ablation study for MAG, and we chose not to bold the different MAG configurations to simplify the presentation. Re. Table 3, this is indeed a mistake. 
3. The results for FG2V only on maze maps are not presented in the paper, but we have them as part of the comprehensive ablation study we performed. We intend to make all our datasets and results publicly available once the paper is accepted. 
We thank the reviewer for the vote of confidence and affirm that we will fix all the writing issues and make several deep passes to ensure no additional issues remain. 

To Reviewer 2:

We are fully committed to making all our code, datasets, and results publicly available once the paper is accepted. Also, we apologize for all the typos and writing issues and will ensure such issues are fixed for the final version. 

Answers to Questions:

1) For the between-grid-type experiments, the training set included instances from all grid types except the grid type used for testing. Training on grids from a single grid type is an interesting direction for future work. We conjecture that grid types that are more similar to the test grid type will be more effective for training, e.g., "Random" grids might be an effective training set for "Empty" grids but not for "Maze" grids.  

Responses to detailed feedback:
We will change MAPF problem to MAPF instance as suggested and correct all the typos and writing issues. 

To Reviewer 3:

1. KBS+G2V is part of the ablation study for MAG, and the baselines for comparison should be KBS and G2V. Indeed, our ablation study shows that the main strength of MAG is in the combination of handcrafted features and graph embedding features, and its advantage over KBS+G2V is modest. Note that KBS+G2V itself is a contribution of our work because (a) G2V was not a practical algorithm prior to our contributions (see the first paragraph of section 2.2, section 3.2, and footnote 4); and (b) part of our contribution is how to combine KBS and G2V in a single model (see section 3.3).  
2. The runtime required for feature extraction of a single instance was a single second on average, mostly devoted to running FEATHER. The prediction (inference) runtime of our XGBoost model is also extremely fast, since it is not a large deep NN. We will add this information to the paper. 
3. The entire training process (over all training set instances) required approximately 10~12 hours, including feature creation, training, and hyperparameter tuning. All experiments were run on a server with an Intel 12th Gen i7-12850HX 2.10 GHz CPU, 64.0 GB RAM, and a 64-bit OS. Indeed, high accuracy can potentially be obtained by "winning" on the easy cases. This is why we also have the other metrics, and in particular RT, which is less susceptible to such issues. 
4. We are fully committed to making the code, datasets, and results publicly available once the paper is accepted. 

Answers to Questions:

1. Prior work on AS for optimal MAPF showed that KBS outperforms these best-at-grid baselines. Since MAG significantly outperformed KBS, we are confident it also outperforms these baselines. We already report on the Oracle in the paper. 
2. As noted above, generation and prediction are very fast (~1-2 seconds), which certainly pays off in our context. 
3. As noted above, 10~12 hours. 
4. We are not sure what the reviewer is suggesting here wrt PAR1, but the RT and regret metrics serve to some extent in observing the impact of wrong choices. 
5. The metrics are the coefficients of the XGBoost model. A high coefficient roughly corresponds to a stronger impact on the prediction outcome. Other feature importance metrics such as SHAP values also exist. 
6. Yes, "average" is "mean". We are happy to report other statistics if required. Also, all our results will be published, so the interested reader will be able to also see the raw results. 
